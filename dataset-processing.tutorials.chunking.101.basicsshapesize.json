{"version":3,"kind":"Notebook","sha256":"df62576c49f4e819a25c8b37c3a0247670865e6f4ae11e92f6c60701cf8213e2","slug":"dataset-processing.tutorials.chunking.101.basicsshapesize","location":"/dataset_processing/tutorials/chunking/101/BasicsShapeSize.ipynb","dependencies":[],"frontmatter":{"title":"Basics of Chunk Shape and Size","content_includes_title":false,"github":"https://github.com/hytest-org/hytest","copyright":"2023","numbering":{"title":{"offset":2}},"source_url":"https://github.com/hytest-org/hytest/blob/main/dataset_processing/tutorials/chunking/101/BasicsShapeSize.ipynb","edit_url":"https://github.com/hytest-org/hytest/edit/main/dataset_processing/tutorials/chunking/101/BasicsShapeSize.ipynb","exports":[{"format":"ipynb","filename":"BasicsShapeSize.ipynb","url":"/hytest//build/BasicsShapeSize-301235fb743ea9cef1d71bd805123d28.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","data":{"tags":[]},"children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"The goal of this notebook is to learn the basics about chunk shape and size.\nWe will discuss several factors to think about when deciding on chunk shape and size for datasets being written to storage.\nThese factors can affect the read pattern from storage and subsequently the computations.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"YRl77lGI5A"}],"key":"c11fPWpO2O"}],"visibility":"show","key":"mSSN99OnVq"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import xarray as xr\nimport fsspec","key":"jHdvmAI7Kr"},{"type":"outputs","id":"xtRCCGj_UueZZAj3Fv-AD","children":[],"key":"WSKk6mTUTy"}],"key":"F4EaJs2ntY"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Accessing the Example Dataset","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cOqpiixNBj"}],"identifier":"accessing-the-example-dataset","label":"Accessing the Example Dataset","html_id":"accessing-the-example-dataset","implicit":true,"key":"D1v9DSKhNs"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"In this notebook, we will use the monthly PRISM v2 dataset as an example for understanding the effects of chunk shape and size.\nLet’s go ahead and read in the file using xarray.\nTo do this, we will use ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Ow4aIMdYbr"},{"type":"link","url":"https://filesystem-spec.readthedocs.io/en/latest/","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"fsspec","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"VTCCo7PV2m"}],"urlSource":"https://filesystem-spec.readthedocs.io/en/latest/","key":"siI8XgVmNw"},{"type":"text","value":" to get a mapper to the Zarr file on the HyTEST OSN.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"PgtcQi28wB"}],"key":"dwal0Sgect"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"q4EMJTW5Q1"}],"key":"o0FyOeI9UH"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"The xarray loader is “lazy”, meaning it will read just enough of the data to make decisions about its shape, structure, etc.\nIt will pretend like the whole dataset is in memory (and we can treat it that way), but it will only load data as required.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"qVBHTB2tkA"}],"key":"BhTCXOiXat"}],"key":"aFiM3oKDIN"}],"key":"qkvB8EtqHv"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"fs = fsspec.filesystem(\n    's3',\n    anon=True,   # anonymous = does not require credentials\n    client_kwargs={'endpoint_url': 'https://usgs.osn.mghpcc.org/'}\n)\nds = xr.open_dataset(\n    fs.get_mapper('s3://mdmf/gdp/PRISM_v2.zarr/'),\n    engine='zarr'\n)\nds","key":"TISPALBf3r"},{"type":"outputs","id":"-xTCav4SyiHck9xFkZNdx","children":[],"key":"GDNK5feCIT"}],"key":"paa8YnVvWH"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Chunk Shape and Size","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xEuslAQgjB"}],"identifier":"chunk-shape-and-size","label":"Chunk Shape and Size","html_id":"chunk-shape-and-size","implicit":true,"key":"O2rJvxWvPB"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Given what we know about this data, we can apply some storage principles to form a strategy for how best to chunk the data if we were to write it to storage (assuming it isn’t already).\nBroadly, we need to specify chunk shape and size.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"UHGvA1VD3w"}],"key":"K7xPXW7iPF"},{"type":"heading","depth":3,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Shape Considerations","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"suLgAydF3f"}],"identifier":"shape-considerations","label":"Shape Considerations","html_id":"shape-considerations","implicit":true,"key":"smP0CL1Ljx"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"“","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"UccX6siMKW"},{"type":"crossReference","url":"/dataset-processing/tutorials/chunking/back/glossary","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"strong","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Chunk shape","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"WUM9BAdmtw"}],"key":"tTPIFE9Hgo"}],"urlSource":"../back/Glossary.md#term-Chunk-shape","dataUrl":"/dataset-processing.tutorials.chunking.back.glossary.json","identifier":"term-chunk-shape","label":"term-Chunk-shape","remote":true,"protocol":"file","key":"AtY5ZWjiZp"},{"type":"text","value":"” is the shape of a chunk, which specifies the number of elements in each dimension.\nSo, we will need to decide on the size of each of the dimensions of the chunks.\nThe preferred shape of each chunk will depend on the read pattern for future analyses.\nOur goal is to chunk the data so that future reads will be performant, and that depends on whether the analyses favor one dimension or another.\nFor some datasets, this will be very apparent.\nFor example, streamflow gage data is very likely to be consumed along the ","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"eZgjnYp4Pn"},{"type":"inlineCode","value":"time","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"mZCCPf4CWx"},{"type":"text","value":" dimension.\nSo, a collection of data from multiple gages is ","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"wPLa9LN3Nv"},{"type":"emphasis","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"more likely","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"up4Stjt48Q"}],"key":"SjIExz14xU"},{"type":"text","value":" to have the individual time series analyzed as opposed to analyzing all gages at a given time.\nTherefore, we would want a chunk shape that is larger along the time dimension.\nFor datasets where there is no clear preference, we can try to chunk based on likely read patterns, but allow for other patterns without too much of a performance penalty.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"io3FEsApIo"}],"key":"sIlaaci34q"},{"type":"paragraph","position":{"start":{"line":18,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"Let’s see how we might do this for our example dataset.\nBeing this dataset spans space and time, it will likely be used in one of two dominant read patterns:","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"eup5CJtBtD"}],"key":"M75wFRKq4a"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":21,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Time series for a given location (or small spatial extent)","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"inVfPVLrlk"}],"key":"EKvx1aBYtN"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Special case: Is it likely that the time series will be subset by a logical unit (e.g., will this monthly data be consumed in blocks of 12 (i.e., yearly))?","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"MF1gGYzXvh"}],"key":"UMImFpIuhS"}],"key":"ZtnxC8Fzn6"}],"key":"B4xmgUGnUy"}],"key":"KkmZkXIk15"},{"type":"listItem","spread":true,"position":{"start":{"line":23,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Full spatial extent for a given point in time.","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"acK2G9x57J"}],"key":"ciNp9wZnKl"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":24,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":24,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Special case: Are specific spatial regions more used than others?","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"gCrdZuEdnY"}],"key":"Tci5iosY5z"}],"key":"RYMnzSRiQT"}],"key":"UTrja4M7jP"}],"key":"BFBMhsxpSE"}],"key":"mZMGWrj8Fk"},{"type":"paragraph","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"text","value":"Let’s look at a couple of options for space and time chunking:","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"Va1MsqA5Ul"}],"key":"ghqNtNRoyS"}],"key":"FvJu3y7WC9"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":4,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Time Dimension","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HtGdeZ1Aol"}],"identifier":"time-dimension","label":"Time Dimension","html_id":"time-dimension","implicit":true,"key":"fWPncGrpOF"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"As we can see above, the example dataset has 1555 monthly time steps.\nHow many chunks would we have if we chunked in groups of twelve (i.e., a year at a time)?","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"i6PjRQEhJP"}],"key":"sIuNBoJnL5"}],"key":"CWAetY4iQ6"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print(f\"Number of chunks: {len(ds.time) / 12:0.2f}\")","key":"onC3n80Hx1"},{"type":"outputs","id":"xjkP9HDaK7-42uzU4a1Kt","children":[],"key":"jr6KFtj8om"}],"key":"l3JItfkE4X"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"In this case, a user could get a single year of monthly data as a single chunk.\nIt is important to note that we have just over a round number of chunks. Having ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QQCVjYEwnF"},{"type":"inlineCode","value":"129.58","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SnlU2To92r"},{"type":"text","value":" time chunks means we will have 130 chunks in practice, but the last one is not full-sized. The last chunk would be a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XfR7j3F2g3"},{"type":"crossReference","url":"/dataset-processing/tutorials/chunking/back/glossary","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"“","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hdiyAQRViN"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"partial chunk","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HhccVv3rDb"}],"key":"rVYjXlr3R0"},{"type":"text","value":"”","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QCyhJFpqZz"}],"urlSource":"../back/Glossary.md#term-Partial-Chunk","dataUrl":"/dataset-processing.tutorials.chunking.back.glossary.json","identifier":"term-partial-chunk","label":"term-Partial-Chunk","remote":true,"protocol":"file","key":"hHeL6gXZQb"},{"type":"text","value":" because we do not have a full year of data for 2024.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YCVcrniiM1"}],"key":"b6cQ0aMCsl"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"So this is where the judgement call gets made: Which is the more likely read pattern for time: year-by-year, or the whole time series (or some sequence of a few years)?\nFor PRISM, it is more likely that someone will want more than just one year of data.\nA happy medium for chunk shape along the time dimension could be 6 years of data per chunk.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"sb5ywEuvBk"}],"key":"UttBo11fyT"}],"key":"zufqEW0tQM"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"time_chunk_shape = 12 * 6\nprint(f\"Number of chunks: {len(ds.time) / time_chunk_shape:0.2f}; Chunk of shape: {time_chunk_shape}\")","key":"HLFHD2eRqW"},{"type":"outputs","id":"hYn6sFCGSn8DV7xlICT-c","children":[],"key":"T8onca6cHM"}],"key":"HnaFIF4Msk"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"This pattern means only 22 chunks (instead of the 126 chunks we were considering a moment ago) are needed for a full time series in a given location.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zT9KrZefXy"}],"key":"kllU5G18E4"}],"key":"F4WH2fxvaN"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":4,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Spatial Dimension","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NsJiHs9vzU"}],"identifier":"spatial-dimension","label":"Spatial Dimension","html_id":"spatial-dimension","implicit":true,"key":"ocs0VE43sR"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"As we can see in our example dataset, it technically contains two spatial dimensions: ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"tbZyggFEnX"},{"type":"inlineCode","value":"lat","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"hUCBxOWNUo"},{"type":"text","value":" and ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"d4glUMk2R7"},{"type":"inlineCode","value":"lon","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"LyCZ0oerU1"},{"type":"text","value":".\nSo, we’re really chunking both of these dimensions when we talk about chunking with respect to space.\nWhile we will consider them both together here, it is important to point out that they can have separate chunk shapes.\nThis leads to the the question of whether future users of this data will want strips of latitude or longitude, square “tiles” in space, or some proportionally-sized tiles of latitude and longitude?\nThat is, is it important that the North-South extent be broken into the same number of chunks as the East-West extent?\nLet’s start by chunking this into square tiles.\nBeing that there are more ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Aq2NJJwrzr"},{"type":"inlineCode","value":"lon","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"xGOKOqGzWK"},{"type":"text","value":" elements than ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"nqvbbLZWkH"},{"type":"inlineCode","value":"lat","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"lslb4FAvkv"},{"type":"text","value":" elements, this means there will be more ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"cp2mccq7g9"},{"type":"inlineCode","value":"lon","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"zCFWKjBtZM"},{"type":"text","value":" chunks than ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"DN7pputbrp"},{"type":"inlineCode","value":"lat","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"eDCVjJGbgo"},{"type":"text","value":" chunks.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"YnjrobeVhg"}],"key":"EsDMnW9dCQ"}],"key":"mg2uy8bEr6"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"nlon = len(ds.lon)\nnlat = len(ds.lat)\nspace_chunk_size = nlat // 4 # split the smaller of the two dimensions into 4 chunks\nprint(f\"Number of 'lon' chunks: {nlon / space_chunk_size:0.3f}; Chunk of shape {space_chunk_size}; Size of last chunk: {nlon % space_chunk_size}\")\nprint(f\"Number of 'lat' chunks: {nlat / space_chunk_size:0.3f}; Chunk of shape {space_chunk_size}; Size of last chunk: {nlat % space_chunk_size}\")","key":"l7wKNlQgzb"},{"type":"outputs","id":"0UeeB4vjasJeMpLnyE9-S","children":[],"key":"pq7bWeAgM4"}],"key":"ws3sul5dqY"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Having ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fEwTbI8igL"},{"type":"inlineCode","value":"9.06","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zPM7io2Nl2"},{"type":"text","value":" longitude chunks means we will have 10 chunks in practice, but that last one is not full-sized.\nIn this case, this means that the last chunk in the given dimension will be extremely thin.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PAEymtixYA"}],"key":"ZRnv9SiXnh"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"In the case of the latitude chunks, the extra ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"k2n7jH2LIk"},{"type":"inlineCode","value":"0.006","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"N0mBA1ui5A"},{"type":"text","value":" of a chunk means that the last, fractional chunk (or ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"gCZptaXM8L"},{"type":"crossReference","url":"/dataset-processing/tutorials/chunking/back/glossary","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"“","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"TekRgd7Oj9"},{"type":"strong","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"partial chunk","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"tJfWc9sahy"}],"key":"CQyF4p3sMC"},{"type":"text","value":"”","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"TCB13Gu3bf"}],"urlSource":"../back/Glossary.md#term-Partial-Chunk","dataUrl":"/dataset-processing.tutorials.chunking.back.glossary.json","identifier":"term-partial-chunk","label":"term-Partial-Chunk","remote":true,"protocol":"file","key":"aH21OWGqrJ"},{"type":"text","value":") is only one ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"Opu5HdOzgK"},{"type":"inlineCode","value":"lat","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"ldKpVaWwNk"},{"type":"text","value":" observation.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"DkpIa8b7WN"}],"key":"PmmEa3KqvM"},{"type":"admonition","kind":"tip","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Tip","key":"O0J1Uvuyst"}],"key":"MXPfP37NhZ"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Ideally, we would want partial chunks to be at least half the size of the standard chunk.\nThe bigger that “remainder” fraction, the better.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"G8O6r3H1CY"}],"key":"HHwa7nLc8u"}],"key":"Ll8EgZhZps"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Let’s adjust the chunk shape a little so that we don’t have that sliver.\nWe’re still committed to square tiles, so let’s try a larger chunk shape to change the size of that last fraction.\nIncreasing the chunk size a little should get us bigger “remainders”.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"bwwlRGQ3oP"}],"key":"G2mxWsX877"}],"key":"kZ8iffRv0I"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"space_chunk_size = 157\nprint(f\"Number of 'lon' chunks: {nlon / space_chunk_size:0.2f}; Chunk of shape {space_chunk_size}; Size of last chunk: {nlon % space_chunk_size}\")\nprint(f\"Number of 'lat' chunks: {nlat / space_chunk_size:0.2f}; Chunk of shape {space_chunk_size}; Size of last chunk: {nlat % space_chunk_size}\")","key":"mdIctUoegl"},{"type":"outputs","id":"kqLqizGM01cQ-R_06jzJ9","children":[],"key":"WiCg4Uu4PK"}],"key":"B1WEPYfhjE"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"With this pattern, the “remainder” latitude chunk will have a shape of 150 in the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XoadxYpcU4"},{"type":"inlineCode","value":"lat","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VpFVI100fn"},{"type":"text","value":" dimension, and the “remainder” longitude chunk will have a shape of 149 in the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"i7KTlIdDb5"},{"type":"inlineCode","value":"lon","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TEkHneGQ64"},{"type":"text","value":" dimension.\nAll others will be a square 157 observations in both dimensions.\nThis amounts to a 9x4 chunk grid, with the last chunk in each dimension being partial.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BEF5kVIcQP"}],"key":"O8FBbTkZng"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"The entire spatial extent for a single time step can be read in 36 chunks with this chunk shape.\nThat seems a little high, given that this dataset will likely be taken at full spatial extent for a typical analysis.\nLet’s go a little bigger to see what that gets us:","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"jux85pCR2y"}],"key":"sOJ9huK09F"}],"key":"CGUbcsf7sZ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"space_chunk_size = 354 # 157 * 2\nprint(f\"Number of 'lon' chunks: {nlon / space_chunk_size:0.2f}; Chunk of shape {space_chunk_size}; Size of last chunk: {nlon % space_chunk_size}\")\nprint(f\"Number of 'lat' chunks: {nlat / space_chunk_size:0.2f}; Chunk of shape {space_chunk_size}; Size of last chunk: {nlat % space_chunk_size}\")","key":"dC1afiBCLw"},{"type":"outputs","id":"aFAr-meZe_0LikjMgsFXD","children":[],"key":"oZMfaynv4m"}],"key":"eiy1rpBQLr"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"This is just as good in terms of full-chunk remainders, and the whole extent can be read in with only 8 chunks.\nThe smallest remainder is still >75% of a full-sized square tile, which is acceptable.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DqQnqpIeqp"}],"key":"xMTSBVCYSN"}],"key":"R0pyvPRFQy"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Alternatively, we could stop being committed to square tiles and try and split the spatial regions more evenly.\nFor example, we could get as close to a 4x2 split as possible:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"laWUF6GLgl"}],"key":"HuxdGwA8I2"}],"key":"AXC0erOihQ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Add one to do a ceil divide\nlon_space_chunk_size = nlon // 4 + 1\nlat_space_chunk_size = nlat // 2 + 1\nprint(f\"Number of 'lon' chunks: {nlon / lon_space_chunk_size:0.3f}; Chunk of shape {lon_space_chunk_size}; Size of last chunk: {nlon % lon_space_chunk_size}\")\nprint(f\"Number of 'lat' chunks: {nlat / lat_space_chunk_size:0.3f}; Chunk of shape {lat_space_chunk_size}; Size of last chunk: {nlat % lat_space_chunk_size}\")","key":"cU791ou5Wp"},{"type":"outputs","id":"LhEYyn9A6V4UH723OV2U_","children":[],"key":"GJBOZoKMos"}],"key":"o2JUjU05Nw"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Or we could aim for a 3x3 split:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VZ9Lu4t8NN"}],"key":"ZhWVTUC0sd"}],"key":"Dzu4LGGWQb"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Add one to do a ceil divide\nlon_space_chunk_size = nlon // 3 + 1\nlat_space_chunk_size = nlat // 3 + 1\nprint(f\"Number of 'lon' chunks: {nlon / lon_space_chunk_size:0.3f}; Chunk of shape {lon_space_chunk_size}; Size of last chunk: {nlon % lon_space_chunk_size}\")\nprint(f\"Number of 'lat' chunks: {nlat / lat_space_chunk_size:0.3f}; Chunk of shape {lat_space_chunk_size}; Size of last chunk: {nlat % lat_space_chunk_size}\")","key":"F2GKMvoLBy"},{"type":"outputs","id":"0gwhts4MpELLefDHDYZ1I","children":[],"key":"uV2lA4LaNd"}],"key":"Vgz92lbqdP"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"As you might be getting, the chunking proportion between latitude and longitude is not super important.\nWhat is important for basic chunk shape is the total number of chunks between the two and the minimization of the remainder in the final chunk of each dimension.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"R9IqOZ5LUi"}],"key":"PXjMkZk6aZ"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"o2AuljzMSu"}],"key":"aikvefyx1z"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"If we were really confident that most analyses wanted the full extent, we might be better off to just put the whole lat/lon dimensions into single chunks each.\nThis would ensure (and require) that we read the entire spatial extent.\nHowever, our poor time-series analysis would then be stuck reading the entire dataset to get all time values for a single location.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"UPS1i9W2gp"}],"key":"AK9WlQsXG9"}],"key":"tgXqbhXCq9"}],"key":"y8kiYLx893"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Size Considerations","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rxtv5Itsrd"}],"identifier":"size-considerations","label":"Size Considerations","html_id":"size-considerations","implicit":true,"key":"MVfZ7D6ZDk"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Shape is only part of the equation.\nTotal “","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"eMZjWUQurx"},{"type":"crossReference","url":"/dataset-processing/tutorials/chunking/back/glossary","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"strong","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"chunk size","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"n4YFTdjRGo"}],"key":"NQtoXvJeIo"}],"urlSource":"../back/Glossary.md#term-Chunk-size","dataUrl":"/dataset-processing.tutorials.chunking.back.glossary.json","identifier":"term-chunk-size","label":"term-Chunk-size","remote":true,"protocol":"file","key":"tDZ63Y06C2"},{"type":"text","value":"” also matters.\nSize considerations come into play mostly as a consideration of how the chunks are stored on disk.\nThe retrieval time is influenced by the size of each chunk.\nHere are some constraints:","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"WuBKI0991S"}],"key":"oAio8Ld7JO"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Files Too Big:\nIn a Zarr dataset, each ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"lkGbGJoTFp"},{"type":"strong","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"chunk","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"f20ZOTG4Fw"}],"key":"tdOHutArD8"},{"type":"text","value":" is stored as a separate binary file (and the entire zarr dataset is a directory grouping these many “chunk” files).\nIf we need data from a particular chunk, no matter how little or how much, that file gets opened, decompressed, and the whole thing read into memory.\nA large chunk size means that there may be a lot of data transferred in situations when only a small subset of that chunk’s data is actually needed.\nIt also means there might not be enough chunks to allow the dask workers to stay busy loading data in parallel.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"GgwOMgYpWV"}],"key":"k3pnlTrv9m"}],"key":"s3l1G6DlKI"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":15,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"Files Too Small:\nIf the chunk size is too small, the time it takes to read and decompress the data for each chunk can become comparable to the latency of S3 (typically 10-100ms).\nWe want the reads to take at least a second or so, so that the latency is not a significant part of the overall timing.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"YVxUdf7Ej1"}],"key":"zBF2ZLQ26K"}],"key":"bGKz0bFlgJ"}],"key":"YbVTPANBya"},{"type":"admonition","kind":"tip","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Tip","key":"acHwNpIdsS"}],"key":"n6ru6hRmcK"},{"type":"paragraph","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"As a general rule, aim for chunk sizes between 10 and 200 MB, depending on shape and expected read pattern of a user.","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"jT6ec5wx5O"}],"key":"saYj2WyHud"}],"key":"MkM5F9cDvr"},{"type":"heading","depth":4,"position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"Total Chunk Size","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"wiuWd44DDN"}],"identifier":"total-chunk-size","label":"Total Chunk Size","html_id":"total-chunk-size","implicit":true,"key":"jh8ukLs9Ak"},{"type":"paragraph","position":{"start":{"line":25,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"To esimate the total chunk size, all we need is the expected chunk shape and data type to know how many bytes a value takes up.\nAs an example, let’s use a chunk shape of ","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"aRhgf2ZfoV"},{"type":"inlineCode","value":"{'time': 72, 'lat': 354, 'lon': 354}","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"KqJPNHZI5l"},{"type":"text","value":"\nThis will tell us if we’ve hit our target of between 10 and 200 MB per chunk.","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"iPo5vQZH9j"}],"key":"LY5jwgDBwh"}],"key":"Y8PE7aR4r6"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"chunks = {'time': 72, 'lat': 354, 'lon': 354}\nbytes_per_value = ds.tmn.dtype.itemsize\ntotal_bytes = chunks['time'] * chunks['lat'] * chunks['lon'] * bytes_per_value\nkiB = total_bytes / (2 ** 10)\nMiB = kiB / (2 ** 10)\nprint(f\"TMN chunk size: {total_bytes} ({kiB=:.2f}) ({MiB=:.2f})\")","key":"BXqAjUC6Yk"},{"type":"outputs","id":"OmAek0DmR0Ssh-c-pGFLG","children":[],"key":"eleFPvjZUM"}],"key":"ahzW0jtELt"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"We’re looking really good for size: about 69 MiB.\nThis maybe even a bit low.\nBut we’re in the (admittedly broad) range of 10-200 MiB of uncompressed data (i.e., in-memory) per chunk.\nTherefore, this seems like it would be a reasonable chunk shape and size for our dataset.\nIf we were curious about other chunk shapes, like a non-square ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"arNZdlYjew"},{"type":"inlineCode","value":"lat","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rKotu3WCkX"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NeP7oRHbs6"},{"type":"inlineCode","value":"lon","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"v68AcL7b09"},{"type":"text","value":" chunk, we could repeat this computation to estimate its size and determine if it is reasonable.\nHowever, we aren’t going to do that here, but it is something you could try on your own if you are curious.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MZkZweHpwZ"}],"key":"x2lpymWCQZ"}],"key":"QvYxhXI8YG"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Review and Final Considerations","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IKJtamH6in"}],"identifier":"review-and-final-considerations","label":"Review and Final Considerations","html_id":"review-and-final-considerations","implicit":true,"key":"g183JF628z"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Now that you have a general idea on how to pick chunk shape and size, let’s review and add a few final considerations.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"KOl8iZduIf"}],"key":"LKjOC9363i"},{"type":"heading","depth":3,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Basic Chunking Recommendations","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"IaWMAScZFj"}],"identifier":"basic-chunking-recommendations","label":"Basic Chunking Recommendations","html_id":"basic-chunking-recommendations","implicit":true,"key":"o3K7JkwyWO"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"When determining the basic chunk shape and size, the choice will depend on the future read pattern and analysis.\nIf this pattern is unknown, then it is important to take a balanced chunking approach that does not favor one dimension over the others (i.e., larger overall shape in a given dimension).\nNext, choosing a chunk shape should try to prevent partial chunks if possible.\nOtherwise, partial chunks should be at least half the size of the standard chunk.\nFinally, the total chunk size should be between 10 and 200 MiB for optimal performance.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"IAeuxy1IGO"}],"key":"vy3VmmAw5j"},{"type":"heading","depth":3,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Final Considerations","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"uWoYewsHD6"}],"identifier":"final-considerations","label":"Final Considerations","html_id":"final-considerations","implicit":true,"key":"npNPWI8VR4"},{"type":"paragraph","position":{"start":{"line":15,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"One final thing to consider is that these basic recommendations assume that your chunked data will be static and not updated.\nHowever, some datasets, especially climate related ones, are periodically updated in their time dimension.\nThese datasets are commonly updated at regular intervals (e.g., every year with the previous years data).\nThis can change the choice of chunk shape such that adding the next year’s worth of data does not require rechunking the whole data set or result in small partial chunks.\nFor our PRISM example, if we chose a temporal chunk shape of length 72 (i.e., six years per chunk), adding a year worth of data would require appending the partial chunk until it becomes full.\nThen, further new data would require starting a new partial chunk.\nThis could be prevented if we chose a chunk size of 12 (i.e., one year per chunk).\nThen, additional data would only require making new chunks versus editing existing chunks.\nTherefore, considering updates to the dataset when deciding the chunking plan can save a lot of time when appending the dataset in the future.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"MqUXxvUuwS"}],"key":"K8CJXqjDU8"},{"type":"paragraph","position":{"start":{"line":25,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"Additionally, all of the information provided here does not discuss proper optimization of chunk shape and size.\nProper optimization would attempt to select chunk sizes that are near powers of two (i.e., ","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"E1tbCZfi6D"},{"type":"inlineMath","value":"2^N","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mn>2</mn><mi>N</mi></msup></mrow><annotation encoding=\"application/x-tex\">2^N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413em;\"></span><span class=\"mord\"><span class=\"mord\">2</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></span></span></span></span></span></span>","key":"feJDjQo8gF"},{"type":"text","value":") to facilitate optimal storage and disk retrieval.\nDetails on this topic can be found in the advanced topic notebook of ","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"jraYQN00wh"},{"type":"link","url":"/dataset-processing/tutorials/chunking/201/optimalchunkselection","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"Choosing an Optimal Chunk Size and Shape","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"JhSoUyKEjg"}],"urlSource":"../201/OptimalChunkSelection.ipynb","dataUrl":"/dataset-processing.tutorials.chunking.201.optimalchunkselection.json","internal":true,"protocol":"file","key":"PYpVZYwVP8"},{"type":"text","value":".","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"BzvHTs2Xic"}],"key":"gYSzdYfe11"}],"key":"S80tZt4096"}],"key":"DEM73atRuN"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"How to Examine a Stored Dataset’s Chunk Shape","url":"/dataset-processing/tutorials/chunking/101/examinedatachunking","group":"Zarr Creation"},"next":{"title":"Writing Chunked Files","url":"/dataset-processing/tutorials/chunking/101/writechunkedfiles","group":"Zarr Creation"}}},"domain":"http://localhost:3005"}