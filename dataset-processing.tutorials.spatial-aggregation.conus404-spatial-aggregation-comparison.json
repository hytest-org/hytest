{"version":3,"kind":"Notebook","sha256":"72949e8c8b339c2b11b1ed3c00f1a70c710419592e35929ccc6aef85c8c0e2d5","slug":"dataset-processing.tutorials.spatial-aggregation.conus404-spatial-aggregation-comparison","location":"/dataset_processing/tutorials/spatial_aggregation/conus404_spatial_aggregation_comparison.ipynb","dependencies":[],"frontmatter":{"title":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","content_includes_title":false,"github":"https://github.com/hytest-org/hytest","copyright":"2023","numbering":{"title":{"offset":2}},"source_url":"https://github.com/hytest-org/hytest/blob/main/dataset_processing/tutorials/spatial_aggregation/conus404_spatial_aggregation_comparison.ipynb","edit_url":"https://github.com/hytest-org/hytest/edit/main/dataset_processing/tutorials/spatial_aggregation/conus404_spatial_aggregation_comparison.ipynb","exports":[{"format":"ipynb","filename":"conus404_spatial_aggregation_comparison.ipynb","url":"/hytest//build/conus404_spatial_agg-535cce9f4f0fe582c96699203d5478ab.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","data":{"tags":[]},"children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"In this notebook, we will be comparing two spatial aggregation methods to aggregate from grids to polygons. One uses ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"zkHegmX5Rl"},{"type":"link","url":"https://gdptools.readthedocs.io/en/latest/index.html","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"inlineCode","value":"gdptools","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"yevSfo6LLy"}],"urlSource":"https://gdptools.readthedocs.io/en/latest/index.html","key":"IG5aKBkTFT"},{"type":"text","value":". The other uses conservative regional methods with ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"IcVvGLIvUY"},{"type":"link","url":"https://docs.xarray.dev/en/stable/index.html","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"inlineCode","value":"xarray","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"k1zS38ELhF"}],"urlSource":"https://docs.xarray.dev/en/stable/index.html","key":"Gb91GfEJu1"},{"type":"text","value":" and ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ZR4BdjKu2T"},{"type":"link","url":"https://geopandas.org/en/stable/index.html","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"inlineCode","value":"geopandas","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"YVBKlQTSfl"}],"urlSource":"https://geopandas.org/en/stable/index.html","key":"TqhYhkvMG4"},{"type":"text","value":" natively (see this ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"yC705RqkRm"},{"type":"link","url":"https://discourse.pangeo.io/t/conservative-region-aggregation-with-xarray-geopandas-and-sparse/2715","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Pangeo Discourse","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"KDrKbUBGyn"}],"urlSource":"https://discourse.pangeo.io/t/conservative-region-aggregation-with-xarray-geopandas-and-sparse/2715","key":"cMP8QyeOMG"},{"type":"text","value":" for details).","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"OuAWeFyCR6"}],"key":"Chv0P857cc"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"The goal of this comparision is to see how the results of the two methods compare to help judge the efficacy of one versus the other.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"AxhgbfcOSQ"}],"key":"KAdUtpyhN4"}],"visibility":"show","key":"nGh9nmCXJy"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%xmode minimal\nimport os\n# Needed when boto3 >= 1.36.0 or the rechunking process will fail\n# This needs to be set before the boto3 library gets loaded\n# See: https://github.com/aws/aws-cli/issues/9214#issuecomment-2606619168\nos.environ['AWS_REQUEST_CHECKSUM_CALCULATION'] = 'when_required'\nos.environ['AWS_RESPONSE_CHECKSUM_VALIDATION'] = 'when_required'\nimport time\nimport xarray as xr\nimport geopandas as gp\nimport pandas as pd\nimport numpy as np\nimport sparse\n\nimport hvplot.pandas\nimport hvplot.xarray\nimport dask\nimport cf_xarray\n\nfrom pynhd import NLDI, WaterData\nfrom pygeohydro import watershed\nimport cartopy.crs as ccrs\nfrom shapely.geometry import Polygon\n\nimport pyproj\nfrom gdptools import WeightGen, AggGen, UserCatData\nimport pystac\nfrom packaging.version import Version\nimport zarr","key":"RL0b7qGctl"},{"type":"outputs","id":"NwroHzD5_Q4cvFlaBFvd8","children":[],"key":"dmLe5lrb4h"}],"key":"Jis579PluS"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Open dataset from Intake Catalog","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PrLj9YlokI"}],"identifier":"open-dataset-from-intake-catalog","label":"Open dataset from Intake Catalog","html_id":"open-dataset-from-intake-catalog","implicit":true,"key":"XfQyxHLair"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"First, let’s begin by loading the CONUS404 daily data.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"dyJomhIUJQ"}],"key":"bAQCeiUrXu"}],"key":"WhIzh5LpWw"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def get_children(catalog, collection_id=None):\n    \"\"\"\n    This function retrieves a specified collection from a STAC catalog/collection and prints key metadata \n    for exploring/accessing the datasets contained within it.\n    If there is no collection ID provided, the collections in the top level of the catalog will be printed.\n    If a collection ID is provided, it will retrieve the collection with that ID from the input catalog/collection.\n    If the collection ID points to a dataset, it will print the assets available for the dataset.\n    If the collection ID points to another collection, it will list the child collections in the IDed collection.\n\n    Args:\n        catalog (pystac.Catalog | pystac.Collection): The STAC catalog/collection object.\n        collection_id (str): The ID of the collection or dataset to retrieve from catalog.\n    \n    Returns:\n        collection (pystac.Catalog | pystac.Collection): The collection object corresponding to the provided ID\n                                                         or the top-level catalog if no ID is provided.\n    \"\"\"\n    dataset = False\n    if collection_id:\n        collection = catalog.get_child(collection_id)\n        if collection.assets:\n            dataset = True\n            print(f\"{collection_id} is a dataset. Please review the assets below and select one to open.\")\n\n        else:\n            print(f\"{collection_id} is a collection. Please review the child items and select one to open in the next cell.\")\n    else:\n        collection = catalog\n    if dataset==True:\n        # List the assets\n        for asset in collection.assets:\n            print(f\"Asset ID: {asset}\")\n            print(f\"    Title: {collection.assets[asset].title}\")\n            print(f\"    Description: {collection.assets[asset].description}\")\n    else:\n        collections = list(collection.get_collections())\n        print(f\"Number of collections: {len(collections)}\")\n        print(\"Collections IDs:\")\n        for child_collection in collections:\n            id = child_collection.id\n            cite_as = \"Not available\"\n            for link in child_collection.links:\n                if link.rel == \"cite-as\":\n                    cite_as = link.target\n            print(f\"- {id}, Source: {cite_as}\")\n    return collection","key":"rJo5Z6Rai3"},{"type":"outputs","id":"mGoPE2YGlGerPXc0ETMEK","children":[],"key":"ICH4BVmShT"}],"key":"enJNq49hdJ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# url for the WMA STAC Catalog\ncatalog_url = \"https://api.water.usgs.gov/gdp/pygeoapi/stac/stac-collection/\"\n\n# use pystac to read the catalog\ncatalog = pystac.Catalog.from_file(catalog_url)\n\n# list the collections in the catalog\ncatalog = get_children(catalog)","key":"RTGKFqM2Kg"},{"type":"outputs","id":"MbeTCO-6a5aPG1WGQ5J3w","children":[],"key":"ECqZVC5FGQ"}],"key":"G0qJONPE1I"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(catalog, collection_id=\"conus404\")","key":"zqPxZxSOHU"},{"type":"outputs","id":"N5FeQuwzBRX6r-oNXYV9j","children":[],"key":"r1VyOqgGIG"}],"key":"QrI2CFvEQL"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(collection, collection_id=\"conus404_daily\")","key":"R2UBEAEG8K"},{"type":"outputs","id":"vogFwBtqKnHaiWm4Ru7AN","children":[],"key":"qqDWfRjgHj"}],"key":"XKdAwZNrub"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"As we can see there are two different locations for the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bPbqhfSYV3"},{"type":"inlineCode","value":"conus404_daily","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CVMh22EHMy"},{"type":"text","value":" data set. The locations are (1) ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"x9dblz71vp"},{"type":"inlineCode","value":"-hovenweep","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"i6PhlPuwSC"},{"type":"text","value":" meaning it is stored on the USGS Hovenweep HPC and (2) ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZBND4kePyr"},{"type":"inlineCode","value":"-osn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BCBRVgQxIM"},{"type":"text","value":" meaning the data is on the USGS open storage network (OSN). As the OSN is free to access from any environment, we will use that for this example, but the location can easily be changed depending on your needs.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HVm2llAi9r"}],"key":"tchlHKeA7W"}],"key":"hnlBRfEev9"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# replace with the asset ID you want to use:\nselected_asset_id = \"zarr-s3-osn\"\n\n# read the asset metadata\nasset = collection.assets[selected_asset_id]","key":"IP9eT2oTfg"},{"type":"outputs","id":"L53qbPVCZIEnz3HWO__pG","children":[],"key":"liAP6pY8Gx"}],"key":"FdYDASwcvC"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# uncomment the lines below to read in your AWS credentials if you want to access data from a requester-pays bucket (-cloud)\n# os.environ['AWS_PROFILE'] = 'default'\n# %run ../../../environment_set_up/Help_AWS_Credentials.ipynb","key":"G25oUrO3cG"},{"type":"outputs","id":"gy4SHehNf1f-PGDxk9ZD6","children":[],"key":"JbJ9v0PQHX"}],"key":"yt0MtHKCfu"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Finally, read in the daily CONUS404 data set and select the accumulated grid scale precipitation. We select the precipitation rather then all variables to keep things simple for this example, but aggregation of other variables would follow the same methodology.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FIBnjureLH"}],"key":"UXLPsfkVDg"}],"key":"p6HLduo9sP"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"if Version(zarr.__version__) < Version(\"3.0.0\"):\n    conus404 = xr.open_dataset(\n        asset.href,\n        storage_options=asset.extra_fields['xarray:storage_options'],\n        **asset.extra_fields['xarray:open_kwargs']\n    )\nelse:\n    conus404 = xr.open_dataset(\n    asset.href,\n    storage_options=asset.extra_fields['xarray:storage_options'],\n    **asset.extra_fields['xarray:open_kwargs'],\n    zarr_format=2\n    )\n\n# Include the crs as we will need it later\nconus404 = conus404[['PREC_ACC_NC', 'crs']]\nconus404","key":"TlAZV55EAn"},{"type":"outputs","id":"pjVmClPhaCSj1MOoV6-my","children":[],"key":"DuMVKpaJMm"}],"key":"Mb41KyMDrz"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Parallelize with Dask (optional)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GCUTFnPc0c"}],"identifier":"parallelize-with-dask-optional","label":"Parallelize with Dask (optional)","html_id":"parallelize-with-dask-optional","implicit":true,"key":"jZXc50UyTQ"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Some of the steps we will take are aware of parallel clustered compute environments using ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"FcHfrZlka1"},{"type":"link","url":"https://www.dask.org/","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"inlineCode","value":"dask","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"FLEO3EqP5z"}],"urlSource":"https://www.dask.org/","key":"RmJPPbhLCt"},{"type":"text","value":". We can start a cluster now so that future steps take advantage\nof this ability. This is an optional step, but speed ups data loading significantly, especially when accessing data from the cloud.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"oEHqs9qfok"}],"key":"EgehL4Gs9S"},{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"We have documentation on how to start a Dask Cluster in different computing environments ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"WlMajjNEhI"},{"type":"link","url":"/environment-set-up/clusters","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"here","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"CJyqcgerkn"}],"urlSource":"../../../environment_set_up/clusters.md","dataUrl":"/environment-set-up.clusters.json","internal":true,"protocol":"file","key":"MfwMQ5vzqu"},{"type":"text","value":". Uncomment the cluster start up that works for your compute environment.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"XG1ZIJwVEL"}],"key":"ZTRdvqBsps"}],"key":"oIeXW5mKZl"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%run ../../../environment_set_up/Start_Dask_Cluster_Nebari.ipynb\n## If this notebook is not being run on Nebari, replace the above \n## path name with a helper appropriate to your compute environment.  Examples:\n# %run ../../../environment_set_up/Start_Dask_Cluster_Denali.ipynb\n# %run ../../../environment_set_up/Start_Dask_Cluster_Tallgrass.ipynb\n# %run ../../../environment_set_up/Start_Dask_Cluster_Desktop.ipynb","key":"dMvsT6I9MC"},{"type":"outputs","id":"K0CTCJenDsYG1OrEbtkw7","children":[],"key":"af9R3vccXP"}],"key":"tqGrXgl5Rq"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Load the Feature Polygons","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uiabNKhbzV"}],"identifier":"load-the-feature-polygons","label":"Load the Feature Polygons","html_id":"load-the-feature-polygons","implicit":true,"key":"aQxytOEcBY"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Now that we have read in the CONUS404 data, we need to read in some polygons to aggregate the data. For this example, we will use the HUC12 basins within the Delaware River Basin. To get these HUC12 polygons, we can use ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"md0eKfWBxv"},{"type":"link","url":"https://docs.hyriver.io/autoapi/pygeohydro/watershed/","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"inlineCode","value":"pygeohydro.watershed","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"KrzgKCIjiM"}],"urlSource":"https://docs.hyriver.io/autoapi/pygeohydro/watershed/","key":"Nj5IY3jXIN"},{"type":"text","value":" to query the Hydro Network Linked Data Index (NLDI). All we need to get the basins is the general IDs of the HUC12 basins. For the Delaware Basin those are ones that start with 020401 or 020402.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"aulk2OYzEq"}],"key":"cLRBwSJQvT"}],"key":"XQnFiHGdkW"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%time\nwbd = watershed.WBD(\"huc4\")\ndelaware_basin = wbd.byids(field=\"huc4\", fids=\"0204\")\nhuc12_basins = WaterData('wbd12').bygeom(delaware_basin.iloc[0].geometry)\nhuc12_basins = huc12_basins[huc12_basins['huc12'].str.startswith(('020401', '020402'))]","key":"PzvNZs8y4T"},{"type":"outputs","id":"2IWj4QDT0dVSeLpWl_4Gs","children":[],"key":"WbHctqDUJw"}],"key":"aKxqfEKHPm"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s plot the HUC12 basins to see how they look.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"u4pgniIB3c"}],"key":"WfFpLJzSLT"}],"key":"ttUZ67V7Es"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"huc12_basins.hvplot(\n    c='huc12', title=\"Delaware River HUC12 basins\",\n    coastline='50m', geo=True,\n    aspect='equal', legend=False, frame_width=300\n)","key":"z6ZJMyW9kL"},{"type":"outputs","id":"sApvCeRfBbiR2-dGEqp2I","children":[],"key":"oWkujTis7i"}],"key":"mmN1e1Xw0z"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"An important thing to note is that all geodataframes should have a coordinate reference system (CRS). Let’s check to make sure our geodataframe has a CRS.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"h7pklkK8sy"}],"key":"hN5vgsBt1N"}],"key":"BcgUQVA2bL"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"huc12_basins.crs","key":"WfuDQAtMU9"},{"type":"outputs","id":"DS1JaC9KEGrkJ3RcETUA0","children":[],"key":"YClt1ScEfV"}],"key":"gUppYUY8Ze"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Limit CONUS404 Spatial Range","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gw1ZFOyt0j"}],"identifier":"limit-conus404-spatial-range","label":"Limit CONUS404 Spatial Range","html_id":"limit-conus404-spatial-range","implicit":true,"key":"TDyzuW3zoz"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"With the HUC12 basins read in, we only need the CONUS404 data that spans these polygons as they are the regions we will be aggregating. So, let’s limit the CONUS404 spatial range to that of the basins. This will save on memory and computation. Note doing this is mainly useful when the regions footprint is much smaller than the footprint of the gridded model.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"HSMEbER0fF"}],"key":"V2r8afQC1U"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"To limit the spatial range, we first need to convert the CRS of the basins to that of CONUS404. Then extract the bounding box of the basins.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"lMVeuzyFjH"}],"key":"Pk7HnqdLQl"}],"key":"sgygDftNig"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"huc12_basins_conus404_crs = huc12_basins.to_crs(conus404.crs.crs_wkt)\nbbox = huc12_basins_conus404_crs.total_bounds\nbbox","key":"s875Ry6G7P"},{"type":"outputs","id":"mKkbjGw4zHYXuqEA40E3S","children":[],"key":"AI9beb2X27"}],"key":"qOxeu0gkx6"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Then select the CONUS404 data within the bounding box. However, when we do this, we will extend the bounds out by 5% of their range to ensure all of our basins are within the spatially limited data. We do this as the reprojections of the CRS can cause slight distortions that make polygons on the bounds not fall fully within the data.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YJHHMMqvqP"}],"key":"jzIlTVwEA0"}],"key":"wAMR4gihTn"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"bbox_x_range = bbox[2] - bbox[0]\nbbox_y_range = bbox[3] - bbox[1]\nx_range = slice(bbox[0] - bbox_x_range * 0.05,\n                bbox[2] + bbox_x_range * 0.05)\ny_range = slice(bbox[1] - bbox_y_range * 0.05,\n                bbox[3] + bbox_y_range * 0.05)\n\nconus404 = conus404.sel(x=x_range, y=y_range)\nconus404","key":"JWysKS0eTM"},{"type":"outputs","id":"SHtYuru5OCFiTwYbEzhb-","children":[],"key":"SS23ZRFsUV"}],"key":"FdXGDWisM6"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"To make sure this worked as intended, let’s plot the full basin over the extracted CONUS404 data.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bPYbfNvPm4"}],"key":"ji7Jl3xQxb"}],"key":"niLss3Jkuz"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Select a single timestamp for simple plotting\ntimestamp = '2000-5-02'\ncutout = conus404.sel(time=timestamp).drop_vars(['lat', 'lon'])\n# We need to write the CRS to the CONUS404 dataset and\n# reproject for clean plotting with hvplot\ncutout = cutout.rio.write_crs(conus404.crs.crs_wkt).rio.reproject('EPSG:4326')\n\ncutout_plt = cutout.hvplot(\n    coastline='50m', geo=True,\n    aspect='equal', cmap='viridis', frame_width=300\n)\nhuc12_plt = huc12_basins.hvplot(\n    geo=True, alpha=0.3, c='r'\n)\ncutout_plt * huc12_plt","key":"wEBa4Ibnsp"},{"type":"outputs","id":"yQivAvzhFgRRSkQ-Ni_NY","children":[],"key":"IljLSIaLit"}],"key":"PiTdAjD3sV"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Looks good!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LKGFb7lTbv"}],"key":"sGhf1VViIJ"}],"key":"tBy6wNPKhc"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Aggregate CONUS404 to Feature Polygons","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ckIJdAcP8A"}],"identifier":"aggregate-conus404-to-feature-polygons","label":"Aggregate CONUS404 to Feature Polygons","html_id":"aggregate-conus404-to-feature-polygons","implicit":true,"key":"NzNyEdeMY5"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Now that we have our gridded data and polygons, it is time to aggregate them using ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"CzQphgMdoG"},{"type":"inlineCode","value":"gdptools","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"aVHTUf8MEU"},{"type":"text","value":" and what we consider the native method that uses ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"mAwqUWkoht"},{"type":"inlineCode","value":"geopandas","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"wctFa3RHo0"},{"type":"text","value":" and ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"kTxqHvk8uQ"},{"type":"inlineCode","value":"xarray","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"wVyEv4paxk"},{"type":"text","value":".","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ZJAbcDbL2P"}],"key":"mV96YjwY0K"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"NOTE: ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"v2ScqyiyLy"},{"type":"inlineCode","value":"gdptools","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"P441OJgmkn"},{"type":"text","value":" handles a number of pre-processing steps for the user:","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"SNayP3Eoej"}],"key":"BkMRBbrYFq"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":7,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Subsets the gridded data to a buffered bounding box of the targets polygons.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"bhTIbkhcg6"}],"key":"JEzyX64Fer"}],"key":"ZmxugGINt2"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Checks latitude bounds and if it’s in the interval 0-360, it’s rotated into -180 - 180.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"iFB9E4CgX9"}],"key":"gqcDm01Y6c"}],"key":"h6pnW7UO7Z"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Checks the order of the longitude bounds, i.e. top-to-bottom or bottom-to-top, and autmatically acconts for this is the sub-setting operation above.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"lDV4Yfaqbs"}],"key":"u54ngOj8S7"}],"key":"wM8l22bKF2"}],"key":"TBEeM3Afup"}],"key":"D91TFL9vS1"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"gdptools Aggregation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"f9F2OKlYRl"}],"identifier":"gdptools-aggregation","label":"gdptools Aggregation","html_id":"gdptools-aggregation","implicit":true,"key":"y3kqyIlMgR"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Let’s start by using the gdptools aggregation method, where we use three data classes provided by ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"pFtz6sE9rB"},{"type":"inlineCode","value":"gdptools","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"FyNRLhioAK"},{"type":"text","value":", in the order discussed below.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"h1JqeR8iYf"}],"key":"tHqj6sTRgP"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"UserCatData","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"QjL4RT1aJa"},{"type":"text","value":" stores the data required to perfom the aggregation.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"oZfgJAZOaM"}],"key":"Aomw6EZQFY"}],"key":"Di79UncZos"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"WeightGen","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"lAmOG5sCwW"},{"type":"text","value":" is a class used to generate the areal-weights used to calculate the areal-weighted interpolation. The weights generated between a source and target dataset can be reused as long as the source and target are consistent. For example, If a new time-period became available, or a different set of variables is needed, the same weights can be used.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"XrjTNNvrCP"}],"key":"IIQzpluQxd"}],"key":"g938xMsmA8"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"AggGen","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"mXU35Q51WR"},{"type":"text","value":" is a class that is used to calculate the aggregation.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"jJigprAkPi"}],"key":"Ki5mIvCt9f"}],"key":"IbGwUXpxIS"}],"key":"zsx7AsCk9h"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"The first step to aggregating with ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"QVHqIu2nka"},{"type":"inlineCode","value":"gdptools","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"WSHS0RkgL9"},{"type":"text","value":" is to convert the input data to a ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"zy3icPSmQQ"},{"type":"inlineCode","value":"UserCatData","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"oGJlF1L28P"},{"type":"text","value":" class. Note additionally that the ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"I063Uxx9a4"},{"type":"inlineCode","value":"var","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"eeV3b9769N"},{"type":"text","value":" parameter could be a list of variables, such that when the user_data object is used in ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"OhT9WyPy9g"},{"type":"inlineCode","value":"AggGen","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"BVzgpooeyc"},{"type":"text","value":", the ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"c1b8jnIGdk"},{"type":"inlineCode","value":"calculuate_agg()","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"bmntU7Ib3J"},{"type":"text","value":" method will perform the aggregation over all the list of variables.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"RSAZWv8lHf"}],"key":"GLJwRo8pCL"}],"key":"MBVubX94vk"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"user_data = UserCatData(\n    ds=conus404,\n    proj_ds=conus404.crs.crs_wkt,\n    x_coord='x',\n    y_coord='y',\n    t_coord='time',\n    var='PREC_ACC_NC',\n    f_feature=huc12_basins,\n    proj_feature=huc12_basins.crs,\n    id_feature='huc12',\n    period=[pd.Timestamp(conus404.time.values.min()),\n            pd.Timestamp(conus404.time.values.max())],\n)","key":"HkZf2xmxoL"},{"type":"outputs","id":"QuvEVo80A2DUQMDQeKula","children":[],"key":"uunjGRM9Sd"}],"key":"QSYX1tbrcj"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tT8laNERru"},{"type":"inlineCode","value":"UserCatData","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"n9nZFw3IDN"},{"type":"text","value":" can then be used to generate weights for each polygon. An important thing to note is that when generating the weights we need to use an equal area projection (i.e., equal area CRS).","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vzFKgkQmCM"}],"key":"JQNu80sNUd"}],"key":"oRWG60epnk"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"crs_area = \"EPSG:6931\" # Good for northern hemisphere\n# crs_area = \"EPSG:5070\" # Good for CONUS\n\n# time the weight generation for later comparison\nt0 = time.time()\n\nweight_gen = WeightGen(\n    user_data=user_data,\n    # use serial here vs dask as the dask overhead would cause\n    # a slow down since our example is relatively small scale\n    method=\"serial\",\n    weight_gen_crs=crs_area,\n)\n\ndf_gdptools_weights = weight_gen.calculate_weights()\n\ngdptools_weights_time = time.time() - t0\n\ndf_gdptools_weights","key":"lkPPOhquGn"},{"type":"outputs","id":"n2EtcttxO-TBy3K6pBGpV","children":[],"key":"JNukLmbDSC"}],"key":"b0CENtKpHZ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"With the weights, we can now perform the aggregation.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tcKosPO5bq"}],"key":"NAbF060Yev"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Note that the return values of ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"aZSWKq1MCi"},{"type":"inlineCode","value":"calculate_agg()","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"HGo6Fll12L"},{"type":"text","value":" are:","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"pLh63DFy6B"}],"key":"kVZYgajzlR"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":4,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"ngdf","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"B7wG3J3VFH"},{"type":"text","value":" the target ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"HV3sqKEISj"},{"type":"inlineCode","value":"GeoDataFrame","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"B1dBPGcYxC"},{"type":"text","value":", sorted by ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"ReMTd5tbqx"},{"type":"inlineCode","value":"id_feature","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"d6sB2XxEwN"},{"type":"text","value":" and filtered to only those ids that have weights.\nIn other words if, there was not complete overlay of the source to target datasets, some target ids will not have values.\nIf the user wishes to plot the resulting interpolated data, the returned GeoDataFrame’s id order is the same as the ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"STQSI8cHDK"},{"type":"inlineCode","value":"gdptools_aggregation","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"zzKlOhh8a9"},{"type":"text","value":".","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"H6coOrGT0O"}],"key":"qsvtUxUbwM"}],"key":"gHV7yFb2Yh"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"gdptools_aggregation","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"nqxydAfnVR"},{"type":"text","value":", which is an ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"YD73LlPHHK"},{"type":"inlineCode","value":"xarray.Dataset","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"kZbd6nCnqL"},{"type":"text","value":" containing the interpolated output with dimensions of ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"N5jzPKG6y6"},{"type":"inlineCode","value":"time","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"BolPdKDcIM"},{"type":"text","value":" and ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"ZtBCgCzhjt"},{"type":"inlineCode","value":"id_feature","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"SYDYmU04W8"},{"type":"text","value":".\nIn the case below, the ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"ezkCPufwzF"},{"type":"inlineCode","value":"agg_writer","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"PDfpW5kb0m"},{"type":"text","value":" parameter is set to ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"H6C99Ew9gV"},{"type":"inlineCode","value":"'none'","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"okP0NV6Yav"},{"type":"text","value":", it can be set to ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"Ylx2hpgw7a"},{"type":"inlineCode","value":"'netcdf'","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"dp1q4NxtA9"},{"type":"text","value":", ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"wJJHySAwsn"},{"type":"inlineCode","value":"'csv'","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"j4YpB02QzB"},{"type":"text","value":", or ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"ggqmh4aJia"},{"type":"inlineCode","value":"'parquet'","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"GnYPIl881j"},{"type":"text","value":" for archiving the results to a file.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"drVyy9gbM0"}],"key":"XHORzOnqeE"}],"key":"nQAMVKzMMS"}],"key":"fD10riUyOb"}],"key":"x2WuvdEPEY"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"t0 = time.time()\n\nagg_gen = AggGen(\n    user_data=user_data,\n    # Use masked to ignore NaNs\n    # Note that a we use mean vs sum as sum seems to ignore\n    # weights even though they should be equivalent methods\n    # (i.e., weighted sum = weighted mean)\n    stat_method=\"masked_mean\",\n    agg_engine=\"dask\",\n    weights=df_gdptools_weights,\n    agg_writer='none',\n)\n_, gdptools_aggregation = agg_gen.calculate_agg()\n\ngdptools_agg_time = time.time() - t0\n\ngdptools_aggregation","key":"HiwhxLLtFw"},{"type":"outputs","id":"q648w2JrL9Ank3m36DeGJ","children":[],"key":"PuLHud0I4u"}],"key":"l40QxNeo8J"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s make a nice plot of the aggregated HUC12 basins to make sure the aggregation worked as expected.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"toYzI6QzpH"}],"key":"HFuENNZCfv"}],"key":"d7Ftz8mo92"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# xarray holds the huc12s in sorted order\ngdptools_huc12_basins = huc12_basins.copy().sort_values('huc12')\ngdptools_huc12_basins['aggregation'] = gdptools_aggregation.sel(time=timestamp)['PREC_ACC_NC']\n\ngdptools_plt = gdptools_huc12_basins.hvplot(\n    c='aggregation', title=\"Accumulated Precipitation over HUC12 basins\",\n    coastline='50m', geo=True, cmap='viridis',\n    aspect='equal', legend=False, frame_width=300\n)\n\ncutout_plt * gdptools_plt + cutout_plt","key":"wGQG3AFifg"},{"type":"outputs","id":"FgUoKrNS662K9wOLovBGQ","children":[],"key":"MXDjdR6H1E"}],"key":"lI9CLRTXCA"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Native Method","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xFTNBIeMyj"}],"identifier":"native-method","label":"Native Method","html_id":"native-method","implicit":true,"key":"o5WopG2LAW"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"For the native method, we first need to extract the grid information from our CONUS404 data set. We then use it to create polygon boxes that we overlay with the basin polygons to generate weights. Finally like ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"pfnC1GQJOa"},{"type":"inlineCode","value":"gdptools","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"QmCiSZkiYX"},{"type":"text","value":", we use the weights to aggregate via a weighted sum.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"UlBCx6chei"}],"key":"ww6kY9zn3y"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"To give a fair computational time comparison with ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"z0Gy9ZNMVt"},{"type":"inlineCode","value":"gdptools","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"yjZkYFmkN6"},{"type":"text","value":", we will group all steps to generate the weights into one timed cell.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"o0zGf6duif"}],"key":"FhqGeKphBV"}],"key":"jUg6gs88IY"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":4,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Create Weights","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ccgdk0HrCk"}],"identifier":"create-weights","label":"Create Weights","html_id":"create-weights","implicit":true,"key":"lKZD2F2ahy"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"To generate the weights, we (1) extract grid information (includes extracting the ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"d5G3ypwDs6"},{"type":"inlineCode","value":"x","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"JRVxS8ugQ4"},{"type":"text","value":" and ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ij45oEeWTk"},{"type":"inlineCode","value":"y","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"nY8pQgAndD"},{"type":"text","value":" grid and getting their bounds), (2) use these bounds to create polygons of the grid, (3) assign the polygons to a ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ASDwJZLf7Q"},{"type":"inlineCode","value":"GeoDataFrame","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"bYpFEJmL9O"},{"type":"text","value":" with the CONUS404 dataset’s CRS, (4) overlay the grid polygons and basin polygons, (5) use the overlay to get fractional area weights.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"w7EkWgk1hm"}],"key":"oueESaGEa8"}],"key":"GYjyKHgtQh"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%time\nt0 = time.time()\n# (1) extract grid info\ngrid = conus404[['x', 'y']].drop_vars(['lat', 'lon']).reset_coords()\ngrid = grid.cf.add_bounds(['x', 'y'])\n\n\n# (2) create polygons of the grid\n# use a simple helper function. This way we can use xarray to parallelize.\ndef bounds_to_poly(x_bounds, y_bounds):\n    return Polygon([\n        (x_bounds[0], y_bounds[0]),\n        (x_bounds[0], y_bounds[1]),\n        (x_bounds[1], y_bounds[1]),\n        (x_bounds[1], y_bounds[0])\n    ])\n\n# Stack the grid cells into a single stack (i.e., x-y pairs)\npoints = grid.stack(point=('y', 'x'))\n\n# Apply the function to create polygons from bounds\nboxes = xr.apply_ufunc(\n    bounds_to_poly,\n    points.x_bounds,\n    points.y_bounds,\n    input_core_dims=[(\"bounds\",),  (\"bounds\",)],\n    output_dtypes=[np.dtype('O')],\n    vectorize=True\n)\n\n\n# (3) assign polygons to geodataframe with CRS\ngrid_polygons = gp.GeoDataFrame(\n    data={\"geometry\": boxes.values, \"y\": boxes['y'], \"x\": boxes['x']},\n    index=boxes.indexes[\"point\"],\n    crs=conus404.crs.crs_wkt\n)\n\n\n# (4) overlay the grid polygons with basin polygons\n# transform both to an area preserving projection\nhuc12_basins_area = huc12_basins.to_crs(crs_area)\ngrid_polygons = grid_polygons.to_crs(crs_area)\n\n# overlay the polygons.\noverlay = grid_polygons.overlay(huc12_basins_area, keep_geom_type=True)\n\n\n# (5)calculate the area fraction for each region\ngrid_cell_fraction = overlay.geometry.area.groupby(overlay['huc12']).transform(lambda x: x / x.sum())\n\n# turn this into a series\nmulti_index = overlay.set_index(['y', 'x', 'huc12']).index\ndf_native_weights = pd.Series(grid_cell_fraction.values, index=multi_index)\n\nda_native_weights_stacked = xr.DataArray(df_native_weights)\n\n# unstack to a sparse array.\nnative_weights = da_native_weights_stacked.unstack(sparse=True, fill_value=0.)\n\nnative_weights_time = time.time() - t0\n\nnative_weights","key":"rRfgtLQmz6"},{"type":"outputs","id":"gbalUoYIbI3exh9VBfhTN","children":[],"key":"mP6kciqKk8"}],"key":"LO8JgHKuOt"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now that we have our weights, we can clearly see that this is a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"odD23KWDAA"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"sparse","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tHbOiBwSkM"}],"key":"CjnFOjex4u"},{"type":"text","value":" matrix, with a density of ~0.0025 (i.e., only 0.25% of values are non-zero). So, maintaining it as a sparse martix is the right move for memory conservation, especially as this process scales up.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IPPdRnPQCq"}],"key":"tNxxuVPd3w"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Also, this process is area conserving. We can verify this for each basin’s area with a simple area calculation.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"nGM8mu6RPG"}],"key":"mLjlNbuqtQ"}],"key":"IQIjyDECo1"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# calculate areas of HUC12s from overlay and original polygons\noverlay_area = overlay.geometry.area.groupby(overlay['huc12']).sum()\nhuc12_area = huc12_basins_area.geometry.area.groupby(huc12_basins_area['huc12']).sum()\n# find the max fractional difference\n(np.abs(overlay_area - huc12_area) / huc12_area).max()","key":"VUSQeIS6wD"},{"type":"outputs","id":"4u1NXtz0yo2g_Qq5oAMUM","children":[],"key":"VefnY1vdeE"}],"key":"bSERKeK8Re"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Nice! This means the differences can be attributed to machine precision.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HH3rTbkMYO"}],"key":"S5EDxdfoMz"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"We can also verify that the cell fractions all sum up to one.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"yUy81C0lPm"}],"key":"M2Wzz2q3Mp"}],"key":"rMGUOFlGyS"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"grid_cell_fraction.groupby(overlay['huc12']).sum().unique()","key":"xrPOIeZQmH"},{"type":"outputs","id":"34xMGC9rkdDDEHf_bzhN6","children":[],"key":"DVLsXCeVwH"}],"key":"zSpMxTiw5a"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":4,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Perform Aggregation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"k88KTJ3kuo"}],"identifier":"perform-aggregation","label":"Perform Aggregation","html_id":"perform-aggregation","implicit":true,"key":"epzesVcBvA"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"To aggregate the data, we can use ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"aNs5piaOm8"},{"type":"link","url":"https://docs.xarray.dev/en/stable/generated/xarray.Dataset.weighted.html","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"inlineCode","value":"xarray.Dataset.weighted","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"vG4DdY04AX"}],"urlSource":"https://docs.xarray.dev/en/stable/generated/xarray.Dataset.weighted.html","key":"Z0v0wNSkU9"},{"type":"text","value":" to do our weighted calculations. This is simple as it will take a sparse array as weights and compute the aggregation.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"yWFCaRwX4N"}],"key":"eLmBi6xESk"}],"key":"wslxlzT5QC"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%time\nt0 = time.time()\n\nnative_aggregation = conus404.drop_vars('crs').weighted(native_weights).sum(dim=['x', 'y']).compute()\n\nnative_agg_time = time.time() - t0\n\nnative_aggregation","key":"uAdvMW9s66"},{"type":"outputs","id":"PmE3hAJNNGnXgiDu0N-az","children":[],"key":"vw9zYIgkSX"}],"key":"waXrUBHGBO"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Like the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SqUyTntXyC"},{"type":"inlineCode","value":"gdptools","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nSmUHKQgQT"},{"type":"text","value":" aggregation results, let’s make some plots to make sure this worked as expected.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KcOUXz9n9j"}],"key":"yl5KTMTLR9"}],"key":"TWLDoDHiPh"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# xarray holds the huc12s in sorted order\nnative_huc12_basins = huc12_basins.copy().sort_values('huc12')\nnative_huc12_basins['aggregation'] = native_aggregation.sel(time=timestamp)['PREC_ACC_NC'].data.todense()\nnative_plt = native_huc12_basins.hvplot(\n    c='aggregation', title=\"Accumulated Precipitation over HUC12 basins\",\n    coastline='50m', geo=True, cmap='viridis',\n    aspect='equal', legend=False, frame_width=300\n)\n\ncutout_plt * native_plt + cutout_plt","key":"DiWN5XEFCQ"},{"type":"outputs","id":"tHX3kxI4qNziRsN-BK-KB","children":[],"key":"xTILvxk02R"}],"key":"ZO9n0MuBZ3"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Compare the Results","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TdqYGYknqW"}],"identifier":"compare-the-results","label":"Compare the Results","html_id":"compare-the-results","implicit":true,"key":"ibA32JlPE1"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"With both aggregation methods complete, we are now ready to compare the results. We can do this both for the final output and the intermediate weights.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"iRaO12Pfbo"}],"key":"Ow5ep9eIT1"}],"key":"RcoPuWxQrL"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Weight Comparison","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JAMzfVqQqc"}],"identifier":"weight-comparison","label":"Weight Comparison","html_id":"weight-comparison","implicit":true,"key":"iqzm2qmhgA"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"To do the weight comparison, we first need to standardize the weight outputs. This is relatively simple as we just need to convert the ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"T5CpbjLmH1"},{"type":"inlineCode","value":"gdptools","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"oUPrqHZz7z"},{"type":"text","value":" ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"XAjQR5JYIw"},{"type":"inlineCode","value":"DataFrame","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"eUcwysRVIM"},{"type":"text","value":" weights into an ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"I6PJn5ivMM"},{"type":"inlineCode","value":"xarray.DataArray","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"COnRqnigZ0"},{"type":"text","value":". We can do this just like we did for the conservative method, but assigning the ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"c5jtet7FA9"},{"type":"inlineCode","value":"x","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"HjHbIBBqkq"},{"type":"text","value":" and ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"zVNptobQaB"},{"type":"inlineCode","value":"y","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"JmdkmjMULC"},{"type":"text","value":" values to the ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"HZt3vCIeEh"},{"type":"inlineCode","value":"gdptools","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Jgxps1hSrI"},{"type":"text","value":" data frame using the given indices.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"GGnwBkVyNA"}],"key":"CZa7k1asd5"}],"key":"NFxySkNFpG"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Due to the buffer region, gdptools weights i index values\n# are off by 3 and j are off by 1. This was found from a manual inspection\ndf_gdptools_weights['y'] = conus404['y'].isel(y=df_gdptools_weights['i']+3).data\ndf_gdptools_weights['x'] = conus404['x'].isel(x=df_gdptools_weights['j']+1).data\ngdptool_weights = xr.DataArray(\n    df_gdptools_weights.set_index(['y', 'x', 'huc12'])['wght']\n).unstack(sparse=True, fill_value=0)","key":"KZF6BzxmlE"},{"type":"outputs","id":"lPSyBf3e8z9xu1sbmo4oM","children":[],"key":"IyCT00ogjE"}],"key":"OEXhqY71K7"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, a simple max fractional difference is the simple check for how they compare.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"a4L2cIAH2H"}],"key":"iPu15fSNZ8"}],"key":"yz0Fo5AAuS"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"(np.abs(gdptool_weights - native_weights) / native_weights).max()","key":"fgRaacOJsw"},{"type":"outputs","id":"YFEFkKR7FNS3QboIlTLtF","children":[],"key":"AwwQ5fDq68"}],"key":"APMchHGe97"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Look at that. They are identical (up to machine precision). So, the only other thing to compare would be the time required for the computation.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"o1KB11fHFP"}],"key":"fo6mD7ymC9"}],"key":"GSsrunf2n1"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print(f'gdptools weights computation time: {gdptools_weights_time:0.3f} seconds')\nprint(f'native weights computation time: {native_weights_time:0.3f} seconds')\nprint(f'computation time difference: {(gdptools_weights_time - native_weights_time):0.3f} seconds')\nprint(f'computation time ratio: {(gdptools_weights_time / native_weights_time):0.3f}')","key":"n9ABWDJ9la"},{"type":"outputs","id":"iyg95uZPA7XlqnlMf0SPk","children":[],"key":"HjXSpTsyZc"}],"key":"CW5UIw5OcN"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So, from this comparison, we can see that both methods give the same weights, but the method using ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"t70NTYcddO"},{"type":"inlineCode","value":"xarray","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BxE3CDJsZr"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"trJcgZXnKb"},{"type":"inlineCode","value":"geopandas","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"op7eSpW0nm"},{"type":"text","value":" slightly faster (and likely not significantly). However, this does not test how well either of the two methods scale.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"alSOPCRyky"}],"key":"lEoIIDtehS"}],"key":"Anmypjl5gP"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Aggregation Comparison","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"X3Dm8guav9"}],"identifier":"aggregation-comparison","label":"Aggregation Comparison","html_id":"aggregation-comparison","implicit":true,"key":"kRPK7KWY1D"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"To do the aggregated data comparison, there is no need for any data formatting, as both ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"QfTJr34lpF"},{"type":"inlineCode","value":"gdptools","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"pEWIdfOF7P"},{"type":"text","value":" and the native method have matching ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"R67XAwFM0V"},{"type":"inlineCode","value":"xarray.Dataset","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"DAwirh2aJT"},{"type":"text","value":" formats. So, let’s start with the simple max fractional difference to compare.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Lli7Pfi7AA"}],"key":"DEIQzpNph3"}],"key":"MIDEMMWVxX"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"(np.abs(gdptools_aggregation - native_aggregation) / native_aggregation).max()","key":"zyjYchabwy"},{"type":"outputs","id":"rOsAY8E1_yCI3BTVpsAZE","children":[],"key":"dKkeVxOCyg"}],"key":"tjaXsYwcfA"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Well, as expected, they are nearly identical, since they had nearly identical weights.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SJotXroHaf"}],"key":"hRQCPjHV4B"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Let’s plot the fractional difference for a timestamp just to see how they compare.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"yzS8BmgJgH"}],"key":"J2FaiaKUe2"}],"key":"VCbAc6l6Z2"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# xarray holds the huc12s in sorted order\ndiff_huc12_basins = huc12_basins.copy().sort_values('huc12')\ndiff_huc12_basins['aggregation'] = (np.abs(gdptools_aggregation - native_aggregation) / native_aggregation).sel(time=timestamp)['PREC_ACC_NC']\n\ndiff_huc12_basins.hvplot(\n    c='aggregation', title=\"Difference in Precipitation over HUC12 basins\",\n    coastline='50m', geo=True, cmap='viridis',\n    aspect='equal', legend=False, frame_width=300\n)","key":"C7KHHmWf3y"},{"type":"outputs","id":"EZaUQ0OpDUhhenIMSeGJd","children":[],"key":"uKLfgODXbr"}],"key":"XYfRmbt9P0"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Finally, let’s compare the computational times.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RNjq9ai2xv"}],"key":"B2io61A09N"}],"key":"bbOKyTrPAu"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print(f'gdptools aggregation computation time: {gdptools_agg_time:0.3f} seconds')\nprint(f'native aggregation computation time: {native_agg_time:0.3f} seconds')\nprint(f'computation time difference: {(gdptools_agg_time - native_agg_time):0.3f} seconds')\nprint(f'computation time ratio: {(gdptools_agg_time / native_agg_time):0.3f}')","key":"Qwu69guIwQ"},{"type":"outputs","id":"gIgn-eDsqbVK38VBde8Tb","children":[],"key":"IkZosesAfW"}],"key":"b1hPDy718z"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"It looks like this step takes about the same time for both. So, let’s compare the total computation time (weights and aggregation).","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GVTfU4olCm"}],"key":"UnyqHPFPkd"}],"key":"vnVuN9qRUY"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"gdptools_total_time = gdptools_weights_time+gdptools_agg_time\nnative_total_time = native_weights_time + native_agg_time\nprint(f'gdptools total computation time: {gdptools_total_time:0.3f} seconds')\nprint(f'native total computation time: {native_total_time:0.3f} seconds')\nprint(f'total computation time difference: {(gdptools_total_time - native_total_time):0.3f} seconds')\nprint(f'total computation time ratio: {(gdptools_total_time / native_total_time):0.3f}')","key":"rUk2X4TBZI"},{"type":"outputs","id":"rYN75fLSHsi1RlevMOtqc","children":[],"key":"Veq4vbL1Ie"}],"key":"nwXGVjLLM8"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Alright, since both the aggregation and weights times are about equal, the overall performance of both is equal as well. Therefore, it appears that either method is a solid choice. The only other thing to test would be how well each method scales to larger feature polygons and larger grids. However, we will leave that comparison for another notebook.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"htXAWuBLvd"}],"key":"ZNSvnZsR9V"}],"key":"JsgOl6M3If"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Shut down the Dask Client","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FwKcmmYuYb"}],"identifier":"shut-down-the-dask-client","label":"Shut down the Dask Client","html_id":"shut-down-the-dask-client","implicit":true,"key":"GR2apJDxsv"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"If utilized, we should shut down the dask client.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"QB02DQUBgy"}],"key":"uOacud31NR"}],"key":"eWeYd1xKgD"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"client.close()","key":"sZf7zmntFy"},{"type":"outputs","id":"k2yIy7ZLwyEca4hgHse4w","children":[],"key":"ZcqBcq7w50"}],"key":"hGiKKfjBXH"}],"key":"h4yv49kSaW"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation","group":"Dataset Processing"},"next":{"title":"CONUS404 Site Data Selection","url":"/dataset-processing/tutorials/conus404-point-selection","group":"Dataset Processing"}}},"domain":"http://localhost:3005"}