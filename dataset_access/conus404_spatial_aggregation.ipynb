{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e332933a-960c-4dfc-92a7-6f39ad95f13c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CONUS404 Spatial Aggregation \n",
    "Using Xarray, GeoPandas and Sparse\n",
    "\n",
    "**Goal:** Spatially aggregate a model data variable _conservatively_, i.e. by exactly partitioning each grid cell into the precise region boundaries.\n",
    "\n",
    "**Approach:** \n",
    "- Represent both the original model grid and target grid as GeoPandas GeoSeries objects (with vector geometries)\n",
    "- Compute their area overlay and turn it into a sparse matrix\n",
    "- Perform matrix multiplication on the full Xarray dataset (with a time dimension)\n",
    "\n",
    "It is quite fast and transparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c5e445-cb29-4c40-89ff-f97e72193d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xmode minimal\n",
    "import os\n",
    "import xarray as xr\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sparse\n",
    "\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import dask\n",
    "import cf_xarray\n",
    "\n",
    "from pynhd import NLDI, WaterData\n",
    "import intake\n",
    "import cartopy.crs as ccrs\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb8bf1",
   "metadata": {},
   "source": [
    "## Open dataset from Intake Catalog\n",
    "* Select `on-prem` dataset from /caldera if running on prem (Denali/Tallgrass)\n",
    "* Select `cloud`/`osn` object store data if running elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e0a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_PROFILE'] = 'default'\n",
    "%run ../environment_set_up/Help_AWS_Credentials.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e540466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the hytest data intake catalog\n",
    "hytest_cat = intake.open_catalog(\"https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml\")\n",
    "list(hytest_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c04b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the conus404 sub-catalog\n",
    "cat = hytest_cat['conus404-catalog']\n",
    "list(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff32473",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select the dataset you want to read into your notebook and preview its metadata\n",
    "dataset = 'conus404-daily-osn' \n",
    "cat[dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2474ae8-f221-4f4a-a31a-d001ee2e1cf7",
   "metadata": {},
   "source": [
    "## 2) Set Up AWS Credentials (Optional)\n",
    "\n",
    "This notebook reads data from the OSN pod by default, which is object store data on a high speed internet connection that is free to access from any environment. If you change this notebook to use one of the CONUS404 datasets stored on S3 (options ending in `-cloud`), you will be pulling data from a `requester-pays` S3 bucket. This means you have to set up your AWS credentials, else we won't be able to load the data. Please note that reading the `-cloud` data from S3 may incur charges if you are reading data outside of the us-west-2 region or running the notebook outside of the cloud altogether. If you would like to access one of the `-cloud` options, uncomment and run the following code snippet to set up your AWS credentials. You can find more info about this AWS helper function [here](../environment_set_up/Help_AWS_Credentials.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f990e72-1609-4751-b264-13eb948b972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the lines below to read in your AWS credentials if you want to access data from a requester-pays bucket (-cloud)\n",
    "# os.environ['AWS_PROFILE'] = 'default'\n",
    "# %run ../environment_set_up/Help_AWS_Credentials.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65a7c98",
   "metadata": {},
   "source": [
    "## Parallelize with Dask \n",
    "Some of the steps we will take are aware of parallel clustered compute environments\n",
    "using `dask`. We're going to start a cluster now so that future steps can take advantage\n",
    "of this ability. \n",
    "\n",
    "This is an optional step, but speed ups data loading significantly, especially \n",
    "when accessing data from the cloud.\n",
    "\n",
    "We have documentation on how to start a Dask Cluster in different computing environments [here](../environment_set_up/clusters.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023931a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../environment_set_up/Start_Dask_Cluster_Nebari.ipynb\n",
    "## If this notebook is not being run on Nebari/ESIP, replace the above \n",
    "## path name with a helper appropriate to your compute environment.  Examples:\n",
    "# %run ../environment_set_up/Start_Dask_Cluster_Denali.ipynb\n",
    "# %run ../environment_set_up/Start_Dask_Cluster_Tallgrass.ipynb\n",
    "# %run ../environment_set_up/Start_Dask_Cluster_Desktop.ipynb\n",
    "# %run ../environment_set_up/Start_Dask_Cluster_PangeoCHS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc96de-d6c2-49d6-bd7c-b6bf76449869",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = cat['conus404-daily-osn'].to_dask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a26f08f-cf8e-4f11-b44d-ed80fda4150f",
   "metadata": {},
   "source": [
    "## Load the feature polygons (e.g. here catchment basins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47aaa91-de9d-4653-9697-d6c6adbfb288",
   "metadata": {},
   "source": [
    "Load with geopandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e89c3b9-4476-418f-be92-3718d3607c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# USGS gage 01482100 Delaware River at Del Mem Bridge at Wilmington De\n",
    "gage_id = '01482100'\n",
    "nldi = NLDI()\n",
    "del_basins = nldi.get_basins(gage_id)\n",
    "huc12_basins = WaterData('wbd12').bygeom(del_basins.geometry[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051cb85c-370a-4f5b-b09d-3dad7131c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_df = huc12_basins\n",
    "region_name = 'name'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ded419-504e-4a95-96f1-f6f14a10d943",
   "metadata": {},
   "source": [
    "Check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33a2263-1f65-4d74-8a3d-9ed31870bb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_df.plot(column=region_name, figsize=(10,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf425d5-adea-4315-8dfc-0b1ac1e12ae7",
   "metadata": {},
   "source": [
    "All geodataframes should have a coordinate reference system. This is important (and sometimes unfamiliar to users coming from the global climate world).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd5dd8-64cd-48c4-aa5c-ff060f46b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_orig = f'EPSG:{regions_df.crs.to_epsg()}'\n",
    "crs_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dcdd5d-4362-4dcb-8a30-0a432d976e7a",
   "metadata": {},
   "source": [
    "## Aggregate CONUS404 to feature polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780e1b80-1500-4e76-b892-1704af205354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = 'x'  # projected x coordinate name\n",
    "y = 'y'  # projected y coordinate name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d2755-abfc-4b96-96ae-afc1eb53e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4b8375-5c26-40df-b7eb-4af9ba6b7311",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_info = ds.crs\n",
    "xx = ds.x.values\n",
    "yy = ds.y.values\n",
    "globe = ccrs.Globe(ellipse='sphere', semimajor_axis=6370000, semiminor_axis=6370000)\n",
    "lcc = ccrs.LambertConformal(globe=globe,\n",
    "                            central_longitude=crs_info.longitude_of_central_meridian, \n",
    "                            central_latitude=crs_info.latitude_of_projection_origin,\n",
    "                            standard_parallels=crs_info.standard_parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b60ea3-060f-4e04-a4ee-02ee4f8dd5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcc_wkt = lcc.to_wkt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f83dbfa-1a0a-48ef-b602-1cf40039141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_df = regions_df.to_crs(lcc_wkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b5522-51e3-44cb-ac80-bdc29b5e9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = tuple(regions_df.total_bounds)\n",
    "bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167e81c0-d5db-4ac5-9b95-7a1e1169cc26",
   "metadata": {},
   "source": [
    "Subset gridded model results to bounds of spatial dataframe to save on memory and computation. More useful when the regions_df is much smaller than the footprint of the gridded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60103ca9-6373-4e5a-91b7-f4a4c3983c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.sel(x=slice(bbox[0],bbox[2]), y=slice(bbox[1],bbox[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364d52da-f10c-4305-9ef1-9248db3175a5",
   "metadata": {},
   "source": [
    "Now we extract just the horizontal grid information.\n",
    "The dataset has information about the lat and lon bounds of each cell, which we need to create the CONUS404 grid cell polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5668a117-7236-42a5-aabc-fb16aeb85fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'T2'  # 2m Temp\n",
    "var = 'PREC_ACC_NC' # precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518f137-6742-41a2-b1c4-3e19648b6f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = ds[[var]].drop(['time', 'lon', 'lat', var]).reset_coords().load()\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d05c230-409c-4384-b8d3-a0c55266676d",
   "metadata": {},
   "source": [
    "Now we \"stack\" the data into a single 1D array. This is the first step towards transitioning to pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da6044f-7236-4a43-8542-06251a8cb260",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = grid.cf.add_bounds([x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02bfde-c749-40e1-b6f8-17db360e3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = grid.stack(point=(y,x))\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b99d30-0a51-4a7d-9ab8-5b8a56c8bc3a",
   "metadata": {},
   "source": [
    "This function creates geometries for a single pair of bounds.\n",
    "It is not fast, but it is fast enough here.\n",
    "Perhaps could be vectorized using pygeos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a472e-2143-4417-ba27-6fda01392f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounds_to_poly(x_bounds, y_bounds):\n",
    "    return Polygon([\n",
    "        (x_bounds[0], y_bounds[0]),\n",
    "        (x_bounds[0], y_bounds[1]),\n",
    "        (x_bounds[1], y_bounds[1]),\n",
    "        (x_bounds[1], y_bounds[0])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f136cd-8fb8-4a47-8b1a-a11c3f0082cf",
   "metadata": {},
   "source": [
    "We apply this function to each grid cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9716bc0-6c5b-44db-8ccd-d555e9f1d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "boxes = xr.apply_ufunc(\n",
    "    bounds_to_poly,\n",
    "    points.x_bounds,\n",
    "    points.y_bounds,\n",
    "    input_core_dims=[(\"bounds\",),  (\"bounds\",)],\n",
    "    output_dtypes=[np.dtype('O')],\n",
    "    vectorize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de8dacc-8b87-4ac2-9a7e-6d565f08fea7",
   "metadata": {},
   "source": [
    "Finally, we convert to a GeoDataframe, specifying the projected CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f7c5e-31c5-4a06-91e4-7437c7686d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df= gp.GeoDataFrame(\n",
    "    data={\"geometry\": boxes.values, \"y\": boxes[y], \"x\": boxes[x]},\n",
    "    index=boxes.indexes[\"point\"],\n",
    "    crs=lcc_wkt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dedb7e-66a2-43ef-9589-707dc5fb1801",
   "metadata": {},
   "source": [
    "We will now transform to an area preserving projection. This is imporant because we want to do area-weighted regridding. Here we use the [NSIDC EASE-Grid 2.0](https://nsidc.org/data/user-resources/help-center/guide-ease-grids) grid for the Northern Hemisphere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e5419-32e7-4103-863e-04af7a402bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_area = \"EPSG:6931\"\n",
    "\n",
    "regions_df = regions_df.to_crs(crs_area)\n",
    "grid_df = grid_df.to_crs(crs_area)\n",
    "\n",
    "grid_df.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a46779-1b8e-4ca0-a17f-7e51831d562a",
   "metadata": {},
   "source": [
    "### Key Step: Overlay the two geometries\n",
    "\n",
    "This is the magic of geopandas; it can calculate the overlap between the original grid and the regions.\n",
    "It is expensive because it has to compare 64800 grid boxes with 242 regions. \n",
    "\n",
    "In this dataframe, the `latitude` and `longitude` values are from the grid, while all the other columns are from the regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dc00dc-63d7-4561-a113-a04732f8554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = grid_df.overlay(regions_df, keep_geom_type=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5624f-7c5b-41e6-875c-72c188aef887",
   "metadata": {},
   "source": [
    "This is essentially already a sparse matrix mapping one grid space to the other. How sparse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e3f3b0-1e72-4392-980b-5af7f2a4c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = len(overlay) / (len(grid_df) * len(regions_df))\n",
    "sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419302f4-45c3-449e-a42f-cea3e414fd4b",
   "metadata": {},
   "source": [
    "Let's explore these overlays a little bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e11210f-81ee-4b46-a139-940000894fde",
   "metadata": {},
   "source": [
    "We can verify that each basin's area is preserved in the overlay operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a823e44-46e6-4d25-8f52-c7b7f2ec01a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay.geometry.area.groupby(overlay[region_name]).sum().nlargest(10)/1e6  # km2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b15ff6e-d27a-41fe-9edc-d42f6d410b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_df.geometry.area.groupby(regions_df[region_name]).sum().nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ee3e3-2f06-4fb0-807a-01f3ddc459ff",
   "metadata": {},
   "source": [
    "### Calculate the area fraction for each region\n",
    "\n",
    "This is another key step. This transform tells us _how much of a country's total area comes from each of the grid cells._\n",
    "This is accurate because we used an area-preserving CRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288707a4-68a2-4b64-939c-4ba8faad87de",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cell_fraction = overlay.geometry.area.groupby(overlay[region_name]).transform(lambda x: x / x.sum())\n",
    "grid_cell_fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11503a-73e7-4969-8dc8-f21426da7fc1",
   "metadata": {},
   "source": [
    "We can verify that these all sum up to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e26cca-183b-4208-a698-135994250aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cell_fraction.groupby(overlay[region_name]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c761e17-d9b6-4374-ae77-c53c690701a0",
   "metadata": {},
   "source": [
    "### Turn this into a sparse Xarray DataArray\n",
    "\n",
    "The first step is making a MultiIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ad60b-db35-4b56-95d8-d385386e9817",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_index = overlay.set_index([y, x, region_name]).index\n",
    "df_weights = pd.DataFrame({\"weights\": grid_cell_fraction.values}, index=multi_index)\n",
    "df_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a005718-1ac8-48ff-b109-f13b5dd92f40",
   "metadata": {},
   "source": [
    "We can bring this directly into xarray as a 1D Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c2720d-b49a-4f6e-b181-14dfe1503fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_weights = xr.Dataset(df_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b834cd0-3844-4556-b548-9824d21c06af",
   "metadata": {},
   "source": [
    "Now we unstack it into a sparse array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a0714d-94cd-40d6-8111-92571b98fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_sparse = ds_weights.unstack(sparse=True, fill_value=0.).weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385348ce-7e05-414c-a2a5-fcd00efdc844",
   "metadata": {},
   "source": [
    "Here we can clearly see that this is a sparse matrix, mapping the input space (lat, lon) to the output space (SOVEREIGNT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94d0f1-a29b-4fcb-b668-03702dfcdb3b",
   "metadata": {},
   "source": [
    "### Perform Matrix Multiplication\n",
    "\n",
    "To regrid the data, we just have to multiply the original precip dataset by this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed039339-a8fd-4a30-a64e-15e03489e1f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#regridded = xr.dot(ds[var], weights_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b3a7c0-7a5b-401c-b176-3b82a7c8c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regridded = sparse.einsum('ij,jk->ik', ds[var].data, weights_sparse.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6e3db9-63c1-4993-95fd-751b40603579",
   "metadata": {
    "tags": []
   },
   "source": [
    "Unfortunately, that doesn't work out of the box, because sparse doesn't implement einsum (see https://github.com/pydata/sparse/issues/31)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e070818-6815-45f2-be58-a671645449f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regridded[0].compute()  # fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b7675-edef-496b-a028-3b88fba0159e",
   "metadata": {},
   "source": [
    "Sparse does implement matmul, so we can use that. But we have to do some reshaping to make it work with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38302721-76e4-4ba1-9fef-0b16dba330ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_weights_matmul_sparse(weights, data):\n",
    "\n",
    "    assert isinstance(weights, sparse.SparseArray)\n",
    "    assert isinstance(data, np.ndarray)\n",
    "    data = sparse.COO.from_numpy(data)\n",
    "    data_shape = data.shape\n",
    "    # k = nlat * nlon\n",
    "    n, k = data_shape[0], data_shape[1] * data_shape[2]\n",
    "    data = data.reshape((n, k))\n",
    "    weights_shape = weights.shape\n",
    "    k_, m = weights_shape[0] * weights_shape[1], weights_shape[2]\n",
    "    assert k == k_\n",
    "    weights_data = weights.reshape((k, m))\n",
    "\n",
    "    regridded = sparse.matmul(data, weights_data)\n",
    "    assert regridded.shape == (n, m)\n",
    "    return regridded.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5080e0-68e6-422b-8a2e-2d9c7be14d55",
   "metadata": {},
   "source": [
    "### Variables at the same location on the grid can use the same weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ef0d5-1dd0-4a9d-bc53-b105695333f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#var = 'T2'  # 2m Temp, grid cell centers\n",
    "#var = 'PREC_ACC_NC' # precip, grid cell centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb5dda3-da7a-490d-b595-1a21fe7350b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "    var_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        weights_sparse,\n",
    "        ds[var],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[y, x, region_name], [y, x]],\n",
    "        output_core_dims=[[region_name]],\n",
    "        dask=\"parallelized\",\n",
    "        dask_gufunc_kwargs=dict(meta=[np.ndarray((0,))])\n",
    "    )\n",
    "    \n",
    "\n",
    "var_regridded.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade5c8b5-bf48-4465-9d17-a5c86bc082fa",
   "metadata": {},
   "source": [
    "### Explore timeseries data by region:\n",
    "Plot monthly-average time series for two selected HUC12s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d2bcd-c7ae-4f9b-a731-2be08e08c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_var = var_regridded.sel(name=['Mallory Brook-West Branch Delaware River', 'Yaleville Brook-Susquehanna River']).resample(time=\"MS\").mean().to_dataset(region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad289493-e3d0-4394-b500-83ef274f42a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_var.hvplot(x='time', grid=True, frame_width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ede5400-e486-47a6-a658-185f06cbb0af",
   "metadata": {},
   "source": [
    "### Explore the mean var by region\n",
    "Calculate the average value over all time steps for every HU12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67abe4d0-002b-4d0a-aeef-1d2f30aff384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = var_regridded.to_pandas().mean()\n",
    "df_mean.name = var\n",
    "df_mean = pd.DataFrame(df_mean).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d861c5c-6e01-41f1-a7ea-3dbac5e80140",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged = pd.merge(regions_df, df_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9956f1-d47b-499b-a9ba-91eb32d539fb",
   "metadata": {},
   "source": [
    "Convert back to geographic coordinates for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda08da-db11-4516-babb-d6453be4ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_geo = 'EPSG:4326'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3534f45c-b302-4045-bfb7-ad730a96dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_geo = merged.to_crs(crs_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aea4d4-9ecd-40c3-9bcd-867fe650d02e",
   "metadata": {},
   "source": [
    "Holoviz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3203715-13b8-4432-93b0-3a26627afbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_geo.hvplot(c=var, geo=True, cmap='viridis_r', frame_width=600, tiles='EsriTerrain', \n",
    "               title='CONUS404', alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c0709-4a9e-4f85-820d-9d6b38fe5274",
   "metadata": {},
   "source": [
    "### Be a good citizen and shut down the cluster\n",
    "... Even though it would shut down in 15 minutes with no activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b111b1d-b478-4a75-a924-8b5e9007d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close(); cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
