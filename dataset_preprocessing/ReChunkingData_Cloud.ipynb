{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Chunking Data\n",
    "\n",
    "This notebook extends ideas covered in the [basic workflow](./ReChunkingData.ipynb).  This \n",
    "notebook will perfrom the same operations, but will work on the **much** larger dataset, and \n",
    "involve some parallelization using the dask scheduler. \n",
    "\n",
    ":::{Warning}\n",
    "\n",
    "You should run this **only** on a cloud compute node -- on ESIP Nebari, for example. We \n",
    "will be reading and writing **enormous** amounts of data to S3 buckets. To do that over a \n",
    "typical network connection will saturate your bandwidth and take days to complete.\n",
    "\n",
    ":::\n",
    "\n",
    "## System Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, force=True)\n",
    "\n",
    "## Let's see how big your compute environment is:\n",
    "import os\n",
    "print(f\"CPUS: {os.cpu_count()}\")\n",
    "import psutil\n",
    "svmem = psutil.virtual_memory()\n",
    "print(f\"Total Virtual Memory: {svmem.total/(1024*1024*1024):.2f} Gb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plumb Data Source\n",
    "We're going to look at a particular dataset from the National Water Model Reanalysis Version 2.1. \n",
    "The dataset is part of the AWS Open Data Program.  Let's look at what's available by just listing\n",
    "the S3 bucket holding the NWM data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "fs = fsspec.filesystem('s3', anon=True)\n",
    "fs.ls('s3://noaa-nwm-retrospective-2-1-zarr-pds/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the zarr data\n",
    "The dataset we'll operate on is the `chrtout` dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chrtout\n",
    "import xarray as xr\n",
    "fileHandle = fs.get_mapper('noaa-nwm-retrospective-2-1-zarr-pds/chrtout.zarr')\n",
    "ds = xr.open_zarr(fileHandle, consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spin up Dask Cluster\n",
    "Our rechunking operation will be able to work in parallel. To do that, we will\n",
    "spin up a `dask` cluster on the cloud hardware to schedule the various workers.\n",
    "Note that this cluster must be configured with a specific user **profile** with \n",
    "permissions to write to our eventual output location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set AWS Credentials\n",
    "import configparser\n",
    "awsconfig = configparser.ConfigParser()\n",
    "awsconfig.read(\n",
    "    os.path.expanduser('~/.aws/credentials') # default location... if yours is elsewhere, change this.\n",
    ")\n",
    "_profile_nm  = 'osn-renci'\n",
    "_endpoint = 'https://renc.osn.xsede.org'\n",
    "# Set environment vars based on parsed awsconfig\n",
    "#os.environ['AWS_PROFILE'] = _profile_nm\n",
    "os.environ['AWS_ACCESS_KEY_ID']     = awsconfig[_profile_nm]['aws_access_key_id']    \n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = awsconfig[_profile_nm]['aws_secret_access_key']    \n",
    "## Your profile may require that you specify an endpoint by which  you access S3 object storage\n",
    "os.environ['AWS_S3_ENDPOINT'] = _endpoint\n",
    "try: \n",
    "    del os.environ['AWS_PROFILE']\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "\n",
    "# NOTE: This cluster configuration is VERY specific to the JupyterHub cloud environment on ESIP/QHUB\n",
    "from dask_gateway import Gateway\n",
    "gateway = Gateway()\n",
    "options = gateway.cluster_options()\n",
    "options.conda_environment='users/users-pangeo'  \n",
    "##                         ^^^^^^ \n",
    "## This conda environment is correct for nebari.esipfed.org\n",
    "## You may need to specify a different conda environment if you are running elsewhere. \n",
    "options.profile = 'Medium Worker'\n",
    "options.environment_vars = dict(\n",
    "    DASK_DISTRIBUTED__SCHEDULER__WORKER_SATURATION=\"1.0\"\n",
    ")\n",
    "# pass environment vars to workers\n",
    "# this includes AWS environment vars needed to access requester-pays and private buckets\n",
    "options.environment_vars.update(dict(os.environ))\n",
    "cluster = gateway.new_cluster(options)\n",
    "cluster.adapt(minimum=10, maximum=30)\n",
    "\n",
    "# get the client for the cluster\n",
    "client = cluster.get_client()\n",
    "client.dashboard_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "    smplData = ds.where(ds.gage_id != ''.rjust(15).encode(), drop=True) # subset to only those features with a valid gage_id\n",
    "    smplData.drop('crs') # Not needed/wanted for this analysis\n",
    "smplData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Chunk Plan\n",
    "We will configure a new chunking plan which will favor time-series analysis. \n",
    "Using the dimensions of the data: \n",
    "* 367439 time steps\n",
    "* 7994 feature IDs\n",
    "\n",
    "We can write the new plan as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The new chunking plan:\n",
    "chunk_plan = {\n",
    "    'streamflow': {'time': 367439, 'feature_id': 1}, # all time records in one chunk for each feature_id\n",
    "    'velocity': {'time': 367439, 'feature_id': 1},\n",
    "    'elevation': (7994,),\n",
    "    'gage_id': (7994,),\n",
    "    'latitude': (7994,),\n",
    "    'longitude': (7994,),    \n",
    "    'order': (7994,),    \n",
    "    'time': (367439,), # all time coordinates in one chunk\n",
    "    'feature_id': (7994,) # all feature_id coordinates in one chunk\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually reset the chunking metadata in prep for re-chunking\n",
    "smplData = smplData.chunk(chunks={'feature_id':1, 'time': 367439})\n",
    "for x in smplData.variables:\n",
    "    smplData[x].encoding['chunks'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up output location\n",
    "\n",
    "With this plan, we can ask `rechunker` to re-write the data using the prescribed chunking pattern.\n",
    "\n",
    "Unlike with the smaller dataset, we need to write this very large dataset to an object store in the datacenter: an S3 'bucket'.  So we need to set that up so that `rechunker` will have a suitable place to write data. This new data will be a complete copy of the original, just re-organized a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with anon=False, we force the use of the environment variable 'AWS_PROFILE', set above.\n",
    "from getpass import getuser\n",
    "uname=getuser()\n",
    "\n",
    "fsw = fsspec.filesystem('s3', anon=False, default_fill_cache=False, skip_instance_cache=True, \n",
    "                                 client_kwargs={'endpoint_url': _endpoint}\n",
    ")\n",
    "workspace = 's3://rsignellbucket2/'\n",
    "testDir = workspace + \"testing/\"\n",
    "myDir = testDir + f'{uname}_ReChunkTutorial/'\n",
    "fsw.ls(testDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsw.mkdir(myDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['rechunked.zarr', 'staging.zarr']:\n",
    "    if fsw.exists(myDir + f):\n",
    "        fsw.rm(myDir + f, recursive=True)\n",
    "staging = fsw.get_mapper(myDir + 'staging.zarr')\n",
    "outfile = fsw.get_mapper(myDir + 'rechunked.zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to rechunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rechunker\n",
    "## Recall that merely invoking rechunker does not do any work... just sorts out \n",
    "## the rechunking plan and writes metadata.\n",
    "result = rechunker.rechunk(\n",
    "    smplData,\n",
    "    chunk_plan,\n",
    "    \"2GB\",\n",
    "    outfile, \n",
    "    temp_store=staging \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import progress, performance_report\n",
    "\n",
    "with performance_report(filename=\"dask-report.html\"):\n",
    "    r = result.execute(retries=10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "_ = zarr.consolidate_metadata(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Let's read in the resulting re-chunked dataset to see how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reChunkedData = xr.open_zarr(outfile)\n",
    "reChunkedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before:\n",
    "sampleData['streamflow'].sel(feature_id=1343034)\n",
    "# Note: three chunks needed to service a single feature_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After:\n",
    "reChunkedData['streamflow'].sel(feature_id=1343034) \n",
    "# All data for the specified feature_id is in a single chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-users-pangeo",
   "language": "python",
   "name": "conda-env-users-users-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "4100cc85ffefb381c538d28dd18cb927e5a99f05bbed6aaad5313d7bb1c2079e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
