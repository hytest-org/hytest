{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e332933a-960c-4dfc-92a7-6f39ad95f13c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CONUS404 Spatial Aggregation Comparison\n",
    "\n",
    "In this notebook, we will be comparing two spatial aggregation methods to aggregate from grids to polygons. One uses [`gdptools`](https://gdptools.readthedocs.io/en/latest/index.html). The other uses conservative regional methods with [`xarray`](https://docs.xarray.dev/en/stable/index.html) and [`geopandas`](https://geopandas.org/en/stable/index.html) natively (see this [Pangeo Discourse](https://discourse.pangeo.io/t/conservative-region-aggregation-with-xarray-geopandas-and-sparse/2715) for details).\n",
    "\n",
    "The goal of this comparision is to see how the results of the two methods compare to help judge the efficacy of one versus the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c5e445-cb29-4c40-89ff-f97e72193d17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%xmode minimal\n",
    "import os\n",
    "import time\n",
    "import xarray as xr\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sparse\n",
    "\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import dask\n",
    "import cf_xarray\n",
    "\n",
    "from pynhd import NLDI, WaterData\n",
    "import intake\n",
    "import cartopy.crs as ccrs\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import pyproj\n",
    "from gdptools import WeightGen, AggGen, UserCatData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb8bf1",
   "metadata": {},
   "source": [
    "## Open dataset from Intake Catalog\n",
    "\n",
    "First, let's begin by loading the CONUS404 daily data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e8c7e-de3a-4642-ab6d-16004fedc4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hytest_cat = intake.open_catalog(\"https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml\")\n",
    "catalog = hytest_cat['conus404-catalog']\n",
    "list(catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c988d1-12d0-491d-9cc5-326f3b0f1388",
   "metadata": {},
   "source": [
    "As we can see there are three different locations for the `conus404-daily` data set. The locations are (1) `-onprem-hw` meaning it is stored on the USGS Hovenweep HPC, (2) `-cloud` meaning it is store in an S3 bucket, or (3) `-osn` meaning the data is on the USGS open storage network (OSN). As the OSN is free to access from any environment, we will use that for this example, but the location can easily be changed depending on your needs.\n",
    "\n",
    "> If you change this notebook to use the CONUS404 dataset stored on S3 (options ending in `-cloud`), you will be pulling data from a `requester-pays` S3 bucket. This means you have to set up your AWS credentials, else we won't be able to load the data. Please note that reading the `-cloud` data from S3 may incur charges if you are reading data outside of the us-west-2 region or running the notebook outside of the cloud altogether. If you would like to access one of the `-cloud` options, uncomment and run the following code cell to set up your AWS credentials. You can find more info about this AWS helper function [here](../environment_set_up/Help_AWS_Credentials.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f990e72-1609-4751-b264-13eb948b972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the lines below to read in your AWS credentials if you want to access data from a requester-pays bucket (-cloud)\n",
    "# os.environ['AWS_PROFILE'] = 'default'\n",
    "# %run ../environment_set_up/Help_AWS_Credentials.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de0545b-468b-450f-aa0c-e3448512b161",
   "metadata": {},
   "source": [
    "Finally, read in the daily CONUS404 data set and select the accumulated grid scale precipitation. We select the precipitation rather then all variables to keep things simple for this example, but aggregation of other variables would follow the same methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc96de-d6c2-49d6-bd7c-b6bf76449869",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'conus404-daily-osn'\n",
    "conus404 = catalog[dataset].to_dask()\n",
    "# Include the crs as we will need it later\n",
    "conus404 = conus404[['PREC_ACC_NC', 'crs']]\n",
    "conus404"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65a7c98",
   "metadata": {},
   "source": [
    "## Parallelize with Dask (optional)\n",
    "\n",
    "Some of the steps we will take are aware of parallel clustered compute environments using [`dask`](https://www.dask.org/). We can start a cluster now so that future steps take advantage\n",
    "of this ability. This is an optional step, but speed ups data loading significantly, especially when accessing data from the cloud.\n",
    "\n",
    "We have documentation on how to start a Dask Cluster in different computing environments [here](../environment_set_up/clusters.md). Uncomment the cluster start up that works for your compute environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023931a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %run ../environment_set_up/Start_Dask_Cluster_Nebari.ipynb\n",
    "## If this notebook is not being run on Nebari/ESIP, replace the above \n",
    "## path name with a helper appropriate to your compute environment.  Examples:\n",
    "# %run ../environment_set_up/Start_Dask_Cluster_Denali.ipynb\n",
    "# %run ../environment_set_up/Start_Dask_Cluster_Tallgrass.ipynb\n",
    "# %run ../environment_set_up/Start_Dask_Cluster_Desktop.ipynb\n",
    "# %run ../environment_set_up/Start_Dask_Cluster_PangeoCHS.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afbe9fb-b207-4464-adf9-caf57d16796a",
   "metadata": {},
   "source": [
    "## Load the feature polygons\n",
    "\n",
    "Now that we have read in the CONUS404 data, we need to read in some polygons to aggregate the data. For this example, we will use the HUC12 basins within the Delaware River Basin. To get these HUC12 polygons, we can use [PyNHD](https://docs.hyriver.io/readme/pynhd.html) to query the Hydro Network Linked Data Index (NLDI). All we need to get the basins is a downstream river gage ID. Gage ID 01482100 is what we need as it is at the mouth of the Delaware River leading out to the Delaware Bay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e89c3b9-4476-418f-be92-3718d3607c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# USGS gage 01482100 Delaware River at Del Mem Bridge at Wilmington De\n",
    "gage_id = '01482100'\n",
    "delaware_basin = NLDI().get_basins(gage_id)\n",
    "# Split the Basin into its Watershed Boundary Dataset (WBD) HUC12 components\n",
    "huc12_basins = WaterData('wbd12').bygeom(delaware_basin.geometry[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ded419-504e-4a95-96f1-f6f14a10d943",
   "metadata": {},
   "source": [
    "Let's plot the HUC12 basins to see how they look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33a2263-1f65-4d74-8a3d-9ed31870bb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "huc12_basins.hvplot(\n",
    "    c='huc12', title=\"Delaware River HUC12 basins\",\n",
    "    coastline='50m', geo=True,\n",
    "    aspect='equal', legend=False, frame_width=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf425d5-adea-4315-8dfc-0b1ac1e12ae7",
   "metadata": {},
   "source": [
    "An important thing to note is that all geodataframes should have a coordinate reference system (CRS). Let's check to make sure our geodataframe has a CRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205f801-1cae-4e9b-b002-e10fbbbe3564",
   "metadata": {},
   "outputs": [],
   "source": [
    "huc12_basins.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af81b1bc-0f85-4932-acbc-4366f63aa036",
   "metadata": {},
   "source": [
    "## Limit CONUS404 Spatial Range\n",
    "\n",
    "With the HUC12 basins read in, we only need the CONUS404 data that spans these polygons as they are the regions we will be aggregating. So, let's limit the CONUS404 spatial range to that of the basins. This will save on memory and computation. Note doing this is mainly useful when the regions footprint is much smaller than the footprint of the gridded model.\n",
    "\n",
    "To limit the spatial range, we first need to convert the CRS of the basins to that of CONUS404. Then extract the bounding box of the basins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f83dbfa-1a0a-48ef-b602-1cf40039141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "huc12_basins_conus404_crs = huc12_basins.to_crs(conus404.crs.crs_wkt)\n",
    "bbox = huc12_basins_conus404_crs.total_bounds\n",
    "bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167e81c0-d5db-4ac5-9b95-7a1e1169cc26",
   "metadata": {},
   "source": [
    "Then select the CONUS404 data within the bounding box. However, when we do this, we will extend the bounds out by 5% of their range to ensure all of our basins are within the spatially limited data. We do this as the reprojections of the CRS can cause slight distortions that make polygons on the bounds not fall fully within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60103ca9-6373-4e5a-91b7-f4a4c3983c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_x_range = bbox[2] - bbox[0]\n",
    "bbox_y_range = bbox[3] - bbox[1]\n",
    "x_range = slice(bbox[0] - bbox_x_range * 0.05,\n",
    "                bbox[2] + bbox_x_range * 0.05)\n",
    "y_range = slice(bbox[1] - bbox_y_range * 0.05,\n",
    "                bbox[3] + bbox_y_range * 0.05)\n",
    "\n",
    "conus404 = conus404.sel(x=x_range, y=y_range)\n",
    "conus404"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12be7bf-5f35-4788-8c90-5b401f5c7d08",
   "metadata": {},
   "source": [
    "To make sure this worked as intended, let's plot the full basin over the extracted CONUS404 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb50ec3-34f6-485c-9e11-81b37a65a52d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select a single timestamp for simple plotting\n",
    "timestamp = '2000-5-02'\n",
    "cutout = conus404.sel(time=timestamp).drop_vars(['lat', 'lon'])\n",
    "# We need to write the CRS to the CONUS404 dataset and\n",
    "# reproject for clean plotting with hvplot\n",
    "cutout = cutout.rio.write_crs(conus404.crs.crs_wkt).rio.reproject('EPSG:4326')\n",
    "\n",
    "cutout_plt = cutout.hvplot(\n",
    "    coastline='50m', geo=True,\n",
    "    aspect='equal', cmap='viridis', frame_width=300\n",
    ")\n",
    "huc12_plt = huc12_basins.hvplot(\n",
    "    geo=True, alpha=0.3, c='r'\n",
    ")\n",
    "cutout_plt * huc12_plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfef620-4bf5-4c12-86bf-aaaa4211b1a1",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dcdd5d-4362-4dcb-8a30-0a432d976e7a",
   "metadata": {},
   "source": [
    "## Aggregate CONUS404 to Feature Polygons\n",
    "\n",
    "Now that we have our gridded data and polygons, it is time to aggregate them using `gdptools` and the `geopandas` and `xarray` native method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359fcbd0-a7c9-4ed9-97d3-687ad7a7152d",
   "metadata": {},
   "source": [
    "### gdptools Aggregation\n",
    "\n",
    "Let's start by using the `gdptools` aggregation method.\n",
    "\n",
    "The first step to aggregating with `gdptools` is to convert the input data to a `UserCatData` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f46783a-26ad-4287-b29f-b07f06101e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = UserCatData(\n",
    "    ds=conus404,\n",
    "    proj_ds=conus404.crs.crs_wkt,\n",
    "    x_coord='x',\n",
    "    y_coord='y',\n",
    "    t_coord='time',\n",
    "    var='PREC_ACC_NC',\n",
    "    f_feature=huc12_basins,\n",
    "    proj_feature=huc12_basins.crs,\n",
    "    id_feature='huc12',\n",
    "    period=[pd.Timestamp(conus404.time.values.min()),\n",
    "            pd.Timestamp(conus404.time.values.max())],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f10431-e5af-47f4-91df-fe90f2360f88",
   "metadata": {},
   "source": [
    "The `UserCatData` can then be used to generate weights for each polygon. An important thing to note is that when generating the weights we need to use an equal area projection (i.e., equal area CRS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0393b2-59a9-4bd1-8030-4f331db5ddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_area = \"EPSG:6931\" # Good for northern hemisphere\n",
    "# crs_area = \"EPSG:5070\" # Good for CONUS\n",
    "\n",
    "# time the weight generation for later comparison\n",
    "t0 = time.time()\n",
    "\n",
    "weight_gen = WeightGen(\n",
    "    user_data=user_data,\n",
    "    method=\"dask\",\n",
    "    weight_gen_crs=crs_area,\n",
    ")\n",
    "\n",
    "df_gdptools_weights = weight_gen.calculate_weights()\n",
    "\n",
    "gdptools_weights_time = time.time() - t0\n",
    "\n",
    "df_gdptools_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc93ef46-1359-4b3b-92ce-27ef0405e1f1",
   "metadata": {},
   "source": [
    "With the weights, we can now perform the aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad919d-3c90-4860-9ad2-fbbdccc4c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "agg_gen = AggGen(\n",
    "    user_data=user_data,\n",
    "    # Use masked to ignore NaNs\n",
    "    # Note that a we use mean vs sum as sum seems to ignore\n",
    "    # weights even though they should be equivalent methods\n",
    "    # (i.e., weighted sum = weighted mean)\n",
    "    stat_method=\"masked_mean\",\n",
    "    agg_engine=\"dask\",\n",
    "    weights=df_gdptools_weights,\n",
    "    agg_writer='none',\n",
    ")\n",
    "_, gdptools_aggregation = agg_gen.calculate_agg()\n",
    "\n",
    "gdptools_agg_time = time.time() - t0\n",
    "\n",
    "gdptools_aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ac59f-08b8-47c5-9560-6f45d5a43a53",
   "metadata": {},
   "source": [
    "Let's make a nice plot of the aggregated HUC12 basins to make sure the aggregation worked as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8fb51c-7b7f-4b22-ac6c-9cdb18d8c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xarray holds the huc12s in sorted order\n",
    "gdptools_huc12_basins = huc12_basins.copy().sort_values('huc12')\n",
    "gdptools_huc12_basins['aggregation'] = gdptools_aggregation.sel(time=timestamp)['PREC_ACC_NC']\n",
    "\n",
    "gdptools_plt = gdptools_huc12_basins.hvplot(\n",
    "    c='aggregation', title=\"Accumulated Precipitation over HUC12 basins\",\n",
    "    coastline='50m', geo=True, cmap='viridis',\n",
    "    aspect='equal', legend=False, frame_width=300\n",
    ")\n",
    "\n",
    "cutout_plt * gdptools_plt + cutout_plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a571b-c5f3-4bb7-bc73-18ca64ba30a7",
   "metadata": {},
   "source": [
    "### Native Method\n",
    "\n",
    "For the native method, we first need to extract the grid information from our CONUS404 data set. We then use it to create polygon boxes that we overlay with the basin polygons to generate weights. Finally like `gdptools`, we use the weights to aggregate via a weighted sum. \n",
    "\n",
    "To give a fair computational time comparison with `gdptools`, we will group all steps to generate the weights into one timed cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994836de-efaf-46a5-8d21-b03c19656f71",
   "metadata": {},
   "source": [
    "#### Create Weights\n",
    "\n",
    "To generate the weights, we (1) extract grid information (includes extracting the `x` and `y` grid and getting their bounds), (2) use these bounds to create polygons of the grid, (3) assign the polygons to a `GeoDataFrame` with the CONUS404 dataset's CRS, (4) overlay the grid polygons and basin polygons, (5) use the overlay to get fractional area weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a400d-1a16-4908-9408-22e60b06c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t0 = time.time()\n",
    "# (1) extract grid info\n",
    "grid = conus404[['x', 'y']].drop_vars(['lat', 'lon']).reset_coords()\n",
    "grid = grid.cf.add_bounds(['x', 'y'])\n",
    "\n",
    "\n",
    "# (2) create polygons of the grid\n",
    "# use a simple helper function. This way we can use xarray to parallelize.\n",
    "def bounds_to_poly(x_bounds, y_bounds):\n",
    "    return Polygon([\n",
    "        (x_bounds[0], y_bounds[0]),\n",
    "        (x_bounds[0], y_bounds[1]),\n",
    "        (x_bounds[1], y_bounds[1]),\n",
    "        (x_bounds[1], y_bounds[0])\n",
    "    ])\n",
    "\n",
    "# Stack the grid cells into a single stack (i.e., x-y pairs)\n",
    "points = grid.stack(point=('y', 'x'))\n",
    "\n",
    "# Apply the function to create polygons from bounds\n",
    "boxes = xr.apply_ufunc(\n",
    "    bounds_to_poly,\n",
    "    points.x_bounds,\n",
    "    points.y_bounds,\n",
    "    input_core_dims=[(\"bounds\",),  (\"bounds\",)],\n",
    "    output_dtypes=[np.dtype('O')],\n",
    "    vectorize=True\n",
    ")\n",
    "\n",
    "\n",
    "# (3) assign polygons to geodataframe with CRS\n",
    "grid_polygons = gp.GeoDataFrame(\n",
    "    data={\"geometry\": boxes.values, \"y\": boxes['y'], \"x\": boxes['x']},\n",
    "    index=boxes.indexes[\"point\"],\n",
    "    crs=conus404.crs.crs_wkt\n",
    ")\n",
    "\n",
    "\n",
    "# (4) overlay the grid polygons with basin polygons\n",
    "# transform both to an area preserving projection\n",
    "huc12_basins_area = huc12_basins.to_crs(crs_area)\n",
    "grid_polygons = grid_polygons.to_crs(crs_area)\n",
    "\n",
    "# overlay the polygons.\n",
    "overlay = grid_polygons.overlay(huc12_basins_area, keep_geom_type=True)\n",
    "\n",
    "\n",
    "# (5)calculate the area fraction for each region\n",
    "grid_cell_fraction = overlay.geometry.area.groupby(overlay['huc12']).transform(lambda x: x / x.sum())\n",
    "\n",
    "# turn this into a series\n",
    "multi_index = overlay.set_index(['y', 'x', 'huc12']).index\n",
    "df_native_weights = pd.Series(grid_cell_fraction.values, index=multi_index)\n",
    "\n",
    "da_native_weights_stacked = xr.DataArray(df_native_weights)\n",
    "\n",
    "# unstack to a sparse array.\n",
    "native_weights = da_native_weights_stacked.unstack(sparse=True, fill_value=0.)\n",
    "\n",
    "native_weights_time = time.time() - t0\n",
    "\n",
    "native_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e87e6d1-2e05-4e1c-8ef5-0d5ecce17032",
   "metadata": {},
   "source": [
    "Now that we have our weights, we can clearly see that this is a **sparse** matrix, with a density of ~0.0025 (i.e., only 0.25% of values are non-zero). So, maintaining it as a sparse martix is the right move for memory conservation, especially as this process scales up.\n",
    "\n",
    "Also, this process is area conserving. We can verify this for each basin's area with a simple area calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dfb754-1a7d-4919-b0f7-cdd8996d10d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate areas of HUC12s from overlay and original polygons\n",
    "overlay_area = overlay.geometry.area.groupby(overlay['huc12']).sum()\n",
    "huc12_area = huc12_basins_area.geometry.area.groupby(huc12_basins_area['huc12']).sum()\n",
    "# find the max fractional difference\n",
    "(np.abs(overlay_area - huc12_area) / huc12_area).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc9ca7-ad27-47b3-961d-01524bc1fd84",
   "metadata": {},
   "source": [
    "Nice! This means the differences can be attributed to machine precision.\n",
    "\n",
    "We can also verify that the cell fractions all sum up to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339befff-b804-49e2-b3c1-7888ece86a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cell_fraction.groupby(overlay['huc12']).sum().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94d0f1-a29b-4fcb-b668-03702dfcdb3b",
   "metadata": {},
   "source": [
    "#### Perform Aggregation\n",
    "\n",
    "To aggregate the data, we can use [`xarray.Dataset.weighted`](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.weighted.html) to do our weighted calculations. This is simple as it will take a sparse array as weights and compute the aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d484b-88d2-4fc5-a67a-199b1bfde14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t0 = time.time()\n",
    "\n",
    "native_aggregation = conus404.drop_vars('crs').weighted(native_weights).sum(dim=['x', 'y']).compute()\n",
    "\n",
    "native_agg_time = time.time() - t0\n",
    "\n",
    "native_aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5a4000-0bc5-4144-892e-565740a196b5",
   "metadata": {},
   "source": [
    "Like the `gdptools` aggregation results, let's make some plots to make sure this worked as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a5cfd7-2b58-4de4-99e6-14278f496d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xarray holds the huc12s in sorted order\n",
    "native_huc12_basins = huc12_basins.copy().sort_values('huc12')\n",
    "native_huc12_basins['aggregation'] = native_aggregation.sel(time=timestamp)['PREC_ACC_NC'].data.todense()\n",
    "native_plt = native_huc12_basins.hvplot(\n",
    "    c='aggregation', title=\"Accumulated Precipitation over HUC12 basins\",\n",
    "    coastline='50m', geo=True, cmap='viridis',\n",
    "    aspect='equal', legend=False, frame_width=300\n",
    ")\n",
    "\n",
    "cutout_plt * native_plt + cutout_plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade5c8b5-bf48-4465-9d17-a5c86bc082fa",
   "metadata": {},
   "source": [
    "## Compare the Results\n",
    "\n",
    "With both aggregation methods complete, we are now ready to compare the results. We can do this both for the final output and the intermediate weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2452655-2066-499d-abbb-ca18ddffddd9",
   "metadata": {},
   "source": [
    "### Weight Comparison\n",
    "\n",
    "To do the weight comparison, we first need to standardize the weight outputs. This is relatively simple as we just need to convert the `gdptools` `DataFrame` weights into an `xarray.DataArray`. We can do this just like we did for the conservative method, but assigning the `x` and `y` values to the `gdptools` data frame using the given indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67004d0-ea2b-4afc-bf62-c36863e6f613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the buffer region, gdptools weights index values\n",
    "# are off by 3. This was found from a manual inspection\n",
    "df_gdptools_weights['y'] = conus404['y'].isel(y=df_gdptools_weights['i']+3).data\n",
    "df_gdptools_weights['x'] = conus404['x'].isel(x=df_gdptools_weights['j']).data\n",
    "gdptool_weights = xr.DataArray(\n",
    "    df_gdptools_weights.set_index(['y', 'x', 'huc12'])['wght']\n",
    ").unstack(sparse=True, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba92618-73ca-4ad7-859b-9924e980b9ae",
   "metadata": {},
   "source": [
    "Now, a simple max fractional difference is the simple check for how they compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0184fe-d4b9-40a7-bc86-7efc8bec88aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.abs(gdptool_weights - native_weights) / native_weights).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ec403-71ea-4c78-ac86-7ddfe83c722b",
   "metadata": {},
   "source": [
    "Look at that. They are identical (up to machine precision). So, the only other thing to compare would be the time required for the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc2a9e1-5544-4353-9344-018e5a118c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'gtptools weights computation time: {gdptools_weights_time:0.3f} seconds')\n",
    "print(f'native weights computation time: {native_weights_time:0.3f} seconds')\n",
    "print(f'computation time difference: {(gdptools_weights_time - native_weights_time):0.3f} seconds')\n",
    "print(f'computation time ratio: {(gdptools_weights_time / native_weights_time):0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc355141-0fda-426f-ad82-ee6f4964202e",
   "metadata": {},
   "source": [
    "So, from this comparison, we can see that both methods give the same weights, but the method using `xarray` and `geopandas` natively is ~10x faster. However, this does not test how well either of the two methods scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ede5400-e486-47a6-a658-185f06cbb0af",
   "metadata": {},
   "source": [
    "### Aggregation Comparison\n",
    "\n",
    "To do the aggregated data comparison, there is no need for any data formatting, as both `gdptools` and the native method have matching `xarray.Dataset` formats. So, let's start with the simple max fractional difference to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab5e42-bebd-4a2c-9aee-cafbf37eef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.abs(gdptools_aggregation - native_aggregation) / native_aggregation).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9218cd7-4693-4a58-98b5-9927787c1bb0",
   "metadata": {},
   "source": [
    "Well, as expected, they are nearly identical, since they had nearly identical weights.\n",
    "\n",
    "Let's plot the fractional difference for a timestamp just to see how they compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b851f716-8937-4e9c-b947-b16a90b8fbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xarray holds the huc12s in sorted order\n",
    "native_huc12_basins = huc12_basins.copy().sort_values('huc12')\n",
    "native_huc12_basins['aggregation'] = (np.abs(gdptools_aggregation - native_aggregation) / native_aggregation).sel(time=timestamp)['PREC_ACC_NC']\n",
    "native_plt = native_huc12_basins.hvplot(\n",
    "    c='aggregation', title=\"Accumulated Precipitation over HUC12 basins\",\n",
    "    coastline='50m', geo=True, cmap='viridis',\n",
    "    aspect='equal', legend=False, frame_width=300\n",
    ")\n",
    "\n",
    "native_plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52db7cd4-0931-4b5a-ba3e-25d14eab0881",
   "metadata": {},
   "source": [
    "Finally, let's compare the computational times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7813f4-0183-4afa-9e1b-8efa88d93748",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'gtptools aggregation computation time: {gdptools_agg_time:0.3f} seconds')\n",
    "print(f'native aggregation computation time: {native_agg_time:0.3f} seconds')\n",
    "print(f'computation time difference: {(gdptools_agg_time - native_agg_time):0.3f} seconds')\n",
    "print(f'computation time ratio: {(gdptools_agg_time / native_agg_time):0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fe873f-7620-41dc-9182-5c81ae76323e",
   "metadata": {},
   "source": [
    "It looks like this step takes about the same time for both. So, let's compare the total computation time (weights and aggregation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7a58ce-c148-4b47-aa68-ccddc3151f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdptools_total_time = gdptools_weights_time+gdptools_agg_time\n",
    "native_total_time = native_weights_time + native_agg_time\n",
    "print(f'gtptools total computation time: {gdptools_total_time:0.3f} seconds')\n",
    "print(f'native total computation time: {native_total_time:0.3f} seconds')\n",
    "print(f'total computation time difference: {(gdptools_total_time - native_total_time):0.3f} seconds')\n",
    "print(f'total computation time ratio: {(gdptools_total_time / native_total_time):0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e97f72-2e5d-47cf-bf7b-db2cad0c0e92",
   "metadata": {},
   "source": [
    "Alright, while the aggregation times are about equal, the decrease in weight creation for the native method makes it ~1.3x faster overall. Therefore, it appears that the native method would be the choice for computational speed. However, generating its weights was more complex than `gdptools`. So, that should be considered when choosing a method to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c0709-4a9e-4f85-820d-9d6b38fe5274",
   "metadata": {},
   "source": [
    "## Shut down the Dask Client\n",
    "\n",
    "If utilized, we should shut down the dask client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b111b1d-b478-4a75-a924-8b5e9007d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
