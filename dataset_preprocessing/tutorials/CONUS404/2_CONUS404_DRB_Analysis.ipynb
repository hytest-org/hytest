{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create zonal statistics and point extractions for comparing CONUS404 and reference datasets\n",
    "\n",
    "<img src='../../../doc/assets/Eval_Analysis.svg' width=600>\n",
    "\n",
    "Now that the data has been prepared, it is time to compute zonal statistics and perform point extractions. \n",
    "\n",
    "<details>\n",
    "  <summary>Guide to pre-requisites and learning outcomes...&lt;click to expand&gt;</summary>\n",
    "  \n",
    "  <table>\n",
    "    <tr>\n",
    "      <td>Pre-Requisites\n",
    "      <td>To get the most out of this notebook, you should already have an understanding of these topics: \n",
    "        <ul>\n",
    "        <li>pre-req one\n",
    "        <li>pre-req two\n",
    "        </ul>\n",
    "    <tr>\n",
    "      <td>Expected Results\n",
    "      <td>At the end of this notebook, you should be able to: \n",
    "        <ul>\n",
    "        <li>outcome one\n",
    "        <li>outcome two\n",
    "        </ul>\n",
    "  </table>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# library imports\n",
    "import cf_xarray\n",
    "import dask\n",
    "import fsspec \n",
    "import geopandas as gpd\n",
    "import hvplot.xarray\n",
    "import intake\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygeohydro\n",
    "import sparse \n",
    "import warnings\n",
    "import xarray as xr\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# run script for available functions\n",
    "%run ../../../model_evaluation/Metrics_StdSuite_v1.ipynb\n",
    "\n",
    "# data\n",
    "# connect to HyTEST catalog\n",
    "url = 'https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml'\n",
    "cat = intake.open_catalog(url)\n",
    "\n",
    "# access tutorial catalog\n",
    "conus404_drb_cat = cat[\"conus404-drb-eval-tutorial-catalog\"]\n",
    "list(conus404_drb_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update to helper function after repo consolidation\n",
    "## **Start a Dask client using an appropriate Dask Cluster** \n",
    "This is an optional step, but can speed up data loading significantly, especially when accessing data from the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_cluster(machine):\n",
    "    ''' Helper function to configure cluster\n",
    "    '''\n",
    "    if machine == 'denali':\n",
    "        from dask.distributed import LocalCluster, Client\n",
    "        cluster = LocalCluster(threads_per_worker=1)\n",
    "        client = Client(cluster)\n",
    "    \n",
    "    elif machine == 'tallgrass':\n",
    "        from dask.distributed import Client\n",
    "        from dask_jobqueue import SLURMCluster\n",
    "        cluster = SLURMCluster(queue='cpu', cores=1, interface='ib0',\n",
    "                               job_extra=['--nodes=1', '--ntasks-per-node=1', '--cpus-per-task=1'],\n",
    "                               memory='6GB')\n",
    "        cluster.adapt(maximum_jobs=30)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "    elif machine == 'local':\n",
    "        import os\n",
    "        import warnings\n",
    "        from dask.distributed import LocalCluster, Client\n",
    "        warnings.warn(\"Running locally can result in costly data transfers!\\n\")\n",
    "        n_cores = os.cpu_count() # set to match your machine\n",
    "        cluster = LocalCluster(threads_per_worker=n_cores)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "    elif machine in ['esip-qhub-gateway-v0.4']:   \n",
    "        import sys, os\n",
    "        sys.path.append(os.path.join(os.environ['HOME'],'shared','users','lib'))\n",
    "        import ebdpy as ebd\n",
    "        aws_profile = 'esip-qhub'  \n",
    "        ebd.set_credentials(profile=aws_profile)\n",
    "\n",
    "        aws_region = 'us-west-2'\n",
    "        endpoint = f's3.{aws_region}.amazonaws.com'\n",
    "        ebd.set_credentials(profile=aws_profile, region=aws_region, endpoint=endpoint)\n",
    "        worker_max = 30\n",
    "        client,cluster = ebd.start_dask_cluster(profile=aws_profile, worker_max=worker_max, \n",
    "                                              region=aws_region, use_existing_cluster=True,\n",
    "                                              adaptive_scaling=True, wait_for_cluster=False, \n",
    "                                              worker_profile='Medium Worker', propagate_env=True)\n",
    "        \n",
    "    return client, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup your cluster\n",
    "\n",
    "#### QHub...\n",
    "Uncomment single commented spaces (#) to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set machine\n",
    "machine = 'esip-qhub-gateway-v0.4'\n",
    "\n",
    "# use configure cluster helper function to setup dask\n",
    "client, cluster = configure_cluster(machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### or HPC\n",
    "Uncomment single commented spaces (#) to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set machine\n",
    "# machine = os.environ['SLURM_CLUSTER_NAME']\n",
    "\n",
    "## use configure_cluster helper function to setup dask\n",
    "# client, cluster = configure_cluster(machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to catalog of tutorial datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow outline:\n",
    "1. Read in the prepared dataset\n",
    "2. Read in the HUC6 boundaries and transform to same coordinate reference system as prepared dataset\n",
    "3. Make a data mask with the HUC6 boundaries to calculate zonal statistics\n",
    "4. Compute zonal statistics with data mask and prepared data\n",
    "\n",
    "Once all calculations are done: \n",
    "\n",
    "5. Combine each reference with benchmark into single dataset\n",
    "6. Export gridded data zonal statistics\n",
    "<br>\n",
    "\n",
    "**CONUS404 zonal statistics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Compute zonal statistics for gridded datasets**\n",
    "\n",
    "In the last tutorial, we prepared three gridded datasets: CONUS404 (benchmark), PRISM (reference), and CERES-EBAF (reference). The goal of this section is compute [zonal statistics](https://gisgeography.com/zonal-statistics/) for each HUC6 zone in the Delaware River Basin (DRB) by using the [conservative regridding method put forth by Ryan Abernathy](https://discourse.pangeo.io/t/conservative-region-aggregation-with-xarray-geopandas-and-sparse/2715) to regrid and perform an area-weight analysis.\n",
    "\n",
    "Dataset outline:\n",
    "<ol>\n",
    "    <li>Read in the prepared dataset</li>\n",
    "    <li>Compute bounding bands for latitude and longitude (if necessary) then use these to create polygons in area-preserving CRS</li>\n",
    "    <li>Read in the HUC6 boundaries and transform to same coordinate reference system as prepared dataset</li>\n",
    "    <li>Overlay the dataset polygons over the HUC6 boundaries and create spatial weights matrices</li>\n",
    "    <li>Perform matrix multiplication between The prepared dataset and the spatial weights matrices</li>\n",
    "    <li>Perform zonal statistics</li>\n",
    "</ol>\n",
    "\n",
    "The following two functions will be used for regridding each dataset. Review them for now and an explanation of what they do will be provided when they are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounds_to_poly(x_bounds, y_bounds):\n",
    "    \"\"\"Return a polygon based on the x (longitude) and y (longitude) bounding band DataArrays\"\"\"\n",
    "    return Polygon([\n",
    "        (x_bounds[0], y_bounds[0]),\n",
    "        (x_bounds[0], y_bounds[1]),\n",
    "        (x_bounds[1], y_bounds[1]),\n",
    "        (x_bounds[1], y_bounds[0])\n",
    "    ])\n",
    "\n",
    "def apply_weights_matmul_sparse(weights, data):\n",
    "    \"\"\"Apply weights in a sparse matrices to data and regrid\"\"\"\n",
    "    assert isinstance(weights, sparse.SparseArray)\n",
    "    assert isinstance(data, np.ndarray)\n",
    "    data = sparse.COO.from_numpy(data)\n",
    "    data_shape = data.shape\n",
    "    n, k = data_shape[0], data_shape[1] * data_shape[2]\n",
    "    data = data.reshape((n, k))\n",
    "    weights_shape = weights.shape\n",
    "    k_, m = weights_shape[0] * weights_shape[1], weights_shape[2]\n",
    "    assert k == k_\n",
    "    weights_data = weights.reshape((k, m))\n",
    "\n",
    "    regridded = sparse.matmul(data, weights_data)\n",
    "    assert regridded.shape == (n, m)\n",
    "    return regridded.todense()\n",
    "\n",
    "    assert isinstance(weights, sparse.SparseArray)\n",
    "    assert isinstance(data, np.ndarray)\n",
    "    data = sparse.COO.from_numpy(data)\n",
    "    data_shape = data.shape\n",
    "    # k = nlat * nlon\n",
    "    n, k = data_shape[0], data_shape[1] * data_shape[2]\n",
    "    data = data.reshape((n, k))\n",
    "    weights_shape = weights.shape\n",
    "    k_, m = weights_shape[0] * weights_shape[1], weights_shape[2]\n",
    "    assert k == k_\n",
    "    weights_data = weights.reshape((k, m))\n",
    "\n",
    "    regridded = sparse.matmul(data, weights_data)\n",
    "    assert regridded.shape == (n, m)\n",
    "    return regridded.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the following `fsspec.filesystem` will be using to read in each dataset from an [Open Storage Network](https://www.openstoragenetwork.org/) bucket, which is read only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_read = fsspec.filesystem('s3', anon=True, skip_instance_cache=True,\n",
    "                            client_kwargs={'endpoint_url': 'https://renc.osn.xsede.org'})\n",
    "\n",
    "x = \"x\"\n",
    "y = \"y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the geometries for the DRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in HUC6 boundaries found in the DRB\n",
    "drb_gdf = pygeohydro.WBD(\"huc6\", outfields=[\"huc6\", \"name\"]).byids(\"huc6\", [\"020401\", \"020402\"])\n",
    "\n",
    "# area preserving crs\n",
    "crs_area = \"ESRI:53034\"\n",
    "\n",
    "# set CRS to match c404_drb\n",
    "drb_gdf = drb_gdf.to_crs(crs_area)\n",
    "\n",
    "#visualize\n",
    "# drb_gdf.plot(edgecolor=\"orange\", facecolor=\"purple\", linewidth=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONUS404 zonal statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# open dataset\n",
    "c404_drb = conus404_drb_cat['conus404-drb-OSN'].to_dask()\n",
    "\n",
    "# crs\n",
    "c404_crs = c404_drb.rio.crs.to_proj4()\n",
    "\n",
    "# c404_drb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c404_drb.PREC_ACC_NC.hvplot(x=\"x\", y=\"y\", rasterize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the grid of c404_drb using any of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vars\n",
    "c404_var = \"TK\"\n",
    "\n",
    "# drop unneeded variable and coordinates\n",
    "c404_grid = c404_drb[[c404_var]].drop(['time', 'lon', 'lat', c404_var]).reset_coords().load() #load in Richs code\n",
    "c404_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create bounding bands then stack into points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bounds\n",
    "c404_grid = c404_grid.cf.add_bounds(x)\n",
    "c404_grid = c404_grid.cf.add_bounds(y)\n",
    "\n",
    "# stack\n",
    "c404_points = c404_grid.stack(point=(y,x))\n",
    "c404_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the `xarray apply_ufunc` function to apply the `bounds_to_poly` function above to the _c404_points_ DataSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_boxes = xr.apply_ufunc(\n",
    "    bounds_to_poly,\n",
    "    c404_points.x_bounds,\n",
    "    c404_points.y_bounds,\n",
    "    input_core_dims=[(\"bounds\",),  (\"bounds\",)],\n",
    "    output_dtypes=[np.dtype('O')],\n",
    "    vectorize=True\n",
    ")\n",
    "c404_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create geodataframe from boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_grid_df= gpd.GeoDataFrame(\n",
    "    data={\"geometry\": c404_boxes.values, \"y\": c404_boxes[y], \"x\": c404_boxes[x]},\n",
    "    index=c404_boxes.indexes[\"point\"],\n",
    "    crs=c404_crs\n",
    ")\n",
    "c404_grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c404_grid_df.plot(edgecolor=\"red\", facecolor=\"white\", linewidth=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlay the two grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert DRB to conus404 crs\n",
    "c404_drb_gdf = drb_gdf.to_crs(c404_crs)\n",
    "\n",
    "#perform overly\n",
    "c404_overlay = c404_grid_df.overlay(c404_drb_gdf, keep_geom_type=True)\n",
    "c404_overlay.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot overlay for single HUC6\n",
    "# c404_overlay[c404_overlay.huc6 == \"020402\"].geometry.plot(edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid cell fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_grid_cell_fraction = c404_overlay.geometry.area.groupby(c404_overlay.huc6).transform(lambda x: x / x.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse DataArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_multi_index = c404_overlay.set_index([y, x, \"huc6\"]).index\n",
    "c404_df_weights = pd.DataFrame({\"weights\": c404_grid_cell_fraction.values}, index=c404_multi_index)\n",
    "\n",
    "c404_ds_weights = xr.Dataset(c404_df_weights)\n",
    "\n",
    "c404_weights_sparse = c404_ds_weights.unstack(sparse=True, fill_value=0.).weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication across each DataArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "    c404_precip_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        c404_weights_sparse,\n",
    "        c404_drb[\"PREC_ACC_NC\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )\n",
    "\n",
    "    c404_rnet_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        c404_weights_sparse,\n",
    "        c404_drb[\"RNET\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )\n",
    "\n",
    "    c404_tk_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        c404_weights_sparse,\n",
    "        c404_drb[\"TK\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge DataArrays into Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_regridded = xr.Dataset({\"PREC_NC_ACC\":c404_precip_regridded, \"RNET\":c404_rnet_regridded, \"TK\": c404_tk_regridded})\n",
    "c404_regridded = c404_regridded.drop(\"crs\")\n",
    "c404_regridded.attrs = c404_drb.attrs\n",
    "c404_regridded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c404_df = c404_regridded.load().to_dataframe()\n",
    "c404_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "c404_zonal_stats = c404_df.reset_index(drop=False)\n",
    "c404_zonal_stats[\"time\"] = c404_zonal_stats[\"time\"].astype(str).str[:-3]\n",
    "c404_zonal_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRISM zonal statistics**\n",
    "\n",
    "PRISM has two variables: TK and PREC_ACC_NC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset\n",
    "prism_drb = conus404_drb_cat['prism-drb-OSN'].to_dask()\n",
    "\n",
    "# prism crs\n",
    "prism_crs = 4269\n",
    "\n",
    "# create the grid of c404_drb using any of the variables\n",
    "prism_var = \"TK\"\n",
    "\n",
    "# drop unneeded variable and coordinates\n",
    "prism_grid = prism_drb[[prism_var]].drop(\n",
    "    ['time', prism_var]).reset_coords().load()\n",
    "\n",
    "\n",
    "# add bounds\n",
    "prism_grid = prism_grid.cf.add_bounds(x)\n",
    "prism_grid = prism_grid.cf.add_bounds(y)\n",
    "\n",
    "# stack\n",
    "prism_points = prism_grid.stack(point=(y,x))\n",
    "\n",
    "# apply bounds_to method\n",
    "prism_boxes = xr.apply_ufunc(\n",
    "    bounds_to_poly,\n",
    "    prism_points.x_bounds,\n",
    "    prism_points.y_bounds,\n",
    "    input_core_dims=[(\"bounds\",),  (\"bounds\",)],\n",
    "    output_dtypes=[np.dtype('O')],\n",
    "    vectorize=True\n",
    ")\n",
    "\n",
    "# create geodataframe from boxes\n",
    "prism_grid_df= gpd.GeoDataFrame(\n",
    "    data={\"geometry\": prism_boxes.values, \"y\": prism_boxes[y], \"x\": prism_boxes[x]},\n",
    "    index=prism_boxes.indexes[\"point\"],\n",
    "    crs=prism_crs\n",
    ")\n",
    "\n",
    "# convert DRB to conus404 crs\n",
    "prism_drb_gdf = drb_gdf.to_crs(epsg=prism_crs)\n",
    "\n",
    "# overlay the two grids\n",
    "prism_overlay = prism_grid_df.overlay(prism_drb_gdf, keep_geom_type=True)\n",
    "\n",
    "# grid cell fractions\n",
    "prism_grid_cell_fraction = prism_overlay.geometry.area.groupby(prism_overlay.huc6).transform(lambda x: x / x.sum())\n",
    "\n",
    "# create sparse dataarray\n",
    "prism_multi_index = prism_overlay.set_index([y, x, \"huc6\"]).index\n",
    "prism_df_weights = pd.DataFrame({\"weights\": prism_grid_cell_fraction.values}, index=prism_multi_index)\n",
    "\n",
    "prism_ds_weights = xr.Dataset(prism_df_weights)\n",
    "\n",
    "prism_weights_sparse = prism_ds_weights.unstack(sparse=True, fill_value=0.).weights\n",
    "\n",
    "# Matrix multiplication across each DataArray\n",
    "with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "    prism_precip_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        prism_weights_sparse,\n",
    "        prism_drb[\"PREC_ACC_NC\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )\n",
    "\n",
    "    prism_tk_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        prism_weights_sparse,\n",
    "        prism_drb[\"TK\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )\n",
    "# merge DataArrays into Dataset\n",
    "prism_regridded = xr.Dataset({\"PREC_NC_ACC\":prism_precip_regridded, \"TK\": prism_tk_regridded})\n",
    "prism_regridded.attrs = prism_drb.attrs\n",
    "\n",
    "# Covert to DataFrame\n",
    "prism_df = prism_regridded.load().to_dataframe()\n",
    "prism_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index and add time back\n",
    "prism_zonal_stats = prism_df.reset_index(drop=False)\n",
    "prism_zonal_stats[\"time\"] = prism_zonal_stats[\"time\"].astype(str).str[:-3]\n",
    "prism_zonal_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the PRISM and CONUS404 zonals stats together based on the HUC6 code and time\n",
    "prism_c404_zonal = prism_zonal_stats.merge(c404_zonal_stats, left_on=['huc6', 'time'], right_on=['huc6', 'time'], suffixes=[\"_prism\", \"_c404\"])\n",
    "\n",
    "#drop RNET\n",
    "prism_c404_zonal.drop(\"RNET\", axis=1, inplace=True)\n",
    "\n",
    "prism_c404_zonal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time column to datetime type\n",
    "prism_c404_zonal[\"time\"] = pd.to_datetime(prism_c404_zonal[\"time\"], format=\"%Y-%m\")\n",
    "prism_c404_zonal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_c404_yearly = prism_c404_zonal.resample(\"1Y\", on=\"time\").mean()\n",
    "prism_c404_yearly.reset_index(drop=False, inplace=True)\n",
    "prism_c404_yearly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean, median, standard devation\n",
    "prism_c404_mean = prism_c404_yearly.mean()\n",
    "prism_c404_median = prism_c404_yearly.median()\n",
    "prism_c404_stdev = prism_c404_yearly.std()\n",
    "\n",
    "#create dataframe\n",
    "prism_c404_stats = pd.DataFrame({\"annual_mean\": prism_c404_mean, \"median\": prism_c404_median, \"stdev\": prism_c404_stdev}).T.drop(\"time\", axis=1)\n",
    "\n",
    "# reset index and rename\n",
    "prism_c404_stats = prism_c404_stats.reset_index(drop=False).rename({\"index\":\"stat\"}, axis=1)\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias\n",
    "prism_c404_stats_annual_mean = prism_c404_stats.loc[prism_c404_stats['stat'] == \"annual_mean\"]\n",
    "prism_c404_bias_precip = float(prism_c404_stats_annual_mean[\"PREC_NC_ACC_c404\"] - prism_c404_stats_annual_mean[\"PREC_NC_ACC_prism\"])\n",
    "prism_c404_bias_tk = float(prism_c404_stats_annual_mean[\"TK_c404\"] - prism_c404_stats_annual_mean[\"TK_prism\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "prism_c404_stats.loc[len(prism_c404_stats.index)] = [\"bias\", prism_c404_bias_precip, None, prism_c404_bias_tk, None]\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE\n",
    "prism_c404_mae_precip = sum(abs(prism_c404_yearly[\"PREC_NC_ACC_c404\"] - prism_c404_yearly[\"PREC_NC_ACC_prism\"]))/len(prism_c404_yearly)\n",
    "prism_c404_mae_tk = sum(abs(prism_c404_yearly[\"TK_c404\"] - prism_c404_yearly[\"TK_prism\"]))/len(prism_c404_yearly)\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "prism_c404_stats.loc[len(prism_c404_stats.index)] = [\"MAE\", prism_c404_mae_precip, None, prism_c404_mae_tk, None]\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE\n",
    "prism_c404_rmse_precip = math.sqrt(np.square(np.subtract(prism_c404_yearly[\"PREC_NC_ACC_c404\"], prism_c404_yearly[\"PREC_NC_ACC_prism\"])).mean())\n",
    "prism_c404_rmse_tk = math.sqrt(np.square(np.subtract(prism_c404_yearly[\"TK_c404\"], prism_c404_yearly[\"TK_prism\"])).mean())\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "prism_c404_stats.loc[len(prism_c404_stats.index)] = [\"RMSE\", prism_c404_rmse_precip, None, prism_c404_rmse_tk, None]\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../../model_evaluation/Metrics_StdSuite_v1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearsons correlation\n",
    "prism_c404_pearson_precip = pearson_r(prism_c404_yearly[\"PREC_NC_ACC_c404\"], prism_c404_yearly[\"PREC_NC_ACC_prism\"])\n",
    "prism_c404_pearson_tk = pearson_r(prism_c404_yearly[\"TK_c404\"], prism_c404_yearly[\"TK_prism\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "prism_c404_stats.loc[len(prism_c404_stats.index)] = [\"Pearson\", prism_c404_pearson_precip, None, prism_c404_pearson_tk, None]\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman's correlation\n",
    "prism_c404_spearman_precip = spearman_r(prism_c404_yearly[\"PREC_NC_ACC_c404\"], prism_c404_yearly[\"PREC_NC_ACC_prism\"])\n",
    "prism_c404_spearman_tk = spearman_r(prism_c404_yearly[\"TK_c404\"], prism_c404_yearly[\"TK_prism\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "prism_c404_stats.loc[len(prism_c404_stats.index)] = [\"Spearman\", prism_c404_spearman_precip, None, prism_c404_spearman_tk, None]\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent bias\n",
    "prism_c404_pbias_precip = pbias(prism_c404_yearly[\"PREC_NC_ACC_c404\"], prism_c404_yearly[\"PREC_NC_ACC_prism\"])\n",
    "prism_c404_pbias_tk = pbias(prism_c404_yearly[\"TK_c404\"], prism_c404_yearly[\"TK_prism\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "prism_c404_stats.loc[len(prism_c404_stats.index)] = [\"pbias\", prism_c404_pbias_precip, None, prism_c404_pbias_tk, None]\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CERES-EBAF zonal statistics**\n",
    "\n",
    "CERES-EBAF has a single variable: RNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset\n",
    "ceres_drb = conus404_drb_cat['ceres-drb-OSN']\n",
    "\n",
    "# crs\n",
    "ceres_crs = 4326\n",
    "\n",
    "# create the grid of c404_drb using any of the variables\n",
    "ceres_var = \"RNET\"\n",
    "\n",
    "# drop unneeded variable and coordinates\n",
    "ceres_grid = ceres_drb[[ceres_var]].drop(\n",
    "    ['time', ceres_var]).reset_coords().load()\n",
    "\n",
    "\n",
    "# add bounds\n",
    "ceres_grid = ceres_grid.cf.add_bounds(x)\n",
    "ceres_grid = ceres_grid.cf.add_bounds(y)\n",
    "\n",
    "# stack\n",
    "ceres_points = ceres_grid.stack(point=(y, x))\n",
    "\n",
    "# apply bounds_to method\n",
    "ceres_boxes = xr.apply_ufunc(\n",
    "    bounds_to_poly,\n",
    "    ceres_points.x_bounds,\n",
    "    ceres_points.y_bounds,\n",
    "    input_core_dims=[(\"bounds\",),  (\"bounds\",)],\n",
    "    output_dtypes=[np.dtype('O')],\n",
    "    vectorize=True\n",
    ")\n",
    "\n",
    "# create geodataframe from boxes\n",
    "ceres_grid_df = gpd.GeoDataFrame(\n",
    "    data={\"geometry\": ceres_boxes.values,\n",
    "          \"y\": ceres_boxes[y], \"x\": ceres_boxes[x]},\n",
    "    index=ceres_boxes.indexes[\"point\"],\n",
    "    crs=ceres_crs\n",
    ")\n",
    "\n",
    "# convert DRB to conus404 crs\n",
    "ceres_drb_gdf = drb_gdf.to_crs(epsg=ceres_crs)\n",
    "\n",
    "# overlay the two grids\n",
    "ceres_overlay = ceres_grid_df.overlay(ceres_drb_gdf, keep_geom_type=True)\n",
    "\n",
    "# grid cell fractions\n",
    "ceres_grid_cell_fraction = ceres_overlay.geometry.area.groupby(\n",
    "    ceres_overlay.huc6).transform(lambda x: x / x.sum())\n",
    "\n",
    "# create sparse dataarray\n",
    "ceres_multi_index = ceres_overlay.set_index([y, x, \"huc6\"]).index\n",
    "ceres_df_weights = pd.DataFrame(\n",
    "    {\"weights\": ceres_grid_cell_fraction.values}, index=ceres_multi_index)\n",
    "\n",
    "ceres_ds_weights = xr.Dataset(ceres_df_weights)\n",
    "\n",
    "ceres_weights_sparse = ceres_ds_weights.unstack(\n",
    "    sparse=True, fill_value=0.).weights\n",
    "\n",
    "# Matrix multiplication across each DataArray\n",
    "with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "    ceres_rnet_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        ceres_weights_sparse,\n",
    "        ceres_drb[\"RNET\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )\n",
    "\n",
    "# merge DataArrays into Dataset\n",
    "ceres_regridded = xr.Dataset({\"RNET\": ceres_rnet_regridded})\n",
    "ceres_regridded.attrs = ceres_drb.attrs\n",
    "\n",
    "# Covert to DataFrame\n",
    "ceres_df = ceres_regridded.load().to_dataframe()\n",
    "ceres_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index \n",
    "ceres_zonal_stats = ceres_df.reset_index(drop=False)\n",
    "ceres_zonal_stats[\"time\"] = ceres_zonal_stats[\"time\"].astype(str).str[:-3]\n",
    "ceres_zonal_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the PRISM and CONUS404 zonals stats together based on the HUC6 code and time\n",
    "ceres_c404_zonal = ceres_zonal_stats.merge(c404_zonal_stats, left_on=['huc6', 'time'], right_on=['huc6', 'time'], suffixes=[\"_ceres\", \"_c404\"])\n",
    "\n",
    "#drop RNET\n",
    "ceres_c404_zonal.drop([\"PREC_NC_ACC\", \"TK\"], axis=1, inplace=True)\n",
    "\n",
    "ceres_c404_zonal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time column to datetime type\n",
    "ceres_c404_zonal[\"time\"] = pd.to_datetime(ceres_c404_zonal[\"time\"], format=\"%Y-%m\")\n",
    "ceres_c404_zonal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceres_c404_yearly = ceres_c404_zonal.resample(\"1Y\", on=\"time\").mean()\n",
    "ceres_c404_yearly.reset_index(drop=False, inplace=True)\n",
    "ceres_c404_yearly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean, median, standard devation\n",
    "ceres_c404_mean = ceres_c404_yearly.mean()\n",
    "ceres_c404_median = ceres_c404_yearly.median()\n",
    "ceres_c404_stdev = ceres_c404_yearly.std()\n",
    "\n",
    "#create dataframe\n",
    "ceres_c404_stats = pd.DataFrame({\"annual_mean\": ceres_c404_mean, \"median\": ceres_c404_median, \"stdev\": ceres_c404_stdev}).T.drop(\"time\", axis=1)\n",
    "\n",
    "# reset index and rename\n",
    "ceres_c404_stats = ceres_c404_stats.reset_index(drop=False).rename({\"index\":\"stat\"}, axis=1)\n",
    "\n",
    "ceres_c404_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias\n",
    "ceres_c404_stats_annual_mean = ceres_c404_stats.loc[ceres_c404_stats['stat'] == \"annual_mean\"]\n",
    "ceres_c404_bias_rnet = float(ceres_c404_stats_annual_mean[\"RNET_c404\"] - ceres_c404_stats_annual_mean[\"RNET_ceres\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "ceres_c404_stats.loc[len(ceres_c404_stats.index)] = [\"bias\", ceres_c404_bias_rnet, None]\n",
    "\n",
    "# MAE\n",
    "ceres_c404_mae_rnet = sum(abs(ceres_c404_yearly[\"RNET_c404\"] - ceres_c404_yearly[\"RNET_ceres\"]))/len(ceres_c404_yearly)\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "ceres_c404_stats.loc[len(ceres_c404_stats.index)] = [\"MAE\", ceres_c404_mae_rnet, None]\n",
    "\n",
    "# RMSE\n",
    "ceres_c404_rmse_rnet = math.sqrt(np.square(np.subtract(ceres_c404_yearly[\"RNET_c404\"], ceres_c404_yearly[\"RNET_ceres\"])).mean())\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "ceres_c404_stats.loc[len(ceres_c404_stats.index)] = [\"RMSE\", ceres_c404_rmse_rnet, None]\n",
    "\n",
    "# Pearsons correlation\n",
    "ceres_c404_pearson_rnet = pearson_r(ceres_c404_yearly[\"RNET_c404\"], ceres_c404_yearly[\"RNET_ceres\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "ceres_c404_stats.loc[len(ceres_c404_stats.index)] = [\"Pearson\", ceres_c404_pearson_rnet, None]\n",
    "\n",
    "# Spearman's correlation\n",
    "ceres_c404_spearman_rnet = spearman_r(ceres_c404_yearly[\"RNET_c404\"], ceres_c404_yearly[\"RNET_ceres\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "ceres_c404_stats.loc[len(ceres_c404_stats.index)] = [\"Spearman\", ceres_c404_spearman_rnet, None]\n",
    "\n",
    "# percent bias\n",
    "ceres_c404_pbias_rnet = pbias(ceres_c404_yearly[\"RNET_c404\"], ceres_c404_yearly[\"RNET_ceres\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "ceres_c404_stats.loc[len(ceres_c404_stats.index)] = [\"pbias\", ceres_c404_pbias_rnet, None]\n",
    "\n",
    "ceres_c404_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Extract gridded values to points**\n",
    "\n",
    "The goal of this section is extract values from CONUS404 where they intersect with station data. This process is described in article about the ESRI tool [Extract Values to Points](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-analyst/extract-values-to-points.htm). This tabular data will then be exported for use in the next notebook, **CONUS404 Analysis**.\n",
    "\n",
    "Dataset outline:\n",
    "1. Read in the prepared dataset\n",
    "2. Extract data from overlapping pixel at same time step as point\n",
    "<br>\n",
    "\n",
    "**Climate Reference Network point extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb_df = conus404_drb_cat['crn-drb-OSN'].read()\n",
    "\n",
    "# create geodataframe\n",
    "crn_drb = gpd.GeoDataFrame(crn_drb_df, crs=4326,\n",
    "                       geometry=gpd.points_from_xy(crn_drb_df.LONGITUDE, \n",
    "                                                         crn_drb_df.LATITUDE))\n",
    "\n",
    "# modify date field\n",
    "crn_drb[\"DATE\"] = crn_drb[\"DATE\"].astype(str).str[:-3]\n",
    "\n",
    "crn_drb.rename({\"DATE\": \"time\",\n",
    "                \"TK\": \"TK_crn\", \n",
    "                \"RNET\": \"RNET_crn\", \n",
    "                \"PREC_ACC_NC\": \"PREC_ACC_NC_crn\"},\n",
    "                  axis=1, inplace=True)\n",
    "\n",
    "crn_drb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get coordinates from crn_drb to index c404_drb by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# isolate single row and transform to c404_drb crs\n",
    "crn_coords_gdf = crn_drb.iloc[[0]].to_crs(c404_crs)\n",
    "\n",
    "# extract lat/long values\n",
    "crn_lat = crn_coords_gdf.iloc[0][\"geometry\"].y\n",
    "crn_lon = crn_coords_gdf.iloc[0][\"geometry\"].x\n",
    "\n",
    "# time\n",
    "crn_time_min = crn_drb_df[\"time\"].min()\n",
    "crn_time_max = crn_drb_df[\"time\"].max()\n",
    "crn_time_min, crn_time_max\n",
    "\n",
    "# subset c404_drb to lat/long using nearest\n",
    "c404_crn_sub = c404_drb.sel(x=crn_lon, y=crn_lat, method=\"nearest\")\n",
    "\n",
    "# slice to time-steps of crn_drb\n",
    "c404_crn_sub = c404_crn_sub.sel(time=slice(crn_time_min, crn_time_max))\n",
    "\n",
    "c404_crn_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert subset to dataframe and reorganize columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_crn_sub_df = c404_crn_sub.to_dataframe().reset_index(drop=False)\n",
    "\n",
    "# trim columns\n",
    "c404_crn_sub_df = c404_crn_sub_df[[\"time\", \"TK\", \"RNET\", \"PREC_ACC_NC\"]]\n",
    "\n",
    "# rename columns\n",
    "c404_crn_sub_df.rename({\"TK\": \"TK_c404\", \n",
    "                    \"RNET\": \"RNET_c404\", \n",
    "                    \"PREC_ACC_NC\": \"PREC_ACC_NC_c404\"},\n",
    "                  axis=1, inplace=True)\n",
    "\n",
    "# trim time\n",
    "c404_crn_sub_df[\"time\"] = c404_crn_sub_df[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "c404_crn_sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine CONUS404 subset with CRN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_c404_point = crn_drb.merge(c404_crn_sub_df, on=\"time\").reset_index(drop=False)\n",
    "\n",
    "# drop columns\n",
    "crn_c404_point.drop([\"index\", \"LATITUDE\", \"LONGITUDE\", \"ID\", \"geometry\"], axis=1, inplace=True)\n",
    "\n",
    "crn_c404_point.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time column to datetime type\n",
    "crn_c404_point[\"time\"] = pd.to_datetime(crn_c404_point[\"time\"], format=\"%Y-%m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample to yearly means\n",
    "crn_c404_yearly = crn_c404_point.resample(\"1Y\", on=\"time\").mean()\n",
    "crn_c404_yearly.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# mean, median, standard devation\n",
    "crn_c404_mean = crn_c404_yearly.mean()\n",
    "crn_c404_median = crn_c404_yearly.median()\n",
    "crn_c404_stdev = crn_c404_yearly.std()\n",
    "\n",
    "#create dataframe\n",
    "crn_c404_stats = pd.DataFrame({\"annual_mean\": crn_c404_mean, \"median\": crn_c404_median, \"stdev\": crn_c404_stdev}).T.drop(\"time\", axis=1)\n",
    "\n",
    "# reset index and rename\n",
    "crn_c404_stats = crn_c404_stats.reset_index(drop=False).rename({\"index\":\"stat\"}, axis=1)\n",
    "\n",
    "# bias\n",
    "crn_c404_stats_annual_mean = crn_c404_stats.loc[crn_c404_stats['stat'] == \"annual_mean\"]\n",
    "crn_c404_bias_precip = float(crn_c404_stats_annual_mean[\"PREC_ACC_NC_c404\"] - crn_c404_stats_annual_mean[\"PREC_ACC_NC_crn\"])\n",
    "crn_c404_bias_rnet = float(crn_c404_stats_annual_mean[\"RNET_c404\"] - crn_c404_stats_annual_mean[\"RNET_crn\"])\n",
    "crn_c404_bias_tk = float(crn_c404_stats_annual_mean[\"TK_c404\"] - crn_c404_stats_annual_mean[\"TK_crn\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "crn_c404_stats.loc[len(crn_c404_stats.index)] = [\"bias\", crn_c404_bias_precip, None, crn_c404_bias_rnet, None, crn_c404_bias_tk, None]\n",
    "\n",
    "# MAE\n",
    "crn_c404_mae_precip = sum(abs(crn_c404_yearly[\"PREC_ACC_NC_c404\"] - crn_c404_yearly[\"PREC_ACC_NC_crn\"]))/len(crn_c404_yearly)\n",
    "crn_c404_mae_rnet = sum(abs(crn_c404_yearly[\"RNET_c404\"] - crn_c404_yearly[\"RNET_crn\"]))/len(crn_c404_yearly)\n",
    "crn_c404_mae_tk = sum(abs(crn_c404_yearly[\"TK_c404\"] - crn_c404_yearly[\"TK_crn\"]))/len(crn_c404_yearly)\n",
    "# add stat to bottom of dataframe\n",
    "crn_c404_stats.loc[len(crn_c404_stats.index)] = [\"MAE\", crn_c404_mae_precip, None, crn_c404_mae_rnet, None, crn_c404_mae_tk, None]\n",
    "\n",
    "# RMSE\n",
    "crn_c404_rmse_precip = math.sqrt(np.square(np.subtract(crn_c404_yearly[\"PREC_ACC_NC_c404\"], crn_c404_yearly[\"PREC_ACC_NC_crn\"])).mean())\n",
    "crn_c404_rmse_rnet = math.sqrt(np.square(np.subtract(crn_c404_yearly[\"RNET_c404\"], crn_c404_yearly[\"RNET_crn\"])).mean())\n",
    "crn_c404_rmse_tk = math.sqrt(np.square(np.subtract(crn_c404_yearly[\"TK_c404\"], crn_c404_yearly[\"TK_crn\"])).mean())\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "crn_c404_stats.loc[len(crn_c404_stats.index)] = [\"RMSE\", crn_c404_rmse_precip, None, crn_c404_rmse_rnet, None, crn_c404_rmse_tk, None]\n",
    "\n",
    "# Pearsons correlation\n",
    "crn_c404_pearson_precip = pearson_r(crn_c404_yearly[\"PREC_ACC_NC_c404\"], crn_c404_yearly[\"PREC_ACC_NC_crn\"])\n",
    "crn_c404_pearson_rnet = pearson_r(crn_c404_yearly[\"RNET_c404\"], crn_c404_yearly[\"RNET_crn\"])\n",
    "crn_c404_pearson_tk = pearson_r(crn_c404_yearly[\"TK_c404\"], crn_c404_yearly[\"TK_crn\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "crn_c404_stats.loc[len(crn_c404_stats.index)] = [\"pearson\", crn_c404_pearson_precip, None, crn_c404_pearson_rnet, None, crn_c404_pearson_tk, None]\n",
    "\n",
    "# Spearman's correlation\n",
    "crn_c404_spearman_precip = spearman_r(crn_c404_yearly[\"PREC_ACC_NC_c404\"], crn_c404_yearly[\"PREC_ACC_NC_crn\"])\n",
    "crn_c404_spearman_rnet = spearman_r(crn_c404_yearly[\"RNET_c404\"], crn_c404_yearly[\"RNET_crn\"])\n",
    "crn_c404_spearman_tk = spearman_r(crn_c404_yearly[\"TK_c404\"], crn_c404_yearly[\"TK_crn\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "crn_c404_stats.loc[len(crn_c404_stats.index)] = [\"spearman\", crn_c404_spearman_precip, None, crn_c404_spearman_rnet, None, crn_c404_spearman_tk, None]\n",
    "\n",
    "# percent bias\n",
    "crn_c404_pbias_precip = pbias(crn_c404_yearly[\"PREC_ACC_NC_c404\"], crn_c404_yearly[\"PREC_ACC_NC_crn\"])\n",
    "crn_c404_pbias_rnet = pbias(crn_c404_yearly[\"RNET_c404\"], crn_c404_yearly[\"RNET_crn\"])\n",
    "crn_c404_pbias_tk = pbias(crn_c404_yearly[\"TK_c404\"], crn_c404_yearly[\"TK_crn\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "crn_c404_stats.loc[len(crn_c404_stats.index)] = [\"pbias\", crn_c404_pbias_precip, None, crn_c404_pbias_rnet, None, crn_c404_pbias_tk, None]\n",
    "\n",
    "crn_c404_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crn_c404_point.to_parquet(\"s3://nhgf-development/workspace/tutorial/CONUS404/crn_c404_point.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Historical Climate Network (HCN) point extraction**\n",
    "\n",
    "The HCN data is different than the CRN data as the HCN data comes from multiple stations whereas the CRN data was from a single station. This will involve using multiple sets of geographic coordinates to extract data from CONUS404."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dataset\n",
    "hcn_drb_df = conus404_drb_cat['hcn-drb-OSN'].read()\n",
    "\n",
    "#rename columns\n",
    "hcn_drb_df.rename({\"DATE\": \"time\",\n",
    "                \"TK\": \"TK_hcn\",  \n",
    "                \"PREC_ACC_NC\": \"PREC_ACC_NC_hcn\"},\n",
    "                  axis=1, inplace=True)\n",
    "\n",
    "# change DATE field to \n",
    "hcn_drb_df[\"time\"] = hcn_drb_df[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "hcn_drb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a DataFrame of the station IDs, lats, and longs to use for extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hcn_stations = hcn_drb_df.copy().drop([\"time\", \"TK_hcn\", \"PREC_ACC_NC_hcn\"], axis=1)\n",
    "hcn_stations[\"LONGITUDE\"] = pd.to_numeric(hcn_stations[\"LONGITUDE\"])\n",
    "hcn_stations[\"LATITUDE\"] = pd.to_numeric(hcn_stations[\"LATITUDE\"])\n",
    "\n",
    "hcn_stations = hcn_stations.groupby('ID').mean().reset_index(drop=False)\n",
    "# hcn_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a GeoDataFrame to convert the lat and long to the coordinate system of CONUS404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_stations_gdf = gpd.GeoDataFrame(hcn_stations, crs=4326,\n",
    "                       geometry=gpd.points_from_xy(hcn_stations.LONGITUDE, \n",
    "                                                         hcn_stations.LATITUDE))\n",
    "\n",
    "# transform to c404_drb crs\n",
    "hcn_stations_gdf = hcn_stations_gdf.to_crs(c404_crs)\n",
    "\n",
    "# extract lat/long values\n",
    "hcn_stations_gdf[\"y\"] = hcn_stations_gdf[\"geometry\"].y\n",
    "hcn_stations_gdf[\"x\"] = hcn_stations_gdf[\"geometry\"].x\n",
    "\n",
    "#drop lat/lon/geo\n",
    "hcn_stations_df = hcn_stations_gdf.drop([\"LATITUDE\", \"LONGITUDE\", \"geometry\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset c404_drb to time period of HCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time min/max\n",
    "hcn_time_min = hcn_drb_df[\"time\"].min()\n",
    "hcn_time_max = hcn_drb_df[\"time\"].max()\n",
    "\n",
    "# slice c404 to HCN time\n",
    "c404_hcn_timesub = c404_drb.sel(time=slice(hcn_time_min, hcn_time_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Dataframe rows to extract data from c404_drb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list of extracted data\n",
    "c404_hcn_subs = []\n",
    "\n",
    "for index, data in hcn_stations_df.iterrows():\n",
    "    c404_hcn_sub_step = c404_hcn_timesub.sel(x=data.x, y=data.y, method=\"nearest\").to_dataframe()\n",
    "    c404_hcn_sub_step[\"ID\"] = data.ID\n",
    "    c404_hcn_subs.append(c404_hcn_sub_step)\n",
    "\n",
    "# concat list of extracted data into single Dataframe\n",
    "c404_hcn_sub = pd.concat(c404_hcn_subs)\n",
    "\n",
    "#reset index\n",
    "c404_hcn_sub.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# drop columns\n",
    "c404_hcn_sub.drop([\"RNET\", \"lon\", \"lat\", \"y\", \"x\", \"crs\"], axis=1, inplace=True)\n",
    "\n",
    "# rename columns\n",
    "c404_hcn_sub.rename({\"TK\":\"TK_c404\",\n",
    "                    \"PREC_ACC_NC\": \"PREC_ACC_NC_c404\"},\n",
    "                   axis=1, inplace=True)\n",
    "\n",
    "# trim time\n",
    "c404_hcn_sub[\"time\"] = c404_hcn_sub[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "# c404_hcn_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge CONUS404 observations to HCN observations using the station ID and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_c404_point = hcn_drb_df.merge(c404_hcn_sub, left_on=[\"ID\", \"time\"], right_on=[\"ID\", \"time\"])\n",
    "\n",
    "# drop columns\n",
    "hcn_c404_point.drop([\"LATITUDE\", \"LONGITUDE\"], axis=1, inplace=True)\n",
    "\n",
    "hcn_c404_point.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time column to datetime type\n",
    "hcn_c404_point[\"time\"] = pd.to_datetime(hcn_c404_point[\"time\"], format=\"%Y-%m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary stastics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample to yearly means\n",
    "hcn_c404_yearly = hcn_c404_point.resample(\"1Y\", on=\"time\").mean()\n",
    "hcn_c404_yearly.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# mean, median, standard devation\n",
    "hcn_c404_mean = hcn_c404_yearly.mean()\n",
    "hcn_c404_median = hcn_c404_yearly.median()\n",
    "hcn_c404_stdev = hcn_c404_yearly.std()\n",
    "\n",
    "#create dataframe\n",
    "hcn_c404_stats = pd.DataFrame({\"annual_mean\": hcn_c404_mean, \"median\": hcn_c404_median, \"stdev\": hcn_c404_stdev}).T.drop(\"time\", axis=1)\n",
    "\n",
    "# reset index and rename\n",
    "hcn_c404_stats = hcn_c404_stats.reset_index(drop=False).rename({\"index\":\"stat\"}, axis=1)\n",
    "\n",
    "# bias\n",
    "hcn_c404_stats_annual_mean = hcn_c404_stats.loc[hcn_c404_stats['stat'] == \"annual_mean\"]\n",
    "hcn_c404_bias_precip = float(hcn_c404_stats_annual_mean[\"PREC_ACC_NC_c404\"] - hcn_c404_stats_annual_mean[\"PREC_ACC_NC_hcn\"])\n",
    "hcn_c404_bias_tk = float(hcn_c404_stats_annual_mean[\"TK_c404\"] - hcn_c404_stats_annual_mean[\"TK_hcn\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "hcn_c404_stats.loc[len(hcn_c404_stats.index)] = [\"bias\", hcn_c404_bias_precip, None, hcn_c404_bias_tk, None]\n",
    "\n",
    "# MAE\n",
    "hcn_c404_mae_precip = sum(abs(hcn_c404_yearly[\"PREC_ACC_NC_c404\"] - hcn_c404_yearly[\"PREC_ACC_NC_hcn\"]))/len(hcn_c404_yearly)\n",
    "hcn_c404_mae_tk = sum(abs(hcn_c404_yearly[\"TK_c404\"] - hcn_c404_yearly[\"TK_hcn\"]))/len(hcn_c404_yearly)\n",
    "# add stat to bottom of dataframe\n",
    "hcn_c404_stats.loc[len(hcn_c404_stats.index)] = [\"MAE\", hcn_c404_mae_precip, None, hcn_c404_mae_tk, None]\n",
    "\n",
    "# RMSE\n",
    "hcn_c404_rmse_precip = math.sqrt(np.square(np.subtract(hcn_c404_yearly[\"PREC_ACC_NC_c404\"], hcn_c404_yearly[\"PREC_ACC_NC_hcn\"])).mean())\n",
    "hcn_c404_rmse_tk = math.sqrt(np.square(np.subtract(hcn_c404_yearly[\"TK_c404\"], hcn_c404_yearly[\"TK_hcn\"])).mean())\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "hcn_c404_stats.loc[len(hcn_c404_stats.index)] = [\"RMSE\", hcn_c404_rmse_precip, None, hcn_c404_rmse_tk, None]\n",
    "\n",
    "# Pearsons correlation\n",
    "hcn_c404_pearson_precip = pearson_r(hcn_c404_yearly[\"PREC_ACC_NC_c404\"], hcn_c404_yearly[\"PREC_ACC_NC_hcn\"])\n",
    "hcn_c404_pearson_tk = pearson_r(hcn_c404_yearly[\"TK_c404\"], hcn_c404_yearly[\"TK_hcn\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "hcn_c404_stats.loc[len(hcn_c404_stats.index)] = [\"pearson\", hcn_c404_pearson_precip, None, hcn_c404_pearson_tk, None]\n",
    "\n",
    "# Spearman's correlation\n",
    "hcn_c404_spearman_precip = spearman_r(hcn_c404_yearly[\"PREC_ACC_NC_c404\"], hcn_c404_yearly[\"PREC_ACC_NC_hcn\"])\n",
    "hcn_c404_spearman_tk = spearman_r(hcn_c404_yearly[\"TK_c404\"], hcn_c404_yearly[\"TK_hcn\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "hcn_c404_stats.loc[len(hcn_c404_stats.index)] = [\"spearman\", hcn_c404_spearman_precip, None, hcn_c404_spearman_tk, None]\n",
    "\n",
    "# percent bias\n",
    "hcn_c404_pbias_precip = pbias(hcn_c404_yearly[\"PREC_ACC_NC_c404\"], hcn_c404_yearly[\"PREC_ACC_NC_hcn\"])\n",
    "hcn_c404_pbias_tk = pbias(hcn_c404_yearly[\"TK_c404\"], hcn_c404_yearly[\"TK_hcn\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "hcn_c404_stats.loc[len(hcn_c404_stats.index)] = [\"pbias\", hcn_c404_pbias_precip, None, hcn_c404_pbias_tk, None]\n",
    "\n",
    "hcn_c404_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hcn_c404_point.to_parquet(\"s3://nhgf-development/workspace/tutorial/CONUS404/hcn_c404_point.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shut down the client and cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close(); cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: CONUS404 Visualization notebook\n",
    "\n",
    "Now that we have moved through our zonal and point statistics, we can move on to visualizing the results in the CONUS404 Visualization notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Last code cell of the notebook\n",
    "# import watermark.watermark as watermark\n",
    "# print(watermark(iversions=True, python=True, machine=True, globals_=globals()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-users-pangeo",
   "language": "python",
   "name": "conda-env-users-users-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "6b8b815d206080047d0881750c260f2e84eb4576ca4137e2355fa3de469693c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
