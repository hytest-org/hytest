{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create zonal statistics and point extractions for comparing CONUS404 and reference datasets\n",
    "\n",
    "<img src='./Eval_PreProc.svg' width=600>\n",
    "\n",
    "The pre-processing step is needed in order to align the two datasets for analysis.  The specific \n",
    "steps needed to prepare a given dataset may differ, depending on the source and the variable of\n",
    "interest. \n",
    "\n",
    "Some steps might include: \n",
    "\n",
    "* Organizing the time-series index such that the time steps for both simulated and observed are congruent\n",
    "    * This may involve interpolation to estimate a more granular time-step than is found in the source data\n",
    "    * More often, an agregating function is used to 'down-sample' the dataset to a coarser time step (days vs hours).\n",
    "* Coordinate aggregation units between simulated and observed \n",
    "    * Gridded data may be sampled per HUC-12, HUC-6, etc. to match modeled data indexed by these units. \n",
    "    * Index formats may be adjusted (e.g. a 'gage_id' may be 'USGS-01104200' in one data set, vs '01104200' in another)\n",
    "* Re-Chunking the data to make time-series analysis more efficient (see [here](/dev/null) for a primer on re-chunking).\n",
    "<details>\n",
    "  <summary>Guide to pre-requisites and learning outcomes...&lt;click to expand&gt;</summary>\n",
    "  \n",
    "  <table>\n",
    "    <tr>\n",
    "      <td>Pre-Requisites\n",
    "      <td>To get the most out of this notebook, you should already have an understanding of these topics: \n",
    "        <ul>\n",
    "        <li>pre-req one\n",
    "        <li>pre-req two\n",
    "        </ul>\n",
    "    <tr>\n",
    "      <td>Expected Results\n",
    "      <td>At the end of this notebook, you should be able to: \n",
    "        <ul>\n",
    "        <li>outcome one\n",
    "        <li>outcome two\n",
    "        </ul>\n",
    "  </table>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# library imports\n",
    "import fsspec #testing\n",
    "import hvplot.xarray #testing\n",
    "import intake #testing\n",
    "import os #testing\n",
    "import warnings #testing\n",
    "import rioxarray #testing\n",
    "import dask #testing\n",
    "import metpy #testing\n",
    "import calendar #testing\n",
    "import sparse \n",
    "import cf_xarray\n",
    "import pygeohydro\n",
    "\n",
    "from shapely.geometry import Polygon #testing\n",
    "from dask.distributed import LocalCluster, Client #testing\n",
    "from fsspec.implementations.ftp import FTPFileSystem #testing\n",
    "from holoviews.streams import PolyEdit, PolyDraw #testing\n",
    "from geocube.api.core import make_geocube #testing\n",
    "\n",
    "import xarray as xr #testing\n",
    "import geopandas as gpd #testing\n",
    "import pandas as pd #testing\n",
    "import geoviews as gv #testing\n",
    "import dask.dataframe as dd #testing\n",
    "import numpy as np #testing\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ceres_drb = xr.open_dataset(fs_read.open('s3://rsignellbucket2/hytest/tutorials/conus404_model_evaluation/ceres_drb.nc'), chunks={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update to helper function after repo consolidation\n",
    "## **Start a Dask client using an appropriate Dask Cluster** \n",
    "This is an optional step, but can speed up data loading significantly, especially when accessing data from the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_cluster(machine):\n",
    "    ''' Helper function to configure cluster\n",
    "    '''\n",
    "    if machine == 'denali':\n",
    "        from dask.distributed import LocalCluster, Client\n",
    "        cluster = LocalCluster(threads_per_worker=1)\n",
    "        client = Client(cluster)\n",
    "    \n",
    "    elif machine == 'tallgrass':\n",
    "        from dask.distributed import Client\n",
    "        from dask_jobqueue import SLURMCluster\n",
    "        cluster = SLURMCluster(queue='cpu', cores=1, interface='ib0',\n",
    "                               job_extra=['--nodes=1', '--ntasks-per-node=1', '--cpus-per-task=1'],\n",
    "                               memory='6GB')\n",
    "        cluster.adapt(maximum_jobs=30)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "    elif machine == 'local':\n",
    "        import os\n",
    "        import warnings\n",
    "        from dask.distributed import LocalCluster, Client\n",
    "        warnings.warn(\"Running locally can result in costly data transfers!\\n\")\n",
    "        n_cores = os.cpu_count() # set to match your machine\n",
    "        cluster = LocalCluster(threads_per_worker=n_cores)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "    elif machine in ['esip-qhub-gateway-v0.4']:   \n",
    "        import sys, os\n",
    "        sys.path.append(os.path.join(os.environ['HOME'],'shared','users','lib'))\n",
    "        import ebdpy as ebd\n",
    "        aws_profile = 'esip-qhub'  \n",
    "        ebd.set_credentials(profile=aws_profile)\n",
    "\n",
    "        aws_region = 'us-west-2'\n",
    "        endpoint = f's3.{aws_region}.amazonaws.com'\n",
    "        ebd.set_credentials(profile=aws_profile, region=aws_region, endpoint=endpoint)\n",
    "        worker_max = 30\n",
    "        client,cluster = ebd.start_dask_cluster(profile=aws_profile, worker_max=worker_max, \n",
    "                                              region=aws_region, use_existing_cluster=True,\n",
    "                                              adaptive_scaling=True, wait_for_cluster=False, \n",
    "                                              worker_profile='Medium Worker', propagate_env=True)\n",
    "        \n",
    "    return client, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup your cluster\n",
    "\n",
    "#### QHub...\n",
    "Uncomment single commented spaces (#) to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set machine\n",
    "machine = 'esip-qhub-gateway-v0.4'\n",
    "\n",
    "# use configure cluster helper function to setup dask\n",
    "client, cluster = configure_cluster(machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### or HPC\n",
    "Uncomment single commented spaces (#) to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set machine\n",
    "# machine = os.environ['SLURM_CLUSTER_NAME']\n",
    "\n",
    "## use configure_cluster helper function to setup dask\n",
    "# client, cluster = configure_cluster(machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset outline:\n",
    "1. Read in the prepared dataset\n",
    "2. Read in the HUC6 boundaries and transform to same coordinate reference system as prepared dataset\n",
    "3. Make a data mask with the HUC6 boundaries to calculate zonal statistics\n",
    "4. Compute zonal statistics with data mask and prepared data\n",
    "\n",
    "Once all calculations are done: \n",
    "\n",
    "5. Combine each reference with benchmark into single dataset\n",
    "6. Export gridded data zonal statistics\n",
    "<br>\n",
    "\n",
    "**CONUS404 zonal statistics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Compute zonal statistics for gridded datasets**\n",
    "\n",
    "In the last tutorial, we prepared three gridded datasets: CONUS404 (benchmark), PRISM (reference), and CERES-EBAF (reference). The goal of this section is compute [zonal statistics](https://gisgeography.com/zonal-statistics/) for each HUC6 zone in the Delaware River Basin (DRB) by using the [conservative regridding method put forth by Ryan Abernathy](https://discourse.pangeo.io/t/conservative-region-aggregation-with-xarray-geopandas-and-sparse/2715) to regrid and perform an area-weight analysis.\n",
    "\n",
    "Dataset outline:\n",
    "<ol>\n",
    "    <li>Read in the prepared dataset</li>\n",
    "    <li>Compute bounding bands for latitude and longitude (if necessary) then use these to create polygons in area-preserving CRS</li>\n",
    "    <li>Read in the HUC6 boundaries and transform to same coordinate reference system as prepared dataset</li>\n",
    "    <li>Overlay the dataset polygons over the HUC6 boundaries and create spatial weights matrices</li>\n",
    "    <li>Perform matrix multiplication between The prepared dataset and the spatial weights matrices</li>\n",
    "    <li>Perform zonal statistics</li>\n",
    "</ol>\n",
    "\n",
    "The following two functions will be used for regridding each dataset. Review them for now and an explanation of what they do will be provided when they are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounds_to_poly(x_bounds, y_bounds):\n",
    "    \"\"\"Return a polygon based on the x (longitude) and y (longitude) bounding band DataArrays\"\"\"\n",
    "    return Polygon([\n",
    "        (x_bounds[0], y_bounds[0]),\n",
    "        (x_bounds[0], y_bounds[1]),\n",
    "        (x_bounds[1], y_bounds[1]),\n",
    "        (x_bounds[1], y_bounds[0])\n",
    "    ])\n",
    "\n",
    "def apply_weights_matmul_sparse(weights, data):\n",
    "    \"\"\"Apply weights in a sparse matrices to data and regrid\"\"\"\n",
    "    assert isinstance(weights, sparse.SparseArray)\n",
    "    assert isinstance(data, np.ndarray)\n",
    "    data = sparse.COO.from_numpy(data)\n",
    "    data_shape = data.shape\n",
    "    n, k = data_shape[0], data_shape[1] * data_shape[2]\n",
    "    data = data.reshape((n, k))\n",
    "    weights_shape = weights.shape\n",
    "    k_, m = weights_shape[0] * weights_shape[1], weights_shape[2]\n",
    "    assert k == k_\n",
    "    weights_data = weights.reshape((k, m))\n",
    "\n",
    "    regridded = sparse.matmul(data, weights_data)\n",
    "    assert regridded.shape == (n, m)\n",
    "    return regridded.todense()\n",
    "\n",
    "    assert isinstance(weights, sparse.SparseArray)\n",
    "    assert isinstance(data, np.ndarray)\n",
    "    data = sparse.COO.from_numpy(data)\n",
    "    data_shape = data.shape\n",
    "    # k = nlat * nlon\n",
    "    n, k = data_shape[0], data_shape[1] * data_shape[2]\n",
    "    data = data.reshape((n, k))\n",
    "    weights_shape = weights.shape\n",
    "    k_, m = weights_shape[0] * weights_shape[1], weights_shape[2]\n",
    "    assert k == k_\n",
    "    weights_data = weights.reshape((k, m))\n",
    "\n",
    "    regridded = sparse.matmul(data, weights_data)\n",
    "    assert regridded.shape == (n, m)\n",
    "    return regridded.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the following `fsspec.filesystem` will be using to read in each dataset from an [Open Storage Network](https://www.openstoragenetwork.org/) bucket, which is read only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_read = fsspec.filesystem('s3', anon=True, skip_instance_cache=True,\n",
    "                            client_kwargs={'endpoint_url': 'https://renc.osn.xsede.org'})\n",
    "\n",
    "x = \"x\"\n",
    "y = \"y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the geometries for the DRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in HUC6 boundaries found in the DRB\n",
    "drb_gdf = pygeohydro.WBD(\"huc6\", outfields=[\"huc6\", \"name\"]).byids(\"huc6\", [\"020401\", \"020402\"])\n",
    "\n",
    "# area preserving crs\n",
    "crs_area = \"ESRI:53034\"\n",
    "\n",
    "# set CRS to match c404_drb\n",
    "drb_gdf = drb_gdf.to_crs(crs_area)\n",
    "\n",
    "#visualize\n",
    "drb_gdf.plot(edgecolor=\"orange\", facecolor=\"purple\", linewidth=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONUS404 zonal statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# c404_drb = xr.open_dataset(fs_read.open('s3://rsignellbucket2/hytest/tutorials/conus404_model_evaluation/c404_drb.nc'), chunks={})\n",
    "# url to c404_drb\n",
    "c404_drb_url = 's3://rsignellbucket2/hytest/tutorials/conus404_model_evaluation/c404_drb.nc'\n",
    "\n",
    "# open dataset\n",
    "c404_drb = xr.open_dataset(fs_read.open(c404_drb_url), decode_coords='all') #, chunks={}\n",
    "\n",
    "# crs\n",
    "c404_crs = c404_drb.rio.crs.to_proj4()\n",
    "\n",
    "# c404_drb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_drb.PREC_ACC_NC.hvplot(x=\"x\", y=\"y\", rasterize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the grid of c404_drb using any of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vars\n",
    "c404_var = \"TK\"\n",
    "\n",
    "# drop unneeded variable and coordinates\n",
    "c404_grid = c404_drb[[c404_var]].drop(['time', 'lon', 'lat', c404_var]).reset_coords().load() #load in Richs code\n",
    "c404_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create bounding bands then stack into points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bounds\n",
    "c404_grid = c404_grid.cf.add_bounds(x)\n",
    "c404_grid = c404_grid.cf.add_bounds(y)\n",
    "\n",
    "# stack\n",
    "c404_points = c404_grid.stack(point=(y,x))\n",
    "c404_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the `xarray apply_ufunc` function to apply the `bounds_to_poly` function above to the _c404_points_ DataSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_boxes = xr.apply_ufunc(\n",
    "    bounds_to_poly,\n",
    "    c404_points.x_bounds,\n",
    "    c404_points.y_bounds,\n",
    "    input_core_dims=[(\"bounds\",),  (\"bounds\",)],\n",
    "    output_dtypes=[np.dtype('O')],\n",
    "    vectorize=True\n",
    ")\n",
    "c404_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create geodataframe from boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_grid_df= gpd.GeoDataFrame(\n",
    "    data={\"geometry\": c404_boxes.values, \"y\": c404_boxes[y], \"x\": c404_boxes[x]},\n",
    "    index=c404_boxes.indexes[\"point\"],\n",
    "    crs=c404_crs\n",
    ")\n",
    "c404_grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_grid_df.plot(edgecolor=\"red\", facecolor=\"white\", linewidth=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlay the two grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert DRB to conus404 crs\n",
    "c404_drb_gdf = drb_gdf.to_crs(c404_crs)\n",
    "\n",
    "#perform overly\n",
    "c404_overlay = c404_grid_df.overlay(c404_drb_gdf, keep_geom_type=True)\n",
    "c404_overlay.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot overlay for single HUC6\n",
    "c404_overlay[c404_overlay.huc6 == \"020402\"].geometry.plot(edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid cell fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_grid_cell_fraction = c404_overlay.geometry.area.groupby(c404_overlay.huc6).transform(lambda x: x / x.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse DataArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_multi_index = c404_overlay.set_index([y, x, \"huc6\"]).index\n",
    "c404_df_weights = pd.DataFrame({\"weights\": c404_grid_cell_fraction.values}, index=c404_multi_index)\n",
    "\n",
    "c404_ds_weights = xr.Dataset(c404_df_weights)\n",
    "\n",
    "c404_weights_sparse = c404_ds_weights.unstack(sparse=True, fill_value=0.).weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication across each DataArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "    c404_precip_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        c404_weights_sparse,\n",
    "        c404_drb[\"PREC_ACC_NC\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )\n",
    "\n",
    "    c404_rnet_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        c404_weights_sparse,\n",
    "        c404_drb[\"RNET\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )\n",
    "\n",
    "    c404_tk_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        c404_weights_sparse,\n",
    "        c404_drb[\"TK\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge DataArrays into Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_regridded = xr.Dataset({\"PREC_NC_ACC\":c404_precip_regridded, \"RNET\":c404_rnet_regridded, \"TK\": c404_tk_regridded})\n",
    "c404_regridded = c404_regridded.drop(\"crs\")\n",
    "c404_regridded.attrs = c404_drb.attrs\n",
    "c404_regridded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_df = c404_regridded.load().to_dataframe()\n",
    "c404_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRISM zonal statistics**\n",
    "\n",
    "PRISM has two variables: TK and PREC_ACC_NC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url to prism_drb\n",
    "prism_drb_url = 's3://nhgf-development/workspace/tutorial/CONUS404/prism_drb.nc'\n",
    "\n",
    "# open dataset\n",
    "prism_drb = xr.open_dataset(fs_read.open('s3://rsignellbucket2/hytest/tutorials/conus404_model_evaluation/prism_drb.nc'))\n",
    "\n",
    "# Create the grid of c404_drb using any of the variables\n",
    "\n",
    "# set vars\n",
    "prism_var = \"TK\"\n",
    "\n",
    "# drop unneeded variable and coordinates\n",
    "prism_grid = prism_drb[[prism_var]].drop(['time', 'lon', 'lat', prism_var]).reset_coords().load()\n",
    "\n",
    "\n",
    "# create bounding bands then stack into points\n",
    "# add bounds\n",
    "c404_grid = c404_grid.cf.add_bounds(x)\n",
    "c404_grid = c404_grid.cf.add_bounds(y)\n",
    "\n",
    "# stack\n",
    "c404_points = c404_grid.stack(point=(y,x))\n",
    "\n",
    "# apply bounds_to\n",
    "c404_boxes = xr.apply_ufunc(\n",
    "    bounds_to_poly,\n",
    "    c404_points.x_bounds,\n",
    "    c404_points.y_bounds,\n",
    "    input_core_dims=[(\"bounds\",),  (\"bounds\",)],\n",
    "    output_dtypes=[np.dtype('O')],\n",
    "    vectorize=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create geodataframe from boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_grid_df= gpd.GeoDataFrame(\n",
    "    data={\"geometry\": c404_boxes.values, \"y\": c404_boxes[y], \"x\": c404_boxes[x]},\n",
    "    index=c404_boxes.indexes[\"point\"],\n",
    "    crs=c404_crs\n",
    ")\n",
    "c404_grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_grid_df.plot(edgecolor=\"red\", facecolor=\"white\", linewidth=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlay the two grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert DRB to conus404 crs\n",
    "c404_drb_gdf = drb_gdf.to_crs(c404_crs)\n",
    "\n",
    "#perform overly\n",
    "c404_overlay = c404_grid_df.overlay(c404_drb_gdf, keep_geom_type=True)\n",
    "c404_overlay.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot overlay for single HUC6\n",
    "c404_overlay[c404_overlay.huc6 == \"020402\"].geometry.plot(edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid cell fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_grid_cell_fraction = c404_overlay.geometry.area.groupby(c404_overlay.huc6).transform(lambda x: x / x.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse DataArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_multi_index = c404_overlay.set_index([y, x, \"huc6\"]).index\n",
    "c404_df_weights = pd.DataFrame({\"weights\": c404_grid_cell_fraction.values}, index=c404_multi_index)\n",
    "\n",
    "c404_ds_weights = xr.Dataset(c404_df_weights)\n",
    "\n",
    "c404_weights_sparse = c404_ds_weights.unstack(sparse=True, fill_value=0.).weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication across each DataArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "    c404_precip_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        c404_weights_sparse,\n",
    "        c404_drb[\"PREC_ACC_NC\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )\n",
    "\n",
    "    c404_rnet_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        c404_weights_sparse,\n",
    "        c404_drb[\"RNET\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )\n",
    "\n",
    "    c404_tk_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        c404_weights_sparse,\n",
    "        c404_drb[\"TK\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge DataArrays into Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_regridded = xr.Dataset({\"PREC_NC_ACC\":c404_precip_regridded, \"RNET\":c404_rnet_regridded, \"TK\": c404_tk_regridded})\n",
    "c404_regridded = c404_regridded.drop(\"crs\")\n",
    "c404_regridded.attrs = c404_drb.attrs\n",
    "c404_regridded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_df = c404_regridded.load().to_dataframe()\n",
    "c404_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# url to prism_drb\n",
    "prism_drb_url = 's3://nhgf-development/workspace/tutorial/CONUS404/prism_drb.nc'\n",
    "\n",
    "fs = fsspec.filesystem(\"s3\", anon=False, requester_pays=True, skip_instance_cache=True)\n",
    "\n",
    "# open dataset\n",
    "prism_drb = xr.open_dataset(fs.open(prism_drb_url), decode_coords=\"all\")\n",
    "\n",
    "# create an output grid\n",
    "prism_out_grid = make_geocube(\n",
    "    vector_data = drb_gdf,\n",
    "    measurements=[\"huc6\"],\n",
    "    like=prism_drb\n",
    ")\n",
    "\n",
    "# add datarrays to grid\n",
    "prism_out_grid[\"TK\"] = (prism_drb.TK.dims, prism_drb.TK.values,\n",
    "                         prism_drb.TK.attrs, prism_drb.TK.encoding)\n",
    "\n",
    "prism_out_grid[\"PREC_ACC_NC\"] = (prism_drb.PREC_ACC_NC.dims, prism_drb.PREC_ACC_NC.values,\n",
    "                         prism_drb.PREC_ACC_NC.attrs, prism_drb.PREC_ACC_NC.encoding)\n",
    "\n",
    "# groupby\n",
    "prism_grouped = prism_out_grid.drop_vars(\"spatial_ref\").groupby(prism_out_grid.huc6)\n",
    "\n",
    "# Calculate the mean variables\n",
    "prism_grid_mean = prism_grouped.mean().rename({\"TK\": \"prism_TK_mean\", \n",
    "                                       \"PREC_ACC_NC\": \"prism_PREC_ACC_NC_mean\", \"time\":\"time_index\"})\n",
    "\n",
    "#convert to a dataframe\n",
    "prism_zonal_stats = prism_grid_mean.to_dataframe().drop(\"spatial_ref\", axis=1)\n",
    "\n",
    "# reste index and add time back\n",
    "prism_zonal_stats = prism_zonal_stats.reset_index(drop=False)\n",
    "prism_zonal_stats[\"time\"] = prism_drb.coords[\"time\"][prism_zonal_stats[\"time_index\"].values]\n",
    "prism_zonal_stats.drop(\"time_index\", axis=1, inplace=True)\n",
    "prism_zonal_stats[\"time\"] = prism_zonal_stats[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "# change huc6 to string and pad with zeros\n",
    "prism_zonal_stats[\"huc6\"] = prism_zonal_stats[\"huc6\"].astype(int).astype(str).str.zfill(6) # pads with 0's to make all column values lenght == 0\n",
    "\n",
    "prism_zonal_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the PRISM and CONUS404 zonals stats together based on the HUC6 code and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_c404_zonal = prism_zonal_stats.merge(c404_zonal_stats, left_on=['huc6', 'time'], right_on=['huc6', 'time'])\n",
    "prism_c404_zonal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need the CONUS404 RNET value so we'll drop that column before exporting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_c404_zonal.drop(\"c404_RNET_mean\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_c404_zonal.to_parquet(\"s3://nhgf-development/workspace/tutorial/CONUS404/prism_c404_zonal.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CERES-EBAF zonal statistics**\n",
    "\n",
    "CERES-EBAF has a single variable: RNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url to ceres_drb\n",
    "ceres_drb_url = 's3://nhgf-development/workspace/tutorial/CONUS404/ceres_drb.nc'\n",
    "\n",
    "fs = fsspec.filesystem(\"s3\", anon=False, requester_pays=True, skip_instance_cache=True)\n",
    "\n",
    "# open dataset\n",
    "ceres_drb = xr.open_dataset(fs.open(ceres_drb_url), decode_coords=\"all\", chunks={\"time\":10})\n",
    "    \n",
    "# create an output grid\n",
    "ceres_out_grid = make_geocube(\n",
    "    vector_data = drb_gdf,\n",
    "    measurements=[\"huc6\"],\n",
    "    like=ceres_drb\n",
    ")\n",
    "\n",
    "# add datarrays to grid\n",
    "ceres_out_grid[\"RNET\"] = (ceres_drb.RNET.dims, ceres_drb.RNET.values,\n",
    "                         ceres_drb.RNET.attrs, ceres_drb.RNET.encoding)\n",
    "\n",
    "# groupby\n",
    "ceres_grouped = ceres_out_grid.drop_vars(\"spatial_ref\").groupby(ceres_out_grid.huc6)\n",
    "\n",
    "# Calculate the mean variables\n",
    "ceres_grid_mean = ceres_grouped.mean().rename({\"RNET\": \"ceres_RNET_mean\", \"time\":\"time_index\"})\n",
    "\n",
    "#convert to a dataframe\n",
    "ceres_zonal_stats = ceres_grid_mean.to_dataframe().drop(\"spatial_ref\", axis=1)\n",
    "\n",
    "# reste index and add time back\n",
    "ceres_zonal_stats = ceres_zonal_stats.reset_index(drop=False)\n",
    "ceres_zonal_stats[\"time\"] = ceres_drb.coords[\"time\"][ceres_zonal_stats[\"time_index\"].values]\n",
    "ceres_zonal_stats.drop(\"time_index\", axis=1, inplace=True)\n",
    "ceres_zonal_stats[\"time\"] = ceres_zonal_stats[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "# change huc6 to string and pad with zeros\n",
    "ceres_zonal_stats[\"huc6\"] = ceres_zonal_stats[\"huc6\"].astype(int).astype(str).str.zfill(6) # pads with 0's to make all column values lenght == 0\n",
    "\n",
    "ceres_zonal_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the CERES-EBAF and CONUS404 zonals stats together based on the HUC6 code and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceres_c404_zonal = ceres_zonal_stats.merge(c404_zonal_stats, left_on=['huc6', 'time'], right_on=['huc6', 'time'])\n",
    "ceres_c404_zonal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need the CONUS404 TK and PREC_ACC_NC values so we'll drop these columns before exporting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceres_c404_zonal.drop([\"c404_TK_mean\", \"c404_PREC_ACC_NC_mean\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceres_c404_zonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ceres_c404_zonal.to_parquet(\"s3://nhgf-development/workspace/tutorial/CONUS404/ceres_c404_zonal.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Extract gridded values to points**\n",
    "\n",
    "The goal of this section is extract values from CONUS404 where they intersect with station data. This process is described in article about the ESRI tool [Extract Values to Points](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-analyst/extract-values-to-points.htm). This tabular data will then be exported for use in the next notebook, **CONUS404 Analysis**.\n",
    "\n",
    "Dataset outline:\n",
    "1. Read in the prepared dataset\n",
    "2. Extract data from overlapping pixel at same time step as point\n",
    "<br>\n",
    "\n",
    "**Climate Reference Network point extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(\"s3\", anon=False, requester_pays=True, skip_instance_cache=True)\n",
    "\n",
    "crn_drb_df = pd.read_parquet(fs.open(\"s3://nhgf-development/workspace/tutorial/CONUS404/crn_drb.parquet\"))\n",
    "\n",
    "# create geodataframe\n",
    "crn_drb = gpd.GeoDataFrame(crn_drb_df, crs=4326,\n",
    "                       geometry=gpd.points_from_xy(crn_drb_df.LONGITUDE, \n",
    "                                                         crn_drb_df.LATITUDE))\n",
    "\n",
    "# modify date field\n",
    "crn_drb[\"DATE\"] = crn_drb[\"DATE\"].astype(str).str[:-3]\n",
    "\n",
    "crn_drb.rename({\"DATE\": \"time\",\n",
    "                \"TK\": \"crn_TK\", \n",
    "                \"RNET\": \"crn_RNET\", \n",
    "                \"PREC_ACC_NC\": \"crn_PREC_ACC_NC\"},\n",
    "                  axis=1, inplace=True)\n",
    "\n",
    "crn_drb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get coordinates from crn_drb to index c404_drb by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# isolate single row and transform to c404_drb crs\n",
    "crn_coords_gdf = crn_drb.iloc[[0]].to_crs(c404_crs)\n",
    "\n",
    "# extract lat/long values\n",
    "crn_lat = crn_coords_gdf.iloc[0][\"geometry\"].y\n",
    "crn_lon = crn_coords_gdf.iloc[0][\"geometry\"].x\n",
    "\n",
    "# time\n",
    "crn_time_min = crn_drb_df[\"time\"].min()\n",
    "crn_time_max = crn_drb_df[\"time\"].max()\n",
    "crn_time_min, crn_time_max\n",
    "\n",
    "# subset c404_drb to lat/long using nearest\n",
    "c404_crn_sub = c404_drb.sel(x=crn_lon, y=crn_lat, method=\"nearest\")\n",
    "\n",
    "# slice to time-steps of crn_drb\n",
    "c404_crn_sub = c404_crn_sub.sel(time=slice(crn_time_min, crn_time_max))\n",
    "\n",
    "c404_crn_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert subset to dataframe and reorganize columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_sub_crn_df = c404_crn_sub.to_dataframe().reset_index(drop=False)\n",
    "\n",
    "# trim columns\n",
    "c404_sub_crn_df = c404_sub_crn_df[[\"time\", \"TK\", \"RNET\", \"PREC_ACC_NC\"]]\n",
    "\n",
    "# rename columns\n",
    "c404_sub_crn_df.rename({\"TK\": \"c404_TK\", \n",
    "                    \"RNET\": \"c404_RNET\", \n",
    "                    \"PREC_ACC_NC\": \"c404_PREC_ACC_NC\"},\n",
    "                  axis=1, inplace=True)\n",
    "\n",
    "# trim time\n",
    "c404_sub_crn_df[\"time\"] = c404_sub_crn_df[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "c404_sub_crn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine CONUS404 subset with CRN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_c404_point = crn_drb.merge(c404_sub_crn_df, on=\"time\").reset_index(drop=False)\n",
    "\n",
    "# drop columns\n",
    "crn_c404_point.drop([\"index\", \"LATITUDE\", \"LONGITUDE\", \"ID\", \"geometry\"], axis=1, inplace=True)\n",
    "\n",
    "crn_c404_point.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_c404_point.to_parquet(\"s3://nhgf-development/workspace/tutorial/CONUS404/crn_c404_point.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Historical Climate Network (HCN) point extraction**\n",
    "\n",
    "The HCN data is different than the CRN data as the HCN data comes from multiple stations whereas the CRN data was from a single station. This will involve using multiple sets of geographic coordinates to extract data from CONUS404."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the HCN dataset\n",
    "fs = fsspec.filesystem(\"s3\", anon=False, requester_pays=True, skip_instance_cache=True)\n",
    "\n",
    "hcn_drb_df = pd.read_parquet(fs.open(\"s3://nhgf-development/workspace/tutorial/CONUS404/hcn_drb.parquet\"))\n",
    "\n",
    "#rename columns\n",
    "hcn_drb_df.rename({\"DATE\": \"time\",\n",
    "                \"TK\": \"hcn_TK\",  \n",
    "                \"PREC_ACC_NC\": \"hcn_PREC_ACC_NC\"},\n",
    "                  axis=1, inplace=True)\n",
    "\n",
    "# change DATE field to \n",
    "hcn_drb_df[\"time\"] = hcn_drb_df[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "hcn_drb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a DataFrame of the station IDs, lats, and longs to use for extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hcn_stations = hcn_drb_df.copy().drop([\"time\", \"hcn_TK\", \"hcn_PREC_ACC_NC\"], axis=1)\n",
    "hcn_stations[\"LONGITUDE\"] = pd.to_numeric(hcn_stations[\"LONGITUDE\"])\n",
    "hcn_stations[\"LATITUDE\"] = pd.to_numeric(hcn_stations[\"LATITUDE\"])\n",
    "\n",
    "hcn_stations = hcn_stations.groupby('ID').mean().reset_index(drop=False)\n",
    "# hcn_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a GeoDataFrame to convert the lat and long to the coordinate system of CONUS404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_stations_gdf = gpd.GeoDataFrame(hcn_stations, crs=4326,\n",
    "                       geometry=gpd.points_from_xy(hcn_stations.LONGITUDE, \n",
    "                                                         hcn_stations.LATITUDE))\n",
    "\n",
    "# transform to c404_drb crs\n",
    "hcn_stations_gdf = hcn_stations_gdf.to_crs(c404_crs)\n",
    "\n",
    "# extract lat/long values\n",
    "hcn_stations_gdf[\"y\"] = hcn_stations_gdf[\"geometry\"].y\n",
    "hcn_stations_gdf[\"x\"] = hcn_stations_gdf[\"geometry\"].x\n",
    "\n",
    "#drop lat/lon/geo\n",
    "hcn_stations_df = hcn_stations_gdf.drop([\"LATITUDE\", \"LONGITUDE\", \"geometry\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset c404_drb to time period of HCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time min/max\n",
    "hcn_time_min = hcn_drb_df[\"time\"].min()\n",
    "hcn_time_max = hcn_drb_df[\"time\"].max()\n",
    "\n",
    "# slice c404 to HCN time\n",
    "c404_hcn_timesub = c404_drb.sel(time=slice(hcn_time_min, hcn_time_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Dataframe rows to extract data from c404_drb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list of extracted data\n",
    "c404_hcn_subs = []\n",
    "\n",
    "for index, data in hcn_stations_df.iterrows():\n",
    "    c404_hcn_sub_step = c404_hcn_timesub.sel(x=data.x, y=data.y, method=\"nearest\").to_dataframe()\n",
    "    c404_hcn_sub_step[\"ID\"] = data.ID\n",
    "    c404_hcn_subs.append(c404_hcn_sub_step)\n",
    "\n",
    "# concat list of extracted data into single Dataframe\n",
    "c404_hcn_sub = pd.concat(c404_hcn_subs)\n",
    "\n",
    "#reset index\n",
    "c404_hcn_sub.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# drop columns\n",
    "c404_hcn_sub.drop([\"RNET\", \"lon\", \"lat\", \"y\", \"x\", \"crs\"], axis=1, inplace=True)\n",
    "\n",
    "# rename columns\n",
    "c404_hcn_sub.rename({\"TK\":\"c404_TK\",\n",
    "                    \"PREC_ACC_NC\": \"c404_PREC_ACC_NC\"},\n",
    "                   axis=1, inplace=True)\n",
    "\n",
    "# trim time\n",
    "c404_hcn_sub[\"time\"] = c404_hcn_sub[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "# c404_hcn_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge CONUS404 observations to HCH observations using the station ID and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_c404_point = hcn_drb_df.merge(c404_hcn_sub, left_on=[\"ID\", \"time\"], right_on=[\"ID\", \"time\"])\n",
    "\n",
    "# drop columns\n",
    "hcn_c404_point.drop([\"LATITUDE\", \"LONGITUDE\"], axis=1, inplace=True)\n",
    "\n",
    "hcn_c404_point.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_c404_point.to_parquet(\"s3://nhgf-development/workspace/tutorial/CONUS404/hcn_c404_point.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check all of the files that have been created in the data preparation notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(\"s3\", anon=False, requester_pays=True, skip_instance_cache=True)\n",
    "\n",
    "fs.ls(\"s3://nhgf-development/workspace/tutorial/CONUS404\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shut down the client and cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close(); cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: CONUS404 Analysis notebook\n",
    "\n",
    "Now that we have moved through our data preparation and calculated zonal and point statistics, we can move on to analyzing the differences between CONUS404 and the reference data in the CONUS404 Analysis notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Last code cell of the notebook\n",
    "# import watermark.watermark as watermark\n",
    "# print(watermark(iversions=True, python=True, machine=True, globals_=globals()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-users-pangeo",
   "language": "python",
   "name": "conda-env-users-users-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "6b8b815d206080047d0881750c260f2e84eb4576ca4137e2355fa3de469693c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
