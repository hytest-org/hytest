{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Chunking Larger Datasets \n",
    "\n",
    "This notebook extends ideas covered in the [basic workflow](./ReChunkingData.ipynb).  This \n",
    "notebook will perfrom the same operations, but will work on the **much** larger dataset, and \n",
    "involve some parallelization using the dask scheduler. \n",
    "\n",
    ":::{Warning}\n",
    "\n",
    "You should run this **only** on a cloud compute node -- on ESIP Nebari, for example. We \n",
    "will be reading and writing **enormous** amounts of data to S3 buckets. To do that over a \n",
    "typical network connection will saturate your bandwidth and take days to complete.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import dask\n",
    "import intake\n",
    "\n",
    "# Activate logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plumb Data Source\n",
    "We're going to look at a particular dataset from the National Water Model Reanalysis Version 2.1. \n",
    "The dataset is part of the AWS Open Data Program, and is included in the HyTEST data catalog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml'\n",
    "cat = intake.open_catalog(url)\n",
    "cat['nwm21-streamflow-cloud']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the zarr data\n",
    "We'll take advantage of the `intake` mechanism and load the data \n",
    "directly.  We'll need to set up our AWS credentials first, since\n",
    "this data is stored on an S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_PROFILE'] = \"osn-renci\"\n",
    "os.environ['AWS_S3_ENDPOINT'] = \"https://renc.osn.xsede.org\"\n",
    "%run ../../../environment_set_up/Help_AWS_Credentials.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = cat['nwm21-streamflow-cloud'].to_dask()\n",
    "indexer = ds.gage_id != ''.rjust(15).encode()\n",
    "smplData = ds.where(indexer.compute(), drop=True) # subset to only those features with a valid gage_id\n",
    "smplData.drop('crs') # Not needed/wanted for this analysis\n",
    "smplData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restrict for Tutorial\n",
    "This is a demonstration workflow, which means we don't really need to work on the entire \n",
    "dataset -- which is very large. We are going to cut this input dataset down to be just the\n",
    "first 100 `feature_id` values, so that this tutorial will run in reasonable time. \n",
    "\n",
    "For processing a full-sized dataset, you'd just skip this step where we slice off a representative\n",
    "example of the data. Expect run time to increase in proportion to the size of the data being\n",
    "processed. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smplData = smplData.isel(feature_id=slice(0, 100))\n",
    "smplData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this data hass the full range of time values (367439 time steps) and all of the \n",
    "variables (`streamflow`, `velocity`, ...). We just selected 100 feature_ids to work with so \n",
    "that the example data will execute more quickly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spin up Dask Cluster\n",
    "Our rechunking operation will be able to work in parallel. To do that, we will\n",
    "spin up a `dask` cluster on the cloud hardware to schedule the various workers.\n",
    "Note that this cluster must be configured with a specific user **profile** with \n",
    "permissions to write to our eventual output location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../../environment_set_up/Start_Dask_Cluster_Nebari.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Chunk Plan\n",
    "We will configure a new chunking plan which will favor time-series analysis. \n",
    "Using the dimensions of the data: \n",
    "* 367439 time steps\n",
    "* 100 feature IDs\n",
    "\n",
    "We can write the new plan as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The new chunking plan:\n",
    "chunk_plan = {\n",
    "    'streamflow': {'time': 367439, 'feature_id': 1}, # all time records in one chunk for each feature_id\n",
    "    'velocity': {'time': 367439, 'feature_id': 1},\n",
    "    'elevation': (100,),\n",
    "    'gage_id': (100,),\n",
    "    'latitude': (100,),\n",
    "    'longitude': (100,),    \n",
    "    'order': (100,),    \n",
    "    'time': (367439,), # all time coordinates in one chunk\n",
    "    'feature_id': (100,) # all feature_id coordinates in one chunk\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate chunks which are 1(feature_id) x 367439(time) arrays of `float64`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#       time  * id * float64\n",
    "bytes = 367439 * 1 * 8\n",
    "kbytes = bytes / (2**10)\n",
    "mbytes = kbytes / (2**10)\n",
    "print(f\"chunk size: {bytes=} ({kbytes=:.2f})({mbytes=:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually reset the chunking metadata in prep for re-chunking\n",
    "smplData = smplData.chunk(chunks={'feature_id':1, 'time': 367439})\n",
    "for x in smplData.variables:\n",
    "    smplData[x].encoding['chunks'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up output location\n",
    "\n",
    "With this plan, we can ask `rechunker` to re-write the data using the prescribed chunking pattern.\n",
    "\n",
    "Unlike with the smaller dataset, we need to write this very large dataset to an object store in the datacenter: an S3 'bucket'.  So we need to set that up so that `rechunker` will have a suitable place to write data. This new data will be a complete copy of the original, just re-organized a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getuser\n",
    "import fsspec\n",
    "uname=getuser()\n",
    "\n",
    "fsw = fsspec.filesystem(\n",
    "    's3', \n",
    "    anon=False, \n",
    "    default_fill_cache=False, \n",
    "    skip_instance_cache=True, \n",
    "    client_kwargs={'endpoint_url': os.environ['AWS_S3_ENDPOINT'], }\n",
    ")\n",
    "\n",
    "workspace = 's3://rsignellbucket2/'\n",
    "testDir = workspace + \"testing/\"\n",
    "myDir = testDir + f'{uname}/'\n",
    "fsw.mkdir(testDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging = fsw.get_mapper(myDir + 'tutorial_staging.zarr')\n",
    "outfile = fsw.get_mapper(myDir + 'tutorial_rechunked.zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to rechunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rechunker\n",
    "## Recall that merely invoking rechunker does not do any work... just sorts out \n",
    "## the rechunking plan and writes metadata.\n",
    "result = rechunker.rechunk(\n",
    "    smplData,\n",
    "    chunk_plan,\n",
    "    \"16GB\",\n",
    "    outfile, \n",
    "    temp_store=staging, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import progress, performance_report\n",
    "\n",
    "with performance_report(filename=\"dask-report.html\"):\n",
    "    r = result.execute(retries=10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "_ = zarr.consolidate_metadata(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Let's read in the resulting re-chunked dataset to see how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reChunkedData = xr.open_zarr(outfile)\n",
    "reChunkedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before:\n",
    "smplData = ds.where(indexer.compute(), drop=True) # subset to only those features with a valid gage_id\n",
    "smplData['streamflow'].sel(feature_id=417955)\n",
    "# Note: many chunks needed to service a single feature_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After:\n",
    "reChunkedData['streamflow'].sel(feature_id=417955) \n",
    "# All data for the specified feature_id is in a single chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-users-pangeofu",
   "language": "python",
   "name": "conda-env-users-users-pangeofu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4100cc85ffefb381c538d28dd18cb927e5a99f05bbed6aaad5313d7bb1c2079e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
