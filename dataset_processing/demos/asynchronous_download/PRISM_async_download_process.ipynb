{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using concurrency to download data from an HTTP server\n",
    "\n",
    "This demo notebook focuses on using the `httpx` and `asyncio` libraries to connect to the PRISM HTTP server, download zipped files for daily data, and extract the BIL files to a directory. To start, run the next code cell.\n",
    "\n",
    "## Download daily PRISM data \n",
    "\n",
    "This may take several minutes and a message will show when it is complete. For this demo, we will start with 1988, the variable 'ppt', and the rest of the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ./async_prism_download.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`httpx` and `asyncio` makes use of async/await syntax to implement asynchronous programming. Async/await effectively allows us to write asynchronous code that looks like synchronous code i.e. an operation that would normally happen one after another (download data for day 1 then download data for day 2) can happen at the same time. Asynchoronus helps with the code whose only bottleneck is waiting for external events, such as network IO and timeouts.\n",
    "\n",
    "This differs from something such multiprocessing or parallel processing, which is useful for CPU-bound tasks. For example, if you have a list of numbers and you want to square each number, you can use multiprocessing to split the list into chunks and have each core of your CPU square the numbers in each chunk.\n",
    "\n",
    "Now, onto the code. The first step is to import the libraries we will use.\n",
    "\n",
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import io\n",
    "import os\n",
    "from pathlib import Path\n",
    "from time import sleep                                          \n",
    "import zipfile\n",
    "from dask.distributed import LocalCluster, Client\n",
    "from dask import config as cfg\n",
    "import fsspec\n",
    "import hvplot.xarray\n",
    "import httpx\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "\n",
    "cfg.set({'distributed.scheduler.worker-ttl': None})\n",
    "hvplot.extension('bokeh')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster = LocalCluster()\n",
    "cluster.adapt(minimum=1, maximum=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read all 365 BIL files into a single xarray dataset\n",
    "\n",
    "First, we want to check a couple of things. First, is there an exisiting zarr file for that variable? Second, is that year already present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up variables\n",
    "year_str = '1988'\n",
    "var = \"ppt\"\n",
    "\n",
    "def date_range_list(year):\n",
    "    \"\"\"Create list of dates for a given year\"\"\"\n",
    "    date_list = (pd.date_range(year + '-01-01', year + '-12-31')\n",
    "                .strftime(\"%Y%m%d\")\n",
    "                .tolist())\n",
    "    return date_list\n",
    "\n",
    "prism_date = date_range_list(year_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_path_base = Path(\"./zarr/\")\n",
    "\n",
    "zarr_filename = var + \".zarr\"\n",
    "\n",
    "zarr_path = zarr_path_base / zarr_filename\n",
    "\n",
    "if zarr_path.exists():\n",
    "    var_zarr = xr.open_zarr(zarr_path)\n",
    "    if len(var_zarr.sel(time=slice(prism_date[0], prism_date[-1])).coords[\"time\"]) > 0:\n",
    "        print(f\"Year already exists in dataset for variable {var}. Proceed to another year or variable.\")\n",
    "    else:\n",
    "        print(\"Proceed with workflow.\")\n",
    "else:\n",
    "    print(\"Zarr file does not exist. Proceed with workflow.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move on to reading all of the BIL files in lazily as 'xarray DataArrays' using `rioxarray` and `dask`. Lets write a function to handle this in case we want to use it again (spoiler alert: we will)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_da_list(year):\n",
    "    da_list = []\n",
    "    bil_path = f\"download/*_{year}*_bil.bil\"\n",
    "    bil_files_list = glob(bil_path)\n",
    "    for file in bil_files_list:\n",
    "        with rioxarray.open_rasterio(file, chunks={}) as f:\n",
    "            da_list.append(f)\n",
    "            f.close()\n",
    "    return da_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_da_list = create_da_list(year_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to add the time dimension to each dataarray, convert them to a `xarray DataSet`, and then concatenate them into a single `xarray DataSet`. Lets write a function to handle this in case we want to use it again (spoiler alert: we will)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_list_datarrays(da_list, date_list, var):\n",
    "    #create a list to hold the datasets\n",
    "    ds_list = []\n",
    "\n",
    "    # add time dimension to each dataset in pr and convert attributes to data variables\n",
    "    for i in range(len(date_list)):\n",
    "        # get single day\n",
    "        day = pd.date_range(date_list[i], periods=1)\n",
    "\n",
    "        # convert to DataArray\n",
    "        time_da = xr.DataArray(day, [('time', day)])\n",
    "\n",
    "        # expand dims\n",
    "        da_list[i] = da_list[i].expand_dims(time=time_da)\n",
    "\n",
    "        # add name as str(i)\n",
    "        da_list[i].name = var\n",
    "\n",
    "        # squeeze band dimension\n",
    "        da_list[i] = da_list[i].squeeze(\"band\", drop=True)\n",
    "\n",
    "        # convert to dataset\n",
    "        ds_list.append(da_list[i].to_dataset())\n",
    "\n",
    "    # convert to dataset\n",
    "    ds = xr.concat(ds_list, dim='time', combine_attrs='drop') \n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_ds = process_list_datarrays(pr_da_list, prism_date, var)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a little tidying up before exporting the dataset to a zarr file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of attrs from pr_da_list[0]\n",
    "attrs_list = list(pr_da_list[0].attrs.keys())[-3:]\n",
    "\n",
    "# create dict of attrs\n",
    "attrs = dict((k, pr_da_list[0].attrs[k]) for k in attrs_list if k in attrs_list)\n",
    "\n",
    "# add attrs to pr_ds\n",
    "pr_ds.attrs = attrs\n",
    "\n",
    "# create chunk dict\n",
    "# sets time to be monthly chunks step has its own chunk\n",
    "chunk_dict = {'time': pr_ds.dims['time'], 'x': 281, 'y': 207}\n",
    "\n",
    "# rechunk\n",
    "pr_ds_rechunk = pr_ds.chunk(chunk_dict)\n",
    "\n",
    "# if the zarr file exists, append to it along the time dimension\n",
    "if zarr_path.exists():\n",
    "    pr_ds_rechunk.to_zarr(zarr_path, append_dim=\"time\")\n",
    "    print(\"Appending to existing zarr file.\")\n",
    "else:\n",
    "    pr_ds_rechunk.to_zarr(zarr_path)\n",
    "    print(\"Creating new zarr file.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see what our new data looks like when read from file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newly_minted_zarr = xr.open_zarr(zarr_path, decode_coords=\"all\")\n",
    "newly_minted_zarr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But does it plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newly_minted_zarr.hvplot(x=\"x\", y=\"y\", rasterize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up downloaded files \n",
    "\n",
    "Now that the individual days have been combined into a single dataset, we can delete the individual files. We can use the `glob` library to get a list of all of the files in the directory and then use `os.remove` to delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_downloads(year):\n",
    "    \"\"\"Cleanup downloads and use sleep function to wait for file to be released (if needed).\n",
    "    Args:\n",
    "        year (int, float, str): year to cleanup\n",
    "    \"\"\"\n",
    "    files = glob(f\"./download/*{year}*\")\n",
    "\n",
    "    print(f\"Number of files to delete for {year}: {len(files)} files\")\n",
    "\n",
    "    print(\"Starting cleanup process...\")\n",
    "\n",
    "    def cleanup(file_list):\n",
    "    # iterate through files and delete\n",
    "        for file in file_list:\n",
    "            # check if file exists\n",
    "            if Path(file).exists():\n",
    "                for i in range(10):\n",
    "                    try:\n",
    "                        os.remove(file)\n",
    "                        break\n",
    "                    except:\n",
    "                        sleep(1)\n",
    "                        continue\n",
    "\n",
    "        # now a bit of recursion to check if files are still there\n",
    "        file_list_updated = glob(f\"./download/*{year}*\")\n",
    "        if len(file_list_updated) > 0:\n",
    "            cleanup(file_list_updated)\n",
    "        else:\n",
    "            print(\"Cleanup complete.\")\n",
    "\n",
    "    cleanup(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_downloads(year_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add to the zarr file\n",
    "\n",
    "Lets add the next year, 1989, to our zarr file. We will go through the same process as above, reusing the functions we wrote. Just a reminder that the download script takes a couple of minutes to run so feel free to grab a coffee, eat a donut, and/or do some pushups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ./async_prism_download.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set our new variables..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up variables\n",
    "year_str = '1989'\n",
    "var = \"ppt\"\n",
    "\n",
    "# create date list\n",
    "prism_date = date_range_list(year_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...create the list of DataArrays..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_da_list = create_da_list(year_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...create the DataSet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_ds = process_list_datarrays(pr_da_list, prism_date, var)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and add it to the zarr file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of attrs from pr_da_list[0]\n",
    "attrs_list = list(pr_da_list[0].attrs.keys())[-3:]\n",
    "\n",
    "# create dict of attrs\n",
    "attrs = dict((k, pr_da_list[0].attrs[k]) for k in attrs_list if k in attrs_list)\n",
    "\n",
    "# add attrs to pr_ds\n",
    "pr_ds.attrs = attrs\n",
    "\n",
    "# create chunk dict\n",
    "# sets time to be monthly chunks step has its own chunk\n",
    "chunk_dict = {'time': pr_ds.dims['time'], 'x': 281, 'y': 207}\n",
    "\n",
    "# rechunk\n",
    "pr_ds_rechunk = pr_ds.chunk(chunk_dict)\n",
    "\n",
    "# if the zarr file exists, append to it along the time dimension\n",
    "if zarr_path.exists():\n",
    "    pr_ds_rechunk.to_zarr(zarr_path, append_dim=\"time\")\n",
    "    print(\"Appending to existing zarr file.\")\n",
    "else:\n",
    "    pr_ds_rechunk.to_zarr(zarr_path)\n",
    "    print(\"Creating new zarr file.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check out the zarr file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newer_newly_minted_zarr = xr.open_dataset(zarr_path, decode_coords=\"all\", engine=\"zarr\")\n",
    "newer_newly_minted_zarr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newer_newly_minted_zarr.hvplot(x=\"x\", y=\"y\", rasterize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there is just that little matter of cleaning up the downloaded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_downloads(year_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding another variable\n",
    "\n",
    "Lets say we want to add another variable to our zarr file, this time tmax. Lets download the data for 1988 and 1989 and process it like we did for ppt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1988 tmax\n",
    "%run -i ./async_prism_download.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1989 tmax\n",
    "%run -i ./async_prism_download.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the zarr file for tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_str_list = [\"1988\", \"1989\"]\n",
    "\n",
    "var = \"tmax\"\n",
    "\n",
    "zarr_path_base = Path(\"./zarr/\")\n",
    "\n",
    "zarr_filename = var + \".zarr\"\n",
    "\n",
    "zarr_path = zarr_path_base / zarr_filename\n",
    "\n",
    "for year_str in year_str_list:\n",
    "\n",
    "    if zarr_path.exists():\n",
    "        var_zarr = xr.open_zarr(zarr_path)\n",
    "        if len(var_zarr.sel(time=slice(prism_date[0], prism_date[-1])).coords[\"time\"]) > 0:\n",
    "            print(f\"Year already exists in dataset for variable {var}. Proceed to another year or variable.\")\n",
    "        else:\n",
    "            print(\"Proceed with workflow.\")\n",
    "    else:\n",
    "        print(\"Zarr file does not exist. Proceed with workflow.\")\n",
    "\n",
    "    # create date list\n",
    "    prism_date = date_range_list(year_str)\n",
    "    pr_da_list = create_da_list(year_str)\n",
    "    pr_ds = process_list_datarrays(pr_da_list, prism_date, var)\n",
    "    # create list of attrs from pr_da_list[0]\n",
    "    attrs_list = list(pr_da_list[0].attrs.keys())[-3:]\n",
    "\n",
    "    # create dict of attrs\n",
    "    attrs = dict((k, pr_da_list[0].attrs[k]) for k in attrs_list if k in attrs_list)\n",
    "\n",
    "    # add attrs to pr_ds\n",
    "    pr_ds.attrs = attrs\n",
    "\n",
    "    # create chunk dict\n",
    "    # sets time to be monthly chunks step has its own chunk\n",
    "    chunk_dict = {'time': pr_ds.dims['time'], 'x': 281, 'y': 207}\n",
    "\n",
    "    # rechunk\n",
    "    pr_ds_rechunk = pr_ds.chunk(chunk_dict)\n",
    "\n",
    "    # if the zarr file exists, append to it along the time dimension\n",
    "    if zarr_path.exists():\n",
    "        pr_ds_rechunk.to_zarr(zarr_path, append_dim=\"time\")\n",
    "        print(\"Appending to existing zarr file.\")\n",
    "    else:\n",
    "        pr_ds_rechunk.to_zarr(zarr_path)\n",
    "        print(\"Creating new zarr file.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up the tmax zarr file and check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_zarr = xr.open_dataset(zarr_path, decode_coords=\"all\", engine=\"zarr\")\n",
    "tmax_zarr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, can we combine ppt and tmax into a single zarr file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt_zarr = xr.open_dataset(\"./zarr/ppt.zarr\", decode_coords=\"all\", engine=\"zarr\")\n",
    "ppt_zarr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a single `xarray DataSet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_zarr = xr.merge([tmax_zarr, ppt_zarr])\n",
    "prism_zarr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save out to a combined zarr file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_zarr.to_zarr(\"./zarr/prism.zarr\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mandatory file cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for yr in year_str_list:\n",
    "    cleanup_downloads(yr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And shutdown the `dask` cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "So, lets recap what we did in this notebook:\n",
    "1. Used async/await to download daily data from the PRISM HTTP server for multiple years\n",
    "2. Used `rioxarray` and `dask` to read the BIL files into a single `xarray DataSet`\n",
    "3. Used `xarray` to save the `xarray DataSet` to a zarr file\n",
    "4. Used `glob` and `os` to delete the downloaded files\n",
    "5. Created a single zarr file for multiple variables\n",
    "\n",
    "This workflow could easily be adapted to:\n",
    "* Download data for multiple variables for the same year, combine them all into a single `xarray DataSet`, and save them to a single zarr file\n",
    "* Download data for multiple variables for multiple years, combine them all into a single `xarray DataSet`, and save them to a single zarr file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
