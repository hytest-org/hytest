{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a demo for downloading BIL files of PRISM daily data and converting these to a .zarr dataset. This demo particulary uses the `pyPRISMClimate` package to interface with the PRISM V2 web server. [pyPRISMClimate](https://sdtaylor.github.io/pyPRISMClimate/) handles the downloading and unzipping of zipped files and has multiple methods depending on the needed data.\n",
    "\n",
    "Next, the daily BIL files are lazily read into an `xarray` dataset by seting a `chunk={}` parameter using `rioxarray.open_dataset()`. After processing, the xarray dataset is written to zarr locally. \n",
    "\n",
    "Finally, any downloaded files are cleaned up and removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyPRISMClimate import get_prism_dailys\n",
    "from pyPRISMClimate import get_prism_daily_single\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import hvplot.xarray\n",
    "from glob import glob\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask\n",
    "from time import sleep\n",
    "from typing import Union\n",
    "from psutil import cpu_count\n",
    "from multiprocessing import cpu_count\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create local dask client\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "\n",
    "# number of cpu cores\n",
    "num_cores = cpu_count(logical=False)\n",
    "\n",
    "# number of workers\n",
    "num_workers = num_cores - 1\n",
    "cluster.scale(minimum=1, maximum=num_workers)\n",
    "cluster"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing parallel daily singles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function to create dask.delayed tasks of get_prism_daily_single\n",
    "# def get_prism_daily_single_delayed(var, date, dest_path):\n",
    "#     return dask.delayed(get_prism_daily_single)(var, date, dest_path)\n",
    "    \n",
    "# create list of dates for all days in January 1990\n",
    "d_list = pd.date_range(start='1990-01-01', end='1990-01-31').tolist()\n",
    "var = \"ppt\"\n",
    "dest_path = \"./download/\"\n",
    "\n",
    "# create list of dask.delayed tasks\n",
    "download_dask_list = [dask.delayed(get_prism_daily_single)(var, date, dest_path) for date in d_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dask = dask.delayed(get_prism_daily_single)(\"ppt\", \"1999-01-02\")\n",
    "test_dask.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.compute(*download_dask_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap function so date is first argument\n",
    "def get_prism_daily_single_wrapper(date, variable, destin_path):\n",
    "\n",
    "    d = date\n",
    "    v = variable\n",
    "    p = destin_path\n",
    "    return get_prism_daily_single(v, d, p)\n",
    "\n",
    "\n",
    "def get_prism_daily_single_multiprocessing(var, date_list, dest_path):\n",
    "    # get cpu count\n",
    "    num_cores = cpu_count()\n",
    "    with ThreadPool(num_cores-1) as pool:\n",
    "        try:\n",
    "            func = partial(get_prism_daily_single_wrapper, variable=var, destin_path=dest_path)\n",
    "            pool.imap(func, date_list)\n",
    "        except Exception as e:\n",
    "            print(\"did not work\")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_list = pd.date_range(start='1990-01-01', end='1990-01-31').tolist()\n",
    "d_list = [d.strftime('%Y-%m-%d') for d in pd.date_range(start='1990-01-01', end='1990-01-31')]\n",
    "var = \"ppt\"\n",
    "dest_path = \"./download/\"\n",
    "\n",
    "get_prism_daily_single_multiprocessing(var, d_list, dest_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to perform processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_downloads(year: int = None):\n",
    "    \"\"\"Cleanup downloads\n",
    "\n",
    "    Args:\n",
    "        year (int): year to cleanup\n",
    "    \"\"\"\n",
    "\n",
    "    if type(year) == int:\n",
    "        # get list of files with year in name\n",
    "        files = glob(f\"./download/*{str(year)}*\")\n",
    "\n",
    "    else:\n",
    "        # get list of files\n",
    "        files = glob(\"./download/*\")\n",
    "\n",
    "    # iterate through files and delete\n",
    "    for file in files:\n",
    "        # check if file exists\n",
    "        if os.path.exists(file):\n",
    "            # try to remove file 10 times\n",
    "            for i in range(10):\n",
    "                try:\n",
    "                    os.remove(file)\n",
    "                    break\n",
    "                except:\n",
    "                    sleep(1)\n",
    "                    continue\n",
    "\n",
    "def process_PRISM_dailies_byYear(var: str, year: Union[int, str], down_dir: str, zarr_dir: str, alt_zarr_name: str = None):\n",
    "    \"\"\"\n",
    "    Process PRISM daily data for a given year\n",
    "\n",
    "    Args:\n",
    "        var (str): variable to download\n",
    "        year (int): year to process\n",
    "        down_dir (str): download directory\n",
    "        zarr_dir (str): zarr directory\n",
    "        alt_zarr_name (str): alternate name for zarr file if not variable name\n",
    "    \"\"\"\n",
    "\n",
    "    # check if any files in down_dir\n",
    "    if len(glob(down_dir + \"*\")) > 0:\n",
    "        # delete files if any\n",
    "        cleanup_downloads()\n",
    "\n",
    "    # convert year to string\n",
    "    if type(year) == int:\n",
    "        year_str = str(year)\n",
    "\n",
    "    else:\n",
    "        year_str = year\n",
    "\n",
    "    # misc vars\n",
    "    dest_path = down_dir\n",
    "    prism_var = var\n",
    "\n",
    "\n",
    "    # set time range variables\n",
    "    prism_start = year_str + '-01-01'\n",
    "    prism_end = year_str + '-12-31'\n",
    "    \n",
    "    # create range of dates\n",
    "    prism_date = pd.date_range(prism_start, prism_end)\n",
    "\n",
    "    # Get var for 1/1-12/31/YYYY\n",
    "    get_prism_dailys(prism_var, prism_start, prism_end, dest_path=dest_path)\n",
    "\n",
    "    # bil path\n",
    "    bil_path = dest_path + \"*_\" + year_str + \"*.bil\"\n",
    "\n",
    "    # get list of bil files and open with Dask\n",
    "    pr_da_list = []\n",
    "\n",
    "    bil_files_list = glob(bil_path)\n",
    "    \n",
    "    for file in bil_files_list:\n",
    "        with rioxarray.open_rasterio(file, chunks=\"auto\") as f:\n",
    "            pr_da_list.append(f)\n",
    "            f.close()\n",
    "\n",
    "    #create a list to hold the datasets\n",
    "    pr_ds_list = []\n",
    "\n",
    "    # add time dimension to each dataset in pr and convert attributes to data variables\n",
    "    for i in range(len(prism_date)):\n",
    "        # get single day\n",
    "        day = pd.date_range(prism_date[i], periods=1)\n",
    "\n",
    "        # convert to DataArray\n",
    "        time_da = xr.DataArray(day, [('time', day)])\n",
    "\n",
    "        # expand dims\n",
    "        pr_da_list[i] = pr_da_list[i].expand_dims(time=time_da)\n",
    "\n",
    "        # add name as str(i)\n",
    "        pr_da_list[i].name = var\n",
    "\n",
    "        # squeeze band dimension\n",
    "        pr_da_list[i] = pr_da_list[i].squeeze(\"band\", drop=True)\n",
    "\n",
    "        # convert to dataset\n",
    "        pr_ds_list.append(pr_da_list[i].to_dataset())\n",
    "\n",
    "    # convert to dataset\n",
    "    pr_ds = xr.concat(pr_ds_list, dim='time', combine_attrs='drop')\n",
    "\n",
    "    # create list of attrs from pr_da_list[0]\n",
    "    attrs_list = list(pr_da_list[0].attrs.keys())[-3:]\n",
    "\n",
    "    # create dict of attrs\n",
    "    attrs = dict((k, pr_da_list[0].attrs[k]) for k in attrs_list if k in attrs_list)\n",
    "\n",
    "    # add attrs to pr_ds\n",
    "    pr_ds.attrs = attrs\n",
    "\n",
    "    # create chunk dict\n",
    "    # sets time to be monthly chunks step has its own chunk\n",
    "    chunk_dict = {'time': pr_ds.dims['time'], 'x': 281, 'y': 207}\n",
    "\n",
    "    # rechunk\n",
    "    pr_ds_rechunk = pr_ds.chunk(chunk_dict)\n",
    "\n",
    "    # if alt_zarr_name is true then use that as zarr name\n",
    "    if alt_zarr_name:\n",
    "        # zarr store\n",
    "        zarr_path = zarr_dir + alt_zarr_name + \".zarr\"\n",
    "\n",
    "    else:\n",
    "        # zarr store\n",
    "        zarr_path = zarr_dir + var + \".zarr\"\n",
    "\n",
    "    # check if file exists\n",
    "    if os.path.exists(zarr_path):\n",
    "        pr_ds_rechunk.to_zarr(zarr_path, mode='a', append_dim='time')\n",
    "    else:\n",
    "        pr_ds_rechunk.to_zarr(zarr_path)\n",
    "\n",
    "\n",
    "def process_PRISM_dailies(var: str, year: Union[int, list], down_dir: str, zarr_dir: str, alt_zarr_name: str = None, parallel: bool = False):\n",
    "    \"\"\"Process PRISM daily data for a given year\n",
    "\n",
    "    Args:\n",
    "        var (str): variable to download\n",
    "        year (Union[int, list]): year or list of years to process\n",
    "        down_dir (str): download directory\n",
    "        zarr_dir (str): zarr directory\n",
    "        alt_zarr_name (str): alternate name for zarr file if not variable name\n",
    "        parallel (bool): if true then use dask to process data\n",
    "    \"\"\"\n",
    "    # check if year is int and dask is true then raise error\n",
    "    if type(year) != list and parallel == True:\n",
    "        raise ValueError(\"dask must be False if year is int or str\")\n",
    "\n",
    "    if type(year) == int or type(year) == str:\n",
    "\n",
    "        # convert year str if int\n",
    "        if type(year) == int:\n",
    "            year = str(year)\n",
    "        \n",
    "        # download data and create zarr\n",
    "        process_PRISM_dailies_byYear(var, year, down_dir, zarr_dir, alt_zarr_name)\n",
    "\n",
    "        # cleanup downloads\n",
    "        cleanup_downloads(year)\n",
    "\n",
    "    # means year is list\n",
    "    else:\n",
    "\n",
    "        # pluralize year variable name if list\n",
    "        years = year\n",
    "\n",
    "        # check if dask is true\n",
    "        if parallel == True:\n",
    "\n",
    "            # create list of dask objects\n",
    "            process_list_dask = [dask.delayed(process_PRISM_dailies_byYear)(\n",
    "                var, yr, down_dir, zarr_dir, alt_zarr_name) for yr in years]\n",
    "            \n",
    "            # compute list\n",
    "            dask.compute(process_list_dask)\n",
    "\n",
    "            # cleanup downloads\n",
    "            for yr in years:\n",
    "                cleanup_downloads(yr)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # process each year\n",
    "            for yr in years:\n",
    "\n",
    "                # download data and create zarr\n",
    "                process_PRISM_dailies_byYear(var, yr, down_dir, zarr_dir, alt_zarr_name)\n",
    "\n",
    "                cleanup_downloads(yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single year\n",
    "process_PRISM_dailies('ppt', 1985, './download/', './zarr/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two years w/ dask\n",
    "process_PRISM_dailies('ppt', [1986, 1987], './download/', './zarr/', parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# four years w/ dask\n",
    "process_PRISM_dailies('ppt', [1988, 1989, 1990, 1991], './download/', './zarr/', parallel=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test opening zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt_zarr = xr.open_zarr('./zarr/ppt.zarr')\n",
    "ppt_zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt_zarr.hvplot(x='x', y='y', rasterize=True, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shut down cluster and client\n",
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
