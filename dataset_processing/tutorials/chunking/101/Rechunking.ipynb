{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a93f80-def8-48cc-b31f-65772a2bdc12",
   "metadata": {},
   "source": [
    "# Rechunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169ec966-1ad9-489d-92b9-89b8e2240bce",
   "metadata": {},
   "source": [
    "::::{margin}\n",
    ":::{note}\n",
    "This notebook was inspired by the material in [this notebook by James McCreight](https://github.com/NCAR/rechunk_retro_nwm_v21/blob/main/notebooks/usage_example_rerechunk_chrtout.ipynb), and [the Rechunker Tutorial](https://rechunker.readthedocs.io/en/latest/tutorial.html).\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507993a5-5380-47db-b9eb-3347ec6ee28b",
   "metadata": {},
   "source": [
    "The goal of this notebook is to learn how to \"[**rechunk**](../back/Glossary.md#term-Rechunking)\" data.\n",
    "This will be a culmination of all the [previous introductory material](index.md) where we will:\n",
    "\n",
    "1. [Read in a Zarr store](ExamineDataChunking.ipynb)\n",
    "2. [Check the current chunking](ExamineDataChunking.ipynb)\n",
    "3. [Choose a new chunk shape](BasicsShapeSize.ipynb)\n",
    "4. Rechunk using [Rechunker](https://rechunker.readthedocs.io/en/latest/index.html)\n",
    "5. [Confirm the proper creation of the Zarr store by Rechunker](WriteChunkedFiles.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67d60e-ad2b-40a3-9a48-54518e47dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xmode Minimal\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "from rechunker import rechunk\n",
    "import zarr\n",
    "import shutil\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60378aa5-336d-42e4-85b7-ef7f54cee32f",
   "metadata": {},
   "source": [
    "## Read in a Zarr Store\n",
    "\n",
    "For the dataset in this tutorial, we will use the data from the National Water Model Reanalysis Version 2.1.\n",
    "The full dataset is part of the [AWS Open Data Program](https://aws.amazon.com/opendata/), available via the S3 bucket at: `s3://noaa-nwm-retro-v2-zarr-pds/`.\n",
    "\n",
    "As this is a Zarr store, we can easily read it in directly with [`xarray.open_dataset()`](https://docs.xarray.dev/en/stable/generated/xarray.open_dataset.html), including the keyword `chunks={}` to make sure dask loads the data using the stored chunks' shape and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c2d2ac-d9b8-4104-82c5-f10241c0f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = fsspec.get_mapper('s3://noaa-nwm-retro-v2-zarr-pds', anon=True)\n",
    "ds = xr.open_dataset(file, chunks={}, engine='zarr')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f01e966-bd18-4971-be94-797ade9b82d0",
   "metadata": {},
   "source": [
    "## Check the Current Chunk Shape and Size\n",
    "\n",
    "From the output, we can see there are two dimensions, `time` of length 227,904 and `feature_id` of length 2,729,077.\n",
    "Both `time` and `feature_id` are coordinates, along with two extra coordinates of `latitude` and `longitude` which are tied to the `feature_id`.\n",
    "Inspecting these extra coordinates in detail, we notice they are \"chunked\", but only to a single chunk of size 10.41 MiB.\n",
    "This is good, as we will want them in a single chunk for writing, such that they are fully read in with each data variable chunk.\n",
    "Finally, there are eight data variables, all having both of the dimensions, meaning they are 2D data.\n",
    "Examining the variables in detail, they all have chunks of shape `{'time': 672, 'feature_id': 30000}` and size of 153.81 MiB or 76.9 MiB (depending if the variable is 64- or 32-bit, respectively).\n",
    "Additionally, we can see that each variable is a whopping 4.53 or 2.26 TiB in memory, which means the whole dataset is almost 32 TiB!\n",
    "\n",
    "This is a bit more than we want to work with in our example.\n",
    "So, let's go ahead and select a subset of the data that is about 1 GiB in memory.\n",
    "While this is not a larger-than-memory dataset anymore, it is still reasonably large and it will work well for this example without taking forever to get data and rechunk.\n",
    "For the subset, let's only look at water year 2018 (i.e., October 2017 to September 2018) for the variables `streamflow` and `velocity` for the first 15,000 `feature_id`s.\n",
    "\n",
    "```{note}\n",
    "As we are not working on AWS, we have selected this subset to minimize the number of chunks that need to be downloaded from the S3 bucket.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd193245-46ac-4aac-a638-b8d19cccd68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[['streamflow', 'velocity']]\n",
    "ds = ds.isel(feature_id=slice(0, 15000))\n",
    "ds = ds.sel(time=slice('2017-10-01', '2018-09-30'))\n",
    "ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04fabae9-9c45-4b69-b829-256f9eee1012",
   "metadata": {},
   "source": [
    "Now, our subset of data is only about 1 GiB per data variable and has a chunk shape of `{'time': 672, 'feature_id': 15000}` with size of 76.9 MiB.\n",
    "This is a good chunk size and between the optimal range of 10 to 200 MiB.\n",
    "However, the chunk shape may not be an optimal choice for our analysis as it is chunked completely by `feature_id` (i.e., all feature IDs for a given time are read in a single chunk).\n",
    "\n",
    "## Choose a New Chunk Shape and Size\n",
    "\n",
    "To decide on a new chunk shape and size, we need to determine how we will use the data.\n",
    "As we just discussed, an analysis which samples all locations for a single point in time would need to fetch only a single chunk, which is perfect for that analysis.\n",
    "However, a time-series analysis (i.e. sampling all time-step values for a single `feature_id`) would require 13 chunks to be read for one feature.\n",
    "This means many more chunks must be fetched for this read pattern.\n",
    "Thinking of data usage, the preferred format for the streamflow data variable is likely time-series wise chunking as this variable is more often used as full time series at a single location.\n",
    "The same goes for velocity.\n",
    "However, for the purpose of this example, let's assume that we don't know how velocity will be used and give it a different chunking pattern, one where we balance the number of chunks between each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6084e9a6-2de1-40ba-b7b5-15e70393f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeature = len(ds.feature_id)\n",
    "ntime = len(ds.time)\n",
    "\n",
    "streamflow_chunk_plan = {'time': ntime, 'feature_id': 1}\n",
    "bytes_per_value = ds.streamflow.dtype.itemsize\n",
    "total_bytes = streamflow_chunk_plan['time'] * streamflow_chunk_plan['feature_id'] * bytes_per_value\n",
    "streamflow_MiB = total_bytes / (2 ** 10) ** 2\n",
    "print(\"STREAMFLOW \\n\"\n",
    "      f\"Shape of chunk: {streamflow_chunk_plan} \\n\"\n",
    "      f\"Partial 'time' chunk remainder: {(ntime % streamflow_chunk_plan['time']) / streamflow_chunk_plan['time']} \\n\"\n",
    "      f\"Partial 'feature_id' chunk remainder: {(nfeature % streamflow_chunk_plan['feature_id']) / streamflow_chunk_plan['feature_id']} \\n\"\n",
    "      f\"Chunk size: {streamflow_MiB:.2f} [MiB] \\n\")\n",
    "\n",
    "chunks_per_dim = 3\n",
    "velocity_chunk_plan = {'time': ntime // chunks_per_dim, 'feature_id': nfeature // chunks_per_dim}\n",
    "bytes_per_value = ds.velocity.dtype.itemsize\n",
    "total_bytes = velocity_chunk_plan['time'] * velocity_chunk_plan['feature_id'] * bytes_per_value\n",
    "velocity_MiB = total_bytes / (2 ** 10) ** 2\n",
    "print(\"VELOCITY \\n\"\n",
    "      f\"Shape of chunk: {velocity_chunk_plan} \\n\"\n",
    "      f\"Partial 'time' chunk remainder: {(ntime % velocity_chunk_plan['time']) / velocity_chunk_plan['time']} \\n\"\n",
    "      f\"Partial 'feature_id' chunk remainder: {(nfeature % velocity_chunk_plan['feature_id']) / velocity_chunk_plan['feature_id']} \\n\"\n",
    "      f\"Chunk size: {velocity_MiB:.2f} [MiB]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ffc944-fe70-413e-b0f8-8bc7bdbb5280",
   "metadata": {},
   "source": [
    "The time-series streamflow chunking for a single feature ID has way too small of a chunk size.\n",
    "If we increase the number of feature IDs per chunk by a factor of 1000, we should have the right size for that chunk\n",
    "As for velocity, the split of the data into three chunks along both dimensions resulted in no partial chunks and a chunk size of 111 MiB.\n",
    "This is within our optimal chunk size range.\n",
    "So, let's stick with the velocity chunks and recheck the streamflow chunking with 1000 features per time series chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8609df-8627-4368-be50-3288ac6c1148",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamflow_chunk_plan = {'time': ntime, 'feature_id': 1000}\n",
    "bytes_per_value = ds.streamflow.dtype.itemsize\n",
    "total_bytes = streamflow_chunk_plan['time'] * streamflow_chunk_plan['feature_id'] * bytes_per_value\n",
    "streamflow_MiB = total_bytes / (2 ** 10) ** 2\n",
    "print(\"STREAMFLOW \\n\"\n",
    "      f\"Chunk of shape {streamflow_chunk_plan} \\n\"\n",
    "      f\"Partial 'time' chunk remainder: {(ntime % streamflow_chunk_plan['time']) / streamflow_chunk_plan['time']} \\n\"\n",
    "      f\"Partial 'feature_id' chunk remainder: {(nfeature % streamflow_chunk_plan['feature_id']) / streamflow_chunk_plan['feature_id']} \\n\"\n",
    "      f\"Chunk size: {streamflow_MiB:.2f} [MiB] \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fdac77-f8ca-4392-b3d4-41806d6c3b2b",
   "metadata": {},
   "source": [
    "Alright, no remainders and a reasonable chunk size.\n",
    "Time to get to rechunking!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3234b824-aa75-402f-9976-0b9d9f90e821",
   "metadata": {},
   "source": [
    "## Rechunk with [Rechunker](https://rechunker.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "This is a relatively trivial example, due to the smaller size of the subset of the dataset.\n",
    "As the whole subset can fit into memory easily, chunking in general is largely unnecesary in terms of optimizing I/O (however, parallelism is still a consideration). \n",
    "But it is worth doing, as the concepts will apply if we take this to the full dataset.\n",
    "\n",
    "First thing we need to determine is a chunk plan to describe the chunk layout we want.\n",
    "We already created a basic version of this above that describes the chunk shapes of streamflow and velocity.\n",
    "We just need to place it into a more cohesive list along with chunk shapes for the coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a072a5-c198-4e2c-a2d7-9f801b9dc081",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_plan = {\n",
    "    'streamflow': streamflow_chunk_plan,\n",
    "    'velocity': velocity_chunk_plan,\n",
    "     # We don't want any of the coordinates chunked\n",
    "    'latitude': (nfeature,),\n",
    "    'longitude': (nfeature,),    \n",
    "    'time': (ntime,),\n",
    "    'feature_id': (nfeature,)\n",
    "}\n",
    "chunk_plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbdfce8-992d-4412-ad19-cdad10dc379c",
   "metadata": {},
   "source": [
    "With this plan, we can ask [Rechunker](https://rechunker.readthedocs.io/en/latest/index.html) to re-write the data using the prescribed chunking pattern.\n",
    "Rechunker will read the data and rechunk it using an intermediate Zarr store for efficiency.\n",
    "This will produce a new Zarr store with the chunk shapes we specified in `chunk_plan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7609cb35-431f-4b2b-a59f-55186d8a286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = \"rechunked_nwm.zarr\"\n",
    "temp_store = \"/tmp/scratch.zarr\"\n",
    "try:\n",
    "    result = rechunk(\n",
    "        ds,\n",
    "        target_chunks=chunk_plan,\n",
    "        max_mem=\"2GB\",\n",
    "        target_store=outfile ,\n",
    "        temp_store=temp_store\n",
    "    )\n",
    "    display(result)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af1b74-f0f0-4b1e-bcfb-5dac59fe4f51",
   "metadata": {},
   "source": [
    "Oh, that is not what we wanted!\n",
    "We seem to have gotten an error indicating overlap in chunks between the read and write.\n",
    "Looking at the error, it is saying that the first `time` chunk we are reading is a partial chunk and not a full chunk.\n",
    "So, when Rechunker tries to read the data and then write the first rechunk, it is having to read two chunks to write to the one chunk.\n",
    "This is a one-to-many write, which can corrupt our file when done in parallel with dask.\n",
    "Thank goodness Rechunker caught this for us!\n",
    "Reading the recommended fix, it seems the only way to go about this is to call `chunk()` and reset the chunking on the original data.\n",
    "In other words, after we select the subset from the dataset, we need to realign the chunks such that the first chunk is not a partial chunk.\n",
    "This is simple enough to do.\n",
    "So much so, we can just do it when passing the dataset subset to Rechunker.\n",
    "\n",
    "```{note}\n",
    "`rechunker.rechunk` does not overwrite any data.\n",
    "If it sees that `rechunked_nwm.zarr` or `/tmp/scratch.zarr` already exist, it will raise an exception.\n",
    "Be sure that these locations do not exist before calling Rechunker. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e7c339-75d1-408a-9f01-3837466fec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must delete the started rechunked zarr stores\n",
    "shutil.rmtree(outfile)\n",
    "shutil.rmtree(temp_store)\n",
    "\n",
    "result = rechunk(\n",
    "    ds.chunk({'time': 672, 'feature_id': 15000}),\n",
    "    target_chunks=chunk_plan,\n",
    "    max_mem=\"2GB\",\n",
    "    target_store=outfile ,\n",
    "    temp_store=temp_store\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47856c25-e913-4ff7-bcc5-b79b5426fecb",
   "metadata": {},
   "source": [
    "Alright, that worked with no problems!\n",
    "Now, we must specifically direct rechunk to calculate.\n",
    "To do this, we can call `execute()` on our `result` `Rechunked` object.\n",
    "Without the call to `execute()`, the Zarr dataset will be empty, and `result` will only hold a 'task graph' outlining the calculation steps.\n",
    "\n",
    "```{tip}\n",
    "Rechunker also writes a minimalist data group, meaning that variable metadata is not consolidated.\n",
    "This is not a required step, but it will really spead up future workflows when the data is read back in using xarray.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d15fca4-0ec1-410a-bee5-877ffcbcbb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = result.execute()\n",
    "_ = zarr.consolidate_metadata(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d79751d-4b6c-4bb8-a033-36440d7fdf5d",
   "metadata": {},
   "source": [
    "## Confirm the Creation of the Zarr Store by Rechunker\n",
    "\n",
    "Let's read in the resulting re-chunked dataset to confirm it turned out how we intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ca918-3ff7-4d46-9d9f-ec2522f30f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rechunked = xr.open_zarr(outfile)\n",
    "ds_rechunked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1267a63c-0822-490a-ac3e-076364805132",
   "metadata": {},
   "source": [
    "Great, our chunk shapes are exactly what we specified!\n",
    "Let's do one final check that will compare our original subset of the dataset with our new rechunked dataset, that way we can confirm nothing unexpected happened during rechunking.\n",
    "\n",
    "```{note}\n",
    "For our small example dataset this is easy as the data will fit into memory.\n",
    "More efficient and better ways should be used for larger datasets.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb5583-b63b-4b4f-8ed3-eca58f70cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(ds - ds_rechunked).compute().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3a8f56-d777-41ec-890a-3e0779f34d2c",
   "metadata": {},
   "source": [
    "Perfect!\n",
    "The maximum absolute difference between each both the `streamflow` and `velocity` variables is 0.\n",
    "In other words, they are exactly the same, and Rechunker worked as expect.\n",
    "\n",
    "Now that you know how to rechunk a Zarr store using Rechunker, you should know all of the basics there are in terms of chunking.\n",
    "You are now ready to explore more [advanced chunking topics in chunking](../201/index.md) if you are interested!\n",
    "\n",
    "## Clean Up\n",
    "\n",
    "As we don't want to keep this rechunked Zarr on our local machine, let's go ahead and delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81014ed7-0c33-438f-915e-af46fd01df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(outfile)\n",
    "shutil.rmtree(temp_store)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
