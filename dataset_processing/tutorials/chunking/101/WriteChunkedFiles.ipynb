{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd95288c-1388-4732-9626-ca4014335d66",
   "metadata": {},
   "source": [
    "# Writing Chunked Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2240372-1697-4083-873b-d5b25653a2d0",
   "metadata": {},
   "source": [
    "::::{margin}\n",
    ":::{note}\n",
    "This notebook is based on the [Xarray User-guide for reading and writing Zarr](https://docs.xarray.dev/en/stable/user-guide/io.html#zarr).\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9262bf36-4d8b-4b1d-9d7c-6730462cf7a6",
   "metadata": {},
   "source": [
    "The goal of this notebook is to learn how to load a collection of NetCDF files, chunk the data, write the data in Zarr format, and confirm the proper creation of the Zarr store.\n",
    "We will be writing to our local storage for simplicity (as this is just a tutorial notebook), but you can easily change the output path to be anywhere including cloud storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c10a55d-a68f-472f-929f-a64e2ddab733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3203b0b-6377-4b12-ae9e-5afb21dacc3a",
   "metadata": {},
   "source": [
    "## Example Dataset\n",
    "\n",
    "In this notebook, we will use the daily gridMET precipitation dataset as an example for reading data and writing to Zarr.\n",
    "The data is currently hosted on the HyTEST OSN as a collection of NetCDF files.\n",
    "To get the files, we will use [fsspec](https://filesystem-spec.readthedocs.io/en/latest/) to open each year of precipitation data to a list.\n",
    "Then, we can read in the all the files at once using [`xarray.open_mfdataset`](https://docs.xarray.dev/en/stable/generated/xarray.open_mfdataset.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad046ac6-2559-4944-bbc2-ca7066cdbc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(\n",
    "    's3',\n",
    "    anon=True,   # anonymous = does not require credentials\n",
    "    client_kwargs={'endpoint_url': 'https://usgs.osn.mghpcc.org/'}\n",
    ")\n",
    "precip_files = [fs.open(file) for file in fs.glob('s3://mdmf/gdp/netcdf/gridmet/gridmet/pr_*.nc')]\n",
    "ds = xr.open_mfdataset(\n",
    "    precip_files,\n",
    "    chunks={},\n",
    "    engine='h5netcdf'\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d41caf-b46e-46bd-a0f7-d4d69e0fc573",
   "metadata": {},
   "source": [
    "## Selecting Chunk Shape and Size\n",
    "\n",
    "As we can see in the rich HTML output of our dataset, the NetCDF files were already chunked with pattern of `{'day': 61, 'lat': 98, 'lon': 231}`.\n",
    "However, the size of these chunks are relatively small and near the lower limit of an acceptable chunk size of 10 MiB.\n",
    "So, it would be better if we could increase our chunk sizes to say 70-110 MiB.\n",
    "To do this, we will simply use multiples of the current chunks so we don't have to completely rechunk the dataset (grouping chunks is way faster than completely changing chunk shape).\n",
    "Our goal will be to try and evenly distribute the number of chunks between the time and space dimensions.\n",
    "\n",
    "First, let's check how many chunks are in the temporal and spatial dimensions with the current chunk shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8dc3b-f5e9-4aa5-86aa-49dcc8c48943",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.precipitation_amount.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc95190-e181-4ce7-b5d4-cf3ccfb8b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_chunk_shape, lat_chunk_shape, lon_chunk_shape = ds.precipitation_amount.encoding['chunksizes']\n",
    "\n",
    "print(f\"Number of temporal chunks: {np.ceil(len(ds.day) / time_chunk_shape)}\")\n",
    "print(f\"Number of spatial chunks: {np.ceil(len(ds.lat) / lat_chunk_shape) * np.ceil(len(ds.lon) / lon_chunk_shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6d4183-6ed3-41cc-9829-ef8a601fb7ae",
   "metadata": {},
   "source": [
    "Okay, so there are about 7 times as many time chunks as there are spatial chunks.\n",
    "Let's try to balance this better so there are an approximately equal amount.\n",
    "We will do this by simply increasing the time chunk shape by a factor of 7.\n",
    "Then, we can double check our chunk size to see if it aligns with our chunk size goal.\n",
    "\n",
    "```{note}\n",
    "Also by increasing our temporal chunk shape by a factor of seven, it is now larger than a whole year.\n",
    "This means if we only wanted a single year, we would at most need to read two chunks.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7477f21-beed-4dda-a14b-b6c05fd4f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_plan = {\n",
    "    \"day\": time_chunk_shape * 7, \n",
    "    \"lat\" : lat_chunk_shape,\n",
    "    \"lon\" : lon_chunk_shape\n",
    "}\n",
    "\n",
    "bytes_per_value = ds.precipitation_amount.dtype.itemsize\n",
    "\n",
    "print(f\"Chunk Plan:{chunk_plan}\")\n",
    "print(f\"Number of temporal chunks: {np.ceil(len(ds.day) / chunk_plan['day'])}\")\n",
    "print(f\"Number of spatial chunks: {np.ceil(len(ds.lat) / chunk_plan['lat']) * np.ceil(len(ds.lon) / chunk_plan['lon'])}\")\n",
    "print(f\"Chunk size in MiB: {(bytes_per_value * chunk_plan['day'] * chunk_plan['lat'] * chunk_plan['lon']) / 2 ** 10 / 2 ** 10:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf00120-5465-4c90-8855-e2dc5c6dca4f",
   "metadata": {},
   "source": [
    "Nice!\n",
    "That got us the exact same number of chunks and the chunk size we were looking for.\n",
    "As a final check, let's make sure none of the ending chunks contain <50% data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49fdfe2-7621-454f-9d43-5ed183aff7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of day chunks: {(len(ds.day) / chunk_plan['day'])}\")\n",
    "print(f\"Number of lat chunks: {(len(ds.lat) / chunk_plan['lat'])}\")\n",
    "print(f\"Number of lon chunks: {(len(ds.lon) / chunk_plan['lon'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0988e8-8174-4d16-9876-d332d24c7421",
   "metadata": {},
   "source": [
    "Perfect!\n",
    "So the final day chunk is only 77% full, but that is good enough.\n",
    "Let's now chunk the dataset to our chunking plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfd13c6-e7be-4e0e-861c-134317a87533",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.chunk(chunk_plan)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d206001-3e5e-4805-b87a-de0c22bcd7d6",
   "metadata": {},
   "source": [
    "Now, let's save this data to a Zarr!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aa84b3-4482-47f3-8e61-105e7b4e3b16",
   "metadata": {},
   "source": [
    "## Writing Chunked Zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad607c-8491-45ea-bb90-ec7394419dd6",
   "metadata": {},
   "source": [
    "As discussed in the [Xarray User-guide for reading and writing Zarr](https://docs.xarray.dev/en/stable/user-guide/io.html#specifying-chunks-in-a-zarr-store), chunks are specified to our Zarr store in one of three ways in the preferential order of:\n",
    "\n",
    "  1. Manual chunk sizing through the use of the `encoding` argument\n",
    "  2. Automatic chunking based on chunks of the dask arrays\n",
    "  3. Default chunk behavior determined by the Zarr library\n",
    "\n",
    "In our case, we updated the dask array chunks by calling `ds.chunk()`.\n",
    "Therefore, we have the correct chunks and should be good to go.\n",
    "\n",
    "```{tip}\n",
    "This is our preferred method over using the `encoding` argument, as the positional ordering of the chunk shape in the `encoding` argument must match the positional ordering of the dimensions in each array.\n",
    "If they do not match you can get incorrect chunk shapes in the Zarr store.\n",
    "\n",
    "If you have multiple variables, using `encoding` could allow for specifying individual chunking shapes for each variable.\n",
    "However, if this is the case, we recommend updating each variable individually using, for example, `ds.precipitation_amount.chunk()` to change the individual variable chunk shape.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd08cea-4469-466b-b8f6-193abd094a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's actually use only one time chunk as this is an\n",
    "# example and we don't want to write 90 GiB\n",
    "ds_write = ds.isel(day=slice(0, chunk_plan['day']))\n",
    "\n",
    "outfile = \"gridmet_precip.zarr\"\n",
    "_ = ds_write.to_zarr(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5fa0fe-7d6a-4808-9a36-b56f469e95d7",
   "metadata": {},
   "source": [
    "Now, let's read in the just-written dataset to verify its integrity and chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044e3c96-60e4-4fb7-b185-a36e693ddc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_read = xr.open_dataset(outfile, engine='zarr', chunks={})\n",
    "ds_read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8cf24d-7ba5-4ccb-b468-521df61df28d",
   "metadata": {},
   "source": [
    "Great! Everything looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f88d0e-e2a6-41fd-9afa-917f5ab4c0dd",
   "metadata": {},
   "source": [
    "## Assessing Compression\n",
    "\n",
    "Now that our Zarr store is made, let's check how much the data was compressed.\n",
    "By default, [Zarr uses the Blosc compressor](https://docs.xarray.dev/en/stable/user-guide/io.html#zarr-compressors-and-filters) when calling [`xarray.Dataset.to_zarr()`](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.to_zarr.html) if we don't specify a compressor in the `encoding`.\n",
    "So, our data should be compressed by default, and we can examine each chunk on disk to confirm their compression factor.\n",
    "\n",
    "To examine this, let's create a new local `filesystem` to get the file info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85378aa-17cf-42ae-9a59-851d5f1222f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_local = fsspec.filesystem('local')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d14b6aa-86d5-4bbb-a546-80e8aa4b6157",
   "metadata": {},
   "source": [
    "Now, estimate the file sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6312d4ea-f1a3-4f52-9392-1fb6b000b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude any hidden files (i.e., starts with .)\n",
    "chunkfiles = fs_local.glob(outfile + \"/precipitation_amount/[!.]*\")\n",
    "filesize = np.array([fs_local.du(file) for file in chunkfiles])\n",
    "filesize_MiB = xr.DataArray(data=filesize / 2**10 / 2**10, name='Size', attrs={'units': 'MiB'})\n",
    "\n",
    "print(\"Stored Chunk Size Summary Statistics [MiB]:\\n\"\n",
    "      f\"Total: {filesize_MiB.sum():.4f}\\n\"\n",
    "      f\"Minimum: {filesize_MiB.min():.4f}\\n\"\n",
    "      f\"Mean:  {filesize_MiB.mean():.4f}\\n\"\n",
    "      f\"Maximum: {filesize_MiB.max():.4f}\")\n",
    "filesize_MiB.hvplot.hist(ylabel='Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0c0dda-8a16-4cc3-8952-460978a6ef6f",
   "metadata": {},
   "source": [
    "As we can see, the total dataset (excluding coordinates) is only 85 MiB on disk, with chunk sizes varying from 75 KiB to 4.9 MiB.\n",
    "This size is drastically smaller than the quoted total size for the xarray output, which said 2.58 GiB.\n",
    "Same for the individual chunks, which were quoted at 73.75 MiB.\n",
    "Let's get an exact comparison and compression ratio for the data we read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ebc894-d4f5-4582-b917-8611797c38ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_per_value = ds_read.precipitation_amount.dtype.itemsize\n",
    "total_size = ds_read['precipitation_amount'].size * bytes_per_value\n",
    "chunk_size = np.array(ds_read['precipitation_amount'].encoding['chunks']).prod() * bytes_per_value\n",
    "print(\"Read-in Chunk Size Summary Statistics:\\n\"\n",
    "      f\"Total: {total_size / (2**10)**3:.4f} [GiB]\\n\"\n",
    "      f\"Chunks: {chunk_size / (2**10)**2:.4f} [MiB]\")\n",
    "\n",
    "print(\"\\n\"\n",
    "      \"Compression Ratio Summary Statistics:\\n\"\n",
    "      f\"Total: {total_size / filesize.sum():.3f}\\n\"\n",
    "      f\"Minimum: {(chunk_size / filesize).min():.4f}\\n\"\n",
    "      f\"Mean:  {(chunk_size / filesize).mean():.4f}\\n\"\n",
    "      f\"Maximum: {(chunk_size / filesize).max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54041c1f-03a5-4544-910b-e8bd79ff5fa2",
   "metadata": {},
   "source": [
    "Ooph! This tells us that we get an astonishing average compression ratio of 30:1 on this particular data.\n",
    "Pretty good results.\n",
    "\n",
    "## Appending New Chunk\n",
    "\n",
    "Since this compression is so good, let's go ahead and add another time chunk onto our existing Zarr store.\n",
    "This is simple in xarray, especially since we are just appending another time chunk.\n",
    "All we have to do is [add `append_dim` to our `.to_zarr()` call to append to the time dimension](https://docs.xarray.dev/en/stable/user-guide/io.html#modifying-existing-zarr-stores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f95b58c-f36f-4204-bb90-509449f87bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_write = ds.isel(day=slice(chunk_plan['day'], chunk_plan['day']* 2))\n",
    "_ = ds_write.to_zarr(outfile, append_dim='day')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c1185f-4292-4450-a354-751e99c54c63",
   "metadata": {},
   "source": [
    "Now, let's read in the appended dataset to verify everything worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f376daab-ca29-4740-a506-afb348b4b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_read = xr.open_dataset(outfile, engine='zarr', chunks={})\n",
    "ds_read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d30e43d-6bf0-4f85-ab22-0c693e08ef1f",
   "metadata": {},
   "source": [
    "That looks like it worked as expected!\n",
    "Let's now double check the compression on these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd24f82f-e026-40bd-a051-60385f822479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude any hidden files (i.e., starts with .)\n",
    "chunkfiles = fs_local.glob(outfile + \"/precipitation_amount/[!.]*\")\n",
    "filesize = np.array([fs_local.du(file) for file in chunkfiles])\n",
    "filesize_MiB = xr.DataArray(data=filesize / 2**10 / 2**10, name='Size', attrs={'units': 'MiB'})\n",
    "\n",
    "print(\"Stored Chunk Size Summary Statistics [MiB]:\\n\"\n",
    "      f\"Total: {filesize_MiB.sum():.4f}\\n\"\n",
    "      f\"Minimum: {filesize_MiB.min():.4f}\\n\"\n",
    "      f\"Mean:  {filesize_MiB.mean():.4f}\\n\"\n",
    "      f\"Maximum: {filesize_MiB.max():.4f}\")\n",
    "filesize_MiB.hvplot.hist(ylabel='Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b98935-4e14-407e-b7a6-aa2a081cfa1a",
   "metadata": {},
   "source": [
    "Looks like we stayed at the same levels of compression which is great.\n",
    "\n",
    "## Clean Up\n",
    "\n",
    "So, hopefully now you know the basics of how to create a Zarr store from some NetCDF files and set its chunks' shape.\n",
    "The same methods would apply when rechunking a dataset, which we will get into next.\n",
    "\n",
    "As we don't want to keep this Zarr on our local machine, let's go ahead and delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761366c-ad04-457b-8b1d-a6d9610054d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up by deleting the zarr store\n",
    "fs_local.rm(outfile, recursive=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
