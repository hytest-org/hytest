{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f48557f-21ec-4fad-805c-3e3ed34bfd08",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Choosing an Optimal Chunk Size and Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8da090-7b8d-4a6b-9450-9086415ca6d3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "::::{margin}\n",
    ":::{note}\n",
    "This notebook is based on a 2013 blog post by Russ Rew at Unidata, [Chunking Data: Choosing Shapes](http://www.unidata.ucar.edu/blogs/developer/en/entry/chunking_data_choosing_shapes).\n",
    "The algorithm utilized in this notebook is based on the algorithm presented in that blog post.\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcb0092-180a-4817-b5ce-acf888b6edd5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The objective of this notebook is to learn and discuss *how* to select the optimal chunk size and shape for a given dataset.\n",
    "In the [Basics of Chunk Shape and Size notebook](../101/BasicsShapeSize.ipynb), we discussed the general considerations for chunking, but not how to apply these consisderations for selecting chunk shape and size.\n",
    "These considerations included:\n",
    "\n",
    "- chunk size between 10-200 MiB,\n",
    "- having partial final chunks at least half the size of the standard chunk,\n",
    "- performant future reads across all dimensions (or optimized for the dominant read pattern), and\n",
    "- chunk shape that is optimized for future data additions (if they occur).\n",
    "\n",
    "Here, we build upon those considerations and present a basic algorithm for automatically selecting the \"optimal\" chunk shape and size.\n",
    "Therefore, giving us a method for selecting chunk shape and size without much trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f6618-d42a-4ab2-8577-9d82f8760392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcabda2-662b-4e71-a4a3-24d3672f17ac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## The Many Considerations for Optimal Chunk Shape and Size Selection\n",
    "\n",
    "As we just listed, there are several things to consider when selecting a chunk size.\n",
    "Creating an \"optimal\" chunk shape and size requires balancing all of these things to make our dataset perform as we want.\n",
    "For example, the \"perfect\" chunking would have:\n",
    "\n",
    "- chunk size around 10 MiB (to accomodate different hardware),\n",
    "- no partial final chunks (i.e., chunk shape is integer divisor of the full data shape),\n",
    "- performant future reads across all dimensions and groups of dimensions, and\n",
    "- a shape that is an integer divisor of future data additions (if they occur).\n",
    "\n",
    "In addition to these, if we can select a chunk size that is optimal for our disk storage (e.g., disk block size), we should further improve read times.\n",
    "However, in practice, there is no way to get the \"perfect\" chunk.\n",
    "We will almost always have to compromise on one or more of these criteria to stay within the constraints created by another.\n",
    "The criteria we compromise on is up to us, which makes determining the \"optimal\" chunk relatively subjective.\n",
    "Therefore, chunk shape and size selection is just as much an art as a science.\n",
    "It depends some firm rules, but it also depends on our preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a5ec8-49e7-457c-a9da-074a707f0af2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Optimal Chunk Shape Algorithm\n",
    "\n",
    "As chunk shape and size selection is relatively subjective, let's create an algorithm using some of our preferences and restrictions to automate the chunk selection process.\n",
    "To take into account the first two considerations (size and partial chunks), this algorithm will focus on determining a chunk shape from the dataset shape given a maximum (uncompressed) chunk size and minimum partial chunk fraction.\n",
    "It will return a chunk shape that has a size as close the maximum size as possible, while ensuring any final chunks contain partial chunk fractions above the specified minimum.\n",
    "For reads across dimensions, we will limit our algorithm to 3D data only and try and balance 1D-to-2D read times.\n",
    "For example, if we have a 3D spatiotemporal dataset, it would propose an \"optimal\" shape that allows for almost equal read times for time-series and spatial reads.\n",
    "Finally, we will assume the algorithm will only be applied to static datasets and that we do not need to worry about any future data additions.\n",
    "\n",
    "So, let's take a look at the algorithm, which we will write as a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c4522a-387b-43d9-a9be-e4d967aaa898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_shape_3D(var_shape, chunk_size=4096, var_value_size=4, partial_chunk_frac=0.5, verbose=False):\n",
    "    \"\"\"\n",
    "    Return a \"optimal\" shape for a 3D variable, assuming balanced 1D-to-2D access.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    var_shape : tuple[int]\n",
    "        Length 3 list giving the variable shape in terms of (T, X, Y).\n",
    "    chunk_size : int\n",
    "        Maximum chunk size desired, in bytes (default = 4096).\n",
    "    var_value_size : int\n",
    "        Size of each variable data value, in bytes (default = 4).\n",
    "    partial_chunk_frac : float\n",
    "        The minimum fraction of data that the partial final chunk must\n",
    "        contain using the returned chunk shape (default = 0.5).\n",
    "    verbose : bool\n",
    "        If True, info on other candidate chunk shapes will be printed\n",
    "        (default = False).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    chunk_shape : tuple[int]\n",
    "        The optimal chunk shape that provides balanced access of 1D subsets\n",
    "        and 2D subsets of a variable with shape (T, X, Y), T is the typically\n",
    "        the time dimension and X and Y are the spatial dimensions. An \"optimal\"\n",
    "        shape for chunks means that the number of chunks accessed to read\n",
    "        either a full time series (1D) or full spatial map (2D) is\n",
    "        approximately equal, and the size of each chunk (uncompressed) is no\n",
    "        more than `chunk_size`, which is often a disk block size.\n",
    "    \"\"\"\n",
    "    rank = len(var_shape)\n",
    "    # Get ideal chunk info using only chunk size and balanced read\n",
    "    ideal_num_vals_per_chunk = chunk_size / float(var_value_size)\n",
    "    ideal_num_chunks = np.prod(var_shape) / ideal_num_vals_per_chunk\n",
    "    if ideal_num_chunks < 1:\n",
    "        ideal_num_chunks = 1\n",
    "    ideal_1d_num_chunks = np.sqrt(ideal_num_chunks)\n",
    "    ideal_2d_num_chunks = np.sqrt(ideal_1d_num_chunks)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Ideal number of values in a chunk = {ideal_num_vals_per_chunk:0.1f}')\n",
    "        print(f'Ideal number of chunks = {ideal_num_chunks:0.1f}')\n",
    "        print(f'Ideal number of chunks along the 1D axis = {ideal_1d_num_chunks:0.1f}')\n",
    "        print(f'Ideal number of chunks along each 2D axis = {ideal_2d_num_chunks:0.1f}')\n",
    "\n",
    "    # Get 1D optimal chunk shape along dimension\n",
    "    # Check if the first dimension has smaller shape then number of chunks\n",
    "    # If so, set to chunk shape to 1 and adjust 2D ideal chunks\n",
    "    if var_shape[0] < ideal_1d_num_chunks:\n",
    "        chunk_dim = 1.0\n",
    "        ideal_2d_num_chunks = (ideal_2d_num_chunks\n",
    "                               / np.sqrt(var_shape[0] / ideal_1d_num_chunks))\n",
    "    else:\n",
    "        chunk_dim = var_shape[0] // ideal_1d_num_chunks\n",
    "    # Add chunk dim to optimal chunk shape list\n",
    "    optimal_chunk_shape = [chunk_dim]\n",
    "\n",
    "    # Get 2D optimal chunk shape along each dimension\n",
    "    prod = 1.0  # factor to increase other dims if some must be increased to 1.0\n",
    "    for i in range(1, rank):\n",
    "        if var_shape[i] < ideal_2d_num_chunks:\n",
    "            prod *= ideal_2d_num_chunks / var_shape[i]\n",
    "            \n",
    "    for i in range(1, rank):\n",
    "        if var_shape[i] < ideal_2d_num_chunks:\n",
    "            chunk_dim = 1.0\n",
    "        else:\n",
    "            chunk_dim = (prod * var_shape[i]) // ideal_2d_num_chunks\n",
    "        optimal_chunk_shape.append(chunk_dim)\n",
    "\n",
    "    # Calculate the partial chunk fraction from the remainder\n",
    "    remainder_frac_per_dim = np.remainder(var_shape, optimal_chunk_shape) / optimal_chunk_shape\n",
    "    # If the remainder fraction is 0, swap with 1 for multiplication of parial chunk fraction\n",
    "    optimal_chunk_frac = np.where(remainder_frac_per_dim == 0, 1, remainder_frac_per_dim).prod()\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Ideal chunk shape = {tuple(map(int, optimal_chunk_shape))}')\n",
    "        print(f'Ideal chunk, partial chunk fraction = {optimal_chunk_frac}')\n",
    "\n",
    "    \"\"\"\n",
    "    Optimal_chunk_shape is typically too small, size(optimal_chunk_shape) < chunk_size\n",
    "    and may have small partial chunks. So, we adjust by adding 1 to some chunk shape\n",
    "    dimensions to get as close as possible to chunk_size without exceeding it and then\n",
    "    check if the shape is over the minimum partial chunk fraction. If it is acceptable,\n",
    "    that is our chunk shape. Otherwise, we continually subtract one from the 1D dimension\n",
    "    to get to the partial fraction minimum, while adding one to the 2D dimensions to\n",
    "    maintain the size request. We then reverse this and subtract from the 2D demensions\n",
    "    and add to the 1D demensions. The optimal chunk is then the one that is the most\n",
    "    balanced of these two increment and decrement methods.\n",
    "    \"\"\"\n",
    "    # Increment the optimal chunk shape by 1\n",
    "    best_chunk_size = 0\n",
    "    best_chunk_shape = []\n",
    "    if verbose:\n",
    "        print('\\n--- Candidates ---')\n",
    "    for dim_increment in itertools.product([0, 1], repeat=3):\n",
    "        candidate_chunk_shape = np.add(optimal_chunk_shape, dim_increment)\n",
    "        \n",
    "        this_chunk_size = int(var_value_size * np.prod(candidate_chunk_shape))\n",
    "        remainder = np.remainder(var_shape, candidate_chunk_shape) / candidate_chunk_shape\n",
    "        this_chunk_frac = np.where(remainder == 0, 1, remainder).prod()\n",
    "        \n",
    "        if verbose:\n",
    "            if (this_chunk_size <= chunk_size) and (this_chunk_frac >= partial_chunk_frac):\n",
    "                print(f'{tuple(map(int, candidate_chunk_shape))}; '\n",
    "                      f'Total size per chunk (MB): {this_chunk_size/2**20:0.3f} '\n",
    "                      f'(ratio: {np.prod(candidate_chunk_shape) / ideal_num_vals_per_chunk:0.3f}); '\n",
    "                      f'Partial chunk fraction: {this_chunk_frac}')\n",
    "\n",
    "        # Only keep if closest to chunk size limit and above partial fraction limit\n",
    "        if (best_chunk_size < this_chunk_size <= chunk_size) and (this_chunk_frac >= partial_chunk_frac):\n",
    "            best_chunk_size = this_chunk_size\n",
    "            best_chunk_shape = list(candidate_chunk_shape) # make a copy of best candidate so far\n",
    "\n",
    "    # Return if a shape was found\n",
    "    if best_chunk_shape:\n",
    "        return list(map(int, best_chunk_shape))\n",
    "\n",
    "    # Increment and decrement 1D and 2D from optimal chunk shape to get a best shape    \n",
    "    increments_decrements = [[[-1, 0, 0], [0, 1, 0], [0, 0, 1]],\n",
    "                             [[1, 0, 0], [0, -1, 0], [0, 0, -1]]]\n",
    "    # Use Euclidean distance to estimate balanced shape\n",
    "    best_shape_balance = np.linalg.norm(np.array(optimal_chunk_shape) - np.array([0, 0, 0]))\n",
    "    for increment_decrement in increments_decrements:\n",
    "        best_chunk_frac = optimal_chunk_frac\n",
    "        candidate_chunk_shape = list(optimal_chunk_shape)\n",
    "        while best_chunk_frac < partial_chunk_frac:\n",
    "            # Quit if any candidate is too big or too small in a dimension\n",
    "            if ((np.array(candidate_chunk_shape) < 1).any()\n",
    "                or (candidate_chunk_shape > np.array(var_shape)).any()):\n",
    "                break\n",
    "\n",
    "            for dim_increment in increment_decrement:\n",
    "                candidate_chunk_shape = np.add(candidate_chunk_shape, dim_increment)\n",
    "                \n",
    "                this_chunk_size = int(var_value_size * np.prod(candidate_chunk_shape))\n",
    "                remainder = np.remainder(var_shape, candidate_chunk_shape) / candidate_chunk_shape\n",
    "                this_chunk_frac = np.where(remainder == 0, 1, remainder).prod()\n",
    "                \n",
    "                if (this_chunk_size <= chunk_size) and (this_chunk_frac >= partial_chunk_frac):\n",
    "                    if verbose:\n",
    "                        print(f'{tuple(map(int, candidate_chunk_shape))}; '\n",
    "                              f'Total size per chunk (MB): {this_chunk_size/2**20:0.3f} '\n",
    "                              f'(ratio: {np.prod(candidate_chunk_shape) / ideal_num_vals_per_chunk:0.3f}); '\n",
    "                              f'Partial chunk fraction: {this_chunk_frac}')\n",
    "                \n",
    "                    best_chunk_frac = this_chunk_frac\n",
    "                    shape_balance = np.linalg.norm(np.array(optimal_chunk_shape) - candidate_chunk_shape)\n",
    "                    # Only save candidate if it is more balanced than previous one\n",
    "                    if shape_balance < best_shape_balance:\n",
    "                        best_shape_balance = shape_balance\n",
    "                        best_chunk_shape = list(candidate_chunk_shape)\n",
    "\n",
    "\n",
    "    return tuple(map(int, best_chunk_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedb4e04-8130-4858-b7c2-385ea124178d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Examples\n",
    "\n",
    "### Toy Example\n",
    "\n",
    "Okay, now that we have our algorithm, let's give it a test using some made up data.\n",
    "We will use a variable of shape `(365, 240, 150)`, aim for a chunk of 1 MiB (`2**20` bytes) in size, and a partial chunk fraction of `0.5` (0.8 in each dimensions; $0.8^3 \\approx 0.5$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4b8326-e846-4f84-ba78-7ad4fd1a1c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_shape = (365, 240, 150)\n",
    "chunk_shape = chunk_shape_3D(\n",
    "    var_shape, chunk_size=2**20, partial_chunk_frac=0.5, verbose=True\n",
    ")\n",
    "print(f'\\n\"Optimal\" chunk shape: {chunk_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9bb4a7-fc11-41b8-9713-5a7ec45f3e0e",
   "metadata": {},
   "source": [
    "Nice!\n",
    "From these results, we can see that the \"ideal\" chunk shape given only our chunk size would have been `(51, 90, 56)`.\n",
    "However, this had some partial chunks that would have been very small.\n",
    "So, its choice would not have been good given this additional constraint.\n",
    "Therefore, it adjusted the chunk shape to `(53, 88, 54)` to meet the partial chunk constraint and provide us with a reasonably close alternative in terms of both dimension balance and chunk size.\n",
    "This is great, exactly what we wanted!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8a8d0d-a75a-409e-ad00-202d0b5cf100",
   "metadata": {},
   "source": [
    "### PRISM Example\n",
    "\n",
    "As a real world example, let's apply the algorithm to the PRISM data from the [Basics of Chunk Shape and Size notebook](../101/BasicsShapeSize.ipynb).\n",
    "First, we need to read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b888f0-c335-4836-bb02-a48d5f882986",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(\n",
    "    's3',\n",
    "    anon=True,   # anonymous = does not require credentials\n",
    "    client_kwargs={'endpoint_url': 'https://usgs.osn.mghpcc.org/'}\n",
    ")\n",
    "ds = xr.open_dataset(\n",
    "    fs.get_mapper('s3://mdmf/gdp/PRISM_v2.zarr/'),\n",
    "    engine='zarr',\n",
    "    chunks={}\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f15d796-7ed8-4841-afa1-94ae82a005e8",
   "metadata": {},
   "source": [
    "Now, we can estimate the \"optimal\" chunk shape using our algorithm.\n",
    "We will only do this for the precipitaiton data variable as the others all have the same shape and data type. If you had additional variables with different data types, you would want to run this algorithm on a variable of each type to find its optimal size.\n",
    "Also, we will use a maximum chunk size that matches the same chunk size of the dataset currently to see how our algorithm compares to the chosen chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149194b3-dad9-4a02-961e-1597449d3910",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_chunk_size = np.prod([chunksize[0] for chunksize in ds['ppt'].chunksizes.values()])\n",
    "current_chunk_size *= ds['ppt'].dtype.itemsize\n",
    "\n",
    "chunk_shape = chunk_shape_3D(\n",
    "    ds['ppt'].shape,\n",
    "    chunk_size=current_chunk_size,\n",
    "    var_value_size=ds['ppt'].dtype.itemsize,\n",
    "    verbose=True\n",
    ")\n",
    "print(f'\\n\"Optimal\" chunk shape: {chunk_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2762d3-b358-474c-b63a-d469c262ba9c",
   "metadata": {},
   "source": [
    "As we can see, our algorithm struggles with this dataset to come up with a balanced chunk shape that meets our size and partial fraction restrictions.\n",
    "Rather than having the \"ideal\" chunk shape of `(125, 178, 404)`, we got a chunk shape of `(168, 135, 362)`, which is quite different than the current chunk shape of `(72, 354, 354)`.\n",
    "The primary driver of this discrepancy is our restriction on balancing the dimensions, as the current chunk shape has a parital fraction of 0.73.\n",
    "Therefore, enforcing the balance can be very restrictive."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7645c50-6f2e-471e-b91c-e961331d1212",
   "metadata": {},
   "source": [
    "## Further Considerations\n",
    "\n",
    "As noted in the PRISM example, this algorithm is a very niche case for chunk shape selection.\n",
    "It assumes you want even read times for the temporal and spatial dimensions, and it only allows for 3D data.\n",
    "What if we had more dimensions or fewer?\n",
    "What if we wanted a different read pattern that was unbalanced?\n",
    "What if we wanted to not have any partial chunks and only use a chunk shape that is a divisor of the variable shape?\n",
    "Therefore, this algorithm is not general by any means, but does give us an idea on how to formulate more general algorithms in the future.\n",
    "For example, one of these more general algorithms can be found in [this repo on dynamic rechunking](https://github.com/jbusecke/dynamic_chunks), which has some algorithms that allow for the user to specify the balancing of the dimensions and a chunk size limit.\n",
    "As algorithms like these become more developed, selecting an optimal chunk size should become easier.\n",
    "However, the subjective component will never go away and the need for someone to make a decision on what critera that influences chunk shape and size is more important will always be required."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
