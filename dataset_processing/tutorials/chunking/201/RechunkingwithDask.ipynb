{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rechunking Larger Datasets with Dask\n",
    "\n",
    "The goal of this notebook is to expand on the rechunking performed in the [Introductory Rechunking tutorial](../101/Rechunking.ipynb).\n",
    "This notebook will perfrom the same operations, but will work on the **much** larger dataset and involve some parallelization using [dask](https://www.dask.org/). \n",
    "\n",
    ":::{Warning}\n",
    "You should only run workflows like this tutorial on a cloud or HPC compute node.\n",
    "In application, this will require reading and writing **enormous** amounts of data.\n",
    "Using a typical network connection and simple compute environment, you would saturate your bandwidth and max out your processor, thereby taking days for the rechunking to complete.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Needed when boto3 >= 1.36.0 or the rechunking process will fail\n",
    "# This needs to be set before the boto3 library gets loaded\n",
    "# See: https://github.com/aws/aws-cli/issues/9214#issuecomment-2606619168\n",
    "os.environ['AWS_REQUEST_CHECKSUM_CALCULATION'] = 'when_required'\n",
    "os.environ['AWS_RESPONSE_CHECKSUM_VALIDATION'] = 'when_required'\n",
    "\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "from rechunker import rechunk\n",
    "import zarr\n",
    "import dask.distributed\n",
    "import dask.diagnostics\n",
    "import logging\n",
    "import dask\n",
    "from dask_gateway import Gateway\n",
    "import configparser\n",
    "\n",
    "dask.config.set({'temporary_directory':'../'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in a Zarr Store\n",
    "\n",
    "Like the [Introductory Rechunking tutorial](../101/Rechunking.ipynb), we will use the data from the National Water Model Retrospective Version 2.1.\n",
    "The full dataset is part of the [AWS Open Data Program](https://aws.amazon.com/opendata/), available via the S3 bucket at: `s3://noaa-nwm-retro-v2-zarr-pds/`.\n",
    "\n",
    "As this is a Zarr store, let's read it in with [`xarray.open_dataset()`](https://docs.xarray.dev/en/stable/generated/xarray.open_dataset.html) and `engine='zarr'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = fsspec.get_mapper('s3://noaa-nwm-retro-v2-zarr-pds', anon=True)\n",
    "ds = xr.open_dataset(file, chunks={}, engine='zarr')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restrict for Tutorial\n",
    "\n",
    "As we saw in the [Introductory Rechunking tutorial](../101/Rechunking.ipynb), this dataset is massive, taking up almost 32 TiB uncompressed.\n",
    "As this is a tutorial, we will still restrict the data to a subset - we don't really need to work on the entire dataset to demonstrate how to use dask to scale up.\n",
    "Following the [Introductory Rechunking tutorial](../101/Rechunking.ipynb) let's only look at `streamflow` and `velocity` for the first 15,000 `feature_id`s, |but we will look at the entire 2000s decade of water years (October 1999 through September 2009) instead of a single water year this time.\n",
    "This will make our dataset larger-than-memory, but it should still run in a reasonable amount of time.\n",
    "\n",
    "For processing the full-sized dataset, you'd just skip this step where we slice off a representative example of the data.\n",
    "Expect run time to increase in proportion to the size of the data being processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[['streamflow', 'velocity']]\n",
    "ds = ds.isel(feature_id=slice(0, 15000))\n",
    "ds = ds.sel(time=slice('1999-10-01', '2009-09-30'))\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our subset of data is only about 10 GiB per data variable and has a chunk shape of `{'time': 672, 'feature_id': 15000}` with size of 76.9 MiB.\n",
    "However, the chunk shape is not an optimal choice for our analysis as it is chunked completely by `feature_id` (i.e., all feature IDs for a given time can be read in a single chunk).\n",
    "Following the [Introductory Rechunking tutorial](../101/Rechunking.ipynb), let's get chunk shapes that are time-series wise chunking (i.e., all `time` for a given `feature_id` in one chunk) for streamflow and balanced for velocity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rechunk Plan\n",
    "\n",
    "Using our general strategy of time-series wise chunking for streamflow and balanced for velocity,\n",
    "let's compute how large the chunk sizes will be if we have chunk shapes of `{'time': 87672, 'feature_id': 1}` for streamflow and 3 chunks per dimension for velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeature = len(ds.feature_id)\n",
    "ntime = len(ds.time)\n",
    "\n",
    "streamflow_chunk_plan = {'time': ntime, 'feature_id': 1}\n",
    "bytes_per_value = ds.streamflow.dtype.itemsize\n",
    "total_bytes = streamflow_chunk_plan['time'] * streamflow_chunk_plan['feature_id'] * bytes_per_value\n",
    "streamflow_MiB = total_bytes / (2 ** 10) ** 2\n",
    "partial_chunks = {'time': ntime -  streamflow_chunk_plan['time'] * (ntime / streamflow_chunk_plan['time']),\n",
    "                  'feature_id': nfeature -  streamflow_chunk_plan['feature_id'] * (nfeature / streamflow_chunk_plan['feature_id']),}\n",
    "print(\"STREAMFLOW \\n\"\n",
    "      f\"Chunk of shape {streamflow_chunk_plan} \\n\"\n",
    "      f\"Partial 'time' chunk remainder: {partial_chunks['time']} ({partial_chunks['time']/streamflow_chunk_plan['time']:.3f}% of a chunk)\\n\"\n",
    "      f\"Partial 'feature_id' chunk remainder: {partial_chunks['feature_id']} ({partial_chunks['feature_id']/streamflow_chunk_plan['feature_id']:.3f}% of a chunk)\\n\"\n",
    "      f\"Chunk size: {streamflow_MiB:.2f} [MiB] \\n\")\n",
    "\n",
    "chunks_per_dim = 3\n",
    "velocity_chunk_plan = {'time': ntime // chunks_per_dim, 'feature_id': nfeature // chunks_per_dim}\n",
    "bytes_per_value = ds.velocity.dtype.itemsize\n",
    "total_bytes = velocity_chunk_plan['time'] * velocity_chunk_plan['feature_id'] * bytes_per_value\n",
    "velocity_MiB = total_bytes / (2 ** 10) ** 2\n",
    "partial_chunks = {'time': ntime -  velocity_chunk_plan['time'] * chunks_per_dim,\n",
    "                  'feature_id': nfeature -  velocity_chunk_plan['feature_id'] * chunks_per_dim,}\n",
    "print(\"VELOCITY \\n\"\n",
    "      f\"Chunk of shape {velocity_chunk_plan} \\n\"\n",
    "      f\"Partial 'time' chunk remainder: {partial_chunks['time']} ({partial_chunks['time']/velocity_chunk_plan['time']:.3f}% of a chunk)\\n\"\n",
    "      f\"Partial 'feature_id' chunk remainder: {partial_chunks['feature_id']} ({partial_chunks['feature_id']/velocity_chunk_plan['feature_id']:.3f}% of a chunk)\\n\"\n",
    "      f\"Chunk size: {velocity_MiB:.2f} [MiB]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we can see that the streamflow chunk size is way to small by a factor of ~100.\n",
    "So, let's include 100 feature IDs per chunk.\n",
    "As for velocity, it is ~10x too big.\n",
    "As it is an even chunk split, that means we need to increase the number of chunks per dimension by ~$\\sqrt{10} \\approx 3$.\n",
    "However knowing that the time dimension is hourly, we can get no partial chunks if our chunk per dimension is a divisor of 24.\n",
    "Luckily, this also applies to the feature ID dimension as 15000 is a multiple of 24.\n",
    "So, rather than increasing our chunks per dimension by a factor of 3 to 9, let's increase them to 12 as this will give no partial chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeature = len(ds.feature_id)\n",
    "ntime = len(ds.time)\n",
    "\n",
    "streamflow_chunk_plan = {'time': ntime, 'feature_id': 100}\n",
    "bytes_per_value = ds.streamflow.dtype.itemsize\n",
    "total_bytes = streamflow_chunk_plan['time'] * streamflow_chunk_plan['feature_id'] * bytes_per_value\n",
    "streamflow_MiB = total_bytes / (2 ** 10) ** 2\n",
    "partial_chunks = {'time': ntime -  streamflow_chunk_plan['time'] * (ntime / streamflow_chunk_plan['time']),\n",
    "                  'feature_id': nfeature -  streamflow_chunk_plan['feature_id'] * (nfeature / streamflow_chunk_plan['feature_id']),}\n",
    "print(\"STREAMFLOW \\n\"\n",
    "      f\"Chunk of shape {streamflow_chunk_plan} \\n\"\n",
    "      f\"Partial 'time' chunk remainder: {partial_chunks['time']} ({partial_chunks['time']/streamflow_chunk_plan['time']:.3f}% of a chunk)\\n\"\n",
    "      f\"Partial 'feature_id' chunk remainder: {partial_chunks['feature_id']} ({partial_chunks['feature_id']/streamflow_chunk_plan['feature_id']:.3f}% of a chunk)\\n\"\n",
    "      f\"Chunk size: {streamflow_MiB:.2f} [MiB] \\n\")\n",
    "\n",
    "chunks_per_dim = 12\n",
    "velocity_chunk_plan = {'time': ntime // chunks_per_dim, 'feature_id': nfeature // chunks_per_dim}\n",
    "bytes_per_value = ds.velocity.dtype.itemsize\n",
    "total_bytes = velocity_chunk_plan['time'] * velocity_chunk_plan['feature_id'] * bytes_per_value\n",
    "velocity_MiB = total_bytes / (2 ** 10) ** 2\n",
    "partial_chunks = {'time': ntime -  velocity_chunk_plan['time'] * chunks_per_dim,\n",
    "                  'feature_id': nfeature -  velocity_chunk_plan['feature_id'] * chunks_per_dim,}\n",
    "print(\"VELOCITY \\n\"\n",
    "      f\"Chunk of shape {velocity_chunk_plan} \\n\"\n",
    "      f\"Partial 'time' chunk remainder: {partial_chunks['time']} ({partial_chunks['time']/velocity_chunk_plan['time']:.3f}% of a chunk)\\n\"\n",
    "      f\"Partial 'feature_id' chunk remainder: {partial_chunks['feature_id']} ({partial_chunks['feature_id']/velocity_chunk_plan['feature_id']:.3f}% of a chunk)\\n\"\n",
    "      f\"Chunk size: {velocity_MiB:.2f} [MiB]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!\n",
    "Now, our chunks are a reasonable size and have no remainders.\n",
    "So, let's use these chunk plans for our rechunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_plan = {\n",
    "    'streamflow': streamflow_chunk_plan,\n",
    "    'velocity': velocity_chunk_plan,\n",
    "     # We don't want any of the coordinates chunked\n",
    "    'latitude': (nfeature,),\n",
    "    'longitude': (nfeature,),    \n",
    "    'time': (ntime,),\n",
    "    'feature_id': (nfeature,)\n",
    "}\n",
    "chunk_plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rechunk with Rechunker\n",
    "\n",
    "With this plan, we can now ask Rechunker to re-write the data using the prescribed chunking pattern.\n",
    "\n",
    "### Set up output location\n",
    "\n",
    "Unlike with the smaller dataset in our previous rechunking tutorial, we will write this larger dataset to an object store (similar to an S3 bucket) on the USGS OSN Pod.\n",
    "So, we need to set that up so that Rechunker will have a suitable place to write data.\n",
    "\n",
    "First, we need to set up our credentials that allow us to write to the OSN Pod and direct it to the endpoint where the OSN Pod is located (while it is available via the S3 API, it is not in AWS, so we have to tell it where to find the pod!). If you do not already have access to the credentials to write to the OSN Pod and you are a USGS staff member or collaborator, please reach out to asnyder@usgs.gov to request them. If you are not a USGS staff or collaborator, you will need to set up a different location that to you have write permissions to. This could be an AWS S3 bucket that you have access too or you could even set up an `fsspec` `LocalFileSystem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awsconfig = configparser.ConfigParser()\n",
    "awsconfig.read(\n",
    "    os.path.expanduser('~/.aws/credentials') \n",
    "    # default location... if yours is elsewhere, change this.\n",
    ")\n",
    "_profile_nm  = 'osn-hytest-scratch'\n",
    "_endpoint = 'https://usgs.osn.mghpcc.org/'\n",
    "\n",
    "# Set environment vars based on parsed awsconfig\n",
    "try:\n",
    "    os.environ['AWS_ACCESS_KEY_ID']     = awsconfig[_profile_nm]['aws_access_key_id']\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = awsconfig[_profile_nm]['aws_secret_access_key']\n",
    "    os.environ['AWS_S3_ENDPOINT']       = _endpoint\n",
    "    os.environ['AWS_PROFILE'] = _profile_nm\n",
    "    os.environ['AWS_DEFAULT_PROFILE'] = _profile_nm\n",
    "except KeyError as e:\n",
    "    logging.error(\"Problem parsing the AWS credentials file. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we make our S3 `fsspec.filesystem` with the required user info and get the mapper to this file to pass to Rechunker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getuser\n",
    "uname=getuser()\n",
    "\n",
    "fs = fsspec.filesystem(\n",
    "    's3', \n",
    "    anon=False, \n",
    "    default_fill_cache=False, \n",
    "    skip_instance_cache=True, \n",
    "    client_kwargs={'endpoint_url': os.environ['AWS_S3_ENDPOINT'], }\n",
    ")\n",
    "\n",
    "output_dir = f's3://hytest-scratch/rechunking_tutorial/{uname}/'\n",
    "\n",
    "temp_store = fs.get_mapper(output_dir + 'temp_store.zarr')\n",
    "target_store = fs.get_mapper(output_dir + 'tutorial_rechunked.zarr')\n",
    "# Check if the objects exist and remove if they do\n",
    "for filename in [temp_store, target_store]:\n",
    "    try:\n",
    "        fs.rm(filename.root, recursive=True)\n",
    "    except:\n",
    "        FileNotFoundError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spin up Dask Cluster\n",
    "\n",
    "Our rechunking operation will be able to work in parallel.\n",
    "To do that, we will spin up a dask cluster to schedule the various workers.\n",
    "\n",
    "This cluster will be configured differently depending on where you compute is performed. We have set up this notebook to run on [Nebari](https://www.nebari.dev/). If you are working on USGS systems, you can likely use one of our [example codes to spin up a dask cluster](https://hytest-org.github.io/hytest/environment_set_up/clusters.html). Otherwise, please refer to the [dask deployment docs](https://docs.dask.org/en/stable/deploying.html) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['DASK_DISTRIBUTED__SCHEDULER__WORKER_SATURATION'] = \"1.0\"\n",
    "\n",
    "gateway = Gateway()\n",
    "_options = gateway.cluster_options()\n",
    "_options.conda_environment='hytest/hytest-rechunking'  ##<< this is the conda environment we use on nebari.\n",
    "_options.profile = 'Medium Worker'\n",
    "_env_to_add={}\n",
    "aws_env_vars=['AWS_ACCESS_KEY_ID',\n",
    "              'AWS_SECRET_ACCESS_KEY',\n",
    "              'AWS_SESSION_TOKEN',\n",
    "              'AWS_DEFAULT_REGION',\n",
    "              'AWS_S3_ENDPOINT',\n",
    "              'AWS_REQUEST_CHECKSUM_CALCULATION',\n",
    "              'AWS_RESPONSE_CHECKSUM_VALIDATION']\n",
    "for _e in aws_env_vars:\n",
    "    if _e in os.environ:\n",
    "        _env_to_add[_e] = os.environ[_e]\n",
    "_options.environment_vars = _env_to_add    \n",
    "cluster = gateway.new_cluster(_options)          ##<< create cluster via the dask gateway\n",
    "cluster.scale(30)                                ##<< Sets scaling parameters. \n",
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rechunk\n",
    "\n",
    "Now, we are ready to rechunk!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rechunk(\n",
    "    # Make sure the base chunks are correct\n",
    "    ds.chunk({'time': 672, 'feature_id': 15000}),\n",
    "    target_chunks=chunk_plan,\n",
    "    max_mem=\"2GB\",\n",
    "    target_store=target_store,\n",
    "    temp_store=temp_store\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that merely invoking Rechunker does not do any work.\n",
    "It just sorts out the rechunking plan and writes metadata.\n",
    "We need to call `.execute` on the `result` object to actually run the rechunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dask.diagnostics.ProgressBar():\n",
    "    r = result.execute(retries=10)  \n",
    "\n",
    "# Also consolidate the metadata for fast reading into xarray\n",
    "_ = zarr.consolidate_metadata(target_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm the Creation of the Zarr Store by Rechunker\n",
    "\n",
    "Let's read in the resulting re-chunked dataset to confirm it turned out how we intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rechunked = xr.open_zarr(target_store)\n",
    "ds_rechunked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, looks good!\n",
    "You may have noticed that the only difference between the [introductory tutorial on rechunking](../101/Rechunking.ipynb) and this is the inclusion of creating the dask cluster and where we saved the files.\n",
    "Picking your compute environment and output location will typically be the only things that vary in other workflows requiring rechunking.\n",
    "Therefore, if you understand this rechunking process you should be able to apply it to your own data efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "As we don't want to keep this rechunked Zarr, let's go ahead and delete it.\n",
    "We will also conform with best practices and close our Dask client and cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.rm(temp_store.root, recursive=True)\n",
    "fs.rm(target_store.root, recursive=True)\n",
    "        \n",
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
