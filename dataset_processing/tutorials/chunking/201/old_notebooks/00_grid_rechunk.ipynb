{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c8086d-13e4-47af-811c-482464eed528",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Rechunk NWM 1km gridded output\n",
    "\n",
    "The NWM 1km gridded output is available on AWS S3 as hourly NetCDF files, so we we would like to rechunk them to be read by the Zarr library.\n",
    "\n",
    "One approach would be to download all the files and work on them locally, like Pangeo forge often does. We will try a different approach, of first kerchunking the NetCDF files to make them more performant, then running rechunker on the kerchunked dataset. In both steps, we will use a Dask Gateway cluster, with workers writing directly to S3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c6a17-a487-4ddd-8596-ac2006b14068",
   "metadata": {},
   "source": [
    "## Chunking and rechunking resources\n",
    "*  [\"Making Earth Science data more accessible\"](https://www.slideserve.com/kiaria/making-earth-science-data-more-accessible-experience-with-chunking-and-compression), AMS presentation slides by Russ Rew, Unidata (2013)\n",
    "*  [\"Rechunker: The missing link for chunked array analytics\"](https://medium.com/pangeo/rechunker-the-missing-link-for-chunked-array-analytics-5b2359e9dc11), Medium blog post by Ryan Abernathy, Columbia University (2020)\n",
    "*  [\"Rechunker\" Python Library Documentation](https://rechunker.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ab4294-7b92-4482-9912-57357075be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f81d41d-93fb-4e04-ad6d-26f23129cd90",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"Python : \", sys.version)\n",
    "print(\"fsspec : \", fsspec.__version__)\n",
    "print(\"xarray : \", xr.__version__)\n",
    "print(\"zarr   : \", zarr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7601239-424c-4ae4-b5f8-760cd58a9141",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('s3', anon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0467a0d8-0759-48c3-92b8-840918481dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = fs.ls('s3://noaa-nwm-retrospective-2-1-pds/')\n",
    "flist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3688846d-8df7-427c-8226-6fd0bf21dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = fs.glob('noaa-nwm-retrospective-2-1-pds/model_output/*')\n",
    "print(flist[0])\n",
    "print(flist[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e34f9c7-adf2-42db-866c-6e4c4487c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = fs.glob('noaa-nwm-retrospective-2-1-pds/model_output/1979/*LDAS*')\n",
    "flist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3755ecf-266a-4403-b3ed-c7edc8b0ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = fs.glob('noaa-nwm-retrospective-2-1-pds/model_output/2020/*LDAS*')\n",
    "flist[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb47465-4eee-4edd-8815-26050b02090c",
   "metadata": {},
   "source": [
    "Okay, so at this point we've learned that we have 3-hourly output over roughly 40 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621ceb7e-191a-4884-a474-b47a2afcc0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# flist = fs.glob('noaa-nwm-retrospective-2-1-pds/model_output/*/*LDAS*')   # this is slow\n",
    "40 * 365 * 24 / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe431e-5564-4c15-99d6-338d6af840d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "flist[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1646169b-7466-4146-8060-5c39985604d9",
   "metadata": {},
   "source": [
    "So about 117,000 NetCDF files! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85914de3-6b6d-456f-8f37-61bc24e6964d",
   "metadata": {},
   "source": [
    "Let's check one out.  Although it's not super efficient, we can open a NetCDF file on S3 as a virtual file object with `fs.open(s3_url_of_netcdf_file)`.  If we open a dataset in xarray using `chunks=` we are telling xarray to use Dask, and `chunks={}` means use the native chunking in the NetCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7355518-8490-440c-99e8-f183b5e73c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(fs.open(flist[0]), chunks={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e54fefb-c522-4c5b-80cb-45c39f88014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f056f13-6cb8-48cb-ac4e-d95c12758ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055162ae-cda2-4d6f-8350-246ce3fbb743",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[['ACCET', 'SNEQV', 'FSNO']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24e41c-afbc-40d4-8211-1be8ab796527",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ddab31-7090-4fd3-bccc-c61c8ff1cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['ACCET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0af67c-70c1-4043-adec-ca95cb15f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ACCET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bd73ae-618a-4342-8b90-54d7e58fe769",
   "metadata": {},
   "source": [
    "The data is chunked as full spatial domain and 1 time step, with about 135MB chunk size.   This is actually great for visualization of maps at specific time steps or for calculations that involve the entire dataset. So kerchunking this data would be a nice first step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3591a978-e5f5-48ca-aeb6-7df31a224779",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "da = ds.ACCET.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a59214-6041-4d47-a5a8-ee61a08c1420",
   "metadata": {},
   "outputs": [],
   "source": [
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e94899e-66a1-4f1e-8568-a4ac8d2a0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.hvplot(x='x', y='y', rasterize=True, cmap='turbo', data_aspect=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a81208-88ea-4921-9e21-aade42fc3952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "21742c3f70e60132a0f53ab9a02119f1da5d00e790acd69466294df8491d8f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
