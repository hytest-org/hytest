{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing OpenDAP Data to ZARR on S3\n",
    "\n",
    "**OBJECTIVE**:  \n",
    "The objective of this chapter is to demonstrate how to read an existing dataset available as an OpenDAP endpoint, and translate it into a cloud-optimized zarr on S3. \n",
    "\n",
    "This notebook will actually write data to S3, using the chunking patterns we \n",
    "decided on based on the {doc}`ExamineSourceData` and {doc}`EffectSizeShape` notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Data\n",
    "We're still looking at readingthe PRISM(v2) dataset via its OpenDAP endpoint: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: \n",
    "OPENDAP_url = 'https://cida.usgs.gov/thredds/dodsC/prism_v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "This is all stuff we are going to need: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import xarray as xr\n",
    "import dask\n",
    "import fsspec\n",
    "import zarr\n",
    "import hvplot.xarray\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%run ../utils.ipynb\n",
    "_versions(['xarray', 'dask', 'fsspec', 'zarr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Credentials\n",
    "Because we will be writing to an S3 object store, we need credentials.\n",
    "This notebook will assume that the correct credentials are already \n",
    "stored in `~/.aws/credentials` . \n",
    "\n",
    "I am using a profile to write to the OSN storage device (profile name \n",
    "`osn-rsignellbucket2`). If you run this notebook and want to write elsewhere \n",
    "with other credentials, you may change the profile name and endpoint \n",
    "in the cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_PROFILE'] = 'osn-rsignellbucket2'\n",
    "os.environ['AWS_ENDPOINT'] = 'https://renc.osn.xsede.org'\n",
    "\n",
    "%run ../AWS.ipynb  # handles credentials for us. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTPUT Location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = f's3://rsignellbucket2/testing/prism/'\n",
    "\n",
    "# OUTPUT Dataset Name:\n",
    "FNAME = 'PRISM2.zarr'\n",
    "\n",
    "# Instantiate a fsspec reference to the workspace: \n",
    "fsw = fsspec.filesystem('s3', \n",
    "    anon=False, \n",
    "    default_fill_cache=False, \n",
    "    skip_instance_cache=True, \n",
    "    client_kwargs={ 'endpoint_url': os.environ['AWS_S3_ENDPOINT'] },\n",
    ") # this will take credentials from the environment variables, \n",
    "# as defined above. No need to specify profile or keys. The endpoint, \n",
    "# unfortunately is necessary to name explicitly.\n",
    "\n",
    "outdir = workspace + FNAME\n",
    "target_store = fsw.get_mapper(outdir)\n",
    "\n",
    "try:\n",
    "    if fsw.exists(workspace + FNAME):\n",
    "        logging.warning(\"Removing existing file/folder: %s\", fname)\n",
    "        fsw.rm(workspace + fname, recursive=True)\n",
    "except:\n",
    "    # Occasionally, the cache doesn't catch up to the fact that we've deleted a file. \n",
    "    # In that case, it throws a FileNotFound exception. Ignore. \n",
    "    pass\n",
    "\n",
    "print(\"READY !!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Source Data\n",
    "\n",
    "Given what we calculated in the {doc}`EffectSizeShape` notebook, we can specify the\n",
    "chunking pattern we want when the data is initially read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_in = xr.open_dataset(OPENDAP_url, decode_times=False, chunks={'time': 72, 'lon': 354, 'lat': 354})\n",
    "ds_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that `xarray` uses lazy-loading -- the entire dataset is not in memory.  It provides\n",
    "us the fiction that it is, and loads data in chunks as needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Data\n",
    "\n",
    "OK... finally we are ready to write out our data.\n",
    "And the good news about using chunked data is that Dask is capable of doing its\n",
    "lazily-loaded data operations *in parallel* and *without hand-holding*.  We don't \n",
    "have to design a parallel workflow: Dask will sort it out.  BUT... to take advantage \n",
    "of that parallelism, we need to start up a cluster: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../StartNebariCluster.ipynb\n",
    "\n",
    "from dask.distributed import progress, performance_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to_zarr()\n",
    "With the data already lazy-loaded into the `ds_in` dataset, we can just\n",
    "call its `to_zarr()` method.  It will write using the chunk pattern already \n",
    "defined in the dataset object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with performance_report('../performance_reports/OpenDAP_to_S3-perfreport.html'):\n",
    "    ds_in.to_zarr(target_store, mode='w', consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify\n",
    "To make sure that we really wrote the whole thing to S3, let's sample the \n",
    "data for some simple plots: \n",
    "\n",
    "### Reading..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = xr.open_dataset(target_store, engine='zarr', chunks={})\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the other variable present, and chunked the same way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds.ppt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot time series for a single location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "da = new_ds.ppt.sel(lon=-75, lat=41.1, method='nearest').load()\n",
    "da.hvplot(x='time', grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a map for a single time-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "da = new_ds.tmx.sel(time=\"1970-01\").load()\n",
    "da.hvplot(x='lon', y='lat', rasterize=True, geo=True, tiles='OSM' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close down cluster\n",
    "Always clean up after yourself...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close(); cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
