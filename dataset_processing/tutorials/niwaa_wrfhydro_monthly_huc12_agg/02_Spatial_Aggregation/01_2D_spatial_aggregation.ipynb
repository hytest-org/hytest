{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49a34541-69af-4144-bf0b-13ad230fdd95",
   "metadata": {},
   "source": [
    "# Aggregating WRF-Hydro modeling application outputs to HUC12s: 2-Dimensional variables\n",
    "**Author:** Kevin Sampson; NCAR\n",
    "\n",
    "#### Note from the Author:\n",
    "This notebook is intended to process the zonal (spatial) statistics between NWM Retrospective outputs and a set of gridded 'zones', which can be any spatial unit such as counties, states, HUCs, etc. Those inputs must already be resolved on the intended NWM grid (LSM - 1km, or routing - 250m) and optionally subset to any spatial subset of the NWM retrospective data (i.e. the grids must match exactly). This script assumes all 'zone' datasets are written in typical GIS fashion from north to south. If an LSM grid is requested, the zone dataset will be flipped south-to-north in this script. \n",
    "\n",
    "## Background\n",
    "WRF-Hydro modeling application outputs should already have been processed from hourly to monthly summaries on the native WRF-Hydro/NWM grids. We will use these monthly datasets to process the zonal statistics at the HUC12 scale. This notebook aggregates the 2-Dimensional WRF-Hydro modeling application outputs and CONUS404-BA outputs.  \n",
    "\n",
    "## Processing Environment\n",
    "This workflow leverages dask and requires 150 GB of allocated memory. \n",
    "The python environment used is a conda environment 'wrfhydro_huc12_agg' here: \n",
    "\n",
    "/path/to/repo/hytest/dataset_processing/tutorials/niwaa_wrfhydro_monthly_huc12_agg/02_Spatial_Aggregation/wrfhydro_huc12_agg.yml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece6e02f-d8cc-44ba-8a9b-7f83a2028073",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72da8374-ac14-451b-8847-d1fb4a353df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import Modules --- #\n",
    "\n",
    "# Import Python Core Modules\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import tracemalloc\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Some environment variables important to dask\n",
    "os.environ[\"MALLOC_TRIM_THRESHOLD_\"] = \"0\"\n",
    "os.environ[\"DASK_DISTRIBUTED__SCHEDULER__ACTIVE_MEMORY_MANAGER__START\"] = \"True\"\n",
    "os.environ[\"DASK_DISTRIBUTED__SCHEDULER__WORKER_SATURATION\"] = \"1.2\"\n",
    "if 'DASK_ROOT_CONFIG' in os.environ:\n",
    "    del os.environ['DASK_ROOT_CONFIG']\n",
    "import dask\n",
    "from dask.distributed import Client, progress, LocalCluster, performance_report\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.diagnostics import ProgressBar\n",
    "import dask.array as da\n",
    "\n",
    "\n",
    "# Import Additional Modules\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import zarr\n",
    "import flox.xarray\n",
    "\n",
    "# Import functions from local repository\n",
    "sys.path.append(r'/path/to/repo/hytest/dataset_processing/tutorials/niwaa_wrfhydro_monthly_huc12_agg/02_Spatial_Aggregation/')\n",
    "from usgs_common import *\n",
    "\n",
    "from rich.console import Console\n",
    "from rich import pretty\n",
    "\n",
    "# Rich library\n",
    "pretty.install()\n",
    "con = Console(record=False, force_jupyter=False)\n",
    "con.width = 200\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message=r'.*Sending large graph of size.*')\n",
    "\n",
    "tic = time.time()\n",
    "con.print(f'Process initiated at {time.ctime()}')\n",
    "# --- End Import Modules --- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc91b816-8070-446b-aad2-ba42490e76a6",
   "metadata": {},
   "source": [
    "## Define the input files and other relevant local variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501fc30e-4e9e-4df4-b35c-12914ffce388",
   "metadata": {},
   "outputs": [],
   "source": [
    "NWM_type = 'LDASOUT'\n",
    "\n",
    "# Variable to process - list form, from LDASOUT, and LDASIN\n",
    "variables = ['deltaACCET',\n",
    "             'deltaACSNOW',\n",
    "             'deltaSNEQV',\n",
    "             'deltaSOILM',\n",
    "             'deltaUGDRNOFF',\n",
    "             'deltaSOILM_depthmean',\n",
    "             'avgSNEQV',\n",
    "             'avgSOILM',\n",
    "             'avgSOILM_depthmean',\n",
    "             'avgSOILM_wltadj_depthmean',\n",
    "             'avgSOILSAT',\n",
    "             'avgSOILSAT_wltadj_top1',\n",
    "             'totPRECIP',\n",
    "             'avgT2D']\n",
    "\n",
    "# Give a name to the zone dataset, which will be the name of the zone variable\n",
    "zone_name = 'WBDHU12'\n",
    "\n",
    "# Perform temporal subset on inputs?\n",
    "temporal_subset = True\n",
    "\n",
    "# Choose the temporal range, if temporal_subset is true\n",
    "time_subset_bounds = slice('2011-10-01', '2012-09-30')     # Test Hytest batch (1 year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4516e30-d401-4707-ad8d-3867a7e48958",
   "metadata": {},
   "source": [
    "## Define the output files and other relevant variables to outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61db1af7-6028-4664-a43a-3d604df96b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory to save 2D aggregation results\n",
    "outDir = Path('/path/to/outputs/agg_out')\n",
    "con.print(f'outDir exists: {outDir.is_dir()}')\n",
    "\n",
    "# Basename for output files - extension will be applied later\n",
    "output_pattern = 'CONUS_HUC12_2D_20111001_20120930'\n",
    "\n",
    "# Other variables to help with the file output naming convention\n",
    "write_CSV = True\n",
    "write_NC = True\n",
    "\n",
    "# Apply a landmask to the weight grid so that water cells are not considered in the spatial statistics? \n",
    "# Only applies to LSM grid variables\n",
    "landmask_results = True\n",
    "\n",
    "# Variables that will be normalized to the full land area (not landmasked land area)\n",
    "non_landmask_vars = ['Precip', 'landmask']\n",
    "\n",
    "# Add variables that we want to process spatial stats for\n",
    "addVars = ['total_gridded_area'] + non_landmask_vars    # For all other processing\n",
    "#addVars = ['total_gridded_area']                        # For the soil moisture top layer variables\n",
    "\n",
    "# Calculate percent soil saturation as a derived output variable\n",
    "pct_sat = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c0a549-1354-43f5-a3d8-7a2951c49179",
   "metadata": {},
   "source": [
    "### Handle the processing of input variables if the source is raw NWM\n",
    "\n",
    "Use the NWM_type to define the input Zarr store, and any other processing requirements (unit conversion, time resampling, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd761ab6-fcab-4af2-847b-f3f5fe677ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will construct a list of files. They must all contain the same time and other dimensions in order to be concatenated using open_mfdataset\n",
    "convert_to_mm = False\n",
    "\n",
    "# Specify the directory where the precip (LDASIN, clim_*.nc) files are stored. This can be different in some cases than the othe files\n",
    "clim_dir = Path('/path/to/temporal/aggregations/output/monthly')\n",
    "con.print(f'clim_dir exists: {clim_dir.is_dir()}')\n",
    "\n",
    "# Specify the directory where the LDASOUT (water_*.nc) files are stored.\n",
    "land_dir = Path('/path/to/temporal/aggregations/output/monthly')\n",
    "con.print(f'land_dir exists: {land_dir.is_dir()}')\n",
    "\n",
    "# Add a second set of variables from a different set of files\n",
    "file_in = get_files_wildcard(land_dir, \n",
    "                             file_pattern='water_*.nc', \n",
    "                             recursive=False)\n",
    "\n",
    "# Obtain list of files from wildcard\n",
    "file_in2 = get_files_wildcard(clim_dir, \n",
    "                             file_pattern='clim_*.nc', \n",
    "                             recursive=False)\n",
    "\n",
    "if len(file_in) != len(file_in2):\n",
    "    con.print('[orange_red1]WARNING[/]: The number of files in clim_dir and land_dir are not the same.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a0a1b-06ad-4f1d-9259-a858c2c62ae0",
   "metadata": {},
   "source": [
    "### Spin up a Dask Cluster\n",
    "\n",
    "Spin up a slurm cluster to parallelize the aggregation process. The scheduler for the dask cluster requires 150 GB of memory to allocate jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f98b20-b622-49fe-b8e6-13cd6074ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    project='impd'\n",
    "    #project = os.environ['SLURM_JOB_ACCOUNT']\n",
    "except KeyError:\n",
    "    logging.error(\"SLURM_JOB_ACCOUNT is not set in the active environment. Are you on the login node? You should not be running this there.\")\n",
    "    raise\n",
    "\n",
    "cluster = SLURMCluster(job_name='dask_niwaa',\n",
    "                       account=project,\n",
    "                       processes=1, \n",
    "                       cores=1, \n",
    "                       memory='10GB', \n",
    "                       interface='ib0',\n",
    "                       walltime='01:00:00',      \n",
    "                       shared_temp_directory='/home/lstaub/tmp',\n",
    "                       #job_extra={'hint': 'multithread'},\n",
    "                       #scheduler_options = {'dashboard_address': ':32939'}\n",
    "                      )\n",
    "\n",
    "cluster.adapt(minimum=10, maximum=30)\n",
    "con.print(cluster.job_script())\n",
    "\n",
    "client = Client(cluster)\n",
    "\n",
    "ood_dashboard_link = f\"https://hw-ood.cr.usgs.gov/node/{os.environ['JUPYTER_SERVER_NODE']}/{os.environ['JUPYTER_SERVER_PORT']}/proxy/{client.dashboard_link.split(':')[2]}\"\n",
    "con.print(f'Dask Dashboard for OnDemand is available at: {ood_dashboard_link}')\n",
    "\n",
    "con.print(\"The 'cluster' object can be used to adjust cluster behavior.  i.e. 'cluster.adapt(minimum=10)'\")\n",
    "con.print(\"The 'client' object can be used to directly interact with the cluster.  i.e. 'client.submit(func)' \")\n",
    "con.print(f\"The link to view the client dashboard is:\\n>  {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce397b5-9f3f-4cb6-a97a-2eb1b57b9e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f44f12f-8c2d-461e-b424-ca8dbc98d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.close()\n",
    "# cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374124f5-d198-446f-84ee-aa955f931a22",
   "metadata": {},
   "source": [
    "#### Open the input file and read some useful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2f1086-a0cc-4f31-974e-80668a36cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# NOTE: 2025-03-27 PAN - added parallel=True to open_mfdataset\n",
    "\n",
    "def extract_dates(in_paths=[], format_str='%Y%m'):\n",
    "    \"\"\"\n",
    "    This function will take an input path and extract a date object from the filename. \n",
    "    Assumes that the filename ends with \"_{datestring}.nc\" (default = YYYYMM)\n",
    "    \"\"\"\n",
    "    dt_strings = [os.path.basename(in_path).split('.nc')[0].split('_')[1] for in_path in in_paths] \n",
    "    dt_obj = pd.to_datetime(dt_strings, format=format_str)\n",
    "    return dt_obj\n",
    "\n",
    "# Open the selected dataset(s), dropping variables as necessary\n",
    "drop_vars = [var_in for var_in in xr.open_dataset(file_in[0]).variables if var_in not in variables+[time_coord]]\n",
    "\n",
    "if len(file_in2) > 1:\n",
    "    drop_vars += [var_in for var_in in xr.open_dataset(file_in2[0]).variables if var_in not in variables+[time_coord]]\n",
    "\n",
    "drop_vars = list(set(drop_vars)) # Eliminate redundancy\n",
    "con.print(f'Dropping {drop_vars} from input file.')\n",
    "\n",
    "# Only use this method if datasets are coming from multiple directories or file types\n",
    "with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "    # This is a little complicated because we will be building multiple datasets\n",
    "    ds_list = [xr.open_mfdataset(in_list, \n",
    "                                 combine='nested', \n",
    "                                 decode_cf=False, \n",
    "                                 concat_dim='time',\n",
    "                                 chunks='auto',\n",
    "                                 parallel=True,\n",
    "                                 drop_variables=drop_vars) for in_list in [file_in, file_in2] if len(in_list) > 0]\n",
    "\n",
    "    datetimes = [extract_dates(in_list) for in_list in [file_in, file_in2] if len(in_list)>0]\n",
    "    ds_list = [ds.assign_coords(time=datetimes_in) for ds, datetimes_in in zip(ds_list, datetimes)]\n",
    "    ds = xr.merge(ds_list)\n",
    "    del ds_list, datetimes\n",
    "    \n",
    "# Perform temporal subset, or not\n",
    "if temporal_subset:\n",
    "    ds = ds.loc[{time_coord:time_subset_bounds}]\n",
    "    \n",
    "# Obtain and print information about the input file\n",
    "ds, timesteps, x_chunk_sizes, y_chunk_sizes, time_chunk_sizes = report_structure(ds, variable=list(ds.data_vars.keys())[0])\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bd4ef5-3689-435c-8829-fcc3b26198e0",
   "metadata": {},
   "source": [
    "#### Obtain the spatial aggregation array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1e8212-8b8a-4dc4-b2ac-6db2b146f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Choose a method for spatial aggregation\n",
    "raster_zones = True\n",
    "spatial_weights = False\n",
    "\n",
    "# Use a 2D grid of zone IDs to perform spatial aggregation.\n",
    "# This is a representation of the zones on the same grid as the analysis data.\n",
    "if raster_zones:\n",
    "    \n",
    "    # Sort out resolution and input files\n",
    "    if NWM_type == 'RTOUT':\n",
    "        zone_raster = r'/caldera/hovenweep/projects/usgs/water/impd/hytest/niwaa_wrfhydro_monthly_huc12_aggregations_sample_data/HUC12_grids/HUC12s_on_250m_grid.tif'\n",
    "        LSM_grid = False\n",
    "    elif NWM_type == 'LDASOUT':\n",
    "        zone_raster = r'/caldera/hovenweep/projects/usgs/water/impd/hytest/niwaa_wrfhydro_monthly_huc12_aggregations_sample_data/HUC12_grids/HUC12s_on_1000m_grid.tif'\n",
    "        LSM_grid = True\n",
    "    print('Using raster grid of zones for spatial aggregation: {0}'.format(zone_raster))\n",
    "    \n",
    "    # Data value to define nodata in the zone raster (anywhere that a zone does not exist).\n",
    "    zone_nodata = 0\n",
    "\n",
    "    # Read in the raster that defines the zones\n",
    "    zone_arr, zone_ndv = return_raster_array(zone_raster)\n",
    "\n",
    "    # Flip the raster if necessary - easier than flipping each input array from the model data\n",
    "    if LSM_grid:\n",
    "        zone_arr = zone_arr[flip_dim(['y', 'x'], DimToFlip='y')]\n",
    "\n",
    "    # Replace nodata values with np.nan, which requires converting to floating point.    \n",
    "    zone_arr = zone_arr.astype('float')    \n",
    "    zone_arr[zone_arr==zone_nodata] = np.nan\n",
    "\n",
    "    # Obtain unique values\n",
    "    zone_uniques = np.unique(zone_arr)\n",
    "    zones_unique = zone_uniques[zone_uniques!=np.nan]\n",
    "    print('{0} zones found in the input dataset'.format(zones_unique.shape[0]-1))\n",
    "    del zone_uniques, zones_unique\n",
    "    \n",
    "    # Add zones to the Xarray DataSet object\n",
    "    zones = xr.DataArray(zone_arr, dims=(\"y\", \"x\"), name=zone_name)\n",
    "    #ds[zone_name] = zones.fillna(-1).astype(int)   # workaround flox bug\n",
    "    ds[zone_name] = zones.fillna(-1).astype(np.int64)   # workaround flox bug\n",
    "    del zones\n",
    "    \n",
    "    # Obtain landmask grid\n",
    "    if landmask_results and NWM_type == 'LDASOUT':\n",
    "        print('  Masking zone grid to LSM LANDMASK variable')\n",
    "        landmask = xr.open_dataset(geogrid)['LANDMASK'].squeeze()\n",
    "        zone_masked = zone_arr.copy()\n",
    "        zone_masked[landmask==0] = np.nan\n",
    "        masked_zone_name = '{0}_masked'.format(zone_name)\n",
    "        zones_ma = xr.DataArray(zone_masked, dims=(\"y\", \"x\"), name=masked_zone_name)\n",
    "        \n",
    "        # Filling NaN areas (water or ocean) with -1 removes it from that HUC.\n",
    "        #ds[masked_zone_name] = zones_ma.fillna(-1).astype(int)   # workaround flox bug\n",
    "        ds[masked_zone_name] = zones_ma.fillna(-1).astype(np.int64)   # workaround flox bug\n",
    "        \n",
    "        # Save the landmask (1s and 0s)\n",
    "        landmask_da = xr.DataArray(landmask, dims=(\"y\", \"x\"), name='landmask')\n",
    "        ds['landmask'] = landmask_da.fillna(0).astype(int)   # workaround flox bug\n",
    "        del landmask, zones_ma\n",
    "    \n",
    "        # Obtain unique values\n",
    "        zone_uniques = np.unique(zone_masked)\n",
    "        zones_unique = zone_uniques[zone_uniques!=np.nan]\n",
    "        print('{0} zones found in the input dataset after land-masking'.format(zones_unique.shape[0]-1))\n",
    "        del zone_uniques, zones_unique, zone_masked\n",
    "        \n",
    "    del zone_arr\n",
    "    \n",
    "# Use a 1D array of pixel weights to perform spatial aggregation\n",
    "### NOT YET WORKING!\n",
    "elif spatial_weights:\n",
    "    sw_file =r'/caldera/hovenweep/projects/usgs/water/impd/hytest/niwaa_wrfhydro_monthly_huc12_aggregations_sample_data/static_niwaa_wrf_hydro_files/WRFHydro_spatialweights_CONUS_250m_NIWAAv1.0.nc'\n",
    "    print('Using pre-computed NWM-style spatial weight file for spatial aggregation: {0}'.format(sw_file))\n",
    "    \n",
    "    # If the raster used to create spatial weights was created in GIS, then it will start with 0,0 in UL corner. \n",
    "    # To flip to south_north, select flip_raster==True\n",
    "    flip_raster = True\n",
    "    \n",
    "    # Open the spatial weight file\n",
    "    sw_ds = xr.open_dataset(sw_file)\n",
    "\n",
    "    # Subset the spatial weight file to just one zone\n",
    "    sw_ds = sw_ds.drop(['overlaps', 'polyid', 'regridweight'])\n",
    "    sw_ds.load()\n",
    "    \n",
    "    display(sw_ds)\n",
    "\n",
    "    # For now, flox need an integer for the zone IDs\n",
    "    sw_ds['IDmask'] = sw_ds['IDmask'].astype(np.int64)\n",
    "    sw_ds = sw_ds.rename({'IDmask':zone_name})\n",
    "\n",
    "    # Obtain indexer arrays and alter the indices to 'flip' the y dimension if requested.\n",
    "    indexer_i = sw_ds['i_index'].astype(int).data\n",
    "    if flip_raster:\n",
    "        indexer_j = LSM_grid_size_y - sw_ds['j_index'].astype(int).data\n",
    "    else:\n",
    "        indexer_j = sw_ds['j_index'].astype(int).data\n",
    "        \n",
    "    # Add the spatial weight variables to the dataset\n",
    "    ds = xr.merge([ds, sw_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a185f-f8cf-45ba-bc5b-f52c2e382c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34899ef6-5581-44f8-bc00-49fbbe43ca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.print(f'{timesteps=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eee781-3e11-47fb-9a7c-d0930a566f4f",
   "metadata": {},
   "source": [
    "## Iterate over time, processing the zonal statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a35f66-ea7b-40f9-b73e-88751424865b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Perform 2D Groupby operation\n",
    "\n",
    "This codeblock will execute the 2D groupby (zonal statistic) operation using the `flox` method `xarray_reduce` or `groupby_reduce`.\n",
    "\n",
    "#### Method of operation\n",
    "\n",
    "For some datasets there may be a memory limitation that will cause individual workers to pause once they reach 80% memory utilization. Thus, we have to carefully select the size of chunks to process. Currently, we use the existing chunk size in the input Zarr store, establishing our iteration strategy on how many time-chunks from the input we can process at once. Keep in mind that the full 2D dataset will be used at each timestep, so only the time chunk will be considered. The `time_chunk_factor` is used to multiply the time-chunk to determine the number of timesteps processed at each iteration. Keep in mind that processing times appear to scale linearly, so this may not be an important factor.\n",
    "\n",
    "Currently, for certain variables, we calculate the sum over a third dimension, such as soil_layers_stag for the `SOIL_M` variable. \n",
    "\n",
    "Currently, the statistical operations provided in the `numpy_groupies` python library are supported:\n",
    "* `sum`, `nansum`\n",
    "* `prod`, `nanprod`\n",
    "* `mean`, `nanmean`\n",
    "* `var`, `nanvar`\n",
    "* `std`, `nanstd`\n",
    "* `min`, `nanmin`\n",
    "* `max`, `nanmax`\n",
    "* `first`, `nanfirst`\n",
    "* `last`, `nanlast`\n",
    "* `argmax`, `nanargmax`\n",
    "* `argmin`, `nanargmin`\n",
    "\n",
    "An output CSV is issued for each iteration and each statistic requested.\n",
    "\n",
    "Other configurations are set to assist in the chunking of the data. A variable `time_chunk_factor` is used to calculate how many timestep chunks to use for each iteration. One CSV file is written out per iteration, per statistic calculated (currently `mean` and `max` are supported)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c38a74c-c9eb-4f72-8abd-dfc0865aa44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "con.print(f'Process initiated at {time.ctime()}')\n",
    "        \n",
    "# Output to file\n",
    "with performance_report(filename=os.path.join(outDir, \"dask-report_2D_2.html\")):  \n",
    "    # Determine how many time chunks we can process at once\n",
    "    time_chunks = [timesteps]    # To process all times at once, provide nested list containing all timesteps\n",
    "    #time_chunk_size = 2  \n",
    "    #time_chunks = [timesteps[i:i + time_chunk_size] for i in range(0, len(timesteps), time_chunk_size)]\n",
    "    con.print(f'There will be {len(time_chunks)} iterations over time.')\n",
    "\n",
    "    # Iterate over variables\n",
    "    datetime_strings = []\n",
    "    con.print(f'There will be up to {len(addVars + variables)} variables processed.')\n",
    "    for varnum, variable in enumerate(addVars + variables):\n",
    "        tic1 = time.time()\n",
    "        #if variable not in ds:\n",
    "        #    print('Skipping variable {0}'.format(variable))\n",
    "        #    continue\n",
    "        con.print(f'Processing variable [bold]{variable}[/]')\n",
    "\n",
    "        # Set the appropriate zone mask\n",
    "        if variable in non_landmask_vars + ['Precip']:\n",
    "            # Use full basin zone array for spatial aggregation. No land-masking\n",
    "            con.print(f'  Using full basin mask for variable {variable}')\n",
    "            zone_da = ds[zone_name]\n",
    "\n",
    "            # Special case where we re-use a variable to produce a secondary result\n",
    "            if variable == 'Precip':\n",
    "                da = ds['totPRECIP']\n",
    "                da.name = variable\n",
    "        else:\n",
    "            con.print('  Using land/water mask to remove water cells from analysis')\n",
    "            # Use land-masked zone array for spatial aggregation\n",
    "            zone_da = ds[masked_zone_name]\n",
    "\n",
    "        # Subset the variable to a DataArray\n",
    "        if variable in ds:\n",
    "            da = ds[variable]\n",
    "\n",
    "        # Special case to gather gridded area considered for each basin\n",
    "        elif variable == 'total_gridded_area':\n",
    "            # Make an array of ones to collect the total gridded area for each basin\n",
    "            da = xr.ones_like(ds['landmask'])\n",
    "            da.name = variable\n",
    "\n",
    "        # Initialize list to store temporary partial DataArrays\n",
    "        outputs = []\n",
    "\n",
    "        # Iterate over time-chunks and process zonal statistics\n",
    "        for n, time_chunk in enumerate(time_chunks):\n",
    "            # Interpret times as strings - for later input to CSV files as a time index\n",
    "            datetime_strings += [pd.to_datetime(time_chunk).strftime('%Y%m%d%H')]\n",
    "\n",
    "            # Subset in time if necessary\n",
    "            if 'time' in da.dims:\n",
    "                data = da.loc[dict(time=slice(time_chunk[0], time_chunk[-1]))]\n",
    "            else:\n",
    "                data = da\n",
    "\n",
    "            # Handle total soil moisture depth\n",
    "            if NWM_type == 'LDASOUT' and variable in ['SOIL_M','deltaSOILM','avgSOILM']:\n",
    "                con.print('\\tConverting soil moisture value to total water depth (mm) in soil column.')\n",
    "\n",
    "                # For Soil Moisture, apply weights to soil depths to get total volume (in mm) in soil column.\n",
    "                soil_dict = dict(soil_weights=('soil_layers_stag', [0,1,2,3]))\n",
    "                weights = xr.DataArray(soil_depths_mm, dims=('soil_layers_stag',), coords=soil_dict)\n",
    "\n",
    "                # Multiply by depth and sum the values over depth dimension\n",
    "                data = (data * weights).sum(dim='soil_layers_stag')\n",
    "                data.name = variable  # reset the dataarray name\n",
    "\n",
    "            # Apply groupby operation\n",
    "            if raster_zones:\n",
    "                if variable == 'total_gridded_area':\n",
    "                    flox_function = 'sum'\n",
    "                else:\n",
    "                    flox_function = 'mean'\n",
    "                    \n",
    "                con.print(f'\\t[{varnum}]    Calculating zonal {flox_function}.')\n",
    "                output = run_flox(data, zone_da, flox_function=flox_function, n=n)\n",
    "            elif spatial_weights:\n",
    "                # Convert from 2D to 1D array using indexer_j and indexer_i\n",
    "                flox_function = 'sum'\n",
    "                con.print(f'\\t[{varnum}]    Calculating spatially weighted value {flox_function}.')\n",
    "                output = run_flox(data.data[indexer_j, indexer_i] * ds['weight'], \n",
    "                                  zone_da, \n",
    "                                  flox_function=flox_function, \n",
    "                                  n=n)\n",
    "                \n",
    "            if variable not in non_landmask_vars+['Precip']:\n",
    "                output = output.rename({masked_zone_name:zone_name})\n",
    "            outputs.append(output)\n",
    "            del data\n",
    "        con.print(f'\\t[{varnum}] Spatial aggregation step completed in {time.time()-tic1:3.2f} seconds.')   # .format(varnum, time.time()-tic1))\n",
    "\n",
    "        # Merge all outputs together\n",
    "        output = xr.merge(outputs)\n",
    "\n",
    "        # Re-arrange dimensions so that time is the fastest varying dimension\n",
    "        if 'time' in output.dims:\n",
    "            output = output[[zone_name, time_coord, variable]]\n",
    "\n",
    "        #if varnum == 0:\n",
    "        if not 'out_ds' in locals():\n",
    "            out_ds = output\n",
    "        else:\n",
    "            out_ds[variable] = output[variable]\n",
    "        con.print(f'\\t[{varnum}] Iteration completed in {time.time()-tic1:3.2f} seconds.')  # .format(varnum, time.time()-tic1))\n",
    "    out_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae8f66-b59e-44fa-bf5f-3f60437b72e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3663c430-6ad2-4a48-ad9d-42a31006bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8096f811-4cfb-4907-bbe1-19c733998485",
   "metadata": {},
   "source": [
    "### Remove unecessary attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121b2ec1-69bd-4587-b841-385f3f3a0848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate any unecessary variable attributes (such as spatial metadata)\n",
    "for variable in out_ds.data_vars:\n",
    "    if 'grid_mapping' in out_ds[variable].attrs:\n",
    "        del out_ds[variable].attrs['grid_mapping']\n",
    "    if 'esri_pe_string' in out_ds[variable].attrs:\n",
    "        del out_ds[variable].attrs['esri_pe_string']\n",
    "    if 'proj4' in out_ds[variable].attrs:\n",
    "        del out_ds[variable].attrs['proj4']\n",
    "    if variable == 'landmask':\n",
    "        out_ds[variable].attrs = {'description':'Fraction of gridded land area in each HUC12'}\n",
    "    if variable == 'total_gridded_area':\n",
    "        out_ds[variable].attrs = {'description':'Number of 1km grid cells for HUC12. Equivalend to square kilometers. Based on grid association of each HUC12'}\n",
    "        \n",
    "# Now eliminate unnecessary global attributes\n",
    "if 'grid_mapping' in out_ds.attrs:\n",
    "    del out_ds.attrs['grid_mapping']\n",
    "if 'units' in out_ds.attrs:\n",
    "    del out_ds.attrs['units']  \n",
    "if 'esri_pe_string' in out_ds.attrs:\n",
    "    del out_ds.attrs['esri_pe_string'] \n",
    "if 'long_name' in out_ds.attrs:\n",
    "    del out_ds.attrs['long_name'] \n",
    "if '_FillValue' in out_ds.attrs:\n",
    "    del out_ds.attrs['_FillValue'] \n",
    "if 'missing_value' in out_ds.attrs:\n",
    "    del out_ds.attrs['missing_value'] \n",
    "out_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e90014-2b0b-4a33-ad9f-e0739ed65633",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ds = out_ds.where(out_ds[zone_name]!=-1, drop=True)\n",
    "out_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87005406-8e5e-4df0-af1f-a5995ca2c1d6",
   "metadata": {},
   "source": [
    "### Output to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e512ce9-3b22-4af2-bd39-0d17f23af49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Read into memory before writing to disk?\n",
    "out_ds.compute()\n",
    "\n",
    "# Write output file (CSV)\n",
    "if write_CSV:\n",
    "    tic1 = time.time()\n",
    "    out_file = os.path.join(outDir, output_pattern+'_2.csv')\n",
    "    print('  Writing output to {0}'.format(out_file))\n",
    "    if os.path.exists(out_file):\n",
    "        tic1 = time.time()\n",
    "        df_in = pd.read_csv(out_file)\n",
    "        df_out = pd.concat([df_in, out_ds.to_dataframe()])\n",
    "        df_out.to_csv(out_file)\n",
    "        print('\\t      Output file written in {0:3.2f} seconds.'.format(time.time()-tic1))\n",
    "    else:\n",
    "        write_csv(out_ds, out_file, columns=output[zone_name], index=[datetime_strings])\n",
    "    print('\\tExport to CSV completed in {0:3.2f} seconds.'.format(time.time()-tic1))\n",
    "    \n",
    "# Write output file (netCDF)\n",
    "if write_NC:\n",
    "    tic1 = time.time()\n",
    "    out_file = os.path.join(outDir, output_pattern+'_2.nc')\n",
    "    if os.path.exists(out_file):\n",
    "        in_ds = xr.open_dataset(out_file).load()\n",
    "        out_ds2 = xr.merge([in_ds, out_ds.transpose()])\n",
    "        in_ds.close()\n",
    "        del in_ds\n",
    "        print('  Writing output to {0}'.format(out_file))\n",
    "        out_ds2.to_netcdf(out_file, mode='w', format=\"NETCDF4\", compute=True)\n",
    "        del out_ds2\n",
    "    else:\n",
    "        print('  Writing output to {0}'.format(out_file))\n",
    "        out_ds.transpose().to_netcdf(out_file, mode='w', format=\"NETCDF4\", compute=True)\n",
    "    print('\\tExport to netCDF completed in {0:3.2f} seconds.'.format(time.time()-tic1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe894c-fbab-4fe4-b5ce-0a3d8a3ab8db",
   "metadata": {},
   "source": [
    "## Spin Down the Cluster and Close datasets\n",
    "##### After we are done, we can spin down our cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874cac1f-69d1-40f4-9d37-df8765322f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the Dask cluster\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c347a11-194d-40a5-98ab-dbca1088e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close dataset\n",
    "ds.close()\n",
    "print('Process completed in {0: 3.2f} seconds.'.format(time.time()-tic))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
