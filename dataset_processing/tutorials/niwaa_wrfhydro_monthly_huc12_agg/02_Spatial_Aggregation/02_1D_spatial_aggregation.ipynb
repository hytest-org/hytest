{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25e6b1cd-c51b-44ad-85f0-d655577fed24",
   "metadata": {},
   "source": [
    "# Aggregating WRF-Hydro modeling application outputs to HUC12s: 2-Dimensional variables\n",
    "**Author:** Kevin Sampson; NCAR\n",
    "\n",
    "## Background\n",
    "WRF-Hydro modeling application outputs should already have been processed from hourly to monthly summaries on the native WRF-Hydro/NWM grids. We will use these monthly datasets to process the zonal statistics at the HUC12 scale. This notebook aggregates the 1-Dimensional WRF-Hydro modeling application outputs. \n",
    "\n",
    "## Processing Environment\n",
    "This workflow leverages dask and requires 150 GB of allocated memory. \n",
    "The python environment used is a conda environment 'wrfhydro_huc12_agg' here: \n",
    "\n",
    "/path/to/repo/hytest/dataset_processing/tutorials/niwaa_wrfhydro_monthly_huc12_agg/02_Spatial_Aggregation/wrfhydro_huc12_agg.yml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cecd50-ef1f-49fa-b608-796fa91611ec",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bc5846-316a-48fc-948f-a95e9a5d07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import Modules --- #\n",
    "\n",
    "# Import Python Core Modules\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Import Additional Modules\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import zarr\n",
    "import flox.xarray\n",
    "\n",
    "# Import functions from local repository\n",
    "sys.path.append(r'/path/to/repo/hytest/dataset_processing/tutorials/niwaa_wrfhydro_monthly_huc12_agg/02_Spatial_Aggregation/')\n",
    "from usgs_common import *\n",
    "\n",
    "tic = time.time()\n",
    "print('Process initiated at {0}'.format(time.ctime()))\n",
    "# --- End Import Modules --- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723edccd-f670-47fe-94f0-56ace598d5cd",
   "metadata": {},
   "source": [
    "### Setup global variables, input and outputs and other options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4663903-0eb7-44a9-adcf-ed40a7eb5d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Globals --- #\n",
    "\n",
    "# Choose a region subset\n",
    "region = 'CONUS'\n",
    "\n",
    "# Input directory containing model outputs\n",
    "inDir = r'/path/to/temporal/aggregations/output/monthly'\n",
    "\n",
    "# Input mapping file for model catchments to analysis catchments\n",
    "Use_Mapping_File = True\n",
    "Mapping_File = r'/caldera/hovenweep/projects/usgs/water/impd/hytest/niwaa_wrfhydro_monthly_huc12_aggregations_sample_data/HUC12_grids/Final_HUC12IDs.csv'\n",
    "\n",
    "# Give a name to the zone dataset, which will be the name of the zone variable\n",
    "zone_name = 'WBDHU12'\n",
    "\n",
    "# Specify the field in the input file that contains local model catchment areas\n",
    "area_field = 'NWM_Catchment_Area' \n",
    "\n",
    "# Specify the field in the input file that contains the mapping between model catchments and aggregation units\n",
    "mapping_field = 'HUC12_FL' # Use this since it maps all flowlines to HUC12, an matches the catchment mapping\n",
    "\n",
    "# Specify the field in the input file that contains the IDs of the model catchments\n",
    "mapping_ID = 'ID'\n",
    "\n",
    "# Peform spatial aggregation?\n",
    "spatial_aggregation = True\n",
    "\n",
    "# Resample the time variable?\n",
    "resample_time = False\n",
    "resample_time_period = \"1D\"\n",
    "\n",
    "# Perform temporal subset on inputs?\n",
    "temporal_subset = True\n",
    "time_subset_bounds = slice('2011-10-01', '2012-09-30')     # Test Hytest batch (1 year)\n",
    "\n",
    "# Fill NaN values in the Area_sqkm (used as weights on the variable) field with a particular value?\n",
    "fill_na_value = 0     \n",
    "\n",
    "# Specify units name - only used in output file names and netCDF variable attribute \"units\"\n",
    "units = 'mm'\n",
    "\n",
    "# Output directory\n",
    "outDir = r'/path/to/outputs/agg_out'\n",
    "\n",
    "# Output filename pattern\n",
    "output_pattern = 'CONUS_HUC12_1D_WY2011_2013'\n",
    "\n",
    "# Select output formats\n",
    "write_NC = True      # Output netCDF file\n",
    "write_CSV = True     # Output CSV file\n",
    "\n",
    "# 1D variables, input files, and other options\n",
    "Variables = ['totOutflow', 'totInflow', 'deltaDepth', 'bucket_depth', 'totqBucket', 'totqSfcLatRunoff', 'totStreamflow'] # Manually specify variables\n",
    "convert_to_mm = True                    # Output is in mm^3s^-1. Divide by m^2 and multiply by 1000 to get mm\n",
    "convert_to_mm_list = ['totOutflow', 'totInflow', 'totqBucket', 'totqSfcLatRunoff', 'totStreamflow']\n",
    "\n",
    "# --- End Globals --- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a9fd17-3bb7-49ab-8080-26b8032b81bd",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95ba2ba-6bdb-4128-8450-9e547dcdcacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Functions --- #\n",
    "\n",
    "def extract_dates(in_paths=[]):\n",
    "    dt_strings = [os.path.basename(in_path).split('.nc')[0].split('_')[1] for in_path in in_paths]\n",
    "    dt_obj = pd.to_datetime(dt_strings, format='%Y%m')\n",
    "    return dt_obj\n",
    "\n",
    "# --- End Functions --- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd2943-c3e3-4d10-91bf-92e45fe4effe",
   "metadata": {},
   "source": [
    "### Read input mapping file from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b33260-f05b-4cc3-a147-24397f03b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine files in input directory\n",
    "file_in = get_files_wildcard(inDir, file_pattern='gw_*.nc', recursive=False)\n",
    "file_in2 = get_files_wildcard(inDir, file_pattern='chrt_*.nc', recursive=False)\n",
    "assert len(file_in) == len(file_in2)\n",
    "\n",
    "# Create a dataframe from the input HUC12 mapping file\n",
    "df = pd.read_csv(Mapping_File, index_col=[0])\n",
    "df2 = pd.read_csv(Mapping_File, index_col=[0], dtype={'HUC12_FL':str, 'HUC12_CA':str})\n",
    "\n",
    "# Create a new field that is a string type based on the HUC12 IDs. This is for outputs\n",
    "df['HUC12_FL_str'] = df2['HUC12_FL']\n",
    "df['HUC12_CA_str'] = df2['HUC12_CA']\n",
    "del df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87076b90-1e65-43ce-857c-ffbf707a75ac",
   "metadata": {},
   "source": [
    "### Define variables to drop from inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2436eeb-303e-44ca-9a0f-e70c361bf3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the selected dataset(s), dropping variables as necessary\n",
    "drop_vars = [variable for variable in xr.open_dataset(file_in[0]) if variable not in Variables+[time_coord]]\n",
    "drop_vars += [variable for variable in xr.open_dataset(file_in2[0]) if variable not in Variables+[time_coord]]\n",
    "drop_vars += ['crs']\n",
    "drop_vars = list(set(drop_vars))\n",
    "print('Dropping {0} from input file.'.format(drop_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb88edf-c0e3-41c6-85a5-d5604686f460",
   "metadata": {},
   "source": [
    "### Iterate over every set of files, one timestep at a time\n",
    "\n",
    "This will keep memory usage low, but all data will be stored in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fb6c25-66d7-4064-b53c-bb6f6e200469",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# -------- Main codeblock -------- #\n",
    "\n",
    "# Iterate over each pair of input timesteps\n",
    "for n, (infile1, infile2) in enumerate(zip(file_in, file_in2)):\n",
    "    tic2 = time.time()\n",
    "\n",
    "    # This is a little complicated because we will be building multiple datasets\n",
    "    ds_list = [xr.open_dataset(infile,\n",
    "                               decode_cf=False,\n",
    "                               drop_variables=drop_vars) for infile in [infile1, infile2]]\n",
    "    datetimes = [extract_dates(in_list) for in_list in [[infile1], [infile2]]]\n",
    "    ds_list = [ds.assign_coords(time=datetimes_in) for ds, datetimes_in in zip(ds_list, datetimes)]\n",
    "    ds_input = xr.merge(ds_list)\n",
    "    print('[{0}]\\t{1}'.format(n, datetimes[0]))\n",
    "    del ds_list, datetimes\n",
    "\n",
    "    # Subset to only the varialbes in the input DataSet\n",
    "    Variables = [variable for variable in Variables if variable in ds_input.data_vars]\n",
    "\n",
    "    # Align (sort) the data to match ID ordering in the xarray dataset\n",
    "    if n == 0:\n",
    "        # Create dataframe from input Xarray dataset feature IDs from NWM output file\n",
    "        feature_IDs = pd.DataFrame(ds_input['feature_id'].to_pandas())\n",
    "        feature_IDs = feature_IDs.rename(columns={feature_IDs.columns[0]:feature_id})\n",
    "        assert (feature_IDs[feature_id] == ds_input['feature_id']).sum() == ds_input['feature_id'].shape[0]\n",
    "\n",
    "        # Perform attribute join to obtain HUC12 for each feature\n",
    "        feature_IDs = feature_IDs.merge(df, how='left', left_on=feature_id, right_on='ID')\n",
    "        assert (feature_IDs[feature_id] == ds_input['feature_id']).sum() == ds_input['feature_id'].shape[0]\n",
    "\n",
    "    # Add HUC data to Xarray dataset to facilitate GroupBy operations later\n",
    "    ds_input[zone_name] = xr.DataArray(feature_IDs[mapping_field], dims='feature_id', coords={'feature_id':ds_input['feature_id']})\n",
    "    ds_input['HUC_12_str'] = xr.DataArray(feature_IDs['HUC12_FL_str'], dims='feature_id', coords={'feature_id':ds_input['feature_id']})\n",
    "    ds_input['Area_sqkm'] = xr.DataArray(feature_IDs[area_field], dims='feature_id', coords={'feature_id':ds_input['feature_id']})\n",
    "\n",
    "    # Iterate over variables, processing each one\n",
    "    for n2,Variable in enumerate(Variables):\n",
    "\n",
    "        # Try doing Flox groupby before applying time aggregation\n",
    "        if spatial_aggregation:\n",
    "\n",
    "            # Get the area totals by HUC\n",
    "            area_totals = ds_input['Area_sqkm'].groupby(ds_input[zone_name]).sum()\n",
    "\n",
    "            # Multiply by basin area to weight the observations\n",
    "            if Variable in ['depth', 'bucket_depth', 'deltaDepth']:\n",
    "                area_weight = True\n",
    "                if area_weight:\n",
    "                    stat = 'sum'\n",
    "                    weight = ds_input['Area_sqkm']\n",
    "                else:\n",
    "                    stat = 'mean'\n",
    "                    weight = 1.\n",
    "            else:\n",
    "                area_weight = False\n",
    "                weight = 1.\n",
    "                stat = 'sum'\n",
    "\n",
    "            # Perform the reduction to get HUC mean for each timestep\n",
    "            tic1 = time.time()\n",
    "            output = flox.xarray.xarray_reduce(\n",
    "                ds_input[Variable] * weight,\n",
    "                ds_input[zone_name],\n",
    "                func=stat).compute()\n",
    "\n",
    "            # Divide by the HUC total area to convert back to original units (mm)\n",
    "            assert (output[zone_name] == area_totals[zone_name]).sum() == output[zone_name].shape[0]\n",
    "            if area_weight and stat=='sum':\n",
    "                output = output/area_totals\n",
    "        else:\n",
    "            output = ds_input\n",
    "\n",
    "            # Get the area totals by original features\n",
    "            area_totals = ds_input['Area_sqkm']\n",
    "            assert (output['feature_id'] == area_totals['feature_id']).sum() == output['feature_id'].shape[0]\n",
    "\n",
    "        # Resample and rechunk using mean over days\n",
    "        if resample_time:\n",
    "            print('  Resampling time using mean over time=\"{0}\".'.format(resample_time_period))\n",
    "            stat = 'mean'\n",
    "\n",
    "            # Find the size of the output array after resampling\n",
    "            resampled_time_length = output.isel({zone_name:slice(0,1,None)}).resample(time=resample_time_period).mean(dim=time_coord).shape[0]\n",
    "\n",
    "            # Peform temporal aggregation\n",
    "            da_output = (\n",
    "                output\n",
    "                .resample(time=resample_time_period)\n",
    "                .mean(dim=time_coord)\n",
    "                .chunk({time_coord: resampled_time_length}))\n",
    "\n",
    "            # Convert to Xarray DataSet\n",
    "            da_output = da_output.to_dataset(name=Variable)\n",
    "\n",
    "        else:\n",
    "            stat = ''\n",
    "            da_output = output\n",
    "            da_output = da_output.to_dataset(name=Variable)\n",
    "\n",
    "        # Put time back in (lost if there is only 1 time in the inputs)\n",
    "        da_output = da_output.assign_coords({time_coord:ds_input[time_coord]})\n",
    "\n",
    "        # Rename the variable\n",
    "        da_output[Variable].attrs['long_name'] = \"{0}\".format(Variable)\n",
    "        da_output[Variable].attrs['units'] = \"{0}\".format(units)\n",
    "        da_output['Area_sqkm'] = area_totals\n",
    "\n",
    "        # convert from rate (m^3/s) to depth (m) over a day (86400s)\n",
    "        if convert_to_mm and Variable in convert_to_mm_list:\n",
    "            mm_per_m = 1000\n",
    "            da_output[Variable] = da_output[Variable]/(da_output['Area_sqkm']*1000000) * mm_per_m\n",
    "\n",
    "        if n2 == 0:\n",
    "            out_ds = da_output\n",
    "        else:\n",
    "            out_ds[Variable] = da_output[Variable]\n",
    "    \n",
    "    if n == 0:\n",
    "        out_ds2 = out_ds\n",
    "    else:\n",
    "        out_ds2 = xr.concat([out_ds2, out_ds], dim=time_coord)        \n",
    "    print('Time elapsed per file pair: {0:3.2f} seconds.'.format(time.time()-tic2))\n",
    "\n",
    "# Fix the 1D variable\n",
    "out_ds2['Area_sqkm'] = out_ds2['Area_sqkm'].isel({time_coord:0}).squeeze()\n",
    "\n",
    "# Transpose the dimension order so that output is (HUC,time)\n",
    "out_ds2 = out_ds2.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181a0da-3d03-4e8a-89c8-449643d5815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440abe70-fc4d-42c7-9093-563fae6855a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform temporal subset, or not\n",
    "if temporal_subset:\n",
    "    out_ds2 = out_ds2.loc[{time_coord:time_subset_bounds}]\n",
    "out_ds2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152ca3ea-ff42-4e17-9ae6-708b6277e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Write output file (CSV)\n",
    "if write_CSV:\n",
    "    tic1 = time.time()\n",
    "    out_file = os.path.join(outDir, output_pattern+'.csv')\n",
    "    \n",
    "    # Interpret times as strings\n",
    "    datetime_strings = pd.to_datetime(out_ds2[time_coord]).strftime('%Y%m%d')\n",
    "\n",
    "    if os.path.exists(out_file):\n",
    "        tic1 = time.time()\n",
    "        df_in = pd.read_csv(out_file)\n",
    "        df_out = pd.concat([df_in, out_ds2.to_dataframe()])\n",
    "        df_out.to_csv(out_file)\n",
    "        print('\\t      Output file written in {0:3.2f} seconds.'.format(time.time()-tic1))\n",
    "\n",
    "    else:\n",
    "        #write_csv(out_ds, out_file, columns=output['HUC_12'], index=[datetime_strings])\n",
    "        write_csv(out_ds2, out_file, columns=out_ds2[zone_name], index=[datetime_strings])\n",
    "    print('\\tExport to CSV completed in {0:3.2f} seconds.'.format(time.time()-tic1))\n",
    "\n",
    "# Write output file (netCDF)\n",
    "if write_NC:\n",
    "    tic1 = time.time()\n",
    "    out_file = os.path.join(outDir, output_pattern+'.nc')\n",
    "    \n",
    "    if os.path.exists(out_file):\n",
    "        in_ds = xr.open_dataset(out_file).load()\n",
    "        out_ds3 = xr.merge([in_ds, out_ds2])\n",
    "        #out_ds3 = xr.concat([in_ds, out_ds], dim=time_coord)\n",
    "        in_ds.close()\n",
    "        del in_ds\n",
    "        print('  Writing output to {0}'.format(out_file))\n",
    "        out_ds3.to_netcdf(out_file, mode='w', format=\"NETCDF4\", compute=True)\n",
    "        out_ds3.close()\n",
    "    else:\n",
    "        print('  Writing output to {0}'.format(out_file))\n",
    "        out_ds2.transpose().to_netcdf(out_file, mode='w', format=\"NETCDF4\", compute=True)\n",
    "    print('\\tExport to netCDF completed in {0:3.2f} seconds.'.format(time.time()-tic1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c06f436-80d9-4427-bef7-f420d8934aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all variables to see ranges of values\n",
    "vars_to_plot = [variable for variable in out_ds2.data_vars if variable not in ['totStreamflow', 'Area_sqkm']] \n",
    "out_ds2[vars_to_plot].isel({zone_name:2}).to_array().plot(row='variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8043ca4-d6dd-4c72-a3ba-6c3ba0a79d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ds2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51daa5a4-1774-42ec-b257-80696014f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Process completed in {0: 3.2f} seconds.'.format(time.time()-tic))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
