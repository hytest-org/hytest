{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e332933a-960c-4dfc-92a7-6f39ad95f13c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s\n",
    "\n",
    "In this notebook, we will be showing how to aggregate gridded data to polygons. The method aggregates gridded data _conservatively_, i.e. by exactly partitioning each grid cell into the precise region boundaries. The method makes use of two key packages [`xarray`](https://docs.xarray.dev/en/stable/index.html) and [`geopandas`](https://geopandas.org/en/stable/index.html). Our implementation is based off of this [Pangeo Discourse](https://discourse.pangeo.io/t/conservative-region-aggregation-with-xarray-geopandas-and-sparse/2715), which we have updated using more streamlined methods.\n",
    "\n",
    "The overall approach consists of:\n",
    "\n",
    "- Represent both the original gridded data and target polygons as [`geopandas.GeoSeries`](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.html) objects (with vector geometries).\n",
    "- Compute their area overlay and turn it into a sparse matrix of cell weights.\n",
    "- Perform weighted aggregation using [`xarray.Dataset.weighted`](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.weighted.html#xarray.Dataset.weighted) along the spatial dimensions.\n",
    "\n",
    "It is quite fast and transparent.\n",
    "\n",
    "The spatial polygons used in this notebook come from the [**NHDPlusV2 snapshot of the Watershed Boundary Dataset HUC12 boundaries**](https://www.sciencebase.gov/catalog/item/60cb5edfd34e86b938a373f4) provided through the [PyGeoHydro](https://docs.hyriver.io/readme/pygeohydro.html) python package.\n",
    "\n",
    "We use the HyTest intake catalog to access CONUS404 from the OSN pod. This notebook provides a relatively simple and efficient workflow that can be easily run on a local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c5e445-cb29-4c40-89ff-f97e72193d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xmode minimal\n",
    "import os\n",
    "import xarray as xr\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sparse\n",
    "\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import dask\n",
    "import cf_xarray\n",
    "\n",
    "from pynhd import NLDI, WaterData\n",
    "from pygeohydro import watershed\n",
    "import intake\n",
    "import cartopy.crs as ccrs\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb8bf1",
   "metadata": {},
   "source": [
    "## Open Dataset from Intake Catalog\n",
    "\n",
    "First, let's begin by loading the CONUS404 daily data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e540466",
   "metadata": {},
   "outputs": [],
   "source": [
    "hytest_cat = intake.open_catalog(\"https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml\")\n",
    "catalog = hytest_cat['conus404-catalog']\n",
    "list(catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509ce711-9413-4018-a914-17b5ae920adc",
   "metadata": {},
   "source": [
    "As we can see there are three different locations for the `conus404-daily` data set. The locations are (1) `-onprem-hw` meaning it is stored on the USGS Hovenweep HPC, (2) `-cloud` meaning it is store in an S3 bucket, or (3) `-osn` meaning the data is on the USGS open storage network (OSN). As the OSN is free to access from any environment, we will use that for this example, but the location can easily be changed depending on your needs. We have a writeup of our different storage locations used in the intake catalog [here](https://hytest-org.github.io/hytest/dataset_catalog/README.html).\n",
    "\n",
    "> If you change this notebook to use the CONUS404 dataset stored on S3 (options ending in `-cloud`), you will be pulling data from a `requester-pays` S3 bucket. This means you have to set up your AWS credentials, else we won't be able to load the data. Please note that reading the `-cloud` data from S3 may incur charges if you are reading data outside of the us-west-2 region or running the notebook outside of the cloud altogether. If you would like to access one of the `-cloud` options, uncomment and run the following code cell to set up your AWS credentials. You can find more info about this AWS helper function [here](../environment_set_up/Help_AWS_Credentials.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0387b3-3351-48d4-906c-40b582c304e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the lines below to read in your AWS credentials if you want to access data from a requester-pays bucket (-cloud)\n",
    "# os.environ['AWS_PROFILE'] = 'default'\n",
    "# %run ../environment_set_up/Help_AWS_Credentials.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67441905-d8ba-48ef-a0ea-77843157c1d6",
   "metadata": {},
   "source": [
    "Finally, read in the daily CONUS404 data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8195584-6720-4c9a-85a2-53bba8b803cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'conus404-daily-osn'\n",
    "conus404 = catalog[dataset].to_dask()\n",
    "conus404"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5e5915-4269-4610-bae2-c1b00eacf6a9",
   "metadata": {},
   "source": [
    "## Parallelize with Dask (optional)\n",
    "\n",
    "Some of the steps we will take are aware of parallel clustered compute environments using [`dask`](https://www.dask.org/). We can start a cluster now so that future steps take advantage\n",
    "of this ability. This is an optional step, but speed ups data loading significantly, especially when accessing data from the cloud.\n",
    "\n",
    "We have documentation on how to start a Dask Cluster in different computing environments [here](../environment_set_up/clusters.md). Uncomment the cluster start up that works for your compute environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b615046-4097-481a-a7fe-2ad7e7a55c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../environment_set_up/Start_Dask_Cluster_Nebari.ipynb\n",
    "## If this notebook is not being run on Nebari/ESIP, replace the above \n",
    "## path name with a helper appropriate to your compute environment.  Examples:\n",
    "# %run ../environment_set_up/Start_Dask_Cluster_Denali.ipynb\n",
    "# %run ../environment_set_up/Start_Dask_Cluster_Tallgrass.ipynb\n",
    "# %run ../environment_set_up/Start_Dask_Cluster_Desktop.ipynb\n",
    "# %run ../environment_set_up/Start_Dask_Cluster_PangeoCHS.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a190a575-a77f-4b98-85f1-a9c5adf364c7",
   "metadata": {},
   "source": [
    "## Load the Feature Polygons\n",
    "\n",
    "Now that we have read in the CONUS404 data, we need to read in some polygons to aggregate the data. For this example, we will use the HUC12 basins within the Delaware River Basin. To get these HUC12 polygons, we can use [`pygeohydro.watershed`](https://docs.hyriver.io/autoapi/pygeohydro/watershed/) to query the Hydro Network Linked Data Index (NLDI). All we need to get the basins is the general IDs of the HUC12 basins. For the Delaware Basin those are ones that start with 020401 or 020402."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f92769-8425-4352-83d9-7df7ae0155f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wbd = watershed.WBD(\"huc4\")\n",
    "delaware_basin = wbd.byids(field=\"huc4\", fids=\"0204\")\n",
    "huc12_basins = WaterData('wbd12').bygeom(delaware_basin.iloc[0].geometry)\n",
    "huc12_basins = huc12_basins[huc12_basins['huc12'].str.startswith(('020401', '020402'))]\n",
    "huc12_basins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c06a5dd-a24d-46fa-8392-f6b1b9d2b1d7",
   "metadata": {},
   "source": [
    "Let's plot the HUC12 basins to see how they look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a0d7e-ac4c-4aeb-885d-f9c8fe60f90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "huc12_basins.hvplot(\n",
    "    c='huc12', title=\"Delaware River HUC12 basins\",\n",
    "    coastline='50m', geo=True,\n",
    "    aspect='equal', legend=False, frame_width=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45ebd3-4ccf-4999-9e53-265f14055e18",
   "metadata": {},
   "source": [
    "An important thing to note is that all geodataframes should have a coordinate reference system (CRS). Let's check to make sure our geodataframe has a CRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5c5b95-8b81-47d7-b05b-8083de307b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "huc12_basins.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105fdba9-05fb-4a8d-b23c-f51f9606e9ae",
   "metadata": {},
   "source": [
    "## Limit CONUS404 Spatial Range\n",
    "\n",
    "With the HUC12 basins read in, we only need the CONUS404 data that spans these polygons as they are the regions we will be aggregating. So, let's limit the CONUS404 spatial range to that of the basins. This will save on memory and computation. Note doing this is mainly useful when the region's footprint is much smaller than the footprint of the gridded model.\n",
    "\n",
    "To limit the spatial range, we first need to convert the CRS of the basins to that of CONUS404. Then extract the bounding box of the basins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0fab67-4425-4d16-b8c0-35798f152703",
   "metadata": {},
   "outputs": [],
   "source": [
    "huc12_basins_conus404_crs = huc12_basins.to_crs(conus404.crs.crs_wkt)\n",
    "bbox = huc12_basins_conus404_crs.total_bounds\n",
    "bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b76808f-9f1f-44af-911c-659db8826923",
   "metadata": {},
   "source": [
    "Then select the CONUS404 data within the bounding box. However, when we do this, we will extend the bounds out by 5% of their range to ensure all of our basins are within the spatially limited data. We do this as the reprojections of the CRS can cause slight distortions that make polygons on the bounds not fall fully within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dabf73-6bbb-4246-ba0f-2aa3c33b440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_x_range = bbox[2] - bbox[0]\n",
    "bbox_y_range = bbox[3] - bbox[1]\n",
    "x_range = slice(bbox[0] - bbox_x_range * 0.05,\n",
    "                bbox[2] + bbox_x_range * 0.05)\n",
    "y_range = slice(bbox[1] - bbox_y_range * 0.05,\n",
    "                bbox[3] + bbox_y_range * 0.05)\n",
    "\n",
    "conus404 = conus404.sel(x=x_range, y=y_range)\n",
    "conus404"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94815d1-dd17-4ddf-8541-d3add5331c6e",
   "metadata": {},
   "source": [
    "To make sure this worked as intended, let's plot the full basin over the extracted CONUS404 data footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5a8f7e-25a7-468f-a1df-634ecb33b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the footprint of the grid\n",
    "cutout = xr.ones_like(conus404.isel(time=0).drop_vars(['lat', 'lon'])['ACDEWC'])\n",
    "# We need to write the CRS to the CONUS404 dataset and\n",
    "# reproject to the crs of the HUC12 basins dataframe for clean plotting with hvplot\n",
    "cutout = cutout.rio.write_crs(conus404.crs.crs_wkt).rio.reproject('EPSG:4326')\n",
    "\n",
    "cutout_plt = cutout.hvplot(\n",
    "    coastline='50m', geo=True,\n",
    "    aspect='equal', frame_width=300, colorbar=False\n",
    ")\n",
    "huc12_plt = huc12_basins.hvplot(\n",
    "    geo=True, c='r'\n",
    ")\n",
    "cutout_plt * huc12_plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da674469-2134-48c0-929f-f5611a76fe63",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dcdd5d-4362-4dcb-8a30-0a432d976e7a",
   "metadata": {},
   "source": [
    "## Aggregate CONUS404 to HUC12 Polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a271e8c6-cb84-4146-ba70-55b0ddfb5fe4",
   "metadata": {},
   "source": [
    "Now that we have our dataset and basin polygons prepared, we are ready to aggregate. \n",
    "\n",
    "### Create Grid Polygons\n",
    "The first step here is to extract the CONUS404 grid information, which consists of getting the grid center points and the grid bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518f137-6742-41a2-b1c4-3e19648b6f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = conus404[['x', 'y']].drop_vars(['lat', 'lon']).reset_coords()\n",
    "grid = grid.cf.add_bounds(['x', 'y'])\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d05c230-409c-4384-b8d3-a0c55266676d",
   "metadata": {},
   "source": [
    "Then, we \"stack\" the data into a single 1D array. This creates an index of (`x`, `y`) pairs of the center points that links to the bounds. This will make generating polygons of the grid cells from the bounds much simpler than any manual looping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02bfde-c749-40e1-b6f8-17db360e3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = grid.stack(point=('y', 'x'))\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc35fb-ef4a-4264-9228-63e6263e306f",
   "metadata": {},
   "source": [
    "Next, we can use the point pairs we just created to make polygons from the bounds. To do this, we we will make a simple function that takes the `x` and `y` bound to generate a polygon for the given grid cell. We can then apply it in parallel using [`xarray.apply_ufunc`](https://docs.xarray.dev/en/stable/generated/xarray.apply_ufunc.html). Note that this step will get slower as we increase the grid size from our limited range. Perhaps could be vectorized using [`pygeos`](https://pygeos.readthedocs.io/en/latest/)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9716bc0-6c5b-44db-8ccd-d555e9f1d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def bounds_to_poly(x_bounds, y_bounds):\n",
    "    return Polygon([\n",
    "        (x_bounds[0], y_bounds[0]),\n",
    "        (x_bounds[0], y_bounds[1]),\n",
    "        (x_bounds[1], y_bounds[1]),\n",
    "        (x_bounds[1], y_bounds[0])\n",
    "    ])\n",
    "    \n",
    "boxes = xr.apply_ufunc(\n",
    "    bounds_to_poly,\n",
    "    points.x_bounds,\n",
    "    points.y_bounds,\n",
    "    input_core_dims=[(\"bounds\",),  (\"bounds\",)],\n",
    "    output_dtypes=[np.dtype('O')],\n",
    "    vectorize=True\n",
    ")\n",
    "boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de8dacc-8b87-4ac2-9a7e-6d565f08fea7",
   "metadata": {},
   "source": [
    "Finally, we convert this `xarray.DataArray` to a `geopandas.GeoDataframe`, specifying the projected CRS to be the same as the CONUS404 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f7c5e-31c5-4a06-91e4-7437c7686d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_polygons= gp.GeoDataFrame(\n",
    "    data={\"geometry\": boxes.values, \"y\": boxes['y'], \"x\": boxes['x']},\n",
    "    index=boxes.indexes[\"point\"],\n",
    "    crs=conus404.crs.crs_wkt\n",
    ")\n",
    "grid_polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a46779-1b8e-4ca0-a17f-7e51831d562a",
   "metadata": {},
   "source": [
    "### Key Step: Overlay the Two Geometries\n",
    "\n",
    "We are finally ready for the magic of this method, the weight generation using `geopandas`. To calculate the weights, we will use the [`overlay`](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.overlay.html#geopandas.GeoDataFrame.overlay) method in `geopandas`. It calculates the area overlap between polygons in two different `GeoDataFrames`, i.e. the original grid polygons and the HUC12 polygons. An important thing to note is that when generating the weights we need to use an equal area projection (i.e., equal area CRS). So, before we overlay, we will need to convert the CRS to an area preserving projection. Here we use the [NSIDC EASE-Grid 2.0](https://nsidc.org/data/user-resources/help-center/guide-ease-grids) grid for the Northern Hemisphere. \n",
    "\n",
    "> As long as the feature polygons only cover a few 100s of grid polygons, this is an extremely fast operation. However, if 1000s of grid cells are covered this can be a wasteful calculation, as we really only need it for the grid polygons that are partially covered. Otherwise, we could use a faster computational method for fully covered cells, but we will leave that complex topic for another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dc00dc-63d7-4561-a113-a04732f8554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "crs_area = \"EPSG:6931\" # Good for northern hemisphere\n",
    "# crs_area = \"EPSG:5070\" # Good for CONUS\n",
    "\n",
    "huc12_basins_area = huc12_basins.to_crs(crs_area)\n",
    "grid_polygons = grid_polygons.to_crs(crs_area)\n",
    "\n",
    "# overlay the polygons\n",
    "overlay = grid_polygons.overlay(huc12_basins_area, keep_geom_type=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5624f-7c5b-41e6-875c-72c188aef887",
   "metadata": {},
   "source": [
    "This is essentially already a sparse matrix, mapping one grid space to the other. How sparse? Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e3f3b0-1e72-4392-980b-5af7f2a4c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = len(overlay) / (len(grid_polygons) * len(huc12_basins_area))\n",
    "sparsity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b37c1c66-a37a-4038-95c9-32fa7eecad41",
   "metadata": {},
   "source": [
    "Let's explore these overlays a little bit more. Mainly, we can verify that each basin's area is preserved in the overlay operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a823e44-46e6-4d25-8f52-c7b7f2ec01a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate areas of HUC12s from overlay and original polygons\n",
    "overlay_area = overlay.geometry.area.groupby(overlay['huc12']).sum()\n",
    "huc12_area = huc12_basins_area.geometry.area.groupby(huc12_basins_area['huc12']).sum()\n",
    "# find the max fractional difference\n",
    "(np.abs(overlay_area - huc12_area) / huc12_area).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f068e-d4db-466e-b8d7-af2a361b3345",
   "metadata": {},
   "source": [
    "Nice! So, it worked and only have differences within machine precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ee3e3-2f06-4fb0-807a-01f3ddc459ff",
   "metadata": {},
   "source": [
    "### Calculate the Weights (i.e., Area Fraction for each Region)\n",
    "\n",
    "Now that we have the overlay of the grid polygons with the HUC12 polygons, we only need to transform this to weights for each grid cell to aggregate. This transform tells us how much of a HUC12 polygon's total area is within each of the grid cells. This is accurate because we used an area-preserving CRS. Calculating this fractional area is again simple with `geopandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288707a4-68a2-4b64-939c-4ba8faad87de",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cell_fraction = overlay.geometry.area.groupby(overlay['huc12']).transform(lambda x: x / x.sum())\n",
    "grid_cell_fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11503a-73e7-4969-8dc8-f21426da7fc1",
   "metadata": {},
   "source": [
    "We can verify that these all sum up to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e26cca-183b-4208-a698-135994250aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cell_fraction.groupby(overlay['huc12']).sum().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f89910-15e8-4842-9b56-c393d84d180b",
   "metadata": {},
   "source": [
    "However, in their current `Series` form, the weights aren't very useful. What we need is to convert them to a sparse array within `xarray`. Thankfully, `xarray` can easily do this if we add a `MultiIndex` for the grid cells' center points and HUC12 IDs to the cell fraction `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ad60b-db35-4b56-95d8-d385386e9817",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_index = overlay.set_index(['y', 'x', 'huc12']).index\n",
    "df_weights = pd.DataFrame({\"weights\": grid_cell_fraction.values}, index=multi_index)\n",
    "df_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a005718-1ac8-48ff-b109-f13b5dd92f40",
   "metadata": {},
   "source": [
    "We can bring this directly into `xarray` as a 1D `Dataset` and then unstack it into a sparse array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf2e716-bf99-4025-8bcd-491088478457",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_weights = xr.Dataset(df_weights)\n",
    "weights_sparse = ds_weights.unstack(sparse=True, fill_value=0.).weights\n",
    "weights_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385348ce-7e05-414c-a2a5-fcd00efdc844",
   "metadata": {},
   "source": [
    "Again, we can clearly see that this is a sparse matrix from the density. This is now all we need to do our aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94d0f1-a29b-4fcb-b668-03702dfcdb3b",
   "metadata": {},
   "source": [
    "#### Perform the Aggregation\n",
    "\n",
    "Unlike deriving the weights, actually performing the aggregation is a simple one line of code. This is because we utilize [`xarray.Dataset.weighted`](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.weighted.html) to do our weighted calculations. It will happily take a sparse array as weights and compute the aggregation.\n",
    "\n",
    "However, rather than aggregating all variables, let's only aggregate two in order to reduce the computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843161c1-51ef-470f-8554-0af124e3ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Note the .compute() at the end to actually do the computation here vs lazy computing\n",
    "huc12_aggregation = conus404[['T2', 'PREC_ACC_NC']].weighted(weights_sparse).sum(dim=['x', 'y']).compute()\n",
    "huc12_aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5345f22-bd70-4f0d-a518-e6b4c2613dff",
   "metadata": {},
   "source": [
    "Note that our aggregations are still sparse arrays, which is the cost of using a sparse array as our weights. However, the density of these sparse arrays is large, meaning we want to convert them out of sparse arrays and back to dense arrays. To do this, we can use `apply_ufunc` with a `lambda` function to convert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568e3084-223e-49dc-a44a-521bfb7a3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "huc12_aggregation = xr.apply_ufunc(lambda data: data.todense(), huc12_aggregation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617d1d7d-55b1-42b4-8204-c49ea4584fb2",
   "metadata": {},
   "source": [
    "It important to note that we used a `sum` aggregation above rather than a `mean`. In theory, the two methods should be the same as a weighted sum is a weighted mean if the sum of weights is one (which they are in our case):\n",
    "\n",
    "$$\\textrm{weighted mean:\\ }\\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}$$\n",
    "$$\\textrm{weighted sum:\\ } \\sum_{i=1}^n w_i x_i$$\n",
    "$$\\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i} = \\sum_{i=1}^n w_i x_i \\ \\textrm{if} \\sum_{i=1}^n w_i = 1$$\n",
    "\n",
    "However, in practice, this will matter if your data contains `NaN` values. When using a sum with `NaN`s, the `NaN`s will effectively be treated as zeros, meaning any all `NaN` aggregation will result in a 0 value. For the mean, `NaN`s are ignored in the calculation, but all `NaN` aggregations will result in a `NaN` value being returned. Therefore, the two methods are the same when you have data mixed with `NaN`s, but if you want all `NaN` aggregations to return `NaN` use a `mean`. Otherwise, if you want it to return `0`, use a `sum`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade5c8b5-bf48-4465-9d17-a5c86bc082fa",
   "metadata": {},
   "source": [
    "## Explore the Aggregation\n",
    "\n",
    "Now that we have the aggregated data and converted it to dense form, let's make some plots!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ede5400-e486-47a6-a658-185f06cbb0af",
   "metadata": {},
   "source": [
    "### Mean of Variable by HUC12\n",
    "\n",
    "First, we will calculate and plot mean value over all time steps for every HU12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67abe4d0-002b-4d0a-aeef-1d2f30aff384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = huc12_aggregation.mean(dim='time').to_dataframe()\n",
    "df_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4242ee28-808c-4dc5-89db-03e53be651a0",
   "metadata": {},
   "source": [
    "We need to merge this with the HUC12 basin `GeoDataFrame` to get the geometry info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d861c5c-6e01-41f1-a7ea-3dbac5e80140",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = pd.merge(huc12_basins, df_mean, left_on='huc12', right_on='huc12')\n",
    "df_mean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aea4d4-9ecd-40c3-9bcd-867fe650d02e",
   "metadata": {},
   "source": [
    "Time to plot our two example variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3203715-13b8-4432-93b0-3a26627afbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_plt = df_mean.hvplot(\n",
    "    c='T2', geo=True, coastline='50m', cmap='viridis',\n",
    "    title='Mean Temperature at 2m [K]', frame_width=300, aspect='equal'\n",
    ")\n",
    "precip_plt = df_mean.hvplot(\n",
    "    c='PREC_ACC_NC', geo=True, coastline='50m', cmap='viridis',\n",
    "    title='Mean 24hr Accumulated Precipitation [mm]', frame_width=300,\n",
    "    aspect='equal'\n",
    ")\n",
    "\n",
    "temp_plt + precip_plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0aae27-4ca0-4a22-bea6-b162918e2543",
   "metadata": {},
   "source": [
    "### Mean Monthly Time Series\n",
    "\n",
    "Finally, let's plot the mean monthly time series for each HUC12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d2bcd-c7ae-4f9b-a731-2be08e08c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_timeseries = huc12_aggregation.resample(time=\"MS\").mean()\n",
    "monthly_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad289493-e3d0-4394-b500-83ef274f42a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_timeseries.hvplot(x='time', grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb148684-2ed6-42d1-a8d9-cd532c6aa55a",
   "metadata": {},
   "source": [
    "## Shut Down the Dask Client\n",
    "\n",
    "If utilized, we should shut down the dask client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84254ec4-f078-4455-89b0-1eabc7243892",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
