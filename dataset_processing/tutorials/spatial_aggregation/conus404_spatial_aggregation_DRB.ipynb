{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s\n",
    "\n",
    "This tutorial demonstrates the use of gdptools, a python package for area-weighted interpolation of *source* gridded datasets, such as conus404, to *target* polygonal geospatial fabrics.  Source datasets can be any gridded dataset that can be opened in XArray.  However it's important to note that gdptools, operations on XArray Datasets or DataArrays with dimensions of (Y,X,Time) generally.  As such climate datasets that have ensemble dimensions will require subsetting by ensemble to obtain the a dataset with the proper dimensions.  The target dataset can be any polygonal dataset that can be read by GeoPandas.  GDPtools also has capabilities of interpolating gridded data to lines as well, but our focus here is interpolating to polygons. \n",
    "\n",
    "In this workflow, CONUS404 is aggregated to Deleware River Basin (DRB) HUC12s. The spatial polygons used in this notebook come from the [**NHDPlusV2 snapshot of the Watershed Boundary Dataset HUC12 boundaries**](https://www.sciencebase.gov/catalog/item/60cb5edfd34e86b938a373f4) provided through the [PyGeoHydro](https://docs.hyriver.io/readme/pygeohydro.html) python package.\n",
    "\n",
    "We use the HyTest intake catalog to access CONUS404 from the OSN pod. This notebook provides a relatively simple and efficient workflow that can be easily run on a local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Common python packages\n",
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "import hvplot.pandas\n",
    "import hvplot.dask\n",
    "import intake\n",
    "import warnings\n",
    "import intake_xarray\n",
    "import datetime\n",
    "import holoviews as hv\n",
    "import geoviews as gv\n",
    "from holoviews import opts\n",
    "import cartopy.crs as ccrs\n",
    "import panel as pn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# HyRiver packages\n",
    "from pynhd import NLDI, WaterData\n",
    "import pygeohydro as gh\n",
    "# GDPTools packages\n",
    "from gdptools import AggGen, UserCatData, WeightGen\n",
    "import os\n",
    "# Until gdptools updates it's numpy dependency to v2, the environment statement below is required\n",
    "os.environ[\"HYRIVER_CACHE_DISABLE\"] = \"true\"\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "pn.extension()\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we setup a variable the sets our local context, working on the HPC or working locally on your Desktop.  This just modifies the access point of the conus404 data, using the Hovenweep access for HPC and the OSN pod access for the Desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sys = \"Desktop\"  # \"HPC\"  # or \"Desktop\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a subset of the [**HyRiver**](https://docs.hyriver.io/index.html) Python packages to access HUC12 geometries representing the Delaware River Basin. The process involves several steps:\n",
    "\n",
    "1. **Select the HUC4 Mid-Atlantic Region**:\n",
    "    - This region encompasses the Delaware River Basin (HUC4 code: 0204).\n",
    "\n",
    "2. **Retrieve HUC12 Basins within the Selected HUC4**:\n",
    "    - Obtain all HUC12 basins that fall within the HUC4 Mid-Atlantic region.\n",
    "\n",
    "3. **Filter HUC12 Basins**:\n",
    "    - Focus on the HUC12 basins within the two HUC6 regions whose drainages terminate in the Delaware River Basin (DRB).\n",
    "    - Exclude basins with drainages that terminate at the coast.\n",
    "\n",
    "We used [**Science in Your Watershed**](https://water.usgs.gov/wsc/map_index.html) to help identify the HUC6 regions that drain directly into the DRB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wbd = gh.WBD(\"huc4\")\n",
    "del_huc4 = wbd.byids(field=\"huc4\", fids=\"0204\")\n",
    "huc12_basins = WaterData('wbd12').bygeom(del_huc4.geometry[0])\n",
    "filtered_gdf = huc12_basins[huc12_basins['huc12'].str.startswith(('020401', '020402'))]\n",
    "filtered_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holoviews.element.tiles import OSM\n",
    "drb = filtered_gdf.hvplot(\n",
    "    geo=True, coastline='50m', alpha=0.2,  frame_width=300,\n",
    "    xlabel=\"longitude\", ylabel=\"latitude\",\n",
    "    title=\"Delaware River HUC12 basins\", aspect='equal'\n",
    ")\n",
    "OSM() * drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access conus404 via the HyTest intake catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the hytest data intake catalog\n",
    "hytest_cat = intake.open_catalog(\"https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml\")\n",
    "list(hytest_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the conus404 sub-catalog\n",
    "cat = hytest_cat['conus404-catalog']\n",
    "list(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of options for accessing **conus404**:\n",
    "\n",
    "1. **HPC Setting (`t_sys = HPC`)**:\n",
    "    - **Assumption**: The notebook is run on the USGS HPC Hovenweep.\n",
    "    - **Access Method**: Utilizes the on-premises version of the data.\n",
    "    - **Benefits**:\n",
    "        - **Workflow Association**: The workflow is directly linked to the data.\n",
    "        - **Speed**: Eliminates the need to download data, significantly reducing access and processing time.\n",
    "\n",
    "2. **Desktop Setting (`t_sys = Desktop`)**:\n",
    "    - **Use Case**: Suitable for workflows that do not require HPC resources or for developing workflows locally before deploying them to the HPC.\n",
    "    - **Access Method**: Connects to the **conus404** data via the OSN pod.\n",
    "    - **Benefits**:\n",
    "        - **Flexibility**: Allows for local development and testing.\n",
    "        - **Performance**: Provides a fast connection to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select the dataset you want to read into your notebook and preview its metadata\n",
    "if t_sys == \"HPC\":\n",
    "    dataset = 'conus404-daily-diagnostic-onprem-hw'\n",
    "elif t_sys == \"Desktop\":\n",
    "    dataset = 'conus404-daily-diagnostic-osn' \n",
    "else:\n",
    "    print(\"Please set the variable t_sys above to one of 'HPC' or 'Desktop'\")        \n",
    "cat[dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dataset and use metpy to parse the crs information on the dataset\n",
    "print(f\"Reading {dataset} metadata...\", end='')\n",
    "ds = cat[dataset].to_dask().metpy.parse_cf()\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GDPTools Background\n",
    "\n",
    "In this section, we utilize three data classes from the `gdptools` package: `UserCatData`, `WeightGen`, and `AggGen`.\n",
    "\n",
    "* [**UserCatData**](https://gdptools.readthedocs.io/en/develop/user_input_data_classes.html):  \n",
    "  Serves as a data container for both the source and target datasets, along with their associated metadata. The instantiated object `user_data` is employed by both the `WeightGen` and `AggGen` classes.\n",
    "\n",
    "* [**WeightGen**](https://gdptools.readthedocs.io/en/develop/weight_gen_classes.html):  \n",
    "  Responsible for calculating the intersected areas between the source and target datasets. It generates normalized area-weights, which are subsequently used by the `AggGen` class to compute interpolated values between the datasets.\n",
    "\n",
    "* [**AggGen**](https://gdptools.readthedocs.io/en/develop/agg_gen_classes.html):  \n",
    "  Facilitates the interpolation of target data to match the source data using the areal weights calculated by `WeightGen`. This process is conducted over the time period specified in the `UserCatData` object.\n",
    "\n",
    "### Instantiation of the `UserCatData` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinate Reference System (CRS) of the conus404 dataset\n",
    "source_crs = ds.crs.crs_wkt\n",
    "\n",
    "# Coordinate names of the conus404 dataset\n",
    "x_coord = \"x\"\n",
    "y_coord = \"y\"\n",
    "t_coord = \"time\"\n",
    "\n",
    "# Time period of interest for areal interpolation of conus404 to DRB HUC12s\n",
    "# using the AggGen class below. Note: The dates follow the same format as the\n",
    "# time values in the conus404 dataset.\n",
    "sdate = \"1979-10-01T00:00:00.000000000\"\n",
    "edate = \"2022-10-01T00:00:00.000000000\"\n",
    "\n",
    "# Variables from the conus404 dataset used for areal interpolation\n",
    "variables = [\"T2MIN\", \"T2MAX\", \"RAINNCVMEAN\"]\n",
    "\n",
    "# CRS of the DRB HUC12 polygons\n",
    "target_crs = 5070\n",
    "\n",
    "# Column name for the unique identifier associated with target polygons.\n",
    "# This ID is used in both the generated weights file and the areal interpolated output.\n",
    "target_poly_idx = \"huc12\"\n",
    "\n",
    "# Common equal-area CRS for reprojecting both source and target data.\n",
    "# This CRS is used for calculating areal weights in the WeightGen class.\n",
    "weight_gen_crs = 5070\n",
    "\n",
    "# Instantiate the UserCatData class, which serves as a container for both\n",
    "# source and target datasets, along with associated metadata. The UserCatData\n",
    "# object provides methods used by the WeightGen and AggGen classes to subset\n",
    "# and reproject the data.\n",
    "user_data = UserCatData(\n",
    "    ds=ds,\n",
    "    proj_ds=source_crs,\n",
    "    x_coord=x_coord,\n",
    "    y_coord=y_coord,\n",
    "    t_coord=t_coord,\n",
    "    var=variables,\n",
    "    f_feature=filtered_gdf,\n",
    "    proj_feature=target_crs,\n",
    "    id_feature=target_poly_idx,\n",
    "    period=[sdate, edate],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Generation with `WeightGen`\n",
    "\n",
    "In this section, we utilize the `WeightGen` class from the `gdptools` package to calculate the normalized areal weights necessary for interpolating the source gridded data (`conus404`) to the target polygonal boundaries (`DRB HUC12s`). The areal weights represent the proportion of each grid cell that overlaps with each polygon, facilitating accurate **areal interpolation** of the data. These weights are calculated using the `calculate_weights()` method.\n",
    "\n",
    "**Weight Calculation Process:**\n",
    "\n",
    "1. **Subset Source Data**: The source data is subset based on the bounds of the target data, with an additional small buffer to ensure coverage. The buffer size is determined based on [specific criteria or methodology].\n",
    "\n",
    "2. **Create cell boundary GeoDataFrame**: A GeoDataFrame of the cell boundaries is created for each node in the subsetted source data, enabling spatial operations.\n",
    "\n",
    "3. **Validate Geometries**: The target file is checked for invalid geometries, which can occur due to various reasons such as topology errors. Invalid geometries are fixed using Shapely's `make_valid()` method to prevent failures during intersection calculations.\n",
    "\n",
    "4. **Calculate and Normalize Areas**: For each polygon, `gdptools` calculates the area of each intersecting grid cell and normalizes it by the total area of the target polygon. This ensures that the weights for each polygon sum to 1, provided the polygon is entirely covered by the source data.\n",
    "   \n",
    "   - **Validation**: A quick check on the weights can be performed by grouping the resulting weights by the `target_poly_idx` and calculating the sum. For all polygons completely covered by the source data, the weights will sum to 1.\n",
    "\n",
    "**Note:** The `method` parameter can be set to one of `\"serial\"`, `\"parallel\"`, or `\"dask\"`. Given the scale of the gridded `conus404` data (4 km × 4 km) and the number and spatial footprint of the `DRB HUC12s`, using `\"serial\"` in this case is the most efficient method. In subsequent sections, we will explore how the `\"parallel\"` and `\"dask\"` methods can provide speed-ups in the areal interpolation process, as well as in the computation of weights for broader CONUS-wide targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wght_gen = WeightGen(\n",
    "    user_data=user_data,\n",
    "    method=\"serial\",\n",
    "    output_file=\"wghts_drb_ser_c404daily.csv\",\n",
    "    weight_gen_crs=6931\n",
    ")\n",
    "\n",
    "wdf = wght_gen.calculate_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Areal Interpolation with the `AggGen` Class\n",
    "\n",
    "In this section, we demonstrate the use of the `AggGen` class and its `calculate_agg()` method from the `gdptools` package to perform areal interpolation. We will explore all three `agg_engine` options: `\"serial\"`, `\"parallel\"`, and `\"dask\"`. The following links provide detailed documentation on the available parameter options:\n",
    "\n",
    "* [**agg_engines**](https://gdptools.readthedocs.io/en/develop/agg_gen_classes.html#gdptools.agg_gen.AGGENGINES)\n",
    "* [**agg_writers**](https://gdptools.readthedocs.io/en/develop/agg_gen_classes.html#gdptools.agg_gen.AGGWRITERS)\n",
    "* [**stat_methods**](https://gdptools.readthedocs.io/en/develop/agg_gen_classes.html#gdptools.agg_gen.STATSMETHODS)\n",
    "\n",
    "When using `AggGen` and the `calculate_agg()` method, it is important to consider the overlap between the source and target data when selecting the `stat_method` parameter value. All statistical methods have a masked variant in addition to the standard method; for example, `\"mean\"` and `\"masked_mean\"`. In cases where the source data has partial overlap with a target polygon, the `\"mean\"` method will return a missing value for the polygon, whereas the `\"masked_mean\"` method will calculate the statistic based on the available overlapping source cells. These considerations help users determine whether using a masked statistic is desirable or if a missing value would be preferred, allowing for post-processing of missing values (e.g., using nearest-neighbor or other approaches to handle the lack of overlap). In the case here conus404 completely covers the footprint of the DRB HUC12s, as such the `\"mean\"` method would be sufficient.  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "agg_gen = AggGen(\n",
    "    user_data=user_data,\n",
    "    stat_method=\"mean\",\n",
    "    agg_engine=\"parallel\",\n",
    "    jobs=4,\n",
    "    agg_writer=\"netcdf\",\n",
    "    weights='wghts_drb_ser_c404daily.csv',\n",
    "    out_path='.',\n",
    "    file_prefix=\"serial_weights\",\n",
    "    precision=8\n",
    ")\n",
    "ngdf, ds_out = agg_gen.calculate_agg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "The [**`calculate_agg()`**](https://gdptools.readthedocs.io/en/develop/agg_gen_classes.html#gdptools.agg_gen.AggGen.calculate_agg) method returns two objects: `ngdf` and `ds_out`.\n",
    "\n",
    "- **`ngdf`**: A `GeoDataFrame` derived from the target GeoDataFrame (`filtered_gdf`) specified in the `UserCatData` container. This GeoDataFrame has been both sorted and dissolved based on the identifiers in the `\"huc12\"` column, as defined by the `target_poly_idx` parameter.\n",
    "  \n",
    "- **`ds_out`**: The areally weighted interpolation output as an XArray Dataset. The Dataset consists of dimensions `time` and `huc12`, and the data variables are `T2MIN`, `T2MAX`, and `RAINNCVMEAN`.  \n",
    "\n",
    "A preview of the `ngdf` GeoDataFrame below shows that it is sorted by `\"huc12\"`. In this case, there are no duplicate `\"huc12\"` values, resulting in the original and output GeoDataFrames having the same number of rows.  Some target datasets such as the GFv1.1, will result in many dissolved geometries.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ngdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results as a quick sanity check\n",
    "\n",
    "Here we plot the results along with the corresponding conus404 values.  To make the plot a little more interesting we choose the time step with the most precipitation.  This provides a quick qualitative sanity check.  If one is intersted in looking in more detail in a graphic presentation of a target polygon, overlayed on the intersecting grid cells, with the grid-cell values and weights shown for each intersection, please look at the tail end of notebook [**ClimateR-Catalog - Terraclime data**](https://gdptools.readthedocs.io/en/develop/Examples/ClimateR-Catalog/terraclime_et.html) for some example code to generate a plot which give a more quantitative expression of the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Format the interpolated results for plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert processed xarray Dataset to DataFrame\n",
    "df = ds_out.to_dataframe().reset_index()\n",
    "# Pivot the DataFrame to have variables as separate columns\n",
    "df_pivot = df.pivot_table(index=['time', 'huc12'], values=['T2MAX', 'T2MIN', 'RAINNCVMEAN']).reset_index()\n",
    "\n",
    "# Merge GeoDataFrame with DataFrame\n",
    "merged_gdf = ngdf.to_crs(5070).merge(df_pivot, on='huc12')\n",
    "\n",
    "# Convert RAINNCVMEAN from kg/m²/s to mm/day\n",
    "merged_gdf['RAINNCVMEAN_mm_day'] = merged_gdf['RAINNCVMEAN'] * 86400\n",
    "# Convert T2MAX and T2MIN from Kelvin to Celsius\n",
    "merged_gdf['T2MAX_C'] = merged_gdf['T2MAX'] - 273.15\n",
    "merged_gdf['T2MIN_C'] = merged_gdf['T2MIN'] - 273.15\n",
    "\n",
    "# Calculate total precipitation for each time step\n",
    "rain_sum = merged_gdf.groupby('time')['RAINNCVMEAN_mm_day'].sum()\n",
    "\n",
    "# Identify the time step with the maximum total precipitation\n",
    "max_rain_time = rain_sum.idxmax()\n",
    "print(f\"Time step with maximum total precipitation: {max_rain_time}\")\n",
    "\n",
    "# Subset the GeoDataFrame for the selected time step\n",
    "subset = merged_gdf[merged_gdf['time'] == max_rain_time]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process the sub-setted conus404 data for plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We can use our agg_gen object to retrieve the subsetted conus404 data\n",
    "da_t2max = agg_gen.agg_data.get(\"T2MAX\").da\n",
    "da_t2min = agg_gen.agg_data.get(\"T2MIN\").da\n",
    "da_rain = agg_gen.agg_data.get(\"RAINNCVMEAN\").da\n",
    "\n",
    "# Get the subsetted raw conus404 cubes used in the areal interpolation\n",
    "da_t2max = agg_gen.agg_data.get(\"T2MAX\").da.sel(time=max_rain_time) - 273.15\n",
    "da_t2min = agg_gen.agg_data.get(\"T2MIN\").da.sel(time=max_rain_time) - 273.15\n",
    "da_rain = agg_gen.agg_data.get(\"RAINNCVMEAN\").da.sel(time=max_rain_time) * 86400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate the plot**\n",
    "* Here we use GeoViews and HoloViews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Cartopy CRS using EPSG code 5070 (NAD83 / Conus Albers)\n",
    "crs_cartopy = ccrs.epsg(5070)\n",
    "\n",
    "# Define color maps for each variable\n",
    "color_maps = {\n",
    "    'T2MAX_C': 'Reds',\n",
    "    'T2MIN_C': 'Blues',\n",
    "    'RAINNCVMEAN_mm_day': 'Greens'\n",
    "}\n",
    "# Define color maps for raw data\n",
    "color_maps_raw = {\n",
    "    'T2MAX': 'Reds',\n",
    "    'T2MIN': 'Blues',\n",
    "    'RAINNCVMEAN': 'Greens'\n",
    "}\n",
    "# Create Polygons for T2MAX_C\n",
    "map_T2MAX = gv.Polygons(\n",
    "    subset.to_crs(5070), \n",
    "    vdims=['T2MAX_C'],\n",
    "    crs=crs_cartopy  # Use Cartopy CRS here\n",
    ").opts(\n",
    "    cmap=color_maps['T2MAX_C'],\n",
    "    colorbar=True,\n",
    "    tools=['hover'],\n",
    "    title='T2MAX (°C)',  # Included units\n",
    "    alpha=0.7,\n",
    "    frame_width=200,\n",
    "    aspect='equal',\n",
    "    padding=0,\n",
    ")\n",
    "\n",
    "# Create Polygons for T2MIN_C\n",
    "map_T2MIN = gv.Polygons(\n",
    "    subset.to_crs(5070), \n",
    "    vdims=['T2MIN_C'],\n",
    "    crs=crs_cartopy\n",
    ").opts(\n",
    "    cmap=color_maps['T2MIN_C'],\n",
    "    colorbar=True,\n",
    "    tools=['hover'],\n",
    "    title='T2MIN (°C)',  # Included units\n",
    "    alpha=0.7,\n",
    "    frame_width=200,\n",
    "    aspect='equal',\n",
    "    padding=0,\n",
    ")\n",
    "\n",
    "# Create Polygons for RAINNCVMEAN_mm_day\n",
    "map_RAINNCVMEAN = gv.Polygons(\n",
    "    subset.to_crs(5070), \n",
    "    vdims=['RAINNCVMEAN_mm_day'],\n",
    "    crs=crs_cartopy\n",
    ").opts(\n",
    "    cmap=color_maps['RAINNCVMEAN_mm_day'],\n",
    "    colorbar=True,\n",
    "    tools=['hover'],\n",
    "    title='RAINNCVMEAN (mm/day)',  # Included units\n",
    "    alpha=0.7,\n",
    "    frame_width=200,\n",
    "    aspect='equal',\n",
    "    padding=0,\n",
    ")\n",
    "\n",
    "# Create raw sub-setted data frames\n",
    "# Assuming 'lon' and 'lat' are the coordinate names; adjust if necessary\n",
    "coord_x = 'x'  # Replace with actual x-coordinate name\n",
    "coord_y = 'y'  # Replace with actual y-coordinate name\n",
    "\n",
    "# Create HoloViews Image for T2MAX (Raw)\n",
    "image_T2MAX_raw = hv.Image(\n",
    "    da_t2max, \n",
    "    kdims=[coord_x, coord_y],\n",
    "    vdims=['T2MAX']\n",
    ").opts(\n",
    "    cmap=color_maps_raw['T2MAX'],\n",
    "    colorbar=True,\n",
    "    tools=['hover'],\n",
    "    title='T2MAX Raw (°C)',\n",
    "    alpha=0.7,\n",
    "    frame_width=200,\n",
    "    aspect='equal'\n",
    ")\n",
    "\n",
    "# Create HoloViews Image for T2MIN (Raw)\n",
    "image_T2MIN_raw = hv.Image(\n",
    "    da_t2min, \n",
    "    kdims=[coord_x, coord_y],\n",
    "    vdims=['T2MIN']\n",
    ").opts(\n",
    "    cmap=color_maps_raw['T2MIN'],\n",
    "    colorbar=True,\n",
    "    tools=['hover'],\n",
    "    title='T2MIN Raw (°C)',\n",
    "    alpha=0.7,\n",
    "    frame_width=200,\n",
    "    aspect='equal'\n",
    ")\n",
    "\n",
    "# Create HoloViews Image for RAINNCVMEAN (Raw)\n",
    "image_RAIN_raw = hv.Image(\n",
    "    da_rain, \n",
    "    kdims=[coord_x, coord_y],\n",
    "    vdims=['RAINNCVMEAN']\n",
    ").opts(\n",
    "    cmap=color_maps_raw['RAINNCVMEAN'],\n",
    "    colorbar=True,\n",
    "    tools=['hover'],\n",
    "    title='RAINNCVMEAN Raw (mm/day)',\n",
    "    alpha=0.7,\n",
    "    frame_width=200,\n",
    "    aspect='equal'\n",
    ")\n",
    "\n",
    "# Create a GridSpec with 3 rows and 3 columns\n",
    "grid = pn.GridSpec(ncols=3, nrows=2, sizing_mode='fixed', width=900, height=1200)\n",
    "\n",
    "# Add title spanning all columns in the first row\n",
    "title = pn.pane.Markdown(\n",
    "    f\"<h2 style='text-align: center;'>Areal Interpolation for {max_rain_time.strftime('%Y-%m-%d')}</h2>\",\n",
    "    width=900,\n",
    "    height=20\n",
    ")\n",
    "\n",
    "# Add raw data plots in the first row\n",
    "grid[0, 0] = image_T2MAX_raw\n",
    "grid[0, 1] = image_T2MIN_raw\n",
    "grid[0, 2] = image_RAIN_raw\n",
    "\n",
    "# Add processed data plots in the second row\n",
    "grid[1, 0] = map_T2MAX\n",
    "grid[1, 1] = map_T2MIN\n",
    "grid[1, 2] = map_RAINNCVMEAN\n",
    "\n",
    "\n",
    "# Display the grid\n",
    "# final_layout\n",
    "grid\n",
    "\n",
    "\n",
    "# Combine the three maps into a single layout arranged horizontally\n",
    "maped_layout = pn.Row(\n",
    "    map_T2MAX,\n",
    "    pn.Spacer(sizing_mode=\"stretch_width\"),\n",
    "    map_T2MIN,\n",
    "    pn.Spacer(sizing_mode=\"stretch_width\"),\n",
    "    map_RAINNCVMEAN,\n",
    "    width=800\n",
    ")\n",
    "\n",
    "# Combine raw data plots horizontally\n",
    "raw_layout = pn.Row(\n",
    "    image_T2MAX_raw,\n",
    "    pn.Spacer(sizing_mode=\"stretch_width\"),\n",
    "    image_T2MIN_raw,\n",
    "    pn.Spacer(sizing_mode=\"stretch_width\"),\n",
    "    image_RAIN_raw,\n",
    "    width=800\n",
    ")\n",
    "\n",
    "# Create a main title using HTML for center alignment within Markdown\n",
    "title = pn.pane.Markdown(\n",
    "    f\"<h3 style='text-align: center;'>Areal Interpolation for {max_rain_time.strftime('%Y-%m-%d')}</h3>\",\n",
    "    # width=1800  # Adjust width as needed\n",
    ")\n",
    "\n",
    "# Combine both rows vertically\n",
    "combined_layout = pn.Column(\n",
    "    raw_layout,\n",
    "    maped_layout\n",
    ")\n",
    "# Combine the title and the layout vertically\n",
    "final_layout = pn.Column(\n",
    "    title,\n",
    "    combined_layout,\n",
    "    width=850\n",
    ")\n",
    "\n",
    "final_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear increasing gradient in temperature from north to south that is visible in the interpolated results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel and Dask Methods\n",
    "\n",
    "The domain of this workflow is small enough that using either the parallel or dask methods are not necessary.  However there is a speedup that we illustrate.  The parallel and dask engines used in the AggGen object operate in a similar manner using `multiprocessing` and `dask bag` respectivly. Using the jobs parameter the user can specify the number of processes to run.  The target data is chunked by the number of processes and each processor recieves a chunked GeoDataFrame along with a copy of the sub-setted source data.  This creates an overhead that can determine how effiently the parallel processing runs.\n",
    "\n",
    "The tradeoff in using parallel processing lies in the balance between the number of processors and the overhead of copying data. While increasing the number of processors can significantly reduce computation time by dividing the workload, it also increases the amount of memory used for duplicate datasets and the coordination time between processes. There is a 'sweet spot' where the number of processors maximizes performance but beyond this point, additional processors may slow down the operation due to the overhead of managing more processes. The optimal number of processors depends on the size of the data, available memory, and system architecture, and can typically be found through experimentation.\n",
    "\n",
    "Importantly, most of the time in processing here is dominated by downloading the data, so the speedup is relatively small.  For larger domains the processing will be a larger percentage of the total time and the speedup should be more pronounced.  Well explore that in the CONUS scale processing of conus404 on Hovenweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gg_gen = AggGen(\n",
    "    user_data=user_data,\n",
    "    stat_method=\"masked_mean\",\n",
    "    agg_engine=\"parallel\",\n",
    "    agg_writer=\"netcdf\",\n",
    "    weights='wghts_drb_ser_c404daily.csv',\n",
    "    out_path='.',\n",
    "    file_prefix=\"testing_p2\",\n",
    "    precision=8,\n",
    "    jobs=4\n",
    ")\n",
    "ngdf, ds_out = agg_gen.calculate_agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "agg_gen = AggGen(\n",
    "    user_data=user_data,\n",
    "    stat_method=\"masked_mean\",\n",
    "    agg_engine=\"dask\",\n",
    "    agg_writer=\"netcdf\",\n",
    "    weights='wghts_drb_ser_c404daily.csv',\n",
    "    out_path='.',\n",
    "    file_prefix=\"testing_p3\",\n",
    "    precision=8,\n",
    "    jobs=4\n",
    ")\n",
    "ngdf, ds_out = agg_gen.calculate_agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_agg = xr.open_dataset(\"testing_p3.nc\")\n",
    "ds_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_agg.T2MAX.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
