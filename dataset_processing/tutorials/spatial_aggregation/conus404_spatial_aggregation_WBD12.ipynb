{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s\n",
    "\n",
    "This tutorial demonstrates the use of gdptools, a python package for area-weighted interpolation of *source* gridded datasets, such as conus404, to *target* polygonal geospatial fabrics.  Source datasets can be any gridded dataset that can be opened in XArray.  However it's important to note that gdptools, operations on XArray Datasets or DataArrays with dimensions of (Y,X,Time) generally.  As such climate datasets that have ensemble dimensions will require subsetting by ensemble to obtain the a dataset with the proper dimensions.  The target dataset can be any polygonal dataset that can be read by GeoPandas.  GDPtools also has capabilities of interpolating gridded data to lines as well, but our focus here is interpolating to polygons. \n",
    "\n",
    "In this workflow, CONUS404 is aggregated to [**NHDPlusV2 snapshot of the Watershed Boundary Dataset HUC12 boundaries**](https://www.sciencebase.gov/catalog/item/60cb5edfd34e86b938a373f4). This is a CONUS scale spatial fabric with ~102,000 polygons. Access to this dataset is provided through a copy of the data release stored on the OSN pod (`'huc12-geoparquet-osn'`).\n",
    "\n",
    "We use the HyTest intake catalog to access the `conus404-daily-diagnostic-onprem-hw` version of CONUS404 on Hovenweep so that we could also run the workflow there to be co-located with the data. However, a user could adapt this workflow to run in other computing environments if they use the version of CONUS404 on the OSN pod instead.\n",
    "\n",
    "Compared to the **gdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s** tutorial, the main difference is that to manage file size and memory overhead we process CONUS404 by year, generating 43 annual netcdf files of the interpolated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common python packages\n",
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "import hvplot.pandas\n",
    "import hvplot.dask\n",
    "import intake\n",
    "import warnings\n",
    "import intake_xarray\n",
    "import intake_parquet\n",
    "import intake_geopandas\n",
    "import datetime\n",
    "import holoviews as hv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# HyRiver packages\n",
    "from pynhd import NLDI, WaterData\n",
    "import pygeohydro as gh\n",
    "# GDPTools packages\n",
    "from gdptools import AggGen, UserCatData, WeightGen\n",
    "import os\n",
    "os.environ[\"HYRIVER_CACHE_DISABLE\"] = \"true\"\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we setup a variable the sets our local context, working on the HPC or working locally on your Desktop.  This just modifies the access point of the conus404 data, using the Hovenweep access for HPC and the OSN pod access for the Desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sys = \"HPC\"  # \"HPC\" or \"Desktop\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access data with HyTest intake catalog.  \n",
    "\n",
    "- Use the `huc12-geoparquet-osn` to read the NHDPlusV2 snapshot of the Watershed Boundary Dataset HUC12 boundaries\n",
    "- Use the `conus404-daily-diagnostic-onprem-hw` to read conus404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the hytest data intake catalog\n",
    "# hytest_cat = intake.open_catalog(\"../dataset_catalog/hytest_intake_catalog.yml\")\n",
    "hytest_cat = intake.open_catalog(\"https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml\")\n",
    "list(hytest_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a column to use as our identifyer.  Printing huc12.columns below to view all the possible columns, we choose the `HUC12` column as our identifyer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the huc12-geoparquet-osn\n",
    "huc12_access = hytest_cat['huc12-geoparquet-osn']\n",
    "huc12 = huc12_access.read()\n",
    "print(huc12.columns)\n",
    "huc12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the conus404 dataset using the HyTest catalog\n",
    "\n",
    "- In this case we are running this notebook on Hovenweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the hytest data intake catalog\n",
    "hytest_cat = intake.open_catalog(\"https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml\")\n",
    "list(hytest_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the conus404 sub-catalog\n",
    "cat = hytest_cat['conus404-catalog']\n",
    "list(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of options for accessing **conus404**:\n",
    "\n",
    "1. **HPC Setting (`t_sys = HPC`)**:\n",
    "    - **Assumption**: The notebook is run on the USGS HPC Hovenweep.\n",
    "    - **Access Method**: Utilizes the on-premises version of the data.\n",
    "    - **Benefits**:\n",
    "        - **Workflow Association**: The workflow is directly linked to the data.\n",
    "        - **Speed**: Eliminates the need to download data, significantly reducing access and processing time.\n",
    "\n",
    "2. **Desktop Setting (`t_sys = Desktop`)**:\n",
    "    - **Use Case**: Suitable for workflows that do not require HPC resources or for developing workflows locally before deploying them to the HPC.\n",
    "    - **Access Method**: Connects to the **conus404** data via the OSN pod.\n",
    "    - **Benefits**:\n",
    "        - **Flexibility**: Allows for local development and testing.\n",
    "        - **Performance**: Provides a fast connection to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select the dataset you want to read into your notebook and preview its metadata\n",
    "if t_sys == \"HPC\":\n",
    "    dataset = 'conus404-daily-diagnostic-onprem-hw'\n",
    "elif t_sys == \"Desktop\":\n",
    "    dataset = 'conus404-daily-diagnostic-osn' \n",
    "else:\n",
    "    print(\"Please set the variable t_sys above to one of 'HPC' or 'Desktop'\")        \n",
    "cat[dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dataset and use metpy to parse the crs information on the dataset\n",
    "print(f\"Reading {dataset} metadata...\", end='')\n",
    "ds = cat[dataset].to_dask().metpy.parse_cf()\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GDPTools Background\n",
    "\n",
    "In this section, we utilize three data classes from the `gdptools` package: `UserCatData`, `WeightGen`, and `AggGen`.\n",
    "\n",
    "* [**UserCatData**](https://gdptools.readthedocs.io/en/develop/user_input_data_classes.html):  \n",
    "  Serves as a data container for both the source and target datasets, along with their associated metadata. The instantiated object `user_data` is employed by both the `WeightGen` and `AggGen` classes.\n",
    "\n",
    "* [**WeightGen**](https://gdptools.readthedocs.io/en/develop/weight_gen_classes.html):  \n",
    "  Responsible for calculating the intersected areas between the source and target datasets. It generates normalized area-weights, which are subsequently used by the `AggGen` class to compute interpolated values between the datasets.\n",
    "\n",
    "* [**AggGen**](https://gdptools.readthedocs.io/en/develop/agg_gen_classes.html):  \n",
    "  Facilitates the interpolation of target data to match the source data using the areal weights calculated by `WeightGen`. This process is conducted over the time period specified in the `UserCatData` object.\n",
    "\n",
    "### Instantiation of the `UserCatData` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinate Reference System (CRS) of the conus404 dataset\n",
    "source_crs = ds.crs.crs_wkt\n",
    "\n",
    "# Coordinate names of the conus404 dataset\n",
    "x_coord = \"x\"\n",
    "y_coord = \"y\"\n",
    "t_coord = \"time\"\n",
    "\n",
    "# Time period of interest for areal interpolation of conus404 to DRB HUC12s\n",
    "# using the AggGen class below. Note: The dates follow the same format as the\n",
    "# time values in the conus404 dataset.\n",
    "sdate = \"1979-10-01T00:00:00.000000000\"\n",
    "edate = \"2022-10-01T00:00:00.000000000\"\n",
    "\n",
    "# Variables from the conus404 dataset used for areal interpolation\n",
    "variables = [\"T2MIN\", \"T2MAX\", \"RAINNCVMEAN\"]\n",
    "\n",
    "# CRS of the DRB HUC12 polygons\n",
    "target_crs = 5070\n",
    "\n",
    "# Column name for the unique identifier associated with target polygons.\n",
    "# This ID is used in both the generated weights file and the areal interpolated output.\n",
    "target_poly_idx = \"HUC12\"\n",
    "\n",
    "# Common equal-area CRS for reprojecting both source and target data.\n",
    "# This CRS is used for calculating areal weights in the WeightGen class.\n",
    "weight_gen_crs = 5070\n",
    "\n",
    "# Instantiate the UserCatData class, which serves as a container for both\n",
    "# source and target datasets, along with associated metadata. The UserCatData\n",
    "# object provides methods used by the WeightGen and AggGen classes to subset\n",
    "# and reproject the data.\n",
    "user_data = UserCatData(\n",
    "    ds=ds,  # conus404 read from the intake catalog\n",
    "    proj_ds=source_crs,\n",
    "    x_coord=x_coord,\n",
    "    y_coord=y_coord,\n",
    "    t_coord=t_coord,\n",
    "    var=variables,\n",
    "    f_feature=huc12,  # huc121 read above from the intake catalog\n",
    "    proj_feature=target_crs,\n",
    "    id_feature=target_poly_idx,\n",
    "    period=[sdate, edate],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Generation with `WeightGen`\n",
    "\n",
    "In this section, we utilize the `WeightGen` class from the `gdptools` package to calculate the normalized areal weights necessary for interpolating gridded data (`conus404`) to polygonal boundaries (`CONUS HUC12s`). The areal weights represent the proportion of each grid cell that overlaps with each polygon, facilitating accurate **areal interpolation** of the data. These weights are calculated using the `calculate_weights()` method.\n",
    "\n",
    "**Weight Calculation Process:**\n",
    "\n",
    "1. **Subset Source Data**: The source data is subset based on the bounds of the target data, with an additional small buffer to ensure coverage. The buffer size is determined based on the cell size of the source data. The buffer is 2 times the max(dx, dy). In other words, the buffer is essentially 2 source grid cells in the x and y dimensions.\n",
    "\n",
    "2. **Create cell boundary GeoDataFrame**: A GeoDataFrame of the cell boundaries is created for each node in the subsetted source data, enabling spatial operations.\n",
    "\n",
    "3. **Validate Geometries**: The target file is checked for invalid geometries, which can occur due to various reasons such as topology errors. Invalid geometries are fixed using Shapely's `make_valid()` method to prevent failures during intersection calculations.\n",
    "\n",
    "4. **Calculate and Normalize Areas**: For each polygon, `gdptools` calculates the area of each intersecting grid cell and normalizes it by the total area of the target polygon. This ensures that the weights for each polygon sum to 1, provided the polygon is entirely covered by the source data.\n",
    "   \n",
    "   - **Validation**: A quick check on the weights can be performed by grouping the resulting weights by the `target_poly_idx` and calculating the sum. For all polygons completely covered by the source data, the weights will sum to 1.\n",
    "\n",
    "**Note:** The `method` parameter in `calculate_weights()` can be set to one of `\"serial\"`, `\"parallel\"`, or `\"dask\"`. Given the scale of the gridded `conus404` data (4 km × 4 km) and the spatial footprint of the `CONUS HUC12s`, using `\"parallel\"`or `\"dask\"` in this case is the most efficient method.\n",
    "\n",
    "\n",
    "### Parallel and Dask Methods\n",
    "\n",
    "The domain in this workflow is large as defined by the number of polygons, the polygon complexity, and the relatively small scale of the conus404 cell geometries.  We can take advantage of the parallel methods to improve performance in both the weight calculation and the interpolation.  The parallel and dask engines used in the `WeightGen` class operate in a similar manner, utilizing Python's `multiprocessing` module and `dask.bag`, respectively.\n",
    "\n",
    "Using the `jobs` parameter, users can specify the number of processes to run. The target data is divided into chunks based on the number of processes, and each processor receives a chunked `GeoDataFrame` along with a copy of the subsetted source data. This setup introduces overhead that can affect how efficiently the parallel processing runs.\n",
    "\n",
    "**Trade-offs in Parallel Processing:**\n",
    "\n",
    "The use of parallel processing involves balancing the number of processors with the overhead of copying data:\n",
    "\n",
    "- **Benefits**: Increasing the number of processors can reduce computation time by dividing the workload.\n",
    "- **Costs**: More processors increase memory usage due to duplicate datasets and add coordination overhead between processes.\n",
    "- **Optimal Performance**: There is a 'sweet spot' where the number of processors maximizes performance. Beyond this point, additional processors may slow down the operation due to overhead.\n",
    "\n",
    "The optimal number of processors depends on factors such as data size, available memory, and system architecture. It often requires experimentation to determine the most efficient configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wght_gen = WeightGen(\n",
    "    user_data=user_data,\n",
    "    method=\"parallel\",\n",
    "    output_file=\"wghts_huc12_c404daily_p.csv\",\n",
    "    weight_gen_crs=weight_gen_crs,\n",
    "    jobs=4\n",
    ")\n",
    "\n",
    "wdf = wght_gen.calculate_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the areal weighted spatial interpolation\n",
    "\n",
    "Because the result will be rather large.  To manage the file size and memory requirements for processing we process by year.  Additionaly, The conus404 data starts and ends on the water year dates, so we chose to process by water year in this case.  The code below generates a list of start_dates, end_dates, and years that we iterate over to process the data by year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start_series = pd.date_range(pd.to_datetime(\"1979-10-01\"), periods=43, freq=\"YS-OCT\")\n",
    "t_end_series = pd.date_range(pd.to_datetime(\"1980-09-30\"), periods=43, freq=\"Y-SEP \")\n",
    "f_time_series = pd.date_range(pd.to_datetime(\"1980\"), periods=43, freq=\"Y\")\n",
    "\n",
    "time_start = [t.strftime(\"%Y-%m-%dT%H:%M:%S.%f\") for t in t_start_series]\n",
    "time_end = [t.strftime(\"%Y-%m-%dT%H:%M:%S.%f\") for t in t_end_series]\n",
    "file_time = [t.strftime(\"%Y\") for t in f_time_series]\n",
    "time_start[:4], time_end[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Areal Interpolation with the `AggGen` Class\n",
    "\n",
    "In this section, we demonstrate the use of the `AggGen` class and its `calculate_agg()` method from the `gdptools` package to perform areal interpolation. We will explore all three `agg_engine` options: `\"serial\"`, `\"parallel\"`, and `\"dask\"`. The following links provide detailed documentation on the available parameter options:\n",
    "\n",
    "* [**agg_engines**](https://gdptools.readthedocs.io/en/develop/agg_gen_classes.html#gdptools.agg_gen.AGGENGINES)\n",
    "* [**agg_writers**](https://gdptools.readthedocs.io/en/develop/agg_gen_classes.html#gdptools.agg_gen.AGGWRITERS)\n",
    "* [**stat_methods**](https://gdptools.readthedocs.io/en/develop/agg_gen_classes.html#gdptools.agg_gen.STATSMETHODS)\n",
    "\n",
    "When using `AggGen` and the `calculate_agg()` method, it is important to consider the overlap between the source and target data when selecting the `stat_method` parameter value. All statistical methods have a masked variant in addition to the standard method; for example, `\"mean\"` and `\"masked_mean\"`. In cases where the source data has partial overlap with a target polygon, the `\"mean\"` method will return a missing value for the polygon, whereas the `\"masked_mean\"` method will calculate the statistic based on the available overlapping source cells. These considerations help users determine whether using a masked statistic is desirable or if a missing value would be preferred, allowing for post-processing of missing values (e.g., using nearest-neighbor or other approaches to handle the lack of overlap). In the case here conus404 completely covers the footprint of the DRB HUC12s, as such the `\"mean\"` method would be sufficient. \n",
    "\n",
    "Because we are processing by year, we have to create a new UserCatData object for each year processed.  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, _ts in enumerate(time_start):\n",
    "    sdate = time_start[index]\n",
    "    edate = time_end[index]\n",
    "    print(sdate, edate)\n",
    "    user_data = UserCatData(\n",
    "        ds=ds,  # conus404 read from the intake catalog\n",
    "        proj_ds=source_crs,\n",
    "        x_coord=x_coord,\n",
    "        y_coord=y_coord,\n",
    "        t_coord=t_coord,\n",
    "        var=variables,\n",
    "        f_feature=huc12,  # GFv1.1 read above from the intake catalog\n",
    "        proj_feature=target_crs,\n",
    "        id_feature=target_poly_idx,\n",
    "        period=[sdate, edate],\n",
    "    )\n",
    "    \n",
    "    agg_gen = AggGen(\n",
    "        user_data=user_data,\n",
    "        stat_method=\"mean\",\n",
    "        agg_engine=\"parallel\",\n",
    "        agg_writer=\"netcdf\",\n",
    "        weights='wghts_huc12_c404daily_p.csv',\n",
    "        out_path='.',\n",
    "        file_prefix=f\"{file_time[index]}_huc12_c404_daily_diagnostic\",\n",
    "        jobs=4\n",
    "    )\n",
    "    ngdf, ds_out = agg_gen.calculate_agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
