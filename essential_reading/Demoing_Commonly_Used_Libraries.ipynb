{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84ffdcab-f25e-405a-be12-2d6a90be3983",
   "metadata": {},
   "source": [
    "# Demoing code for `intake`, `fsspec`, `dask`, and `xarray` libraries\n",
    "\n",
    "These libraries are used in most of the HyTEST notebooks and understanding how they work together will help understand the code more.\n",
    "\n",
    "Author: Andrew Laws - USGS Web Informatics and Mapping (WIM) <br>\n",
    "\n",
    "Date: 3/20/2023\n",
    "\n",
    "Focus: `intake`, `fsspec`, `dask`, and `xarray` libraries use in climate and forcings big data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c792918-142f-4888-90cc-33c28ab048c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import fsspec\n",
    "import hvplot.xarray\n",
    "import intake\n",
    "import os\n",
    "import warnings\n",
    "import rioxarray\n",
    "import dask\n",
    "\n",
    "from dask.distributed import LocalCluster, Client\n",
    "from pygeohydro import pygeohydro\n",
    "\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import geoviews as gv\n",
    "import dask.dataframe as dd\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf594761-fc28-4162-a064-a448e6ede09a",
   "metadata": {},
   "source": [
    "#### Dask is a super useful Python package for parallel and lazy computing. \n",
    "\n",
    "When working with a large dataset that is multiple terrabytes in size such as CONUS404, Dask allows you to perform lazy operations, meaning the whole dataset is not read into memory (RAM) all at once but only once you're ready for it. A prime example is work within HyTEST where we lazily ready in CONUS404, subset the data to certain variables and a limited geographic extend and then load it into memory. For a more in-depth dive and before you start to use it in-depth, make sure you read through the [Dask documentation](https://docs.dask.org/en/stable/).\n",
    "\n",
    "**Another huge benefit of this**: you can avoid the annoying step of downloading data to a local folder, instead just using a  web service, saving time. This means if the base data is updated, you don't have to download the dataset again and it is automatically available for your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780c348-8030-426a-bf18-5d36f43f8a91",
   "metadata": {},
   "source": [
    "#### Starting up dask on HPC or Nebari using a helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605a50b9-df06-44a4-b948-c421b34562d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_cluster(machine):\n",
    "    ''' Helper function to configure cluster\n",
    "    '''\n",
    "    if machine == 'denali':\n",
    "        from dask.distributed import LocalCluster, Client\n",
    "        cluster = LocalCluster(threads_per_worker=1)\n",
    "        client = Client(cluster)\n",
    "    \n",
    "    elif machine == 'tallgrass':\n",
    "        from dask.distributed import Client\n",
    "        from dask_jobqueue import SLURMCluster\n",
    "        cluster = SLURMCluster(queue='cpu', cores=1, interface='ib0',\n",
    "                               job_extra=['--nodes=1', '--ntasks-per-node=1', '--cpus-per-task=1'],\n",
    "                               memory='6GB')\n",
    "        cluster.adapt(maximum_jobs=30)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "    elif machine == 'local':\n",
    "        import os\n",
    "        import warnings\n",
    "        from dask.distributed import LocalCluster, Client\n",
    "        warnings.warn(\"Running locally can result in costly data transfers!\\n\")\n",
    "        n_cores = os.cpu_count() # set to match your machine\n",
    "        cluster = LocalCluster(threads_per_worker=n_cores)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "    elif machine in ['esip-qhub-gateway-v0.4']:   \n",
    "        import sys, os\n",
    "        sys.path.append(os.path.join(os.environ['HOME'],'shared','users','lib'))\n",
    "        import ebdpy as ebd\n",
    "        aws_profile = 'esip-qhub'\n",
    "        ebd.set_credentials(profile=aws_profile)\n",
    "\n",
    "        aws_region = 'us-west-2'\n",
    "        endpoint = f's3.{aws_region}.amazonaws.com'\n",
    "        ebd.set_credentials(profile=aws_profile, region=aws_region, endpoint=endpoint)\n",
    "        worker_max = 30\n",
    "        client,cluster = ebd.start_dask_cluster(profile=aws_profile, worker_max=worker_max, \n",
    "                                              region=aws_region, use_existing_cluster=True,\n",
    "                                              adaptive_scaling=False, wait_for_cluster=False, \n",
    "                                              worker_profile='Medium Worker', propagate_env=True)\n",
    "        \n",
    "    return client, cluster\n",
    "\n",
    "# if-else to determine HPC or Nebari\n",
    "if 'SLURM_CLUSTER_NAME' in os.environ: #USGS HPC use SLURM CLUSTER to handle jobs, otherwise...\n",
    "    machine = os.environ['SLURM_CLUSTER_NAME']\n",
    "    cluster = configure_cluster(machine)\n",
    "else:  # use the Nebari machine\n",
    "    machine = 'esip-qhub-gateway-v0.4'\n",
    "    client, cluster = configure_cluster(machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a16472b-b47d-4460-83a9-c1cd5ff030fe",
   "metadata": {},
   "source": [
    "You can also use Dask on your personal computer. This [bit of Dask documentation](https://docs.dask.org/en/stable/deploying-python.html) explains how easy this is to set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2630d44-f77d-441f-a4f0-bbb5772cd943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster = LocalCluster()\n",
    "# client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f748725-8550-4715-bd9a-55ab5fb707e3",
   "metadata": {},
   "source": [
    "#### With HyTEST, we use `intake` catalogs in our code to making data I/O more uniform. \n",
    "\n",
    "If we change where a dataset gets imported from, we only have to change it in one place rather than in each notebook. They can also be nested as you'll see next.\n",
    "\n",
    "[Intake readthedocs site](https://intake.readthedocs.io/en/latest/overview.html)\n",
    "\n",
    "[Example of HyTEST catalogs](https://github.com/hytest-org/hytest/tree/main/dataset_catalog)\n",
    "\n",
    "Note: Select datasets that end in \"onprem\" if running on Denali/Tallgrass HPC or cloud data if working on QHub or local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7635c-3436-4472-b7db-f0f3ea0fc943",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml'\n",
    "cat = intake.open_catalog(url)\n",
    "list(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe957754-3343-41f4-9367-656669edd33a",
   "metadata": {},
   "source": [
    "Notice the 'conus404-drb-eval-tutorial-catalog'? That is a nested catalog for data we use in a specific tutorial series and can be accessed like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0fca58-56c0-4889-b65d-d75d5155564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_cat = cat['conus404-drb-eval-tutorial-catalog']\n",
    "list(nested_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d9d411-3886-4a0c-b3b2-813f9dbf71ae",
   "metadata": {},
   "source": [
    "And if you want to see the details about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72458b79-5da9-4be3-a1e3-3a9cff96d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_cat['conus404-drb-OSN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be39388-5177-4258-82fb-fa450853abb9",
   "metadata": {},
   "source": [
    "The other handy bit about `intake` is that you can open a dataset as a Dask data object easily (though the base class will be an `xarray` dataset). This is done lazily. But why the `xarray` dataset? From [this read about the Dask ecosystem:](https://docs.dask.org/en/stable/ecosystem.html?highlight=xarray) \"xarray: Wraps Dask Array, offering the same scalability, but with axis labels which add convenience when dealing with complex datasets.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd7f112-8945-4d36-9136-85d05995ae72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c404_drb = nested_cat['conus404-drb-OSN'].to_dask()\n",
    "c404_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648f4998-3e71-4137-8775-c91fafea05c3",
   "metadata": {},
   "source": [
    "#### What is `xarray`? \n",
    "From [the Xarray website](https://docs.xarray.dev/en/stable/getting-started-guide/why-xarray.html): \"Xarray introduces labels in the form of dimensions, coordinates and attributes on top of raw NumPy-like multidimensional arrays, which allows for a more intuitive, more concise, and less error-prone developer experience.\" \n",
    "\n",
    "If you've used `numpy` and `pandas` before, the API will be relatively familiar. As mentioned above, since it wraps a Dask array, it allows computation on large datasets in parallel. It can also take in many cloud and local file types. See [this Xarray documentation about that](https://docs.xarray.dev/en/stable/user-guide/io.html). As with Dask, an upfront time investment in understanding Xarray will pay long-term efficiency rewards.\n",
    "\n",
    "But what if you don't want to use `intake` catalogs for a bunch of datasets you don't maintain?\n",
    "\n",
    "#### `fsspec` allows a user to connect with many different filesystems for querying, reading, and writing data.\n",
    "\n",
    "Want data from an AWS S3 bucket? `fsspec` has you covered, whether it requires credentials (requester pays and no anonymous read) or not (anonymous reads). Local files? Yep. Open data portals using something such as [THREDDS Data Server](https://www.unidata.ucar.edu/software/tds/)? Still good. For a more in-depth look at `fsspec`, read through the [fsspec documentation](https://filesystem-spec.readthedocs.io/en/latest/).\n",
    "\n",
    "An additional benefit is that many libraries (`dask`, `xarray`, and `pandas`) already use `fsspec` in the background for reading and writing data. Next, you'll see a couple examples of calling in data using `fsspec` directly and indirectly (through `xarray`), as well as incorporating `dask`.\n",
    "\n",
    "First up, some data is only available through older file transfer protocal (FTP) methods. Using `fsspec`'s [FTPFileSystem object](https://filesystem-spec.readthedocs.io/en/latest/api.html?highlight=ftpfilesystem#fsspec.implementations.ftp.FTPFileSystem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1fea84-c79a-4648-b634-8455dc2caa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fsspec.implementations.ftp import FTPFileSystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b69e9ce-610b-4e22-b18e-a2025db07af3",
   "metadata": {},
   "source": [
    "One thing to note: FTP server calls typically have a timeout, meaning if you take too long after instantiating the connecting it will disconnect. All you have to do is reconnect by running the code again (next cell). In the next few cells, we are reading in Climate Reference Network station data over an FTP connection.\n",
    "\n",
    "First, create a FTP file system connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b571dabb-23cd-4f1f-a9f6-3edacb56b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FTPFileSystem(\"ftp.ncei.noaa.gov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416eac15-dabd-4c26-9cc0-40182e2a5b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs.ls(\"/pub/data/uscrn/products/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed2d74-f5b4-45d4-83de-a2444d45982a",
   "metadata": {},
   "source": [
    "Since the file type is *tab-separated values (tsv)*, we will use the *pd.read_table* function to create a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505feebc-eae5-4122-8614-30e29b9ba937",
   "metadata": {},
   "outputs": [],
   "source": [
    "uscrn_all = pd.read_table(fs.open(\"/pub/data/uscrn/products/stations.tsv\")) \n",
    "uscrn_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc69286-7def-430c-800b-a9d687403017",
   "metadata": {},
   "source": [
    "What about reading from a permissioned S3 bucket? Lets find the URL for a dataset from the `intake` catalog. Note: this requires you to have AWS credentials.\n",
    "\n",
    "**If you are unsure of what permissioned/requester pays bucket is, read this [resource](https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d4caf3-c23b-45c7-80a2-d734c7c5f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat['conus404-hourly-cloud']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80658298-06b3-4967-939c-ab1ffb9d3dfc",
   "metadata": {},
   "source": [
    "Using the urlpath in `xarray`, we can read in the dataset using saved AWS credentials and the `fsspec.get_mapper()` method to pass the correct parameters to `xarray.open_zarr()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1188d10e-c6bb-4b5f-93ac-d863dfe76645",
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_hourly_url = \"s3://nhgf-development/conus404/conus404_hourly_202209.zarr\"\n",
    "\n",
    "fs_gm = fsspec.get_mapper(c404_hourly_url, # given the url, fsspec will know it is speaking to an S3 file system\n",
    "                          anon=False,\n",
    "                          requester_pays=True, # our S3 credentials will be charged\n",
    "                          client_kwargs={'region_name':'us-west-2'})\n",
    "\n",
    "# open dataset\n",
    "c404_hourly = xr.open_zarr(fs_gm)\n",
    "c404_hourly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665cdf2b-ef10-400a-aa22-3e8e40c291ba",
   "metadata": {},
   "source": [
    "Or you can open the dataset lazily with Dask by setting the parameter chunks={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e9661-0bab-46e3-abf2-ba53491b1a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset lazily \n",
    "# this happens by setting chunks={}\n",
    "# though you can specify chunks where applicable\n",
    "c404_hourly_dask = xr.open_zarr(fs_gm, chunks={})\n",
    "c404_hourly_dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b05db9d-a615-4d54-8718-2ab6edb3dda7",
   "metadata": {},
   "source": [
    "The datasets look the same but the second takes up less space in memory. Now to show an example of lazy computation using radiation variables to calculate a made up variable, FAKERAD. \n",
    "Here are the equations:\n",
    "\\begin{equation}FAKERAD = DNB - UPB\\end{equation}\n",
    "\\begin{equation}DNB = ACSWDNB - ACLWDNB\\end{equation} \n",
    "\\begin{equation}UPB = ACSWUPB - ACLWUPB\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bfc276-a93b-41b8-84c8-8c8339bc781c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c404_hourly_dask_sub = c404_hourly_dask[[\"ACSWDNB\", \"ACSWUPB\", \"ACLWDNB\", \"ACLWUPB\"]]\n",
    "c404_hourly_dask_sub = c404_hourly_dask_sub.sel(time=slice('1979-10-01T00:00:00.000000000', '1979-10-01T00:00:00.000000000')) #subset to single time step\n",
    "c404_hourly_dask_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a851f2-e0bb-41d3-8bd1-ff88fd5205b1",
   "metadata": {},
   "source": [
    "First, calculate DNB and UPB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c5f56c-c612-4025-a909-0e172e086f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_hourly_dask_sub[\"DNB\"] = c404_hourly_dask_sub['ACSWDNB'] - c404_hourly_dask_sub['ACLWDNB']\n",
    "c404_hourly_dask_sub['UPB'] = c404_hourly_dask_sub['ACSWUPB'] - c404_hourly_dask_sub['ACLWUPB']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bace30a3-ea69-4ffc-8c9b-cd4771c6614a",
   "metadata": {},
   "source": [
    "Then calculate FAKERAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339feabd-6ecb-49b2-8fe1-9bef82ff2826",
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_hourly_dask_sub['FAKERAD'] = c404_hourly_dask_sub['DNB'] - c404_hourly_dask_sub['UPB']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29f0d4c-754a-43ed-a456-5a266efe57b5",
   "metadata": {},
   "source": [
    "This has all happened so fast! But, has anything actually happened? Dask has been keeping track of the operations and won't run them all until the compute() method is passed. The execution will take longer time than the last couple of cells but will be much faster than runner all of these operations on an in-memory dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9fd54d-4e95-469a-a472-d5df8d3dde49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c404_hourly_memory_sub = c404_hourly_dask_sub.compute()\n",
    "c404_hourly_memory_sub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b00e6abc-5cd1-48ab-8eb0-994c31920167",
   "metadata": {},
   "source": [
    "If you are working with large tabular data, Dask Dataframes performs the same way and is modeled after `pandas` dataframes. The `.compute()` graphs can be visualized for these and more information can be found [here.](https://docs.dask.org/en/stable/graphviz.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6628abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_hourly_dask_sub['ACSWDNB'].data.visualize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3a5c6ee",
   "metadata": {},
   "source": [
    "Finally, here are some examples of calling in some datasets from various open access sources.\n",
    "\n",
    "##### OpenDAP data\n",
    "\n",
    "Now, looking at CERES-EBAF, called in from an OpenDAP source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e09486-61b0-423b-9655-1425339cd275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ceres-ebaf url\n",
    "ceres_url = \"https://opendap.larc.nasa.gov/opendap/CERES/EBAF/Edition4.1/CERES_EBAF_Edition4.1_200003-202111.nc\"\n",
    "\n",
    "# bring in ceres-ebaf\n",
    "ceres = xr.open_dataset(ceres_url, decode_coords=\"all\")\n",
    "ceres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02b2cb-469e-449a-820b-3c6573887548",
   "metadata": {},
   "source": [
    "##### Data on a THREDD server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931869a-fc34-473d-a501-a702f7a7d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open daymet lazily \n",
    "# this happens by setting chunks={}\n",
    "# though you can specify chunks where applicable\n",
    "daymet = xr.open_dataset(\"https://thredds.daac.ornl.gov/thredds/dodsC/daymet-v4-agg/na.ncml\", chunks={})\n",
    "daymet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d88026-37e2-4954-92c4-e9e5da11b651",
   "metadata": {},
   "source": [
    "#### Clipping xarray datasets to a bounding box\n",
    "Here are two possible ways to clip to a bounding box and when each might suit you best. Why a bounding box and not an exact clip? If storing an intermediate dataset, this makes the data smaller as well as giving flexibility to exact area extraction or only including cells inside of the area of interest.\n",
    "\n",
    "The first is using the `xarray` index methods. This is beneficial because it doesn't require using extra libraries but there have been isues when the spatial resolution of the dataset is very large and the geographical area is small. In some cases, it will drop some grid cells compared to the second method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e291ad-aab1-4d2d-a9d9-6571ca71e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygeohydro import WBD\n",
    "from cartopy import crs as ccrs\n",
    "# bring in boundaries of DRB and create single polygon\n",
    "drb = WBD(\"huc6\", outfields=[\"huc6\", \"name\"]).byids(\"huc6\", [\"020401\", \"020402\"])\n",
    "\n",
    "# create a column where all entries have the same value\n",
    "drb[\"name\"] = \"DRB\"\n",
    "\n",
    "# dissolve by that column\n",
    "drb = drb.dissolve(by=\"name\")\n",
    "\n",
    "# set CRS to match ds\n",
    "drb = drb.iloc[[0]].to_crs(ccrs.LambertConformal())\n",
    "\n",
    "# tuple of bounding box\n",
    "drb_bbox = list(drb.total_bounds)\n",
    "\n",
    "drb_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98580e76-2f82-4942-a0e0-ab678d801675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "daymet_sel = daymet.sel(x=slice(drb_bbox[1],drb_bbox[3]), y=slice(drb_bbox[2],drb_bbox[0]))\n",
    "daymet_sel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4fca87-7e14-4190-b609-5479be126875",
   "metadata": {},
   "source": [
    "In the above, the indexing returned a subset along the x dimension but nothing along the y due to underlying CRS/indexing conflicts.\n",
    "\n",
    "The second is the `rioxarray.clip_box` method. `rioxarray` is used to apply `rasterio` to `xarray` and adds a `rio` accessor to `xarray` objects and [documentation can be found here about rioxarray.](https://corteva.github.io/rioxarray/stable/). This method does require an additional library but does ensure that all grid cells that are inside the bounding box are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8fd33-391c-4dcb-9b56-14666bd4c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "\n",
    "# add crs to dataset\n",
    "daymet.rio.write_crs(ccrs.LambertConformal(), inplace=True)\n",
    "\n",
    "# drop time_bnds, otherwise it will throw an error in the clip_box\n",
    "daymet = daymet.drop([\"time_bnds\"])\n",
    "\n",
    "#clip to bbox\n",
    "daymet_bbox = daymet.rio.clip_box(minx=drb_bbox[0],\n",
    "                            miny=drb_bbox[1],\n",
    "                            maxx=drb_bbox[2],\n",
    "                            maxy=drb_bbox[3],\n",
    "                            crs=ccrs.LambertConformal())\n",
    "\n",
    "daymet_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc5842-db94-4cab-94fa-948fe82b725a",
   "metadata": {},
   "source": [
    "As can be seen, the indexing works great but does have one less grid cell (see y dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58175e6a-7756-4adb-8233-4a73307b0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Index select:\\n\", daymet_sel.dims)\n",
    "print(\"Rioxarray:\\n\", daymet_bbox.dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95f5acf-3e95-4a18-9b9c-818886a8a4d8",
   "metadata": {},
   "source": [
    "Finally, it is always important to shutdown the client and cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff14ba7-ad3c-45a1-97b6-a694054830e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close(); cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-users-pangeo",
   "language": "python",
   "name": "conda-env-users-users-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
