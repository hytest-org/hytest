{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism with Dask\n",
    "\n",
    "The dask folks have a very good introduction to dask data structures and parallelism in their\n",
    "[dask tutorial](https://tutorial.dask.org/).  In that tutorial, you'll get exposure to \n",
    "the general dask architecture, as well as specific hands-on examples with two of the three\n",
    "key dask data structures:  dask arrays, and dask dataframes. \n",
    "\n",
    "The above tutorial does not cover dask **`bag`** in any detail -- this is a key data structure\n",
    "that we use to distribute parallel tasks in clustered environments.  Here's a quick demo of \n",
    "a dask bag and how we use it. A bag is analagous to a standard Python `list`... let's start there \n",
    "with some nomenclature: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lists and Maps\n",
    "\n",
    "A common pattern in Python is a 'list comprehension' -- a way of transforming a list of values into a new list of values using a transformation pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "newList = [x**2 for x in myList]\n",
    "\n",
    "newList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature of python **maps** an action onto each element in a list. The result is a new list\n",
    "holding the result of each action -- one action per element in the list.  \n",
    "\n",
    "The syntax of the above list comprehension is purely for us\n",
    "humans. The python implementation behind the scenes uses a special built-in python function \n",
    "called `map`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFunc(i):\n",
    "    return i**2\n",
    "\n",
    "list(map(myFunc, myList))\n",
    "# maps each element of myList to a separate invokation of myFunc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `map()` call handles the work of calling `myFunc()` once each for the elements of `myList`.  \n",
    "You can see that doing it this way involves a lot of extra hoops and parenthesis to jump through. \n",
    "Which is why it is almost never written that way.  \n",
    "\n",
    "The list element is given to the function as a positional argument.  In essence, the above `map` is the same as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "for x in myList:\n",
    "    result.append(myFunc(x))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Bag\n",
    "\n",
    "A dask bag datastructure is much like a list, but it has the **`map`** function built into it as \n",
    "an object method. The invocation is slightly different, but the concept is the same: it \n",
    "pairs up elements of a list with a function call to execute against those elements.  A dask\n",
    "bag has the ability to spawn the `myFunc()` calls in parallel, distributing those calls to \n",
    "workers around the dask cluster. \n",
    "\n",
    "Let's look at an example... first, let's make `myFunc()` simulate time-consuming work by having \n",
    "it pause for a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "def myFunc(i):\n",
    "    sleep(1) ## Simulates dense computation taking one second\n",
    "    return i**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is one that we want to distribute across a dask compute cluster.\n",
    "\n",
    "For this small demonstration, we'll build a cluster on the local host; no need to \n",
    "make a distributed cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask.bag as db\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster(threads_per_worker=os.cpu_count())\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the bag with our list of items\n",
    "myBag = db.from_sequence(myList)\n",
    "# then 'map' the elements to the worker function we want to execute on each element in the bag\n",
    "result = myBag.map(myFunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this returned immediately (i.e very small execution time). That's because the \n",
    "computation has not been done yet. Dask is a 'lazy' system in which computations are\n",
    "delayed as long as possible.  \n",
    "\n",
    "As with other dask operations, `result` at this point just contains the 'task graph' -- what \n",
    "dask is going to do, and in what order.  We can see that task graph with the `visualize` \n",
    "method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each of those elements is independent from the others, so they can in theory execute in parallel. \n",
    "'Thread' zero will call myFunc with some arguments to produce result zero.  Same with threads 1 throuth 9. \n",
    "Because these threads do not depend on one another, they can operate in parallel, on separate workers. \n",
    "\n",
    "In the original, pure-python operation, these calls to `myFunc` would be serialized: one at a time. \n",
    "\n",
    "Let's compare run times:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time  \n",
    "# should take about 10 seconds: one second per call to myFunc\n",
    "## Pure python mapping:\n",
    "list( map( myFunc, myList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## using the dask bag task-graph\n",
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the pure python execution took right around 10 seconds.  Each call to `myFunc`, remember, is artificially set to take one second each.  Calling them serially should take 10 seconds (plus whatever internal overhead python needs). \n",
    "\n",
    "The dask bag approach took substantially less time. If it were **perfectly** parallel, the result would have been computed in one second (ten simultaneous executions of `myFunc`, each taking one second).  But that parallelism depends on how many cpus/cores you have.  If you only have one core, then parallelism isn't going to help  you -- the dispatched workers still need to take turns on the CPU.  \n",
    "\n",
    "If you had 10 cores (the number of elements in the bag), then you might get close to the perfect parallelism.  Dask does involve more overhead than pure python in order to achieve its parallelism; the administrative overhead for operating the cluster scheduler will prevent this from being perfectly parallel. \n",
    "\n",
    "The above example uses a **local** cluster, meaning that the work is scheduled among the CPUs on the local hardware.  Dask can also utilize **distributed** clusters, meaning that workers can be organized among several computers connected via network. This allows for many more CPUs to attach to a problem, but the overhead of network communication will impact the administrative costs of coordinating the workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shut down cluster scheduler\n",
    "client.close(); del client\n",
    "cluster.close(); del cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Dask Bags in HyTEST Workflows\n",
    "This pattern is used extensively in the benchmarking workflows.  A series of statistics are calculated for each streamgage in a list of 5000+ gages. With a dask bag (the contents of which is the list of gages), the stats package can be dispatched to workers operating independently and in parallel. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
