{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb6eadef-547b-4c85-af9c-c2a2aff58fa7",
   "metadata": {},
   "source": [
    "# Streamflow Drought Statistical Suite\n",
    "## v1, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadfe78f-2449-43c7-bb45-e2c83de375e1",
   "metadata": {},
   "source": [
    "The suite of statistical metrics within this notebook align with the metrics found in [Simeone et al., 2024; Table 1](https://doi.org/10.3390/w16202996) and are geared toward the evaluation of streamflow drought in particular. Within this notebook are custom-defined Python functions to compare modeled/simulated steramflow data against observational streamflow data over time. The code contained within the notebook is adapted from the original functions found in the R package _HyMED_ [(Simeone and Foks, 2024)](https://doi.org/10.5066/P1TLARC5).\n",
    "\n",
    "This notebook will briefly describe each metric and contain the code to calculate each metric.  This notebook can be sourced into analysis notebooks to retrieve access to these functions without having to copy and paste the metric of interest. \n",
    "\n",
    "##### Streamflow drought statistical metrics for daily streamflow drought evaluation for different metric categories. Additional calculation details can be found in Text S1 [Simeone et al., 2024; Table 1](https://doi.org/10.3390/w16202996).\n",
    "\n",
    "| Category\t| Statistic\t| Description\t| Units | Range (Perfect)\t| Comments |\n",
    "| :-----    | :-----    |:-----         |:-----   | :-----    | :----\n",
    "| Event Classification\t| Cohen’s kappa |\tCohen’s kappa statistic for inter-rater reliability,  measure of agreement between categorical variables | unitless |\t−1 to 1 (1)\t| A measure of agreement relative to the probability of achieving results by chance. |\n",
    "|Error Components | Spearman’s r | Spearman’s rank correlation coefficient | unitless | −1 to 1 (1) | A nonparametric estimator of correlation for flow timing. |\n",
    "|Error Components | Ratio of standard deviations | Ratio of simulated to observed standard deviations (for the scorecard this is presented as the absolute deviation from the target of 1)\t| unitless | 0 to Inf (1)\t| Indicates if the flow variability is being over or underestimated. |\n",
    "|Error Components | Percent bias | Percent bias (simulated minus observed) (for the scorecard this is presented as the absolute percent bias) | percent | −100 to Inf (0) |   Indicates if total streamflow volume is being over or underestimated. |\n",
    "| Drought Signatures | Drought Duration | Normalized mean absolute error (NMAE) in the annual time series of drought duration (units in time or days), this is the sum of days of drought each year for a given threshold. |\ttime (days) | 0 to Inf (0) |\tIndicates how well the model simulates annual drought durations. |\n",
    "| Drought Signatures | Drought Intensity | NMAE in annual time series of the distance the minimum percentile is below the drought threshold, this is the overall maximum distance below the threshold for any drought during the year (minimum percentile). | percentile | 0 to Inf (0) | Indicates how well the model simulates annual minimum flow.\n",
    "| Drought Signatures | Drought Severity (Flow Deficit Volume) | NMAE in the annual time series of drought deficit volume, this is the sum of drought deficits for all droughts during the year. | flow units (cms or cfs) - days |\t0 to Inf (0) | Indicates how well the model simulates annual flow deficit. This is a measure of drought severity. |\n",
    "\n",
    "### References:\n",
    "- Simeone, C.E., and Foks, S.S. (2024). HyMED—Hydrologic Model Evaluation for Drought: R package version 1.0.0, U.S. Geological Survey software release, https://doi.org/10.5066/P1TLARC5.\n",
    "- Simeone, C., Foks, S., Towler, E., Hodson, T., & Over, T. (2024). Evaluating Hydrologic Model Performance for Characterizing Streamflow Drought in the Conterminous United States. Water, 16(20), 2996. https://doi.org/10.3390/w16202996."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b1f461-d0da-47c1-86b6-8dfac722036b",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0fd747-6047-4f31-adee-6cc7fe07e870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c039b8cf-2da5-4f92-a0d6-cd960e834c46",
   "metadata": {},
   "source": [
    "## Percentile and Threshold Calculations\n",
    "This function is adapted from _calculate_percentiles_single.R_ in HyMED [(Simeone and Foks, 2024)](https://doi.org/10.5066/P1TLARC5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd45e06f-7af4-4de3-abe0-c79aede2d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentiles(df, units) -> float:\n",
    "    \"\"\"\n",
    "    Pre-processes streamflow data where there is an observed and modeled column. \n",
    "    Calculates percentiles for streamflow using Weibull plotting position for a fixed and variable threshold\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "            |\n",
    "    \"\"\"\n",
    "    # calculate day-of-year or julian day (important for variable threshold)\n",
    "    # add it as a column to dataframe\n",
    "    day_of_year = df.index.dayofyear\n",
    "    df['jd'] = day_of_year\n",
    "\n",
    "    # calculate climate year (important for NMAE drought signature evaluation)\n",
    "    # add it as a column to dataframe\n",
    "    df['cy'] = df.index.year.where(df.index.month < 4, df.index.year + 1)\n",
    "\n",
    "    ## Handling of zeros\n",
    "    # Zero flow cutoff (0.001 for cubic feet per second / 0.00028 for cubic meters per second)\n",
    "    if units == 'cms':\n",
    "        zero_cutoff = 0.00028\n",
    "    elif units == 'cfs':\n",
    "        zero_cutoff = 0.01\n",
    "\n",
    "    # Create zero boolean columns (1 if present, 0 if not)\n",
    "    df['zero_bool_obs'] = (df['observed'] < zero_cutoff).astype(int)\n",
    "    df['zero_bool_mod'] = (df['modeled'] < zero_cutoff).astype(int)\n",
    "\n",
    "    # Separate all zero vs not zero periods\n",
    "    df['zero_id_obs'] = (df['zero_bool_obs'] != df['zero_bool_obs'].shift()).cumsum()\n",
    "    df['zero_id_mod'] = (df['zero_bool_mod'] != df['zero_bool_mod'].shift()).cumsum()\n",
    "    \n",
    "    # Calculate zero stats for observations\n",
    "    df_zero_obs = (df[df['zero_bool_obs'] == 1]\n",
    "                   .groupby('zero_id_obs', as_index=False)\n",
    "                   .apply(lambda x: x.assign(previous_zeros_obs=np.arange(1, len(x) + 1)), include_groups=False)\n",
    "                   .reset_index(drop=True))\n",
    "    df_zero_obs['time'] = df.index[df_zero_obs.index]\n",
    "\n",
    "    # Calculate zero stats for modeled data\n",
    "    df_zero_mod = (df[df['zero_bool_mod'] == 1]\n",
    "                   .groupby('zero_id_mod', as_index=False)\n",
    "                   .apply(lambda x: x.assign(previous_zeros_mod=np.arange(1, len(x) + 1)), include_groups=False)\n",
    "                   .reset_index(drop=True))\n",
    "    df_zero_mod['time'] = df.index[df_zero_mod.index]\n",
    "\n",
    "    # Check if df_zero_obs and df_zero_mod have the expected columns\n",
    "    has_previous_zeros_obs = 'previous_zeros_obs' in df_zero_obs.columns\n",
    "    has_previous_zeros_mod = 'previous_zeros_mod' in df_zero_mod.columns\n",
    "\n",
    "    # Prepare the DataFrames for merging\n",
    "    df_zero_obs_selected = df_zero_obs[['time', 'previous_zeros_obs']] if has_previous_zeros_obs else df_zero_obs[['time']]\n",
    "    df_zero_mod_selected = df_zero_mod[['time', 'previous_zeros_mod']] if has_previous_zeros_mod else df_zero_mod[['time']]\n",
    "\n",
    "    # Attempt to join the two zero datasets together\n",
    "    df_zero = pd.merge(df_zero_obs_selected, df_zero_mod_selected, on='time', how='outer')\n",
    "\n",
    "    # If previous_zeros_obs or previous_zeros_mod is missing, fill with NaN\n",
    "    if not has_previous_zeros_obs:\n",
    "        df_zero['previous_zeros_obs'] = np.nan\n",
    "\n",
    "    if not has_previous_zeros_mod:\n",
    "        df_zero['previous_zeros_mod'] = np.nan\n",
    "\n",
    "    # Combine zero information back with the main dataset for the gage\n",
    "    gage_df = df.merge(df_zero, on=['time'], how='left')\n",
    "    gage_df['previous_zeros_obs'] = gage_df['previous_zeros_obs'].fillna(0)\n",
    "    gage_df['previous_zeros_mod'] = gage_df['previous_zeros_mod'].fillna(0)\n",
    "\n",
    "    # originally multiplied by 0.01...\n",
    "    gage_df['obs_prev_zeros'] = gage_df['observed'] - (gage_df['previous_zeros_obs'] * zero_cutoff)\n",
    "    gage_df['mod_prev_zeros'] = gage_df['modeled'] - (gage_df['previous_zeros_mod'] * zero_cutoff)\n",
    "\n",
    "    # sort values by time\n",
    "    gage_df = gage_df.sort_values('time')\n",
    "\n",
    "    # Compute the Weibull plotting position (r/(n + 1), where r is rank and n is the number of data (Lahaa et al,. 2017; https://doi.org/10.5194/hess-21-3001-2017)\n",
    "    # no CDPM (continuous dry period method; https://doi.org/10.5194/hess-16-2437-2012)\n",
    "    gage_df['weibull_site_obs_no_cdpm'] = 100 * gage_df['observed'].rank(method='min') / (gage_df['observed'].count() + 1)\n",
    "    gage_df['weibull_site_mod_no_cdpm'] = 100 * gage_df['modeled'].rank(method='min') / (gage_df['modeled'].count() + 1)\n",
    "\n",
    "    # CDPM (continuous dry period method; https://doi.org/10.5194/hess-16-2437-2012)\n",
    "    gage_df['weibull_site_obs_cdpm'] = 100 * gage_df['obs_prev_zeros'].rank(method='min') / (gage_df['obs_prev_zeros'].count() + 1)\n",
    "    gage_df['weibull_site_mod_cdpm'] = 100 * gage_df['mod_prev_zeros'].rank(method='min') / (gage_df['mod_prev_zeros'].count() + 1)\n",
    "    \n",
    "    # Assign a 'site' - this is to examine the percentiles over the period of record for the streamflow gage\n",
    "    gage_df['site'] = gage_id\n",
    "\n",
    "    # Group by site and day of year for Weibull calculations\n",
    "    gage_df_grouped = gage_df.groupby(['site', 'jd'])\n",
    "\n",
    "    # No CDPM\n",
    "    gage_df['weibull_jd_obs_no_cdpm'] = gage_df_grouped['observed'].transform(lambda x: 100 * x.rank(method='min') / (x.count() + 1))\n",
    "    gage_df['weibull_jd_mod_no_cdpm'] = gage_df_grouped['modeled'].transform(lambda x: 100 * x.rank(method='min') / (x.count() + 1))\n",
    "\n",
    "    # CDPM\n",
    "    gage_df['weibull_jd_obs_cdpm'] = gage_df_grouped['obs_prev_zeros'].transform(lambda x: 100 * x.rank(method='min') / (x.count() + 1))\n",
    "    gage_df['weibull_jd_mod_cdpm'] = gage_df_grouped['mod_prev_zeros'].transform(lambda x: 100 * x.rank(method='min') / (x.count() + 1))\n",
    "\n",
    "    # Choose cdpm or non cdpm\n",
    "    gage_df['weibull_jd_obs'] = gage_df['weibull_jd_obs_cdpm']\n",
    "    gage_df['weibull_jd_mod'] = gage_df['weibull_jd_mod_cdpm']\n",
    "    gage_df['weibull_site_obs'] = gage_df['weibull_site_obs_cdpm']\n",
    "    gage_df['weibull_site_mod'] = gage_df['weibull_site_mod_cdpm']\n",
    "\n",
    "    gage_df['feature_id'] = gage_df['site']\n",
    "\n",
    "    df_pct = gage_df\n",
    "    \n",
    "    return df_pct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f29fd37-586f-4030-97e0-255d4ae54bcd",
   "metadata": {},
   "source": [
    "## Calculate Drought Properties and Annual Statistics\n",
    "Note: This function may have an issue with flow_volume. Bonus points if you find it.\n",
    "\n",
    "This function is adapted from _calculate_properties_single.R_ in HyMED [(Simeone and Foks, 2024)](https://doi.org/10.5066/P1TLARC5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e744f6-fb7b-4e13-864a-ac96637f8a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_site_properties(df, gage_id, \n",
    "                              thresholds = [5, 10, 20 , 30], \n",
    "                              percent_type_list = ['weibull_jd_obs', 'weibull_jd_mod', 'weibull_site_obs', 'weibull_site_mod'],\n",
    "                              flow_name_list = ['observed', 'modeled', 'observed', 'modeled'], start_cy = 1985, end_cy = 2016) -> float:\n",
    "    \"\"\"\n",
    "    This function identifies drought events and then calculates their properties.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframes\n",
    "            |\n",
    "    \"\"\"\n",
    "    low_flow_list = []\n",
    "    drought_event_list = []\n",
    "    flow_name = []\n",
    "    \n",
    "    df = df_pct\n",
    "    \n",
    "    # Calculate drought properties\n",
    "    for h in range(len(percent_type_list)):\n",
    "        percent_type = percent_type_list[h]\n",
    "        flow_name = flow_name_list[h]\n",
    "        df['flow_value'] = df[flow_name]\n",
    "\n",
    "        # Iterate through each threshold.\n",
    "        for j in range(len(thresholds)):\n",
    "            thresh = thresholds[j]\n",
    "        \n",
    "            # Find periods under/over the threshold\n",
    "            df['value'] = df[percent_type]\n",
    "            df['less_than_thresh'] = np.where(df['value'] <= thresh, 1, 0) # create boolean 1 = under or at threshold (drought)\n",
    "            df.sort_values(['time'], inplace = True)\n",
    "            \n",
    "            # Assign a unique identifier (drought_id) to consecutive periods of drought based on the less_than_thresh column\n",
    "            df['drought_id'] = (df['less_than_thresh'] != df['less_than_thresh'].shift()).cumsum()\n",
    "        \n",
    "            # Group by 'feature_id' and the second part of percent_type, then calculate flow_thresh\n",
    "            df['flow_thresh'] = df.groupby(['feature_id', percent_type.split('_')[1]])['flow_value'].transform(\n",
    "                lambda x: np.quantile(x, thresh / 100, interpolation='linear'))\n",
    "    \n",
    "            # reset the index\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # Analysis on all periods below threshold\n",
    "            df_low_flows = df[df['value'] <= thresh]\n",
    "            df_low_flows = df_low_flows.groupby(['site', 'cy', 'drought_id']).agg(days=('value', 'size'),\n",
    "                                                                          lowest_percent=('value', 'min'),\n",
    "                                                                          severity=('value', lambda x: abs(np.sum(thresh - x))),\n",
    "                                                                          lowest_flow=('flow_value', 'min'),\n",
    "                                                                          flow_volume=('flow_thresh', lambda x: np.sum(x - df['flow_value']))).reset_index()\n",
    "\n",
    "            # Summarize drought metrics\n",
    "            drought_summary = (df_low_flows.groupby(['site', 'cy'])\n",
    "                               .agg(total_duration_below_threshold=('days', 'sum'),\n",
    "                                    longest_duration_below_threshold=('days', 'max'),\n",
    "                                    total_severity_below_threshold=('severity', 'sum'),\n",
    "                                    largest_severity_below_threshold=('severity', 'max'),\n",
    "                                    maximum_drought_intensity=('lowest_percent', lambda x: thresh - x.min()),\n",
    "                                    minimum_flow=('lowest_flow', 'min'),\n",
    "                                    total_flow_volume_below_threshold=('flow_volume', 'sum'),\n",
    "                                    largest_flow_volume_below_threshold=('flow_volume', 'max'))\n",
    "                               .reset_index())\n",
    "\n",
    "            # Complete the DataFrame to ensure all years are represented\n",
    "            drought_summary = (drought_summary.set_index(['site', 'cy'])\n",
    "                               .reindex(pd.MultiIndex.from_product([drought_summary['site'].unique(), range(start_cy, end_cy + 1)], names=['site', 'cy']),\n",
    "                                        fill_value=0).reset_index())\n",
    "\n",
    "            # Set 'minimum_flow' to NaN for the new rows (where the mask is False)\n",
    "            existing_mask = drought_summary['minimum_flow'] != 0\n",
    "            drought_summary['minimum_flow'] = np.where(existing_mask, drought_summary['minimum_flow'], np.nan)\n",
    "    \n",
    "            # Add additional columns\n",
    "            drought_summary['threshold'] = thresh\n",
    "            drought_summary['pct_type'] = percent_type\n",
    "        \n",
    "\n",
    "            # Analysis of drought events ------------------------------------------------\n",
    "            # Amount of time between events for pooling and minimum time of event\n",
    "            inter_event_duration = 5\n",
    "            \n",
    "            # Pooling severity ratio. The ratio of the inter-event excess severity to the preceding event deficit severity\n",
    "            pooling_severity_ratio = 0.1\n",
    "    \n",
    "            # Finding events to be pooled.\n",
    "            df_IC_events = df.groupby(['site', 'drought_id']).agg(duration=('value','count'),    # duration of each event\n",
    "                                                                  drought_bool=('less_than_thresh', 'mean'),   # drought bool 1 is drought, 0 is not\n",
    "                                                                  severity=('value', lambda x: abs((thresh - x).sum()))).reset_index()\n",
    "            # Add columns with duration/severity of previous event\n",
    "            df_IC_events['previous_duration'] = df_IC_events['duration'].shift(1)\n",
    "            df_IC_events['previous_severity'] = df_IC_events['severity'].shift(1)\n",
    "            df_IC_events['severity_ratio'] = df_IC_events['severity'] / df_IC_events['previous_severity']\n",
    "            # Filter \n",
    "            df_IC_events = df_IC_events[(((df_IC_events['duration'] < inter_event_duration) & \n",
    "                                  (df_IC_events['previous_duration'] > df_IC_events['duration'])) | \n",
    "                                 (df_IC_events['severity_ratio'] < pooling_severity_ratio)) & \n",
    "                                 (df_IC_events['drought_bool'] == 0)]\n",
    "            # Add feature id w/ event\n",
    "            df_IC_events['feature_event_id'] = df_IC_events['site'] + \"_\" + df_IC_events['drought_id'].astype(str)\n",
    "\n",
    "            # Run summary statistics on pooled droughts.\n",
    "            df_drought_events = df[~df['drought_id'].isin(df_IC_events['drought_id'])].copy() # Remove inter-event periods that are too short/low severity to stop a drought.\n",
    "            df_drought_events = df_drought_events[df_drought_events['less_than_thresh'] == 1] # Retain droughts\n",
    "            df_drought_events = df_drought_events.groupby(['site', 'drought_id']).agg(\n",
    "                    severity=('value', lambda x: (thresh - x).sum()),\n",
    "                    mean_intensity=('value', 'mean'),\n",
    "                    max_intensity=('value', 'min'),\n",
    "                    duration=('value', 'count'),\n",
    "                    start=('time', 'first'),\n",
    "                    end=('time', 'last'),\n",
    "                    min_flow=('flow_value', 'min'),\n",
    "                    mean_flow=('flow_value', 'mean'),\n",
    "                    max_flow=('flow_value', 'max'),\n",
    "                    flow_volume=('flow_thresh', lambda x: (x - df.loc[x.index, 'flow_value']).sum(skipna=True))).reset_index()  # check this.. some flows are odd.\n",
    "\n",
    "            # Keep drought events if duration is longer than or equal to the inter-event duration length in days\n",
    "            df_drought_events = df_drought_events[df_drought_events['duration'] >= inter_event_duration]\n",
    "    \n",
    "            # Reassign climate year\n",
    "            df_drought_events['cy'] = np.where(df_drought_events['start'].dt.month >= 4, df_drought_events['start'].dt.year + 1, df_drought_events['start'].dt.year)\n",
    "    \n",
    "            # Define the threshold and percentile\n",
    "            df_drought_events['threshold'] = thresh\n",
    "            df_drought_events['pct_type'] = percent_type\n",
    "    \n",
    "            # Append data\n",
    "            low_flow_list.append(drought_summary)\n",
    "            drought_event_list.append(df_drought_events)\n",
    "\n",
    "    # Combine data for each threshold\n",
    "    df_drought_events_all = pd.concat(drought_event_list, ignore_index=True)\n",
    "    df_low_flows_all = pd.concat(low_flow_list, ignore_index=True)\n",
    "    \n",
    "    # Summarize data to the cy level\n",
    "    df_drought_long = df_drought_events_all.groupby(['site', 'cy', 'threshold', 'pct_type']).agg(\n",
    "                           total_drought_severity=('severity', 'sum'),\n",
    "                           total_drought_duration=('duration', 'sum'),\n",
    "                           number_of_drought_events=('severity', 'count'),\n",
    "                           largest_drought_severity=('severity', 'max'),\n",
    "                           longest_drought_duration=('duration', 'max'),\n",
    "                           total_drought_flow_volume=('flow_volume', 'sum'),\n",
    "                           longest_drought_flow_volume=('flow_volume', 'max')).reset_index()\n",
    "                           #start_day_of_longest_drought=('start', lambda x: pd.to_datetime(x).dt.dayofyear.loc[x.idxmax()]),\n",
    "                           #start_date_of_longest_drought=('start', lambda x: pd.to_datetime(x).loc[x.idxmax()]),\n",
    "                           #end_date_of_longest_drought=('end', lambda x: pd.to_datetime(x).loc[x.idxmax()])\n",
    "    \n",
    "    # Ungroup and complete missing years (cy)\n",
    "    start_cy = df_drought_long['cy'].min()\n",
    "    end_cy = df_drought_long['cy'].max()\n",
    "    \n",
    "    # Create a complete DataFrame with all combinations of site, cy, threshold, and pct_type\n",
    "    complete_index = pd.MultiIndex.from_product(\n",
    "        [df_drought_long['site'].unique(), \n",
    "         range(start_cy, end_cy + 1), \n",
    "         df_drought_long['threshold'].unique(), \n",
    "         df_drought_long['pct_type'].unique()],\n",
    "        names=['site', 'cy', 'threshold', 'pct_type'])\n",
    "\n",
    "    # Reindex to complete the DataFrame\n",
    "    df_drought_long = df_drought_long.set_index(['site', 'cy', 'threshold', 'pct_type']).reindex(complete_index, fill_value=0).reset_index()\n",
    "    \n",
    "    # Convert Drought event data to long format\n",
    "    df_drought_long = df_drought_long.melt(id_vars=['site', 'cy', 'threshold', 'pct_type'],\n",
    "                                           value_vars=['total_drought_severity', \n",
    "                                                       'total_drought_duration', \n",
    "                                                       'number_of_drought_events', \n",
    "                                                       'largest_drought_severity', \n",
    "                                                       'longest_drought_duration', \n",
    "                                                       'total_drought_flow_volume', \n",
    "                                                       'longest_drought_flow_volume'],\n",
    "                                           var_name='measure',\n",
    "                                           value_name='value')\n",
    "    \n",
    "    # Select the final columns\n",
    "    df_drought_long = df_drought_long[['site', 'cy', 'measure', 'value', 'threshold', 'pct_type']]\n",
    "    \n",
    "    # Convert low flow threshold data to long format\n",
    "    df_low_flow_long = df_low_flows_all.melt(\n",
    "        id_vars=['site', 'cy', 'threshold', 'pct_type'], \n",
    "        value_vars=[\n",
    "            'largest_flow_volume_below_threshold',\n",
    "            'total_flow_volume_below_threshold',\n",
    "            'minimum_flow',\n",
    "            'total_severity_below_threshold',\n",
    "            'largest_severity_below_threshold',\n",
    "            'maximum_drought_intensity',\n",
    "            'longest_duration_below_threshold',\n",
    "            'total_duration_below_threshold'],  # Columns to unpivot\n",
    "        var_name='measure',  # Name for the new variable column\n",
    "        value_name='value' )  # Name for the new value column\n",
    "    \n",
    "    # Select the final columns (if needed)\n",
    "    df_low_flow_long = df_low_flow_long[['site', 'cy', 'measure', 'value', 'threshold', 'pct_type']]\n",
    "    \n",
    "    # Combine datasets\n",
    "    annual_stats = pd.concat([df_drought_long, df_low_flow_long], ignore_index=True)\n",
    "    \n",
    "    # Sort the combined DataFrame by 'site' and 'cy'\n",
    "    annual_stats = annual_stats.sort_values(by=['site', 'cy'])\n",
    "\n",
    "    # Remove duplicate rows\n",
    "    annual_stats = annual_stats.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    return df_drought_events_all, annual_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abbf118-d11b-4b96-b6b4-17289b9283d0",
   "metadata": {},
   "source": [
    "## Simplified drought/non-drought boolean calculator\n",
    "This function expands drought events into a TRUE/FALSE timeseries of drought/non-drought and is translated from the R script _calculate_site_booleans_threshold_only.R_ within HyMED (Simeone and Foks, 2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af283034-12c1-47df-a5e1-b8d4bd02f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_site_boolean_threshold_only(df_pct):\n",
    "    \"\"\"\n",
    "    Expand drought events into true false timeseries of drought/no drought.\n",
    "    We do this here only using thresholds not including pooling for improved simplicity of methodology.\n",
    "    Returns\n",
    "    -------\n",
    "    dataframes\n",
    "            |\n",
    "    \"\"\"\n",
    "\n",
    "    # Subset dataframe\n",
    "    df_bool_threshold_only = df_pct[['site', 'time', 'weibull_jd_obs', 'weibull_jd_mod', 'weibull_site_obs', 'weibull_site_mod']]\n",
    "    \n",
    "    # Pivot longer to get pct_type\n",
    "    df_bool_threshold_only = df_bool_threshold_only.melt(id_vars=['site', 'time'], \n",
    "                                                               value_vars=['weibull_jd_obs', 'weibull_jd_mod', 'weibull_site_obs', 'weibull_site_mod'], \n",
    "                                                               var_name='pct_type',\n",
    "                                                               value_name='value')\n",
    "    #  Create boolean columns for each threshold                                                    \n",
    "    df_bool_threshold_only['5'] = df_bool_threshold_only['value'] <= 5\n",
    "    df_bool_threshold_only['10'] = df_bool_threshold_only['value'] <= 10\n",
    "    df_bool_threshold_only['20'] = df_bool_threshold_only['value'] <= 20\n",
    "    df_bool_threshold_only['30'] = df_bool_threshold_only['value'] <= 30                                        \n",
    "    \n",
    "    # Pivot longer for the thresholds\n",
    "    df_bool_threshold_only = df_bool_threshold_only.melt(id_vars=['site', 'time', 'pct_type'], \n",
    "                                                         value_vars=['5', '10', '20', '30'], \n",
    "                                                         var_name='threshold', \n",
    "                                                         value_name='drought')\n",
    "        \n",
    "    # Convert threshold to numeric\n",
    "    df_bool_threshold_only['threshold'] = pd.to_numeric(df_bool_threshold_only['threshold'])\n",
    "        \n",
    "    # rename 'time' to 'day'\n",
    "    df_bool_threshold_only = df_bool_threshold_only.rename(columns={'time': 'day'})\n",
    "        \n",
    "    return df_bool_threshold_only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c16b8c-5202-47e9-8920-dbe606e9d336",
   "metadata": {},
   "source": [
    "##  Event Classification: Cohen's kappa\n",
    "Cohen's kappa [(Cohen, 1960)](https://doi.org/10.1177/001316446002000104) is used to measure the agreement between categorical variables. [Landis and Koch (1977)](https://doi.org/10.2307/2529310) provide a guide for interpretation. This script is translated from _site_cohens_kappas.R_ within HyMED (Simeone and Foks, 2024).\n",
    "\n",
    "- Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20, 37–46. https://doi.org/10.1177/001316446002000104\n",
    "- Landis, J.R.; Koch, G.G. The measurement of observer agreement for categorical data. Biometrics 1977, 33, 159–174. https://doi.org/10.2307/2529310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b0ff78-9b36-40a7-84fb-11bb7e802b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def site_cohens_kappa(df_bool, gage_id):\n",
    "    \n",
    "    # Setup lists to be filled.\n",
    "    mod_ca_list = [np.nan] * 8\n",
    "    mod_kappa_list = [np.nan] * 8\n",
    "    pct_type_list = [np.nan] * 8\n",
    "    threshold_list = [np.nan] * 8\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    # Iterate through percent types.\n",
    "    for percent_type in [\"jd\", \"site\"]:\n",
    "        df_type = df_bool[df_bool['pct_type'].str.contains(percent_type)]\n",
    "\n",
    "        # Iterate through percentiles.\n",
    "        for thresh in [5, 10, 20, 30]:\n",
    "            try:\n",
    "                # Subset data to threshold value\n",
    "                df = df_type[df_type['threshold'] == thresh]\n",
    "                \n",
    "                # Pivot the dataframe\n",
    "                # Assign pct_type as a column and the drought False/True flag as the value.\n",
    "                df = df.pivot_table(index=['threshold', 'day', 'site'], \n",
    "                                    columns='pct_type', \n",
    "                                    values='drought', \n",
    "                                    fill_value=False)\n",
    "                df.reset_index(inplace=True)\n",
    "                \n",
    "                # Setup drought data\n",
    "                df['drought_mod'] = df[f'weibull_{percent_type}_mod']\n",
    "                df['drought_obs'] = df[f'weibull_{percent_type}_obs']\n",
    "                df['mod_correct'] = df['drought_mod'] == df['drought_obs']\n",
    "\n",
    "                # Calculate classification accuracy\n",
    "                mod_ca_list[i] = df['mod_correct'].mean()\n",
    "\n",
    "                # Calculate Cohen's kappa\n",
    "                # Using scikit-learn\n",
    "                # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html\n",
    "                mod_kappa_list[i] = cohen_kappa_score(df['drought_mod'], df['drought_obs'])\n",
    "                pct_type_list[i] = percent_type\n",
    "                threshold_list[i] = thresh\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'Error encountered: {e}')\n",
    "                mod_ca_list[i] = np.nan\n",
    "                mod_kappa_list[i] = np.nan\n",
    "                pct_type_list[i] = percent_type\n",
    "                threshold_list[i] = thresh\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    # Setup final DataFrame\n",
    "    df_accuracy = pd.DataFrame({\n",
    "        'site': gage_id,\n",
    "        'threshold': threshold_list,\n",
    "        'pct_type': pct_type_list,\n",
    "        'mod_kappa': mod_kappa_list,\n",
    "        'mod_classification_accuracy': mod_ca_list})\n",
    "\n",
    "    return df_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448b82c3-7a6a-4529-91de-32c7280cad42",
   "metadata": {},
   "source": [
    "### Error Components: Spearman's r\n",
    "Spearman's rank correlation coefficient is calculated to evaluate a modeling applications' ability to reproduce the sequence of observational streamflow data (i.e., the timing of streamflow during streamflow drought periods). Spearman's assesses the monotonicity of the relation between the observed and simulated flows. This function is translated from _site_spearmans.R_ within HyMED (Simeone and Foks, 2024).\n",
    "\n",
    "Observed and modeled streamflow are each first ranked by magnitude. After calculating ranks, we subset the data into periods with observed droughts at the various threshold levels of interest; for example, we took the lowest 20% of flow values for the 20th percentile drought threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565093dd-0a2f-4323-a1c7-f23ce54feccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_r(df_pct, site_name, thresholds = [5,10,20,30]):\n",
    "    # Calculate Spearman's Rho\n",
    "    \n",
    "    # Setup df for use.\n",
    "    df = df_pct.dropna(subset=['observed']).copy()\n",
    "    df['obs_rank'] = df['observed'].rank(method='min')\n",
    "    df['mod_rank'] = df['modeled'].rank(method='min')\n",
    "    df['obs_mod_d'] = df['mod_rank'] - df['obs_rank']\n",
    "    df['obs_mod_d2'] = df['obs_mod_d'] ** 2\n",
    "    \n",
    "    count = 0\n",
    "    df_thresh_list = []\n",
    "    result = {}\n",
    "\n",
    "    # Iterate through thresholds.\n",
    "    for thresh in thresholds:\n",
    "        for pct_type in ['weibull_site', 'weibull_jd']:\n",
    "            count += 1\n",
    "        \n",
    "            # Create the pct_value_obs column\n",
    "            df['pct_value_obs'] = df[f'{pct_type}_obs']\n",
    "\n",
    "            # Subset data to drought period and calculate drought metrics\n",
    "            df_q = df[df['pct_value_obs'] <= thresh]\n",
    "\n",
    "            # Calculate Spearman's Rho using scipy\n",
    "            rho_obs_mod = (np.cov(df_q['obs_rank'], df_q['mod_rank'])) / (np.std(df_q['obs_rank']) * np.std(df_q['mod_rank']))\n",
    "            rho_obs_mod = rho_obs_mod[0][1] # extract the correlation coefficient\n",
    "            quant_length = len(df_q)\n",
    "\n",
    "            # Create a result dictionary to store the results\n",
    "            result = {\n",
    "                'site': site_name,\n",
    "                'pct_type': pct_type,\n",
    "                'threshold': thresh,\n",
    "                'quant_length' : quant_length,\n",
    "                'spearmans_r': rho_obs_mod}\n",
    "        \n",
    "            # Append the result dictionary to results list\n",
    "            df_thresh_list.append(result)\n",
    "            \n",
    "    # Concatenate all results into a single DataFrame\n",
    "    df_spearmans = pd.DataFrame(df_thresh_list)\n",
    "\n",
    "    return df_spearmans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7934029-f1fd-49ba-a8b7-2d151d8bbf1a",
   "metadata": {},
   "source": [
    "### Bias & Ratio of Standard Deviations\n",
    "Bias is a measure of the mean tendency of simulated values to be greater or less than associated observed values. Ratio of standard deviations (sd of modeled / sd of observed) is also calculated here. This function was translated from _site_bias_dist.R_ within HyMED (Simeone and Foks, 2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dedfa23-be48-4776-bdd4-1448843afca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_dist(df_pct, site_name, thresholds = [5,10,20,30]):\n",
    "    \"\"\"\n",
    "    Calculate bias and distributional errors.\n",
    "    \n",
    "    Parameters:\n",
    "    df_pct (DataFrame): Input DataFrame containing 'observed' and 'modeled' values.\n",
    "    site_name (str): Name of the site/gage of interest.\n",
    "    thresholds (list): List of thresholds for calculations.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing calculated bias and distributional metrics.\n",
    "    \"\"\"\n",
    "    # Initialize empty list\n",
    "    df_thresh_list = []\n",
    "\n",
    "    # Drop NA values from the 'q_obs' column\n",
    "    df = df_pct.dropna(subset=['observed'])\n",
    "\n",
    "    # Iterate through thresholds\n",
    "    for thresh in thresholds:\n",
    "        for pct_type in ['weibull_site', 'weibull_jd']:\n",
    "            \n",
    "            # Extract observed and modeled values\n",
    "            df['pct_value_obs'] = df[f'{pct_type}_obs']\n",
    "            df['pct_value_mod'] = df[f'{pct_type}_mod']\n",
    "            \n",
    "            # Filter data based on the threshold for observed values\n",
    "            df_obs = df[df['pct_value_obs'] <= thresh]\n",
    "            mean_obs = df_obs['observed'].mean()\n",
    "            sd_obs = np.std(df_obs['observed'])\n",
    "\n",
    "            # Filter data based on the threshold for modeled values\n",
    "            df_mod = df[df['pct_value_mod'] <= thresh]\n",
    "            mean_mod = df_mod['modeled'].mean()\n",
    "            sd_mod = np.std(df_mod['modeled'])\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mod_sd_ratio = sd_mod / sd_obs if sd_obs != 0 else np.nan\n",
    "            mod_bias = mean_mod - mean_obs\n",
    "            mod_pct_bias = ((mean_mod - mean_obs) / abs(mean_obs)) * 100 if mean_obs != 0 else np.nan\n",
    "            \n",
    "            # Build DataFrame for return\n",
    "            df_q = pd.DataFrame({\n",
    "                'mod_bias': [mod_bias],\n",
    "                'mod_pct_bias': [mod_pct_bias],\n",
    "                'mod_sd_ratio': [mod_sd_ratio],\n",
    "                'thresh': [thresh],\n",
    "                'pct_type': [pct_type],\n",
    "                'site': [site_name]\n",
    "            })\n",
    "            df_thresh_list.append(df_q)\n",
    "\n",
    "    # Concatenate all results into a single DataFrame\n",
    "    df_bias_sd = pd.concat(df_thresh_list, ignore_index=True)\n",
    "\n",
    "    # Return DataFrame\n",
    "    return df_bias_sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e8cc4-23b5-48ec-a6c0-3f4bf354f4b3",
   "metadata": {},
   "source": [
    "### Error on Annual Signatures (NMAE and others)\n",
    "The drought events that are identified each year are aggregated to determine the annual drought signatures of duration, intensity, and severity in each given climate year. The normalized mean absolute error (NMAE) on this annual time series is calculated to compare across streamgages and models. This is adapted from Simeone and Foks (2024), R package HyMED - script \"site_annual_signature_metrics.R\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3424336-d543-4c18-93f0-00e49be8de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annual_signatures(df_annual_stats, site_name):\n",
    "    \"\"\"\n",
    "    This function takes in a site and calculates metrics for various drought measures.\n",
    "\n",
    "    Parameters:\n",
    "    df_annual_stats (pd.DataFrame): The DataFrame containing annual statistics.\n",
    "    site_name (str): The name of the site to filter the data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the calculated metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the list of measures to consider\n",
    "    measure_list = [\n",
    "        \"total_duration_below_threshold\",\n",
    "        \"total_drought_severity\",\n",
    "        \"total_flow_volume_below_threshold\",\n",
    "        \"number_of_drought_events\",\n",
    "        \"maximum_drought_intensity\"]\n",
    "\n",
    "    # Subset data\n",
    "    df = df_annual_stats[df_annual_stats['measure'].isin(measure_list)]\n",
    "    \n",
    "    # Filter for the specific site\n",
    "    df = df[df['site'] == site_name]\n",
    "\n",
    "    # Replace NA values with 0\n",
    "    df['value'] = df['value'].fillna(0)\n",
    "\n",
    "    # Separate 'pct_type' into three columns: 'weibull', 'type', 'source'\n",
    "    df[['weibull', 'type', 'source']] = df['pct_type'].str.split('_', expand=True)\n",
    "\n",
    "    # Pivot wide\n",
    "    df_wide = df.pivot_table(index=['site', 'measure', 'threshold', 'type'],\n",
    "                              columns='source',\n",
    "                              values='value',\n",
    "                              fill_value=0).reset_index()\n",
    "\n",
    "    # Filter thresholds\n",
    "    df_wide = df_wide[df_wide['threshold'].isin([5, 10, 20, 30])]\n",
    "\n",
    "    sum_df = df_wide.groupby(['site', 'measure', 'threshold', 'type']).agg(\n",
    "        mae_mod=('mod', lambda x: np.mean(np.abs(df_wide.loc[x.index, 'obs'] - x))),\n",
    "        mean_obs=('obs', lambda x: np.mean(x)),\n",
    "        nmae_mod=('mod', lambda x: np.mean(np.abs(df_wide.loc[x.index, 'obs'] - x)) / np.mean(df_wide.loc[x.index, 'obs']))).reset_index()\n",
    "\n",
    "    return sum_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global-global-pangeo",
   "language": "python",
   "name": "conda-env-global-global-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
