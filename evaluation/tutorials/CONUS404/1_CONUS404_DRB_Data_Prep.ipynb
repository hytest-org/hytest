{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing CONUS404 and reference data for aggregation and sampling\n",
    "\n",
    "Author: Andrew Laws, alaws@usgs.gov\n",
    "\n",
    "<img src='../../../doc/assets/Eval_PreProc.svg' width=600>\n",
    "The purpose of data preparation notebooks is to process forcing and reference datasets for comparison and evaluation. Each preparation notebook will differ somewhat in its specifications; for instance, by forcing dataset, reference datasets, variables, temporal resolution, spatial extent, etc.\n",
    "\n",
    "Specifications for this notebook:\n",
    "\n",
    "- Forcing data: CONUS404\n",
    "- Reference datasets: PRISM (gridded), CERES-EBAF (gridded), GHCN (point), USCRN (point)\n",
    "- Variables: precipitation, mean temperature, tmin, tmax, radiation components\n",
    "- Processed temporal resolution: monthly\n",
    "- Processed spatial extent: DRB, by HUC6\n",
    "\n",
    "These are the steps this notebook follows:\n",
    "\n",
    "- Step 0: Import libraries\n",
    "- Step 1: Retrieve data from the HPC or Cloud\n",
    "- Step 2: Explore the data\n",
    "- Step 3: Import geographic extents\n",
    "- Step 4: Put it together: Process CONUS404 to variable and spatial extent\n",
    "- Step 5: Prepare gridded reference data\n",
    "- Step 6: Prepare station reference data\n",
    "\n",
    "## **Step 0: Importing libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import calendar\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import fsspec\n",
    "import geopandas as gpd\n",
    "import geoviews as gv\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import intake\n",
    "import metpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from fsspec.implementations.ftp import FTPFileSystem\n",
    "from holoviews.streams import PolyDraw, PolyEdit\n",
    "from pygeohydro import WBD\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1: Retrieving data from HPC or the Cloud**\n",
    "\n",
    "#### The process varies based on where the notebook is being run but generally looks this:\n",
    "\n",
    "1.  Connect to workspace (local, HPC, or cloud-based environment such as ESIP-Nebari) and open notebook **(Done already)**\n",
    "2.  Start Dask client\n",
    "3.  Pull in data from source\n",
    "4.  Process the data into usable file format, size, and extent\n",
    "\n",
    "\n",
    "### Start a Dask client using an appropriate Dask Cluster\n",
    "\n",
    "This is an optional step, but can speed up data loading significantly, especially when accessing data from the cloud.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for existing Dask cluster\n",
    "if \"client\" in locals():\n",
    "    print(\"Shutting down existing Dask cluster.\")\n",
    "    cluster.close()\n",
    "    client.close()\n",
    "\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "\n",
    "print(f\"The link to the Dask dashboard is {client.dashboard_link}. If on HPC, this may not be available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View available datasets from the Intake Catalog and choose which to use\n",
    "\n",
    "For explanation of data naming and storage locations, please see [link](https://hytest-org.github.io/hytest/dataset_catalog/README.html). You will want to use data for where you are working (i.e., if you are working on a cloud-based environment such as Nebari, you will want to use data stored on the cloud or OSN).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml\"\n",
    "# open the hytest data intake catalog\n",
    "hytest_cat = intake.open_catalog(url)\n",
    "list(hytest_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the conus404 sub-catalog\n",
    "cat = hytest_cat[\"conus404-catalog\"]\n",
    "list(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup dataset for the compute environment you are working in\n",
    "\n",
    "Adjust the dataset name below to correspond to the dataset of interest you are working on. For this tutorial, we are focusing on the *CONUS404 hourly dataset*.\n",
    "\n",
    "The data can be read in lazily using Dask with the `to_dask()` method after subsetting using the kev for the dataset value you want to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    \"SLURM_CLUSTER_NAME\" in os.environ\n",
    "):  # USGS HPC use SLURM CLUSTER, so use the on-prem dataset, otherwise...\n",
    "    dataset = \"conus404-hourly-onprem\"\n",
    "else:  # use the OSN dataset\n",
    "    dataset = \"conus404-hourly-osn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = cat[dataset].to_dask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View dataset metadata. To view variables, expand the _Data variables_ section and a greater explanation of the data variables can be found in the [WRFout Data Dictionary on the NCAR website](https://rda.ucar.edu/datasets/ds559.0/documentation/#).\n",
    "\n",
    "For this tutorial, we will be working with the accumulated precipitation (PREC*ACC_NC), air temperature (TK), and surface net radiation (RNET) variables. If you look through the \\_Data variables* section of _ds_, you'll notice RNET doesn't exist. It will be calculated using other existing variables later in the tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2: Explore the data**\n",
    "\n",
    "(sometimes called exploratory data analysis (EDA) or exploratory spatial data analysis (ESDA) when it contains cartographic data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the accumulated precipitation variable by first subsetting the larger dataset.\n",
    "\n",
    "This makes the data lighter and working with it more responsive. <br>\n",
    "\n",
    "Notice the information in the array and chunk columns as well as the coordinates (in particular _time_) and the units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable PREC_ACC_NC\n",
    "prec = ds.PREC_ACC_NC\n",
    "prec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, lets visualize a map of the data at specific time step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_time = prec.sel(time=\"2014-03-01 00:00\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous cell, the .sel() method filters the dataset by the _time_ coordinate through \"time=\" and then uses the .load() method to load the dataset into memory. More on indexing datasets can be found at [Indexing and selecting data](https://docs.xarray.dev/en/stable/user-guide/indexing.html).\n",
    "\n",
    "Now, let's visualize the dataset using the [QuadMesh](https://holoviews.org/reference/elements/bokeh/QuadMesh.html) plot from Holoviews. For a more in-depth tutorial for visualizing gridded data in Holoviews, go to [Gridded Datasets](http://holoviews.org/getting_started/Gridded_Datasets.html).\n",
    "\n",
    "Notice the spatial extent and the pattern of values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_time.hvplot.quadmesh(\n",
    "    x=\"lon\", y=\"lat\", rasterize=True, geo=True, tiles=\"OSM\", alpha=0.7, cmap=\"turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also look at a time-series for a specific grid cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_point = (\n",
    "    prec.isel(y=600, x=600)  # Selects based on y,x\n",
    "    .sel(\n",
    "        time=slice(\"2015-02-11 00:00\", \"2015-04-28 00:00\")\n",
    "    )  # selects from the result to a particular time range\n",
    "    .load()  # loads the subsetted result\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the previous cell uses the .isel() method, which returns the dataset from where the **x** and **y** indexes equal 600 prior to filtering by **time** and loading the data into memory.\n",
    "Note the previous cell uses the .isel() method, which returns the dataset from where the **x** and **y** indexes equal 600 prior to filtering by **time** and loading the data into memory. Essentially, we're plotting a time series at the grid cell whose coordinates are x=y=600, and we're also defining a period of time to look at through definition of the time slice.\n",
    "\n",
    "Lets plot the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_point.hvplot(x=\"time\", grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 3: Importing geographic extents**\n",
    "\n",
    "Sometimes geographic data is brought in and used to clip a larger dataset to an area of interest (AOI).\n",
    "\n",
    "Let's look at two ways this can be done: a user-defined polygon or using the pyNHD package. Data can also be brought in other ways such as a local file or an API request. These are covered in other tutorials such as [Reading and Writing Files (GeoPandas)](https://geopandas.org/en/stable/docs/user_guide/io.html) or using the Python [requests](https://requests.readthedocs.io/en/latest/) package for [Programattically Accesing Geospatial Data Using APIs](https://www.earthdatascience.org/courses/use-data-open-source-python/intro-to-apis/spatial-data-using-apis/).\n",
    "\n",
    "We'll show how to use geometries to spatially clip datasets later in this tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first method will use the the Holoviews and Geoviews libraries to draw a polygon and then add its dimensions to a geopandas GeoDataFrame.\n",
    "\n",
    "When the next code block is run, a map will open and the PolyDraw tool will be automatically selected.\n",
    "\n",
    "**Double-click to add the first vertex, then single-click to add each subsequent vertex, and to finalize the draw action double-click to insert the final vertex or press the ESC key to stop drawing after adding the last vertex.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CartoLight basemap\n",
    "basemap = gv.tile_sources.CartoLight()\n",
    "\n",
    "# x and y limits for CONUS\n",
    "xlim = (-135, -50)\n",
    "ylim = (22, 50)\n",
    "\n",
    "# create blank polygon to draw\n",
    "## redim.range works with Bokeh backend to set default map extent\n",
    "blank_poly = gv.Polygons([]).redim.range(Longitude=xlim, Latitude=ylim)\n",
    "\n",
    "# set PolyDraw for creation and PolyEdit for editing polygon,\n",
    "# num_objects keeps to single object at a time\n",
    "user_poly = PolyDraw(source=blank_poly, show_vertices=True, num_objects=1)\n",
    "user_poly_edit = PolyEdit(source=blank_poly)\n",
    "\n",
    "# create plots\n",
    "## active_tools set to allow instant polygon drawing\n",
    "basemap.options(width=700, height=400) * blank_poly.options(\n",
    "    active_tools=[\"poly_draw\"], fill_alpha=0.2, line_color=\"black\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code block pulls the latitude and longitude coordinates for the polygon vertices that were just drawn and creates a `geopandas` GeoDataFrame object. More info about `geopandas` [can be found here](https://geopandas.org/en/stable/docs/user_guide.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract lists of lat/long coordinates\n",
    "long = user_poly.data[\"xs\"][0]\n",
    "lat = user_poly.data[\"ys\"][0]\n",
    "\n",
    "# create zip of polygon vertices\n",
    "vertices = zip(long, lat)\n",
    "\n",
    "# construct polygon in GDF\n",
    "polygon = gpd.GeoDataFrame(index=[0], crs=\"EPSG:4326\", geometry=[Polygon(vertices)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot polygon to confirm the shape matches what was drawn\n",
    "polygon.hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polygon can then be using to clip rasters or other vector data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The second method will be importing the HUC6 boundaries using the PyGeoHydro library. PyGeoHydro is a part of the HyRiver family of libraries and is documented [here](https://docs.hyriver.io/autoapi/pygeohydro/index.html).\n",
    "\n",
    "The following cell queries the Water Boundary Dataset HUC6 layer and returns a GeoDataFrame from the .byids() function by examining the \"huc6\" field (first argument) for the list of HUC6 id's (second argument). The two HUC6 ids used here correspond to the Delware River Basin (DRB).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drb = WBD(\"huc6\", outfields=[\"huc6\", \"name\"]).byids(\"huc6\", [\"020401\", \"020402\"])\n",
    "drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see there are two polygons in the GeoDataFrame and plotting them confirms this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drb.hvplot(line_color=\"orange\", color=\"purple\", line_width=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to use geometries to refine datasets to an AOI, it is best to have a single, concise geometry. We'll combine the two polygons in the next code cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column where all entries have the same value\n",
    "drb[\"name\"] = \"DRB\"\n",
    "\n",
    "# dissolve by that column\n",
    "drb = drb.dissolve(by=\"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure it worked by examing the tabular and spatial data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabular\n",
    "drb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial\n",
    "drb.hvplot(line_color=\"orange\", color=\"purple\", linewidth=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Step 4: Putting it together: Process CONUS404 to variable and spatial extent**\n",
    "\n",
    "In this section we are going to put together some skills we have learned so far: bring in CONUS404, select our variables, then clip to our spatial extent. This assumes that the notebook is being run on the ESIP QHub. If being run on HPC then comment/uncomment the datasets as needed.\n",
    "\n",
    "Variables: Accumulated precipitation (PREC_ACC_NC), air temperature (TK), and (calculated) surface net radiation (RNET) <br>\n",
    "Spatial extent: Delaware River Basin (DRB)<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dask array from dataset\n",
    "# dataset set above in Step 2\n",
    "ds = cat[dataset].to_dask()\n",
    "\n",
    "# copy crs data variable for later\n",
    "ds_crs_array = ds.variables[\"crs\"]\n",
    "\n",
    "# parse spatial information from CF conventions\n",
    "ds = ds.metpy.parse_cf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the coordinate reference system (CRS) information from CONUS404 dataset to use when setting the CRSs for other datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crs = ds[\"TK\"].metpy.cartopy_crs\n",
    "# crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference datasets that that are brought in might need to be sliced to the same time period as the CONUS404 dataset. Set the start and end dates for the CONUS404 dataset to use later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the minimum time coordinate\n",
    "start_date = ds.coords[\"time\"].values.min()\n",
    "\n",
    "# convert to datetime then extract the year-month as a string \"YYYYmm\"\n",
    "start_date = pd.to_datetime(start_date).strftime(\"%Y-%m\")\n",
    "\n",
    "# add the first day of the month to the date\n",
    "start_date = f\"{start_date}-01\"\n",
    "\n",
    "# get the maximum time coordinate\n",
    "end_date = ds.coords[\"time\"].values.max()\n",
    "\n",
    "# convert to datetime then extract the year-month as a string \"YYYYmm\"\n",
    "end_date = pd.to_datetime(end_date).strftime(\"%Y-%m\")\n",
    "\n",
    "# as the end date of months vary, use the monthrange function,\n",
    "# which returns a tuple of integers as (firstDay, lastDay)\n",
    "# extract the lastDay integer by indexing [1] and convert it to string\n",
    "last_day = calendar.monthrange(int(end_date[0:4]), int(end_date[-2:]))[1]\n",
    "last_day = str(last_day)\n",
    "\n",
    "# add last_data to end_date\n",
    "end_date = f\"{end_date}-{last_day}\"\n",
    "\n",
    "print(\"Start date:\", start_date, \"\\nEnd date:\", end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a subset of desired data variables and then subset the variables:\n",
    "\n",
    "<table align=\"left\">\n",
    "  <tr>\n",
    "    <th>Short name</th>\n",
    "    <th>Long name</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>PREC_ACC_NC</td>\n",
    "    <td>accumulated prescipitation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>TK</td>\n",
    "    <td>average temperature</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ACSWDNB</td>\n",
    "    <td>accumulated downwelling surface flux of shortwave radiation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ACSWUPB</td>\n",
    "    <td>accumulated upwelling surface flux of shortwave radiation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ACLWDNB</td>\n",
    "    <td>accumulated downwelling surface flux of longwave radiation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ACLWUPB</td>\n",
    "    <td>accumulated upwelling surface flux of longwave radiation</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset data variables\n",
    "c404_variables = [\n",
    "    \"PREC_ACC_NC\",\n",
    "    \"TK\",\n",
    "    \"ACSWDNB\",\n",
    "    \"ACSWUPB\",\n",
    "    \"ACLWDNB\",\n",
    "    \"ACLWUPB\",\n",
    "    \"crs\",\n",
    "]\n",
    "c404 = ds[c404_variables]\n",
    "\n",
    "# persist into memory to make calculations go faster\n",
    "# c404.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now bring in our AOI boundaries and as before, we will use the DRB HUC6 polygons. We will calculate the bounding box of the DRB, which will be used in the next step to pare the CONUS404 data down spatially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in boundaries of DRB and create single polygon\n",
    "drb = WBD(\"huc6\", outfields=[\"huc6\", \"name\"]).byids(\"huc6\", [\"020401\", \"020402\"])\n",
    "# create a column where all entries have the same value\n",
    "drb[\"name\"] = \"DRB\"\n",
    "\n",
    "# dissolve by that column\n",
    "drb = drb.dissolve(by=\"name\")\n",
    "\n",
    "# set CRS to match ds\n",
    "drb = drb.iloc[[0]].to_crs(crs)\n",
    "\n",
    "# tuple of bounding box\n",
    "drb_bbox = list(drb.total_bounds)\n",
    "\n",
    "# visualize\n",
    "drb.hvplot(line_color=\"orange\", color=\"purple\", line_width=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now clip by the bounding box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write CRS\n",
    "c404.rio.write_crs(crs, inplace=True)\n",
    "\n",
    "# perform clip by bounding box\n",
    "c404_drb = c404.rio.clip_box(\n",
    "    minx=drb_bbox[0], miny=drb_bbox[1], maxx=drb_bbox[2], maxy=drb_bbox[3], crs=crs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_drb[\"TK\"].isel(time=-1).hvplot(\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    crs=crs,\n",
    "    geo=True,\n",
    "    rasterize=True,\n",
    "    cmap=\"turbo\",\n",
    "    tiles=\"OSM\",\n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a little more processing to do before the dataset is ready for analysis. We need to:\n",
    "\n",
    "1. Calcuate RNET using the radiation columns\n",
    "2. Resample and aggregate the data from the input hourly timestep to the desired time-step (1 month)\n",
    "3. Perform unit conversion to align with desired units\n",
    "\n",
    "RNET is calculated using this equation:<br><br><br>\n",
    "\\begin{equation}RNET = SWDN + LWDN - SWUP - LWUP\\end{equation}\n",
    "\n",
    "and those variables are detailed below.\n",
    "\n",
    "<table align=\"left\" style=\"page-break-after: always;\">\n",
    "  <tr>\n",
    "    <th>Short name</th>\n",
    "    <th>Long name</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>SWDN</td>\n",
    "    <td>downwelling surface flux of shortwave radiation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>LWDN</td>\n",
    "    <td>downwelling surface flux of longwave radiation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>SWUP</td>\n",
    "    <td>upwelling surface flux of shortwave radiation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>LWUP</td>\n",
    "    <td>upwelling surface flux of longwave radiation</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the RNET components is calculated using different data variables.\n",
    "\n",
    "For each component we convert from units of J/m2 to W/m2 by differencing hourly values of ACSWDNB and dividing by the number of accumulated seconds. For example, to compute SWDN in W/m2, we use:\n",
    "\n",
    "\\begin{equation}SWDN = (ACSWDNB[h] - ACSWDNB[h-1]) / 3600\\end{equation}\n",
    "\n",
    "which specifies values at the current hour (h) and the previous hour (h-1).\n",
    "\n",
    "To code, first we define all values at time index h (the second timestep since indices start at 0)....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACSWDNB = c404_drb[\"ACSWDNB\"][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we define all values at time index h-1. Additionally we need to ensure that ACSWDNB and ACSWDNB1 have the same time indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACSWDNB1 = c404_drb[\"ACSWDNB\"][:-1]\n",
    "ACSWDNB1.coords[\"time\"] = ACSWDNB.coords[\"time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm both time coords are the same length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ACSWDNB.coords[\"time\"].values) == len(ACSWDNB1.coords[\"time\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate SWDN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWDN = (ACSWDNB - ACSWDNB1) / 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some of the results. Note the difference in time of day in the two maps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWDN.sel(time=\"2000-06-01 10:00\").hvplot(\n",
    "    x=\"x\", y=\"y\", crs=crs, rasterize=True, cmap=\"turbo\", tiles=\"OSM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWDN.sel(time=\"2000-06-01 23:00\").hvplot(\n",
    "    x=\"x\", y=\"y\", crs=crs, rasterize=True, cmap=\"turbo\", tiles=\"OSM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, pad a NaN to the beginning to match original datasets dimension length and then reset to those dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWDN = SWDN.pad({\"time\": (1, 0)})\n",
    "SWDN.coords[\"time\"] = c404_drb[\"ACSWDNB\"].coords[\"time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will do the same steps to calculate the other three components of _RNET_.\n",
    "\n",
    "_SWUP_\n",
    "\\begin{equation}SWUP = (ACSWUPB[h] - ACSWUPB[h-1]) / 3600\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (h) variables\n",
    "ACSWUPB = c404_drb[\"ACSWUPB\"][1:]\n",
    "\n",
    "# (h-1) variables)\n",
    "ACSWUPB1 = c404_drb[\"ACSWUPB\"][:-1]\n",
    "ACSWUPB1.coords[\"time\"] = ACSWUPB.coords[\"time\"]\n",
    "\n",
    "# calculate variable\n",
    "SWUP = (ACSWUPB - ACSWUPB1) / 3600\n",
    "\n",
    "# pad to match c404_drb time dimension\n",
    "SWUP = SWUP.pad({\"time\": (1, 0)})\n",
    "SWUP.coords[\"time\"] = c404_drb[\"ACSWUPB\"].coords[\"time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_LWDN_\n",
    "\n",
    "\\begin{equation}LWDN = (ACLWDNB[h] - ACLWDNB[h-1]) / 3600\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (h) variables\n",
    "ACLWDNB = c404_drb[\"ACLWDNB\"][1:]\n",
    "\n",
    "# (h-1) variables)\n",
    "ACLWDNB1 = c404_drb[\"ACLWDNB\"][:-1]\n",
    "ACLWDNB1.coords[\"time\"] = ACLWDNB.coords[\"time\"]\n",
    "\n",
    "# calculate variable\n",
    "LWDN = (ACLWDNB - ACLWDNB1) / 3600\n",
    "\n",
    "# pad to match c404_drb time dimension\n",
    "LWDN = LWDN.pad({\"time\": (1, 0)})\n",
    "LWDN.coords[\"time\"] = c404_drb[\"ACLWDNB\"].coords[\"time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_LWUP_\n",
    "\n",
    "\\begin{equation}LWUP = (ACLWUPB[h] - ACLWUPB[h-1]) / 3600\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (h) variables\n",
    "ACLWUPB = c404_drb[\"ACLWUPB\"][1:]\n",
    "\n",
    "# (h-1) variables)\n",
    "ACLWUPB1 = c404_drb[\"ACLWUPB\"][:-1]\n",
    "ACLWUPB1.coords[\"time\"] = ACLWUPB.coords[\"time\"]\n",
    "\n",
    "# calculate variable\n",
    "LWUP = (ACLWUPB - ACLWUPB1) / 3600\n",
    "\n",
    "# pad to match c404_drb time dimension\n",
    "LWUP = LWUP.pad({\"time\": (1, 0)})\n",
    "LWUP.coords[\"time\"] = c404_drb[\"ACLWUPB\"].coords[\"time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the components, calculate RNET...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate\n",
    "RNET = SWDN + LWDN - SWUP - LWUP\n",
    "# RNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...assign its attributes...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of attributes\n",
    "RNET_attrs = {\n",
    "    \"description\": \"SURFACE NET RADIATION\",\n",
    "    \"grid_mapping\": \"crs\",\n",
    "    \"long_name\": \"Surface net radiation\",\n",
    "    \"units\": \"W m-2\",\n",
    "}\n",
    "\n",
    "# assign attributes\n",
    "RNET = RNET.assign_attrs(RNET_attrs)\n",
    "# RNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and assign it back to CONUS404\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_drb = c404_drb.assign(RNET=RNET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now drop the extra radiation variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_variables_drop = [\"ACSWDNB\", \"ACSWUPB\", \"ACLWDNB\", \"ACLWUPB\"]\n",
    "c404_drb = c404_drb.drop_vars(c404_variables_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize RNET, which is accumulated net radiation in watts per m^2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_drb[\"RNET\"].sel(time=\"2000-06-01 23:00\").hvplot(\n",
    "    x=\"x\", y=\"y\", crs=crs, rasterize=True, cmap=\"turbo\", tiles=\"OSM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset has been clipped to the area of interest and all the needed variables calculated. The final bit of engineering is resampling the data from hourly to monthly. Xarray has a built in method `resample()` to do this but it only allows a single aggregation method for all the DataArrays in the DataSet.\n",
    "\n",
    "Unfortunately, the DataArrays need different aggregation techniques: we want to sum values of _PREC_ACC_NC_, but compute means of _RNET_ and _TK_. We'll accomplish this by splitting _PREC_ACC_NC_ from the dataset, resampling it and the dataset separately, then merging them back together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy data\n",
    "PREC_ACC_NC = c404_drb[\"PREC_ACC_NC\"]\n",
    "\n",
    "# resample to 1 month by summing\n",
    "PREC_ACC_NC = PREC_ACC_NC.resample(time=\"1M\").sum()\n",
    "\n",
    "# copy attributes from original\n",
    "PREC_ACC_NC.attrs = c404_drb[\"PREC_ACC_NC\"].attrs\n",
    "\n",
    "# drop from c404_drb\n",
    "c404_drb = c404_drb.drop_vars(\"PREC_ACC_NC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample the dataset -- which now excludes PREC_ACC_NC -- and aggregate by mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.set_options(keep_attrs=True):  # needed, otherwise drops attributes\n",
    "    c404_drb = c404_drb.resample(time=\"1M\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add back the resampled _PREC_ACC_NC_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_drb[\"PREC_ACC_NC\"] = PREC_ACC_NC\n",
    "# c404_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct attributes as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_drb.PREC_ACC_NC.attrs[\"integration_length\"] = \"accumulated over past month\"\n",
    "c404_drb.PREC_ACC_NC.attrs[\"grid_mapping\"] = \"crs\"\n",
    "c404_drb.RNET.attrs[\"description\"] = \"MEAN RADIATION FROM PAST MONTH FOR BUCKET\"\n",
    "c404_drb.RNET.attrs[\"grid_mapping\"] = \"crs\"\n",
    "c404_drb.TK.attrs[\"description\"] = (\n",
    "    \"MEAN AIR TEMPERATURE AT THE LOWEST MODEL LEVEL OVER THE PAST MONTH\"\n",
    ")\n",
    "c404_drb.TK.attrs[\"grid_mapping\"] = \"crs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop coordinates that will not be need in the exported dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_drb = c404_drb.reset_coords([\"metpy_crs\", \"crs\"], drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the crs data variable from the original CONUS404 dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_drb[\"crs\"] = ds_crs_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set chunk size so entire dataset is single chunk (the data is small enough)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now it is time to export the data. For the purposes of this tutorial, you will be shown **how** to export the data to a requester pays (think permissioned) S3 bucket but will **not actually** export the data. The intermediate datasets created with the above and below code have been saved in a read-only space for the next two notebooks.\n",
    "\n",
    "For working with climate and forcings data using the CF conventions, NETCDF is the format of choice.\n",
    "\n",
    "First, `fsspec` is used to create an object to write the file to. Note that in the `open` method, it says `simplecache` when building the S3 file name. This has been found to be the easiest way to write files to a permissioned space using `fsspec`. `simplecache` works by locally caching the file before writing _that_ copy to S3 and then deleting the locally cached file.\n",
    "\n",
    "Next, the object is written to using `xarray`'s built-in `to_netcdf` method.\n",
    "\n",
    "**Note**: running the code below will result in an error and not work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outfile = fsspec.open('simplecache::s3://mybucket/c404_drb.nc',\n",
    "#                       mode='wb', s3=dict(profile='profile'))\n",
    "# with outfile as f:\n",
    "#     c404_drb.load().to_netcdf(f, compute=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Step 5: Prepare the gridded reference data**\n",
    "\n",
    "Now that the CONUS404 dataset has been preprocessed, we need to do the same with datasets used for comparison with the forcings data. In this section, data will be brought in from several sources and preprocessed in data-type-appropriate ways.\n",
    "\n",
    "We'll start by processing the rest of the gridded datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRISM V2 data\n",
    "\n",
    "This time we will open the PRISM V2 dataset, temporarally slice it, spatially clip it, and refine the data. The PRISM V2 is daily data with a spatial resolution of 800 meters and we will be using the precipitation, maximum temperature, and minimum temperature data variables. The latter two will be used to calculate mean temperature. Finally, the PRISM V2 dataset is projected in the [NAD83 coordinate reference system](https://epsg.io/4269). Many of the steps will look the same as the CONUS404 dataset so there will be less explanation of the steps. This dataset has been brought into the NHGF development server for storage but it is also available from the [PRISM Climate Group at Oregon State University](https://prism.oregonstate.edu/downloads/).\n",
    "\n",
    "**NOTE:** If you are unable to access the data, make sure you have updated WMA AWS credentials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create filesystem object\n",
    "fs = fsspec.filesystem(\"s3\", anon=False, requester_pays=True, skip_instance_cache=True)\n",
    "\n",
    "# range of years for data\n",
    "prism_years = range(1979, 2021, 1)\n",
    "\n",
    "# construct list of datasets for each year\n",
    "chunks = {\"time\": 6, \"lon\": 703, \"lat\": 311}\n",
    "pr = [\n",
    "    xr.open_dataset(\n",
    "        fs.open(f\"s3://nhgf-development/thredds/prism_v2/prism_{str(year)}.nc\"),\n",
    "        chunks=chunks,\n",
    "        decode_coords=\"all\",\n",
    "    )\n",
    "    for year in prism_years\n",
    "]\n",
    "\n",
    "# concat datasets\n",
    "prism = xr.concat(pr, dim=\"time\")\n",
    "\n",
    "# drop time_bnds\n",
    "prism = prism.drop_vars(\"time_bnds\")\n",
    "\n",
    "# set NAD83 with EPSG code 4269\n",
    "prism_crs = 4269\n",
    "\n",
    "# write crs to prism\n",
    "prism.rio.write_crs(prism_crs, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the dimensions to match CF conventions used by rioxarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism = prism.rename({\"lon\": \"x\", \"lat\": \"y\", \"ppt\": \"PREC_ACC_NC\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bring in DRB boundaries with NAD83 crs...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in boundaries of DRB and create single polygon\n",
    "drb_NAD83 = WBD(\"huc6\", outfields=[\"huc6\", \"name\"]).byids(\"huc6\", [\"020401\", \"020402\"])\n",
    "\n",
    "# create a column where all entries have the same value and then...\n",
    "drb_NAD83[\"name\"] = \"DRB\"\n",
    "\n",
    "# dissolve by that column\n",
    "drb_NAD83 = drb_NAD83.dissolve(by=\"name\")\n",
    "\n",
    "# set CRS to match prism\n",
    "drb_NAD83 = drb_NAD83.iloc[[0]].to_crs(prism_crs)\n",
    "\n",
    "# get bounds of NAD83 drb\n",
    "drb_NAD83_bounds = list(drb_NAD83.total_bounds)\n",
    "\n",
    "# visualize\n",
    "# drb_NAD83.hvplot(edgecolor=\"orange\", facecolor=\"purple\", linewidth=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and perform clip to DRB extent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip to DRB bbox\n",
    "prism_drb = prism.rio.clip_box(\n",
    "    minx=drb_NAD83_bounds[0],\n",
    "    miny=drb_NAD83_bounds[1],\n",
    "    maxx=drb_NAD83_bounds[2],\n",
    "    maxy=drb_NAD83_bounds[3],\n",
    ")\n",
    "\n",
    "# slice time\n",
    "prism_drb = prism_drb.sel(time=slice(start_date, end_date))\n",
    "prism_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the clipped data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_drb.PREC_ACC_NC.sel(time=\"2000-06-01\", method=\"nearest\").hvplot(\n",
    "    x=\"x\", y=\"y\", rasterize=True, tiles=\"OSM\", alpha=0.7, cmap=\"turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the subset CONUS404 and PRISM data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [caelan] I think it could be nice here to have a comparison plot for\n",
    "# conus 404 vs prism.\n",
    "# (c404_drb[\"PREC_ACC_NC\"]\n",
    "#  .sel(time=\"2000-05-31\", method = 'nearest')\n",
    "#  .hvplot(x='x', y='y', crs=crs, rasterize=True, cmap='turbo', tiles='OSM'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean monthly tempertaure and convert to Kelvin and populate its attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean temperature in Kelvin\n",
    "prism_drb = prism_drb.assign(TK=((prism_drb.tmn + prism_drb.tmx) / 2) + 273.15)\n",
    "\n",
    "# dictionary of attributes\n",
    "prism_tk_attrs = {\"units\": \"K\", \"long_name\": \"Mean monthly temperature\"}\n",
    "\n",
    "# assign attributes\n",
    "prism_drb[\"TK\"] = prism_drb[\"TK\"].assign_attrs(prism_tk_attrs)\n",
    "\n",
    "prism_drb.PREC_ACC_NC.attrs[\"units\"] = \"mm\"\n",
    "prism_drb.PREC_ACC_NC.attrs[\"long_name\"] = \"Accumulated grid scale precipitation\"\n",
    "\n",
    "# drop variables\n",
    "prism_drb = prism_drb.drop_vars([\"tmn\", \"tmx\"])\n",
    "\n",
    "# drop spatial_ref coord in order to export later\n",
    "prism_drb = prism_drb.reset_coords(\"spatial_ref\", drop=True)\n",
    "\n",
    "# rechunk to make export easier\n",
    "prism_drb = prism_drb.chunk(chunks={\"y\": 98, \"x\": 48, \"time\": 492})\n",
    "\n",
    "# check out final dataset\n",
    "prism_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export data as a NetCDF file (this code is once again only an example and will result in an error if ran).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outfile = fsspec.open('simplecache::s3://mybucket/prism_drb.nc',\n",
    "#                       mode='wb', s3=dict(profile='profile'))\n",
    "# with outfile as f:\n",
    "#     prism_drb.load().to_netcdf(f, compute=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NASA's CERES-EBAF Level 3b Dataset\n",
    "\n",
    "The [CERES-EBAF Summary](https://ceres.larc.nasa.gov/documents/DQ_summaries/CERES_EBAF_Ed4.1_DQS.pdf) provides important background information and insights about the data. The [download page](https://ceres.larc.nasa.gov/data/#ebaf-level-3) provides a quicks summary as well. The CERES-EBAF dataset includes radiation data at a monthy time step and at 1-degree latitude-longitude resolution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in ceres-ebaf\n",
    "ceres = xr.open_dataset(\n",
    "    \"https://opendap.larc.nasa.gov/opendap/CERES/EBAF/Edition4.1/CERES_EBAF_Edition4.1_200003-202203.nc\",\n",
    "    decode_coords=\"all\",\n",
    ")\n",
    "\n",
    "# parse_cf\n",
    "ceres = ceres.metpy.parse_cf()\n",
    "\n",
    "# extract crs\n",
    "ceres_crs = ceres.sfc_net_tot_all_mon.metpy.cartopy_crs\n",
    "\n",
    "# rename\n",
    "ceres = ceres.rename({\"lon\": \"x\", \"lat\": \"y\", \"sfc_net_tot_all_mon\": \"RNET\"})\n",
    "\n",
    "# x is in 0:360, need it in -180:180\n",
    "ceres = ceres.assign_coords(x=(((ceres.x + 180) % 360) - 180)).sortby(\"x\")\n",
    "\n",
    "# pare down dataset\n",
    "ceres = ceres[\"RNET\"]\n",
    "\n",
    "# set crs\n",
    "ceres.rio.write_crs(ceres_crs, inplace=True)\n",
    "\n",
    "# bring in boundaries of DRB and create single polygon\n",
    "drb_PC = WBD(\"huc6\", outfields=[\"huc6\", \"name\"]).byids(\"huc6\", [\"020401\", \"020402\"])\n",
    "\n",
    "# create a column where all entries have the same value\n",
    "drb_PC[\"name\"] = \"DRB\"\n",
    "\n",
    "# dissolve by that column\n",
    "drb_PC = drb_PC.dissolve(by=\"name\")\n",
    "\n",
    "# set CRS to match ds\n",
    "drb_PC = drb_PC.iloc[[0]].to_crs(ceres_crs)\n",
    "\n",
    "# get bounds of NAD83 drb\n",
    "drb_PC_bounds = list(drb_PC.total_bounds)\n",
    "\n",
    "# clip to DRB bbox\n",
    "ceres_drb = ceres.rio.clip_box(\n",
    "    minx=drb_PC_bounds[0],\n",
    "    miny=drb_PC_bounds[1],\n",
    "    maxx=drb_PC_bounds[2],\n",
    "    maxy=drb_PC_bounds[3],\n",
    ")\n",
    "\n",
    "# slice time\n",
    "ceres_drb = ceres_drb.sel(time=slice(start_date, end_date))\n",
    "\n",
    "# drop spatial_ref coord in order to export later\n",
    "ceres_drb = ceres_drb.reset_coords([\"metpy_crs\"], drop=True)\n",
    "\n",
    "# check final dataset\n",
    "ceres_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceres_drb.hvplot(x=\"x\", y=\"y\", rasterize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export data as a NetCDF file (this code is once again only an example and will result in an error if run).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outfile = fsspec.open('simplecache::s3://mybucket/ceres_drb.nc',\n",
    "#                       mode='wb', s3=dict(profile='profile'))\n",
    "# with outfile as f:\n",
    "#     ceres_drb.load().to_netcdf(f, compute=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Step 6: Prepare station reference data**\n",
    "\n",
    "Now its on to the point datasets (stations).\n",
    "\n",
    "#### NOAA's Global Historical Climate Network - Daily (GHCN) Dataset\n",
    "\n",
    "The GHCN dataset contains station data for temperature, radiation, and precipitation data variables at a daily temporal resolution all from across the world. We are interested in a subset of these, the United States Historical Climate Network (USHCN). It is always important to review any readme or metadata files for the data you wish to bring in. The [GHCN readme](https://noaa-ghcn-pds.s3.amazonaws.com/readme.txt) is useful because it explains what is in the S3 bucket, the various columns in the datasets, and other information. When we later call in the observational data, the [by station readme](https://noaa-ghcn-pds.s3.amazonaws.com/readme-by_station.txt) provides a more detailed explanation of the data there.\n",
    "\n",
    "The steps for working with the GHCN data will be that we first read in the data that describe the stations (metadata); then we'll read in time-series data for each station.\n",
    "\n",
    "After reading the metadata for the file, we can see that only the first three columns are needed to map the stations: the station ID, latitude, and longitude. However, we want to make sure that we are only using USHCN stations so we need to also use the (G)HCN/CRN Flag column to filter to USHCN sites.\n",
    "\n",
    "Start by getting a list of stations from the AWS S3 bucket where the daily data is housed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghcn_all = pd.read_csv(\"s3://noaa-ghcn-pds/ghcnd-stations.txt\", sep=\"\\t\", header=None)\n",
    "ghcn_all.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the read does not recognize the file's columns and the data are stored as one column per record. So, we have to split the record to create the columns that we need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghcn_all = ghcn_all[0].str.split(\" +\", expand=True)\n",
    "ghcn_all.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns 0-3 now look as we'd expect. However, column 4 is where it starts to get messy as the method for expanding the columns has split up the station names at the spaces between. This means that the HCN flag, which we would expect to be in column 6, could be in columns 6-13. Thankfully the pandas `loc` function handles this problem easily so that we can successfully filter just the 'HCN' stations. Note that the | represents the \"or\" logic and will return any row that matches any of the conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ushcn = ghcn_all.loc[\n",
    "    (ghcn_all[6] == \"HCN\")\n",
    "    | (ghcn_all[7] == \"HCN\")\n",
    "    | (ghcn_all[8] == \"HCN\")\n",
    "    | (ghcn_all[9] == \"HCN\")\n",
    "    | (ghcn_all[10] == \"HCN\")\n",
    "    | (ghcn_all[11] == \"HCN\")\n",
    "    | (ghcn_all[12] == \"HCN\")\n",
    "    | (ghcn_all[13] == \"HCN\")\n",
    "].copy()\n",
    "ushcn = (\n",
    "    ushcn.iloc[:, 0:3].rename({0: \"station\", 1: \"lat\", 2: \"lon\"}, axis=1).copy()\n",
    ")  # after the search, trim the columns and rename to get the data to\n",
    "# what is needed to map\n",
    "ushcn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to clip the points to only those in the DRB. We do that by using the latitude and longitude to create a GeoDataFrame...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ushcn_gdf = gpd.GeoDataFrame(\n",
    "    ushcn, geometry=gpd.points_from_xy(ushcn[\"lon\"], ushcn[\"lat\"], crs=\"EPSG:4326\")\n",
    ")\n",
    "\n",
    "# convert to same crs as drb\n",
    "ushcn_gdf = ushcn_gdf.to_crs(crs)\n",
    "\n",
    "ushcn_gdf.hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...followed by clipping using the _drb_ geodataframe above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_drb_gdf = gpd.clip(ushcn_gdf, drb)\n",
    "# hcn_drb_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to pull in the tabular data for all of the DRB stations indexed by time. These are stored on AWS in an individual CSV file for each station named _station.csv_. So, we need to get the station IDs from our DRB subset in order to create a list of URLs for the corresponding csv files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_drb_data_url = [\n",
    "    f\"s3://noaa-ghcn-pds/csv/by_station/{station}.csv\"\n",
    "    for station in hcn_drb_gdf[\"station\"].unique().tolist()\n",
    "]\n",
    "# print(hcn_drb_data_url[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(hcn_drb_data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now pass that list of URLs to _dask.dataframe.read_csv_, which will read the data files in parallel. We'll then refine the entries to the temporal range of CONUS404.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_options = dict(anon=True, requester_pays=False)\n",
    "hcn_drb_data = dd.read_csv(\n",
    "    hcn_drb_data_url,\n",
    "    parse_dates=[\"DATE\"],\n",
    "    usecols=[\"ID\", \"DATE\", \"ELEMENT\", \"DATA_VALUE\"],\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "\n",
    "hcn_drb_data = hcn_drb_data.loc[\n",
    "    (hcn_drb_data[\"DATE\"] >= start_date) & (hcn_drb_data[\"DATE\"] <= end_date)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we'll refine the dataframe by a list of elements and then compute it, which is a `dask` method for reading the dataset into memory.\n",
    "\n",
    "##### Note: We are using TMAX and TMIN rather than TAVG as TAVG has no records prior to 1998.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of elements we are interested in\n",
    "element_list = [\"PRCP\", \"TMAX\", \"TMIN\"]\n",
    "\n",
    "hcn_drb_data = hcn_drb_data.loc[hcn_drb_data[\"ELEMENT\"].isin(element_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape\n",
    "hcn_drb_data.compute().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much memory does it take up?\n",
    "# hcn_drb_data.compute().memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dask dataframe is about 21 mb in size.\n",
    "\n",
    "Similar to the CONUS404 data, we have a little more engineering to do with the data. We need to calculate the average temperatue using TMIN and TMAX (in Kelvin) as well as resample the data from a 1 day to a 1 month interval. We'll convert the Dask Dataframe into a Pandas Dataframe to do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_drb_data_df = hcn_drb_data.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by whittling down to our two temperature elements, dropping the _ELEMENT_ column, and grouping our data by _ID_ and _DATE_ in order to take the mean of _TMIN_ and _TMAX_ and convert this to degrees Kelvin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paring down data\n",
    "hcn_drb_tk = hcn_drb_data_df.loc[hcn_drb_data_df[\"ELEMENT\"].isin([\"TMAX\", \"TMIN\"])]\n",
    "\n",
    "# dropping ELEMENT\n",
    "hcn_drb_tk = hcn_drb_tk.drop(\"ELEMENT\", axis=1)\n",
    "\n",
    "# calculate mean temperature for each station and date\n",
    "hcn_drb_tk = hcn_drb_tk.groupby([\"ID\", \"DATE\"]).mean()\n",
    "\n",
    "# rename the DATA_VALUE column to TK\n",
    "hcn_drb_tk = hcn_drb_tk.rename({\"DATA_VALUE\": \"TK\"}, axis=1)\n",
    "\n",
    "# convert from tenths of degrees Celsius to degrees Kelvin\n",
    "hcn_drb_tk[\"TK\"] = (hcn_drb_tk[\"TK\"] * 0.1) + 273.15\n",
    "\n",
    "# reset the index\n",
    "hcn_drb_tk.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolate the _PRCP_ element, rename for consistency, and convert from units of tenths of mm to mm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_drb_prcp = hcn_drb_data_df.loc[hcn_drb_data_df[\"ELEMENT\"] == \"PRCP\"].copy()\n",
    "\n",
    "# dropping ELEMENT\n",
    "hcn_drb_prcp = hcn_drb_prcp.drop(\"ELEMENT\", axis=1)\n",
    "\n",
    "# rename the DATA_VALUE column to PREC_ACC_NC\n",
    "hcn_drb_prcp = hcn_drb_prcp.rename({\"DATA_VALUE\": \"PREC_ACC_NC\"}, axis=1)\n",
    "\n",
    "# convert from tenths of mm to mm\n",
    "hcn_drb_prcp[\"PREC_ACC_NC\"] = hcn_drb_prcp[\"PREC_ACC_NC\"] * 0.1\n",
    "\n",
    "# reset the index\n",
    "hcn_drb_prcp.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# hcn_drb_prcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine _TK_ and _PRCP_ DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_drb = hcn_drb_tk.merge(hcn_drb_prcp, how=\"inner\", on=[\"ID\", \"DATE\"])\n",
    "# hcn_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then resample to 1 month and aggregate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_drb = (\n",
    "    hcn_drb.groupby(\"ID\")\n",
    "    .resample(\"1M\", on=\"DATE\")\n",
    "    .agg({\"TK\": \"mean\", \"PREC_ACC_NC\": \"sum\"})\n",
    "    .reset_index(drop=False)\n",
    ")\n",
    "\n",
    "# round TK\n",
    "hcn_drb.TK = round(hcn_drb.TK, 2)\n",
    "# hcn_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add the latitude and longitude coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_drb_coords = pd.DataFrame(hcn_drb_gdf.drop(columns=\"geometry\"))\n",
    "\n",
    "# rename ID columnt o match drb_hcn\n",
    "hcn_drb_coords = hcn_drb_coords.rename(\n",
    "    {\"station\": \"ID\", \"lon\": \"LONGITUDE\", \"lat\": \"LATITUDE\"}, axis=1\n",
    ")\n",
    "\n",
    "# merge\n",
    "hcn_drb = hcn_drb.merge(hcn_drb_coords, on=\"ID\", how=\"left\")\n",
    "\n",
    "hcn_drb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the station data will be a bit different. First, the data type is different as we used the `xarray` dataset for the gridded data and a `pandas` dataframe for the station. Since our station data is essentially column-oriented tabular data, a good file type for storage is [parquet](https://parquet.apache.org/). An additional benefit is that `pandas` uses `fsspec` in the background to handle IO so it will use environmental variables such as an AWS ID and Secret Key to handle writing to many permissioned spaces. An example of this would look like (this is an example and will result in an error if run):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hcn_drb.to_parquet(\"s3://file/path/to/hcn_drb.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOAA's Global Climate Reference Network (GCRN) Dataset\n",
    "\n",
    "For the GCRN, we will be using the accumulated precipitation and average temperature variables. The GCRN will be subset to United States Climate Reference Network (USCRN) stations. The documentation for the GCRN dataset can be found at [the FTP site](https://www.ncei.noaa.gov/pub/data/uscrn/products/monthly01/readme.txt). As this data will be brought in over an FTP call, the process will be a bit different than the other datasets, using _fsspec_ to make FTP call to NOAA for CRN data. <br>\n",
    "\n",
    "First, create a FTP file system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FTPFileSystem(\"ftp.ncei.noaa.gov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the file type is _tab-separated values (tsv)_, we will use the _pd.read_table_ function to create a Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uscrn_all = pd.read_table(fs.open(\"/pub/data/uscrn/products/stations.tsv\"))\n",
    "uscrn_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now turn into GeoDataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uscrn_gdf = gpd.GeoDataFrame(\n",
    "    uscrn_all,\n",
    "    geometry=gpd.points_from_xy(uscrn_all[\"LONGITUDE\"], uscrn_all[\"LATITUDE\"]),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "\n",
    "# convert to same crs as drb\n",
    "uscrn_gdf = uscrn_gdf.to_crs(crs)\n",
    "\n",
    "# uscrn_gdf.hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find which USCRN sites are in DRB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb_gdf = gpd.clip(uscrn_gdf, drb)\n",
    "# crn_drb_gdf.hvplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now would know what CRN sites are in the Delaware River Basin. However, note that the USCRN station network is much sparser than GHCN and only one USCRN station is located in the DRB. We must now retrieve the data for this site from the FTP server.\n",
    "\n",
    "First, we'll get the location name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_stat_name = crn_drb_gdf[\"LOCATION\"].values.tolist()[0]\n",
    "print(crn_stat_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilize the FTP connection again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FTPFileSystem(\"ftp.ncei.noaa.gov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_glob = fs.glob(f\"/pub/data/uscrn/products/daily01/**/*{crn_stat_name}*\")\n",
    "\n",
    "# see list of files\n",
    "# for file in file_list_glob:\n",
    "#     print(file,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb_dfs = [\n",
    "    pd.read_csv(fs.open(file), header=None, sep=\"\\t\") for file in file_list_glob\n",
    "]\n",
    "\n",
    "crn_drb = pd.concat(crn_drb_dfs)\n",
    "\n",
    "# split single column into all\n",
    "crn_drb = crn_drb[0].str.split(\" +\", expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now bring in the headers for the station data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_headers = fs.open(\"/pub/data/uscrn/products/daily01/headers.txt\")\n",
    "crn_data_headers = (\n",
    "    pd.read_csv(crn_headers, sep=\"\\t\", header=None)\n",
    "    .iloc[1, :]\n",
    "    .str.split(\" +\")\n",
    "    .values.tolist()[0][0:28]\n",
    ")\n",
    "# crn_data_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the number of headers equals the number of columns in our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(crn_drb.columns) == len(crn_data_headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now rename the column headers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb.columns = crn_data_headers\n",
    "# crn_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _LST_DATE_ column is the date of the observation and its data type is currently string. We want the data type to be [datetime](https://docs.python.org/3/library/datetime.html) so we will perform this conversion using the `to_datetime` function from the `pandas` library then refine the columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb[\"DATE\"] = pd.to_datetime(crn_drb[\"LST_DATE\"])\n",
    "crn_drb = crn_drb[[\"DATE\", \"P_DAILY_CALC\", \"T_DAILY_AVG\", \"LONGITUDE\", \"LATITUDE\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you examine the data types, you'll see that the 4 columns of numbers are actually data type _object_ when we need them as numeric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets rectify that by applying the `pd.to_numeric` function to the columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = crn_drb.columns.drop(\"DATE\")\n",
    "crn_drb[cols] = crn_drb[cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "crn_drb.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now refine the data by date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb = crn_drb.loc[(crn_drb[\"DATE\"] >= start_date) & (crn_drb[\"DATE\"] <= end_date)]\n",
    "print(crn_drb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CRN dataset has many values of -9999.0, which is where a record was not recorded due to data quality or other issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the value -9999.0 is seen as a valid number, set these values to NaN so they will be ignored during calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to NaN\n",
    "crn_drb = crn_drb.replace(-9999.0, np.nan)\n",
    "\n",
    "print(crn_drb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert columns to the correct units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celsius to Kelvin\n",
    "crn_drb[\"TK\"] = crn_drb[\"T_DAILY_AVG\"] + 273.15\n",
    "\n",
    "# drop columns\n",
    "crn_drb = crn_drb.drop([\"T_DAILY_AVG\"], axis=1)\n",
    "\n",
    "# rename column\n",
    "crn_drb = crn_drb.rename({\"P_DAILY_CALC\": \"PREC_ACC_NC\"}, axis=1)\n",
    "\n",
    "print(crn_drb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample the data and round TK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb = (\n",
    "    crn_drb.resample(\"1M\", on=\"DATE\")\n",
    "    .agg({\"TK\": \"mean\", \"PREC_ACC_NC\": \"sum\", \"LATITUDE\": \"mean\", \"LONGITUDE\": \"mean\"})\n",
    "    .reset_index(drop=False)\n",
    ")\n",
    "\n",
    "# round\n",
    "crn_drb[\"TK\"] = round(crn_drb[\"TK\"], 2)\n",
    "\n",
    "\n",
    "# add a station ID column and modify DATE to be YYYY-dd\n",
    "crn_drb[\"ID\"] = crn_stat_name\n",
    "\n",
    "crn_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Export the dataset (this is an example only and will result in an error if run).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crn_drb.to_parquet(\"s3://file/path/to/crn_drb.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the final, always important step of shutting down the 'dask' client and cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "Congratulations, you made it! I know this notebook contained a lot but it serves as a practical tutorial to see how diverse data input, preparation, and data output methods are handled. The best analogy I can think of is thinking about painting: 85% of the work is preparation.\n",
    "\n",
    "The next notebook will walk you through analysing CONUS404 against the reference data sets and include topics such as area weighted zonal statistics, point extraction of gridded data, spatial statistics, and temporal statistics.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
