{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create zonal statistics and point extractions for comparing CONUS404 and reference datasets\n",
    "\n",
    "Author: Andrew Laws alaws@usgs.gov\n",
    "\n",
    "<img src='../../../doc/assets/Eval_Analysis.svg' width=600>\n",
    "\n",
    "Now that the data has been prepared, it is time to compute zonal statistics and perform point extractions. \n",
    "\n",
    "<details>\n",
    "  <summary>Guide to pre-requisites and learning outcomes...&lt;click to expand&gt;</summary>\n",
    "  \n",
    "  <table>\n",
    "    <tr>\n",
    "      <td>Pre-Requisites\n",
    "      <td>To get the most out of this notebook, you should already have an understanding of these topics: \n",
    "        <ul>\n",
    "        <li>the following summary statistics: mean, median, standard deviation, bias, mean absolute error (MAE), root mean squared error (RMSE), Pearson and Spearman correlation coefficients, and percent bias.\n",
    "        <li>the HUC (Hydrologic Unit Code) system of identification of drainage basins used by the USGS, in particular the HUC6 designation used here.\n",
    "        </ul>\n",
    "    <tr>\n",
    "      <td>Expected Results\n",
    "      <td>At the end of this notebook, you will produce: \n",
    "        <ul>\n",
    "        <li>Tables of descriptive statistics for the Delaware River Basin for each HUC6 designation within the basin, for forcing and reference datasets of precipitation, temperature, and net radiation.\n",
    "        <li>Further processed datasets for subsequent use in a separate visualization notebook\n",
    "        </ul>\n",
    "  </table>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The evaluation of forcings data can be performed either directly to some observational or reference data, or indirectly. An example of an indirect evaluation is to use multiple forcing datasets to drive a hydrological model and compare simulated outputs with observations (e.g., streamflow to gaged streamflow) to assess the influence of different forcing data on hydrologic model outputs. Indirect approaches are computationally expensive, thus we demonstrate in this notebook how to perform a direct evaluation of forcing data to multiple reference datasets. The methods for a direct evaluation consist of calculating descriptive and comparative statistics between forcing and reference datasets for several variables (precipitation, temperature, and net radiation).  \n",
    "\n",
    "### How does this notebook evaluate forcing data?\n",
    "First, data from gridded datasets (forcing: CONUS404; reference: PRISM, CERES-EBAF) are spatially summarized by computing area-weighted means over HUC6 spatial units within the Delaware River Basin. Next, annual means are computed over consistent periods of record. From these, some commonly-used descriptive statistics are calculated and tabulated. Similarly, annual means of point (station-based) reference data (GHCN, CRN) and annual means of forcing data extracted from nearest (to station) grid-points are computed, and descriptive statistics calculated and tabulated. For further evaluation and visualization, data processed within the notebook are output for subsequent use by a separate visualization notebook (see graphic above).\n",
    "    \n",
    "The following statistics will be calculated for comparative analysis between CONUS404 and each reference dataset:\n",
    "\n",
    "<ul>\n",
    "    <li>mean</li>\n",
    "    <li>median</li>\n",
    "    <li>standard deviation (stdev)</li>\n",
    "    <li>bias</li>\n",
    "    <li>mean absolute error (mae)</li>\n",
    "    <li>root-mean square error (rmse)</li>\n",
    "    <li>Pearson's correlation</li>\n",
    "    <li>Spearman's correlation</li>\n",
    "    <li>percent bias (pbias)</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "### Data References\n",
    "- [CONUS404 (**CON**tiguous United States for **40** years at **4**-km resolution)](https://doi.org/10.5066/P9PHPK4F)\n",
    "- [PRISM (**P**arameter-elevation **R**egressions on **I**ndependent **S**lopes **M**odel)](https://prism.oregonstate.edu/)\n",
    "- [CERES-EBAF (**C**louds and **E**arth's **R**adiant **E**nergy **S**ystems - **E**nergy **B**alanced **A**nd **F**illed)](https://ceres.larc.nasa.gov/data/)\n",
    "- [GHCN (**G**lobal **H**istorical **C**limate **N**etwork)](https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily)\n",
    "- [CRN (**C**limate **R**eference **N**etwork)](https://www.ncei.noaa.gov/products/land-based-station/us-climate-reference-network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# library imports\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import dask\n",
    "import geopandas as gpd\n",
    "import hvplot.xarray  # noqa: F401\n",
    "import hvplot.pandas  # noqa: F401\n",
    "import intake\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sparse\n",
    "import xarray as xr\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from pygeohydro import WBD\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the standard suite and additional statistical metrics for our comparative statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run script for available functions from the standard suite\n",
    "%run ../../../evaluation/Metrics_StdSuite_v1.ipynb\n",
    "\n",
    "# run script for additional functions that will be used\n",
    "%run ../../../evaluation/Metrics_Misc_v1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `intake` catalog, we will load CONUS404 data for the Delaware River Basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "# connect to HyTEST catalog\n",
    "url = 'https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml'\n",
    "cat = intake.open_catalog(url)\n",
    "\n",
    "# access tutorial catalog\n",
    "conus404_drb_cat = cat[\"conus404-drb-eval-tutorial-catalog\"]\n",
    "list(conus404_drb_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Start a Dask client using an appropriate Dask Cluster** \n",
    "This is an optional step, but can speed up data loading significantly, especially when accessing data from the cloud.\n",
    "\n",
    "### Setup your client on your local PC or on HPC like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for existing Dask cluster\n",
    "if \"client\" in locals():\n",
    "    print(\"Shutting down existing Dask cluster.\")\n",
    "    cluster.close()\n",
    "    client.close()\n",
    "\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "\n",
    "print(f\"The link to the Dask dashboard is {client.dashboard_link}. If on HPC, this may not be available.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow outline:\n",
    "1. Read in the prepared dataset\n",
    "2. Read in the HUC6 boundaries and transform to same coordinate reference system as prepared dataset\n",
    "3. Make a data mask with the HUC6 boundaries to calculate zonal statistics\n",
    "4. Compute zonal statistics with data mask and prepared data\n",
    "\n",
    "Once all calculations are done: \n",
    "\n",
    "5. Combine each reference with benchmark into single dataset\n",
    "6. Export gridded data zonal statistics\n",
    "<br>\n",
    "\n",
    "## **Compute zonal statistics for gridded datasets**\n",
    "\n",
    "In the last tutorial, we prepared three gridded datasets: CONUS404 (benchmark), PRISM (reference), and CERES-EBAF (reference). The goal of this section is compute [zonal statistics](https://gisgeography.com/zonal-statistics/) for each HUC6 zone in the Delaware River Basin (DRB) by using the [conservative regridding method put forth by Ryan Abernathy](https://discourse.pangeo.io/t/conservative-region-aggregation-with-xarray-geopandas-and-sparse/2715) to regrid and perform an area-weighted analysis.\n",
    "\n",
    "Dataset outline:\n",
    "<ol>\n",
    "    <li>Read in the prepared dataset</li>\n",
    "    <li>Compute bounding bands for grid cells then use these to create polygons in area-preserving CRS</li>\n",
    "    <li>Read in the HUC6 boundaries and transform to same coordinate reference system as prepared dataset</li>\n",
    "    <li>Overlay the dataset polygons over the HUC6 boundaries and create spatial weights matrices</li>\n",
    "    <li>Perform matrix multiplication between The prepared dataset and the spatial weights matrices</li>\n",
    "</ol>\n",
    "\n",
    "The following two functions will be used for regridding each dataset. An explanation of what they do will be provided when they are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounds_to_poly(x_bounds, y_bounds) -> Polygon:\n",
    "    \"\"\"Return a polygon based on the x (longitude) and\n",
    "    y (longitude) bounding band DataArrays.\n",
    "    \"\"\"\n",
    "    return Polygon([\n",
    "        (x_bounds[0], y_bounds[0]),\n",
    "        (x_bounds[0], y_bounds[1]),\n",
    "        (x_bounds[1], y_bounds[1]),\n",
    "        (x_bounds[1], y_bounds[0])\n",
    "    ])\n",
    "\n",
    "\n",
    "def apply_weights_matmul_sparse(weights, data):\n",
    "    \"\"\"Apply weights in a sparse matrices to data and regrid.\"\"\"\n",
    "    assert isinstance(weights, sparse.SparseArray)\n",
    "    assert isinstance(data, np.ndarray)\n",
    "    data = sparse.COO.from_numpy(data)\n",
    "    data_shape = data.shape\n",
    "    n, k = data_shape[0], data_shape[1] * data_shape[2]\n",
    "    data = data.reshape((n, k))\n",
    "    weights_shape = weights.shape\n",
    "    k_, m = weights_shape[0] * weights_shape[1], weights_shape[2]\n",
    "    assert k == k_\n",
    "    weights_data = weights.reshape((k, m))\n",
    "\n",
    "    regridded = sparse.matmul(data, weights_data)\n",
    "    assert regridded.shape == (n, m)\n",
    "    return regridded.todense()\n",
    "\n",
    "    assert isinstance(weights, sparse.SparseArray)\n",
    "    assert isinstance(data, np.ndarray)\n",
    "    data = sparse.COO.from_numpy(data)\n",
    "    data_shape = data.shape\n",
    "    # k = nlat * nlon\n",
    "    n, k = data_shape[0], data_shape[1] * data_shape[2]\n",
    "    data = data.reshape((n, k))\n",
    "    weights_shape = weights.shape\n",
    "    k_, m = weights_shape[0] * weights_shape[1], weights_shape[2]\n",
    "    assert k == k_\n",
    "    weights_data = weights.reshape((k, m))\n",
    "\n",
    "    regridded = sparse.matmul(data, weights_data)\n",
    "    assert regridded.shape == (n, m)\n",
    "    return regridded.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the following `fsspec.filesystem` will be using to read in each dataset from an [Open Storage Network](https://www.openstoragenetwork.org/) bucket, which is read only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x and y below are syntatically longitude (x) and latitude (y) in the datasets\n",
    "\n",
    "\n",
    "x = \"x\"\n",
    "y = \"y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONUS404 zonal statistics**\n",
    "\n",
    "#### 1. Read in the HUC6 boundaries and transform to same coordinate reference system as prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in HUC6 boundaries found in the DRB\n",
    "drb_wbd = WBD(layer=\"huc6\", outfields=[\"huc6\", \"name\"])\n",
    "drb_gdf = drb_wbd.byids(\"huc6\", [\"020401\", \"020402\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the dataframe\n",
    "drb_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area preserving coordinate reference system\n",
    "crs_area = \"ESRI:53034\"\n",
    "\n",
    "# set CRS to match c404_drb\n",
    "drb_gdf = drb_gdf.to_crs(crs_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "drb_gdf.hvplot(line_color=\"orange\", color=\"purple\", line_width=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Read CONUS404 data, previously processed over the DRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conus404_drb_cat['conus404-drb-OSN'].kwargs['decode_coords'] = 'all'\n",
    "conus404_drb_cat['conus404-drb-OSN'].kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# open dataset\n",
    "c404_drb = conus404_drb_cat['conus404-drb-OSN'].to_dask()\n",
    "\n",
    "# crs\n",
    "c404_crs = c404_drb.rio.crs.to_proj4()\n",
    "\n",
    "c404_drb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize (uncomment to view)\n",
    "# c404_drb.PREC_ACC_NC.hvplot(x=\"x\", y=\"y\", rasterize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Compute bounding bands for grid cells and use them to create polygons in area-preserving CRS\n",
    "\n",
    "Create the grid of `c404_drb` using any of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set var\n",
    "c404_var = \"TK\"\n",
    "\n",
    "# drop unneeded variable and coordinates\n",
    "c404_grid = c404_drb[[c404_var]].drop(['time', 'lon', 'lat', c404_var]).reset_coords()\n",
    "c404_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create bounding bands then stack into points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bounds\n",
    "c404_grid = c404_grid.cf.add_bounds(x)\n",
    "c404_grid = c404_grid.cf.add_bounds(y)\n",
    "\n",
    "# stack\n",
    "c404_points = c404_grid.stack(point=(y, x))\n",
    "c404_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the `xr.apply_ufunc` function to apply the `bounds_to_poly` function above to the _c404_points_ DataSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_boxes = xr.apply_ufunc(\n",
    "    bounds_to_poly,\n",
    "    c404_points.x_bounds,\n",
    "    c404_points.y_bounds,\n",
    "    input_core_dims=[(\"bounds\",), (\"bounds\",)],\n",
    "    output_dtypes=[np.dtype('O')],\n",
    "    vectorize=True\n",
    ")\n",
    "c404_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `gpd.GeoDataFrame` from boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_grid_df = gpd.GeoDataFrame(\n",
    "    data={\"geometry\": c404_boxes.values, \"y\": c404_boxes[y], \"x\": c404_boxes[x]},\n",
    "    index=c404_boxes.indexes[\"point\"],\n",
    "    crs=c404_crs\n",
    ")\n",
    "c404_grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize (uncomment to view)\n",
    "# c404_grid_df.hvplot(line_color=\"red\", color=\"white\", line_width=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Overlay the dataset polygons we just created over the HUC6 boundaries and create spatial weights matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert DRB to conus404 crs\n",
    "c404_drb_gdf = drb_gdf.to_crs(c404_crs)\n",
    "\n",
    "# perform overlay\n",
    "c404_overlay = c404_grid_df.overlay(c404_drb_gdf, keep_geom_type=True)\n",
    "c404_overlay.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot overlay for single HUC6\n",
    "c404_overlay[c404_overlay.huc6 == \"020402\"].geometry.plot(edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute grid cell fractions. \n",
    "\n",
    "A cell fraction is defined as the area of the grid cell divided by the area of the target polygon (HUC6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_grid_cell_fraction = (c404_overlay.geometry.area.groupby(c404_overlay.huc6)\n",
    "                           .transform(lambda x: x / x.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse DataArray (for an in-depth description of sparse data, see [this article](https://www.techopedia.com/definition/9480/sparse-array#:~:text=17%20May%2C%202017-,What%20Does%20Sparse%20Array%20Mean%3F,array%20in%20digital%20data%20handling.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_multi_index = c404_overlay.set_index([y, x, \"huc6\"]).index\n",
    "c404_df_weights = pd.DataFrame({\"weights\": c404_grid_cell_fraction.values},\n",
    "                               index=c404_multi_index)\n",
    "\n",
    "# create xarray dataset\n",
    "c404_ds_weights = xr.Dataset(c404_df_weights)\n",
    "\n",
    "# generate sparse data array\n",
    "c404_weights_sparse = c404_ds_weights.unstack(sparse=True, fill_value=0.).weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Perform matrix multiplication between the prepared dataset and the spatial weights matrices\n",
    "\n",
    "Matrix multiplication across each DataArray. We do this for precipitation (PREC_ACC_NC), net radiation (RNET), and temperature (TK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "\n",
    "    # precipitation\n",
    "    c404_precip_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        c404_weights_sparse,\n",
    "        c404_drb[\"PREC_ACC_NC\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )\n",
    "\n",
    "    # net radiation\n",
    "    c404_rnet_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        c404_weights_sparse,\n",
    "        c404_drb[\"RNET\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )\n",
    "\n",
    "    # temperature\n",
    "    c404_tk_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        c404_weights_sparse,\n",
    "        c404_drb[\"TK\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge DataArrays into Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_regridded = xr.Dataset({\"PREC_NC_ACC\": c404_precip_regridded,\n",
    "                             \"RNET\": c404_rnet_regridded,\n",
    "                             \"TK\": c404_tk_regridded})\n",
    "c404_regridded = c404_regridded.drop(\"crs\")\n",
    "c404_regridded.attrs = c404_drb.attrs\n",
    "c404_regridded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert to DataFrame. Now we see we have our timeseries for each variable and each HU06 within DRB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c404_df = c404_regridded.to_dataframe()\n",
    "c404_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up our data. We will reset our index and modify our dates to represent the year-month instead of reporting on the last day of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "c404_zonal_stats = c404_df.reset_index(drop=False)\n",
    "\n",
    "# convert time to string and remove days\n",
    "c404_zonal_stats[\"time\"] = c404_zonal_stats[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "# drop 1979 as it only had three months of data\n",
    "c404_zonal_stats = (c404_zonal_stats[c404_zonal_stats['time']\n",
    "                                     .str.contains(\"1979\") == False])\n",
    "\n",
    "c404_zonal_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zonal stats that were just calculated took a lot of steps and ,depending on the computational power of your environment, time to create. It could be a good idea to export this intermediate data in case you need to come back to this analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only an example\n",
    "# c404_zonal_stats.to_parquet(\"./file/path/to/conus404_drb_zonal_stats.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRISM zonal statistics**\n",
    "\n",
    "Now, in the next cell, we'll run through the same steps to compute zonal statistics for the two PRISM variables, temperature (TK) and precipitation (PREC_ACC_NC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conus404_drb_cat['prism-drb-OSN'].kwargs['decode_coords'] = 'all'\n",
    "prism_drb = conus404_drb_cat['prism-drb-OSN'].to_dask()\n",
    "prism_drb = prism_drb.chunk(dict(x=-1))\n",
    "prism_drb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset\n",
    "conus404_drb_cat['prism-drb-OSN'].kwargs['decode_coords'] = 'all'\n",
    "prism_drb = conus404_drb_cat['prism-drb-OSN'].to_dask()\n",
    "prism_drb = prism_drb.chunk(dict(x=-1))\n",
    "\n",
    "# prism crs\n",
    "prism_crs = 4269  # EPSG:4269 = NAD83\n",
    "\n",
    "# create the grid of c404_drb using any of the variables\n",
    "prism_var = \"TK\"\n",
    "\n",
    "# drop unneeded variable and coordinates\n",
    "prism_grid = prism_drb[[prism_var]].drop(\n",
    "    ['time', prism_var]).reset_coords().load()\n",
    "\n",
    "\n",
    "# add bounds\n",
    "prism_grid = prism_grid.cf.add_bounds(x)\n",
    "prism_grid = prism_grid.cf.add_bounds(y)\n",
    "\n",
    "# stack\n",
    "prism_points = prism_grid.stack(point=(y, x))\n",
    "\n",
    "# apply bounds_to method\n",
    "prism_boxes = xr.apply_ufunc(\n",
    "    bounds_to_poly,\n",
    "    prism_points.x_bounds,\n",
    "    prism_points.y_bounds,\n",
    "    input_core_dims=[(\"bounds\",), (\"bounds\",)],\n",
    "    output_dtypes=[np.dtype('O')],\n",
    "    vectorize=True\n",
    ")\n",
    "\n",
    "# create geodataframe from boxes\n",
    "prism_grid_df = gpd.GeoDataFrame(\n",
    "    data={\"geometry\": prism_boxes.values,\n",
    "          \"y\": prism_boxes[y],\n",
    "          \"x\": prism_boxes[x]},\n",
    "    index=prism_boxes.indexes[\"point\"],\n",
    "    crs=prism_crs\n",
    ")\n",
    "\n",
    "# convert DRB to conus404 crs\n",
    "prism_drb_gdf = drb_gdf.to_crs(epsg=prism_crs)\n",
    "\n",
    "# overlay the two grids\n",
    "prism_overlay = prism_grid_df.overlay(prism_drb_gdf, keep_geom_type=True)\n",
    "\n",
    "# grid cell fractions\n",
    "prism_grid_cell_fraction = (prism_overlay.geometry.area.groupby(prism_overlay.huc6)\n",
    "                            .transform(lambda x: x / x.sum()))\n",
    "\n",
    "# create sparse dataarray\n",
    "prism_multi_index = prism_overlay.set_index([y, x, \"huc6\"]).index\n",
    "prism_df_weights = pd.DataFrame({\"weights\": prism_grid_cell_fraction.values},\n",
    "                                index=prism_multi_index)\n",
    "\n",
    "prism_ds_weights = xr.Dataset(prism_df_weights)\n",
    "\n",
    "prism_weights_sparse = prism_ds_weights.unstack(sparse=True, fill_value=0.).weights\n",
    "\n",
    "# Matrix multiplication across each DataArray\n",
    "with dask.config.set(**{'array.slicing.split_large_chunks': False, 'allow_rechunk': True}):\n",
    "    prism_precip_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        prism_weights_sparse,\n",
    "        prism_drb[\"PREC_ACC_NC\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )\n",
    "\n",
    "    prism_tk_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        prism_weights_sparse,\n",
    "        prism_drb[\"TK\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )\n",
    "# merge DataArrays into Dataset\n",
    "prism_regridded = xr.Dataset({\"PREC_NC_ACC\": prism_precip_regridded,\n",
    "                              \"TK\": prism_tk_regridded})\n",
    "prism_regridded.attrs = prism_drb.attrs\n",
    "\n",
    "# Covert to DataFrame\n",
    "prism_df = prism_regridded.load().to_dataframe()\n",
    "prism_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up the PRISM data as we have done with the CONUS404 data. We will reset the index and clean up the date formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index and add time back\n",
    "prism_zonal_stats = prism_df.reset_index(drop=False)\n",
    "\n",
    "# convert time to string and remove days\n",
    "prism_zonal_stats[\"time\"] = prism_zonal_stats[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "prism_zonal_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example export of zonal stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only an example\n",
    "# prism_zonal_stats.to_parquet(\"./file/path/to/prism_drb_zonal_stats.parquet\", index=False) #noqa : E501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics\n",
    "\n",
    "We will now compute descriptive statistics between the previously computed CONUS404 and PRISM zonal statistics. The statistics that will be calculated are mean, median, standard deviation, bias, MAE, RMSE, Pearson's correlation, Spearman's r, and percent bias. \n",
    "\n",
    "The overall process will look like this:\n",
    "1. Merge zonal stats together on HUC6 and time values\n",
    "2. Resample data to 1 year means of monthly values\n",
    "3. Calculate each statistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the PRISM and CONUS404 zonals stats together based on the HUC6 code and time\n",
    "prism_c404_zonal = prism_zonal_stats.merge(c404_zonal_stats,\n",
    "                                           left_on=['huc6', 'time'],\n",
    "                                           right_on=['huc6', 'time'],\n",
    "                                           suffixes=[\"_prism\", \"_c404\"])\n",
    "\n",
    "# drop RNET\n",
    "prism_c404_zonal.drop(\"RNET\", axis=1, inplace=True)\n",
    "\n",
    "prism_c404_zonal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time column to datetime type\n",
    "prism_c404_zonal[\"time\"] = pd.to_datetime(prism_c404_zonal[\"time\"], format=\"%Y-%m\")\n",
    "prism_c404_zonal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# resample done by HUC6 as the index is HUC6 and year\n",
    "prism_c404_yearly = prism_c404_zonal.resample(\"1Y\", on=\"time\").agg(\n",
    "    {\"PREC_NC_ACC_c404\": \"sum\",\n",
    "    \"PREC_NC_ACC_prism\": \"sum\",\n",
    "    \"TK_c404\": \"mean\",\n",
    "    \"TK_prism\": \"mean\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_c404_yearly.reset_index(drop=False, inplace=True)\n",
    "prism_c404_yearly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate mean, median, and standard deviation of yearly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mean, median, standard deviation\n",
    "prism_c404_mean = prism_c404_yearly.mean()\n",
    "prism_c404_median = prism_c404_yearly.median()\n",
    "prism_c404_stdev = prism_c404_yearly.std()\n",
    "\n",
    "# create dataframe\n",
    "prism_c404_stats = pd.DataFrame({\"annual_mean\": prism_c404_mean,\n",
    "                                 \"median\": prism_c404_median,\n",
    "                                 \"stdev\": prism_c404_stdev}).T.drop(\"time\", axis=1)\n",
    "\n",
    "# reset index and rename\n",
    "prism_c404_stats = prism_c404_stats.reset_index(drop=False).rename({\"index\": \"stat\"}, axis=1) #noqa : E501\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias\n",
    "prism_c404_stats_annual_mean = prism_c404_stats.loc[prism_c404_stats['stat'] == \"annual_mean\"] #noqa : E501\n",
    "prism_c404_bias_precip = float(prism_c404_stats_annual_mean[\"PREC_NC_ACC_c404\"] - prism_c404_stats_annual_mean[\"PREC_NC_ACC_prism\"]) #noqa : E501\n",
    "prism_c404_bias_tk = float(prism_c404_stats_annual_mean[\"TK_c404\"] - prism_c404_stats_annual_mean[\"TK_prism\"]) #noqa : E501\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "prism_c404_stats.loc[len(prism_c404_stats.index)] = [\"bias\", prism_c404_bias_precip, None, prism_c404_bias_tk, None] #noqa : E501\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE and RMSE is then calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE\n",
    "prism_c404_mae_precip = sum(abs(prism_c404_yearly[\"PREC_NC_ACC_c404\"] - prism_c404_yearly[\"PREC_NC_ACC_prism\"]))/len(prism_c404_yearly) #noqa : E501\n",
    "prism_c404_mae_tk = sum(abs(prism_c404_yearly[\"TK_c404\"] - prism_c404_yearly[\"TK_prism\"]))/len(prism_c404_yearly) #noqa : E501\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "prism_c404_stats.loc[len(prism_c404_stats.index)] = [\"MAE\", prism_c404_mae_precip, None, prism_c404_mae_tk, None] #noqa : E501\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE\n",
    "prism_c404_rmse_precip = math.sqrt(np.square(np.subtract(prism_c404_yearly[\"PREC_NC_ACC_c404\"], prism_c404_yearly[\"PREC_NC_ACC_prism\"])).mean()) #noqa : E501\n",
    "prism_c404_rmse_tk = math.sqrt(np.square(np.subtract(prism_c404_yearly[\"TK_c404\"], prism_c404_yearly[\"TK_prism\"])).mean()) #noqa : E501\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "prism_c404_stats.loc[len(prism_c404_stats.index)] = [\"RMSE\", prism_c404_rmse_precip, None, prism_c404_rmse_tk, None] #noqa : E501\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearsons correlation\n",
    "prism_c404_pearson_precip = pearson_r(prism_c404_yearly[\"PREC_NC_ACC_c404\"], prism_c404_yearly[\"PREC_NC_ACC_prism\"]) #noqa : E501\n",
    "prism_c404_pearson_tk = pearson_r(prism_c404_yearly[\"TK_c404\"], prism_c404_yearly[\"TK_prism\"]) #noqa : E501\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "prism_c404_stats.loc[len(prism_c404_stats.index)] = [\"Pearson\", prism_c404_pearson_precip, None, prism_c404_pearson_tk, None] #noqa : E501\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman's correlation\n",
    "prism_c404_spearman_precip = spearman_r(prism_c404_yearly[\"PREC_NC_ACC_c404\"], prism_c404_yearly[\"PREC_NC_ACC_prism\"]) #noqa : E501\n",
    "prism_c404_spearman_tk = spearman_r(prism_c404_yearly[\"TK_c404\"], prism_c404_yearly[\"TK_prism\"]) #noqa : E501\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "prism_c404_stats.loc[len(prism_c404_stats.index)] = [\"Spearman\", prism_c404_spearman_precip, None, prism_c404_spearman_tk, None] #noqa : E501\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent bias\n",
    "prism_c404_pbias_precip = pbias(prism_c404_yearly[\"PREC_NC_ACC_prism\"], prism_c404_yearly[\"PREC_NC_ACC_c404\"]) #noqa : E501\n",
    "prism_c404_pbias_tk = pbias(prism_c404_yearly[\"TK_prism\"], prism_c404_yearly[\"TK_c404\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "prism_c404_stats.loc[len(prism_c404_stats.index)] = [\"pbias\", prism_c404_pbias_precip, None, prism_c404_pbias_tk, None] #noqa : E501\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example export of descriptive stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prism_c404_stats.to_parquet(\"./file/path/to/c404_prism_drb_descriptive_stats.parquet\", index=False) #noqa : E501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CERES-EBAF net radiation zonal statistics**\n",
    "\n",
    "Now, in the next cell, we'll run through the same steps to compute zonal statistics for the single CERES-EBAF variable, net radiation (RNET)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset\n",
    "conus404_drb_cat['ceres-drb-OSN'].kwargs['decode_coords'] = 'all'\n",
    "ceres_drb = conus404_drb_cat['ceres-drb-OSN'].to_dask()\n",
    "\n",
    "# crs\n",
    "ceres_crs = 4326\n",
    "\n",
    "# create the grid of c404_drb using any of the variables\n",
    "ceres_var = \"RNET\"\n",
    "\n",
    "# drop unneeded variable and coordinates\n",
    "ceres_grid = ceres_drb[[ceres_var]].drop(\n",
    "    ['time', ceres_var]).reset_coords().load()\n",
    "\n",
    "\n",
    "# add bounds\n",
    "ceres_grid = ceres_grid.cf.add_bounds(x)\n",
    "ceres_grid = ceres_grid.cf.add_bounds(y)\n",
    "\n",
    "# stack\n",
    "ceres_points = ceres_grid.stack(point=(y, x))\n",
    "\n",
    "# apply bounds_to method\n",
    "ceres_boxes = xr.apply_ufunc(\n",
    "    bounds_to_poly,\n",
    "    ceres_points.x_bounds,\n",
    "    ceres_points.y_bounds,\n",
    "    input_core_dims=[(\"bounds\",), (\"bounds\",)],\n",
    "    output_dtypes=[np.dtype('O')],\n",
    "    vectorize=True\n",
    ")\n",
    "\n",
    "# create geodataframe from boxes\n",
    "ceres_grid_df = gpd.GeoDataFrame(\n",
    "    data={\"geometry\": ceres_boxes.values,\n",
    "          \"y\": ceres_boxes[y], \"x\": ceres_boxes[x]},\n",
    "    index=ceres_boxes.indexes[\"point\"],\n",
    "    crs=ceres_crs\n",
    ")\n",
    "\n",
    "# convert DRB to conus404 crs\n",
    "ceres_drb_gdf = drb_gdf.to_crs(epsg=ceres_crs)\n",
    "\n",
    "# overlay the two grids\n",
    "ceres_overlay = ceres_grid_df.overlay(ceres_drb_gdf, keep_geom_type=True)\n",
    "\n",
    "# grid cell fractions\n",
    "ceres_grid_cell_fraction = ceres_overlay.geometry.area.groupby(\n",
    "    ceres_overlay.huc6).transform(lambda x: x / x.sum())\n",
    "\n",
    "# create sparse dataarray\n",
    "ceres_multi_index = ceres_overlay.set_index([y, x, \"huc6\"]).index\n",
    "ceres_df_weights = pd.DataFrame(\n",
    "    {\"weights\": ceres_grid_cell_fraction.values}, index=ceres_multi_index)\n",
    "\n",
    "ceres_ds_weights = xr.Dataset(ceres_df_weights)\n",
    "\n",
    "ceres_weights_sparse = ceres_ds_weights.unstack(\n",
    "    sparse=True, fill_value=0.).weights\n",
    "\n",
    "# Matrix multiplication across each DataArray\n",
    "with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "    ceres_rnet_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        ceres_weights_sparse,\n",
    "        ceres_drb[\"RNET\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )\n",
    "\n",
    "# merge DataArrays into Dataset\n",
    "ceres_regridded = xr.Dataset({\"RNET\": ceres_rnet_regridded})\n",
    "ceres_regridded.attrs = ceres_drb.attrs\n",
    "\n",
    "# Covert to DataFrame\n",
    "ceres_df = ceres_regridded.load().to_dataframe()\n",
    "ceres_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "ceres_zonal_stats = ceres_df.reset_index(drop=False)\n",
    "\n",
    "# drop spatial_ref\n",
    "ceres_zonal_stats.drop(\"spatial_ref\", axis=1, inplace=True)\n",
    "\n",
    "# convert time to string and drop day\n",
    "ceres_zonal_stats[\"time\"] = ceres_zonal_stats[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "ceres_zonal_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example export of zonal stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only an example\n",
    "# ceres_zonal_stats.to_parquet(\"./file/path/to/ceres_drb_zonal_stats.parquet\", index=False) #noqa : E501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics\n",
    "\n",
    "We will now compute descriptive statistics between the previously computed CONUS404 and CERES-EBAF zonal statistics. This is the same overall process that was done with the precipitation and temperature PRISM zonal stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the PRISM and CONUS404 zonals stats together based on the HUC6 code and time\n",
    "ceres_c404_zonal = ceres_zonal_stats.merge(c404_zonal_stats,\n",
    "                                           left_on=['huc6', 'time'],\n",
    "                                           right_on=['huc6', 'time'],\n",
    "                                           suffixes=[\"_ceres\", \"_c404\"])\n",
    "\n",
    "# drop RNET\n",
    "ceres_c404_zonal.drop([\"PREC_NC_ACC\", \"TK\"], axis=1, inplace=True)\n",
    "\n",
    "ceres_c404_zonal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time column to datetime type\n",
    "ceres_c404_zonal[\"time\"] = pd.to_datetime(ceres_c404_zonal[\"time\"], format=\"%Y-%m\")\n",
    "ceres_c404_zonal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample done by HUC6 as the index is HUC6 and year\n",
    "ceres_c404_yearly = ceres_c404_zonal.resample(\"1Y\", on=\"time\").agg(\n",
    "    {\"RNET_c404\": \"sum\",\n",
    "     \"RNET_ceres\": \"sum\"\n",
    "    }\n",
    ")\n",
    "ceres_c404_yearly.reset_index(drop=False, inplace=True)\n",
    "ceres_c404_yearly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean, median, standard devation\n",
    "ceres_c404_mean = ceres_c404_yearly.mean()\n",
    "ceres_c404_median = ceres_c404_yearly.median()\n",
    "ceres_c404_stdev = ceres_c404_yearly.std()\n",
    "\n",
    "# create dataframe\n",
    "ceres_c404_stats = pd.DataFrame({\"annual_mean\": ceres_c404_mean,\n",
    "                                 \"median\": ceres_c404_median,\n",
    "                                 \"stdev\": ceres_c404_stdev}).T.drop(\"time\", axis=1)\n",
    "\n",
    "# reset index and rename\n",
    "ceres_c404_stats = ceres_c404_stats.reset_index(drop=False).rename({\"index\": \"stat\"}, axis=1) #noqa : E501\n",
    "\n",
    "ceres_c404_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias\n",
    "ceres_c404_stats_annual_mean = ceres_c404_stats.loc[ceres_c404_stats['stat'] == \"annual_mean\"] #noqa : E501\n",
    "ceres_c404_bias_rnet = float(ceres_c404_stats_annual_mean[\"RNET_c404\"] - ceres_c404_stats_annual_mean[\"RNET_ceres\"]) #noqa : E501\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "ceres_c404_stats.loc[len(ceres_c404_stats.index)] = [\"bias\", ceres_c404_bias_rnet, None] #noqa : E501\n",
    "\n",
    "# MAE\n",
    "ceres_c404_mae_rnet = sum(abs(ceres_c404_yearly[\"RNET_c404\"] - ceres_c404_yearly[\"RNET_ceres\"]))/len(ceres_c404_yearly) #noqa : E501\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "ceres_c404_stats.loc[len(ceres_c404_stats.index)] = [\"MAE\", ceres_c404_mae_rnet, None] #noqa : E501\n",
    "\n",
    "# RMSE\n",
    "ceres_c404_rmse_rnet = math.sqrt(np.square(np.subtract(ceres_c404_yearly[\"RNET_c404\"], ceres_c404_yearly[\"RNET_ceres\"])).mean()) #noqa : E501\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "ceres_c404_stats.loc[len(ceres_c404_stats.index)] = [\"RMSE\", ceres_c404_rmse_rnet, None] #noqa : E501\n",
    "\n",
    "# Pearsons correlation\n",
    "ceres_c404_pearson_rnet = pearson_r(ceres_c404_yearly[\"RNET_c404\"], ceres_c404_yearly[\"RNET_ceres\"]) #noqa : E501\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "ceres_c404_stats.loc[len(ceres_c404_stats.index)] = [\"Pearson\", ceres_c404_pearson_rnet, None] #noqa : E501\n",
    "\n",
    "# Spearman's correlation\n",
    "ceres_c404_spearman_rnet = spearman_r(ceres_c404_yearly[\"RNET_c404\"], ceres_c404_yearly[\"RNET_ceres\"]) #noqa : E501\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "ceres_c404_stats.loc[len(ceres_c404_stats.index)] = [\"Spearman\", ceres_c404_spearman_rnet, None] #noqa : E501\n",
    "\n",
    "# percent bias\n",
    "ceres_c404_pbias_rnet = pbias(\n",
    "    ceres_c404_yearly[\"RNET_ceres\"], ceres_c404_yearly[\"RNET_c404\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "ceres_c404_stats.loc[len(ceres_c404_stats.index)] = [\"pbias\", ceres_c404_pbias_rnet, None] #noqa : E501\n",
    "\n",
    "ceres_c404_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example export of descriptive stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ceres_c404_stats.to_parquet(\"./file/path/to/c404_ceres_drb_descriptive_stats.parquet\", index=False) #noqa : E501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Extract gridded values to points**\n",
    "\n",
    "The goal of this section is extract values from CONUS404 where they overlay spatially and temporally with station data. This concept is described in an article about the ESRI tool [Extract Values to Points](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-analyst/extract-values-to-points.htm). After extracting the values, the same descriptive statistics used to compare the gridded datasets will be run.\n",
    "\n",
    "Process outline:\n",
    "1. Read in the prepared dataset\n",
    "2. Use latitude and longitude of each point to extract data from matching CONUS404 grid cell\n",
    "<br>\n",
    "\n",
    "**Climate Reference Network point extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb_df = conus404_drb_cat['crn-drb-OSN'].read()\n",
    "\n",
    "# create geodataframe\n",
    "crn_drb = gpd.GeoDataFrame(crn_drb_df, crs=4326,\n",
    "                       geometry=gpd.points_from_xy(crn_drb_df.LONGITUDE,\n",
    "                                                         crn_drb_df.LATITUDE))\n",
    "\n",
    "crn_drb.rename({\"DATE\": \"time\",\n",
    "                \"TK\": \"TK_crn\",\n",
    "                \"PREC_ACC_NC\": \"PREC_ACC_NC_crn\"},\n",
    "                  axis=1, inplace=True)\n",
    "\n",
    "crn_drb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get coordinates from CRN stations to index CONUS404 DRB by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# isolate single row and transform to c404_drb crs\n",
    "crn_coords_gdf = crn_drb.iloc[[0]].to_crs(c404_crs)\n",
    "\n",
    "# extract lat/long values\n",
    "crn_lat = crn_coords_gdf.iloc[0][\"geometry\"].y\n",
    "crn_lon = crn_coords_gdf.iloc[0][\"geometry\"].x\n",
    "\n",
    "# get min and max time\n",
    "crn_time_min = crn_drb[\"time\"].min()\n",
    "crn_time_max = crn_drb[\"time\"].max()\n",
    "\n",
    "# convert time to str\n",
    "crn_drb[\"time\"] = crn_drb[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "# subset c404_drb to lat/long using nearest\n",
    "c404_crn_sub = c404_drb.sel(x=crn_lon, y=crn_lat, method=\"nearest\")\n",
    "\n",
    "# slice to time-steps of crn_drb\n",
    "c404_crn_sub = c404_crn_sub.sel(time=slice(crn_time_min, crn_time_max))\n",
    "\n",
    "c404_crn_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert subset to dataframe and reorganize columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_crn_sub_df = c404_crn_sub.to_dataframe().reset_index(drop=False)\n",
    "\n",
    "# trim columns\n",
    "c404_crn_sub_df = c404_crn_sub_df[[\"time\", \"TK\", \"PREC_ACC_NC\"]]\n",
    "\n",
    "# rename columns\n",
    "c404_crn_sub_df.rename({\"TK\": \"TK_c404\",\n",
    "                    \"PREC_ACC_NC\": \"PREC_ACC_NC_c404\"},\n",
    "                  axis=1, inplace=True)\n",
    "\n",
    "# trim time\n",
    "c404_crn_sub_df[\"time\"] = c404_crn_sub_df[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "c404_crn_sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine CONUS404 subset with CRN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_c404_point = crn_drb.merge(c404_crn_sub_df, on=\"time\").reset_index(drop=False)\n",
    "\n",
    "# drop columns\n",
    "crn_c404_point.drop([\"index\", \"LATITUDE\", \"LONGITUDE\", \"ID\", \"geometry\"], axis=1, inplace=True) #noqa : E501\n",
    "\n",
    "crn_c404_point.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time column to datetime type\n",
    "crn_c404_point[\"time\"] = pd.to_datetime(crn_c404_point[\"time\"], format=\"%Y-%m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of exporting point data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crn_c404_point.to_parquet(\"./file/path/to/c404_crn_drb_point_values.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics\n",
    "\n",
    "We will now compute descriptive statistics between the point extracted CONUS404 and CRN data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample to yearly means\n",
    "crn_c404_yearly = crn_c404_point.resample(\"1Y\", on=\"time\").agg({\n",
    "    \"PREC_ACC_NC_c404\": \"sum\",\n",
    "    \"PREC_ACC_NC_crn\": \"sum\",\n",
    "    \"TK_c404\": \"mean\",\n",
    "    \"TK_crn\": \"mean\"}\n",
    ")\n",
    "crn_c404_yearly.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# mean, median, standard devation\n",
    "crn_c404_mean = crn_c404_yearly.mean()\n",
    "crn_c404_median = crn_c404_yearly.median()\n",
    "crn_c404_stdev = crn_c404_yearly.std()\n",
    "\n",
    "# create dataframe\n",
    "crn_c404_stats = pd.DataFrame({\"annual_mean\": crn_c404_mean, \"median\": crn_c404_median, \"stdev\": crn_c404_stdev}).T.drop(\"time\", axis=1) #noqa : E501\n",
    "\n",
    "# reset index and rename\n",
    "crn_c404_stats = crn_c404_stats.reset_index(drop=False).rename({\"index\": \"stat\"}, axis=1) #noqa : E501\n",
    "\n",
    "# bias\n",
    "crn_c404_stats_annual_mean = crn_c404_stats.loc[crn_c404_stats['stat'] == \"annual_mean\"] #noqa : E501\n",
    "crn_c404_bias_precip = float(crn_c404_stats_annual_mean[\"PREC_ACC_NC_c404\"] - crn_c404_stats_annual_mean[\"PREC_ACC_NC_crn\"]) #noqa : E501\n",
    "crn_c404_bias_tk = float(crn_c404_stats_annual_mean[\"TK_c404\"] - crn_c404_stats_annual_mean[\"TK_crn\"]) #noqa : E501\n",
    "# add stat to bottom of dataframe\n",
    "crn_c404_stats.loc[len(crn_c404_stats.index)] = [\"bias\", crn_c404_bias_precip, np.nan, crn_c404_bias_tk, np.nan] #noqa : E501\n",
    "\n",
    "# MAE\n",
    "crn_c404_mae_precip = sum(abs(crn_c404_yearly[\"PREC_ACC_NC_c404\"] - crn_c404_yearly[\"PREC_ACC_NC_crn\"]))/len(crn_c404_yearly) #noqa : E501\n",
    "crn_c404_mae_tk = sum(abs(crn_c404_yearly[\"TK_c404\"] - crn_c404_yearly[\"TK_crn\"]))/len(crn_c404_yearly) #noqa : E501\n",
    "# add stat to bottom of dataframe\n",
    "crn_c404_stats.loc[len(crn_c404_stats.index)] = [\"MAE\", crn_c404_mae_precip, np.nan, crn_c404_mae_tk, np.nan] #noqa : E501\n",
    "\n",
    "# RMSE\n",
    "crn_c404_rmse_precip = math.sqrt(np.square(np.subtract(crn_c404_yearly[\"PREC_ACC_NC_c404\"], crn_c404_yearly[\"PREC_ACC_NC_crn\"])).mean()) #noqa : E501\n",
    "crn_c404_rmse_tk = math.sqrt(np.square(np.subtract(crn_c404_yearly[\"TK_c404\"], crn_c404_yearly[\"TK_crn\"])).mean()) #noqa : E501\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "crn_c404_stats.loc[len(crn_c404_stats.index)] = [\"RMSE\", crn_c404_rmse_precip, np.nan, crn_c404_rmse_tk, np.nan] #noqa : E501\n",
    "\n",
    "# Pearsons correlation\n",
    "crn_c404_pearson_precip = pearson_r(crn_c404_yearly[\"PREC_ACC_NC_c404\"], crn_c404_yearly[\"PREC_ACC_NC_crn\"]) #noqa : E501\n",
    "crn_c404_pearson_tk = pearson_r(crn_c404_yearly[\"TK_c404\"], crn_c404_yearly[\"TK_crn\"]) #noqa : E501\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "crn_c404_stats.loc[len(crn_c404_stats.index)] = [\"pearson\", crn_c404_pearson_precip, np.nan, crn_c404_pearson_tk, np.nan] #noqa : E501\n",
    "\n",
    "# Spearman's correlation\n",
    "crn_c404_spearman_precip = spearman_r(crn_c404_yearly[\"PREC_ACC_NC_c404\"], crn_c404_yearly[\"PREC_ACC_NC_crn\"]) #noqa : E501\n",
    "crn_c404_spearman_tk = spearman_r(crn_c404_yearly[\"TK_c404\"], crn_c404_yearly[\"TK_crn\"]) #noqa : E501\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "crn_c404_stats.loc[len(crn_c404_stats.index)] = [\"spearman\", crn_c404_spearman_precip, np.nan, crn_c404_spearman_tk, np.nan] #noqa : E501\n",
    "\n",
    "# percent bias\n",
    "crn_c404_pbias_precip = pbias(crn_c404_yearly[\"PREC_ACC_NC_c404\"], crn_c404_yearly[\"PREC_ACC_NC_crn\"]) #noqa : E501\n",
    "crn_c404_pbias_tk = pbias(crn_c404_yearly[\"TK_c404\"], crn_c404_yearly[\"TK_crn\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "crn_c404_stats.loc[len(crn_c404_stats.index)] = [\"pbias\", crn_c404_pbias_precip, np.nan, crn_c404_pbias_tk, np.nan] #noqa : E501\n",
    "\n",
    "crn_c404_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example export of descriptive stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crn_c404_stats.to_parquet(\"./file/path/to/c404_crn_drb_descriptive_stats.parquet\", index=False) #noqa : E501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Historical Climate Network (HCN) point extraction**\n",
    "\n",
    "The HCN data is different than the CRN data as the HCN data comes from multiple stations whereas the CRN data was from a single station. This will involve using multiple sets of geographic coordinates to extract data from CONUS404."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dataset\n",
    "hcn_drb_df = conus404_drb_cat['hcn-drb-OSN'].read()\n",
    "\n",
    "# rename columns\n",
    "hcn_drb_df.rename({\"DATE\": \"time\",\n",
    "                \"TK\": \"TK_hcn\",\n",
    "                \"PREC_ACC_NC\": \"PREC_ACC_NC_hcn\"},\n",
    "                  axis=1, inplace=True)\n",
    "\n",
    "# change DATE field to string\n",
    "hcn_drb_df[\"time\"] = hcn_drb_df[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "# hcn_drb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a DataFrame of the station IDs, lats, and longs to use for extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hcn_stations = hcn_drb_df.copy().drop([\"time\", \"TK_hcn\", \"PREC_ACC_NC_hcn\"], axis=1)\n",
    "hcn_stations[\"LONGITUDE\"] = pd.to_numeric(hcn_stations[\"LONGITUDE\"])\n",
    "hcn_stations[\"LATITUDE\"] = pd.to_numeric(hcn_stations[\"LATITUDE\"])\n",
    "\n",
    "hcn_stations = hcn_stations.groupby('ID').mean().reset_index(drop=False)\n",
    "# hcn_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a GeoDataFrame to convert the lat and long to the coordinate system of CONUS404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_stations_gdf = gpd.GeoDataFrame(hcn_stations, crs=4326,\n",
    "                       geometry=gpd.points_from_xy(hcn_stations.LONGITUDE,\n",
    "                                                         hcn_stations.LATITUDE))\n",
    "\n",
    "# transform to c404_drb crs\n",
    "hcn_stations_gdf = hcn_stations_gdf.to_crs(c404_crs)\n",
    "\n",
    "# extract lat/long values and give them the ID of the stations\n",
    "target_lon = xr.DataArray(hcn_stations_gdf[\"geometry\"].x.to_numpy(), dims=\"ID\", coords=dict(ID=hcn_stations_gdf.ID)) #noqa : E501\n",
    "target_lat = xr.DataArray(hcn_stations_gdf[\"geometry\"].y.to_numpy(), dims=\"ID\", coords=dict(ID=hcn_stations_gdf.ID)) #noqa : E501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset `c404_drb` to time period of HCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time min/max\n",
    "hcn_time_min = hcn_drb_df[\"time\"].min()\n",
    "hcn_time_max = hcn_drb_df[\"time\"].max()\n",
    "\n",
    "# slice c404 to HCN time and drop unused vars\n",
    "c404_hcn_timesub = c404_drb.sel(time=slice(hcn_time_min, hcn_time_max)).drop_vars(\"RNET\") #noqa : E501\n",
    "\n",
    "\n",
    "# rename data_vars names\n",
    "name_dict = dict(TK=\"TK_c404\", PREC_ACC_NC=\"PREC_ACC_NC_c404\")\n",
    "c404_hcn_timesub = c404_hcn_timesub.rename(name_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data from points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to points\n",
    "c404_stations_sub = c404_hcn_timesub.sel(y=target_lat, x=target_lon, method='nearest').compute() #noqa : E501\n",
    "\n",
    "# covert to a dataframe and reset the index\n",
    "c404_stations_df = c404_stations_sub.to_dataframe().reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge CONUS404 observations to HCN observations using the station ID and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare df to merge back to hcn_drb_df\n",
    "# start by subsetting columns\n",
    "c404_stations_df = c404_stations_df[[\"ID\", \"time\", \"TK_c404\", \"PREC_ACC_NC_c404\"]]\n",
    "\n",
    "# convert time to string\n",
    "c404_stations_df[\"time\"] = c404_stations_df[\"time\"].dt.strftime(\"%Y-%m\")\n",
    "\n",
    "# merge into hcn_drb_df\n",
    "hcn_c404_point = pd.merge(hcn_drb_df, c404_stations_df, on=[\"ID\", \"time\"])\n",
    "\n",
    "# convert time column to datetime type\n",
    "hcn_c404_point[\"time\"] = pd.to_datetime(hcn_c404_point[\"time\"], format=\"%Y-%m\")\n",
    "\n",
    "hcn_c404_point.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of exporting point data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hcn_c404_point.to_parquet(\"./file/path/to/c404_hcn_drb_point_values.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics\n",
    "\n",
    "We will now compute descriptive statistics between the point extracted CONUS404 and HCN data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearly means\n",
    "hcn_c404_yearly = hcn_c404_point.groupby(\"ID\").resample(\"1Y\", on=\"time\").agg(\n",
    "    {\"PREC_ACC_NC_c404\": \"sum\",\n",
    "    \"PREC_ACC_NC_hcn\": \"sum\",\n",
    "    \"TK_c404\": \"mean\",\n",
    "    \"TK_hcn\": \"mean\"}\n",
    ")\n",
    "\n",
    "hcn_c404_yearly = hcn_c404_yearly.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mean\n",
    "hcn_c404_mean = hcn_c404_yearly.groupby(\"ID\").mean()\n",
    "hcn_c404_mean[\"stat\"] = \"mean\"\n",
    "\n",
    "# mean\n",
    "hcn_c404_median = hcn_c404_yearly.groupby(\"ID\").median()\n",
    "hcn_c404_median[\"stat\"] = \"median\"\n",
    "\n",
    "# stdev\n",
    "hcn_c404_std = hcn_c404_yearly.groupby(\"ID\").std()\n",
    "hcn_c404_std[\"stat\"] = \"stdev\"\n",
    "\n",
    "# summary stats\n",
    "hcn_c404_summary = (\n",
    "    pd.concat([\n",
    "    hcn_c404_mean,\n",
    "    hcn_c404_median,\n",
    "    hcn_c404_std\n",
    "    ])\n",
    "    .sort_index()\n",
    "    .reset_index(drop=False)\n",
    ")\n",
    "\n",
    "# reorder columns to make more table more readable\n",
    "hcn_c404_columns = [\"ID\", \"stat\", \"TK_c404\", \"TK_hcn\", \"PREC_ACC_NC_c404\", \"PREC_ACC_NC_hcn\"] #noqa : E501\n",
    "hcn_c404_summary = hcn_c404_summary[hcn_c404_columns]\n",
    "\n",
    "hcn_c404_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create goodness of fit status for each station and then create a dataframe of all the stats.\n",
    "\n",
    "Notice that many of the intermediate `pd.Series` objects are getting a respective `.name` (ex: `hcn_c404_bias_PREC_ACC_NC.name = \"PREC_ACC_NC_c404\"`). This will automatically name the column that when the `pd.Series` is transformed into a `pd.DataFrame` later on. Additionally, there multiple times that a `pd.Series` gets called the same name. This will be to match the column names in `hcn_c404_summary` above so when we can smoothly concantenate the two `pd.DataFrame`s together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes for each goodness of fit stat \n",
    "\n",
    "# functions\n",
    "def dataframe_transform(data: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Transform a Series to DataFrame.\"\"\"\n",
    "    df = data.to_frame().reset_index(drop=False)\n",
    "    if \"level_1\" in df.columns:\n",
    "        df = df.drop(\"level_1\", axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_stat_df(ldata: pd.Series, rdata: pd.Series, stat: str) -> pd.DataFrame:\n",
    "    \"\"\"Combine a list of series into unified dataframe.\"\"\"\n",
    "    # custom transform function\n",
    "    left_data_df = dataframe_transform(ldata)\n",
    "    right_data_df = dataframe_transform(rdata)\n",
    "    \n",
    "    # merge data on ID\n",
    "    df = (pd.merge(left_data_df, right_data_df, on=\"ID\")\n",
    "            .assign(stat=stat))\n",
    "    return df\n",
    "\n",
    "# calculate stats\n",
    "\n",
    "# bias\n",
    "\n",
    "hcn_c404_stats_annual_mean = hcn_c404_summary.loc[hcn_c404_summary['stat'] == \"mean\"]\n",
    "hcn_c404_bias_PREC_ACC_NC = hcn_c404_stats_annual_mean.groupby(\"ID\").apply(lambda x: bias(x[\"PREC_ACC_NC_c404\"], x[\"PREC_ACC_NC_hcn\"])) #noqa : E501\n",
    "hcn_c404_bias_PREC_ACC_NC.name = \"PREC_ACC_NC_c404\"\n",
    "hcn_c404_bias_TK = hcn_c404_stats_annual_mean.groupby(\"ID\").apply(lambda x: bias(x[\"TK_c404\"], x[\"TK_hcn\"])) #noqa : E501\n",
    "hcn_c404_bias_TK.name = \"TK_c404\"\n",
    "\n",
    "hcn_c404_bias = create_stat_df(hcn_c404_bias_PREC_ACC_NC, hcn_c404_bias_TK, \"bias\") #noqa : E501\n",
    "\n",
    "# MAE\n",
    "hcn_c404_mae_PREC_ACC_NC = hcn_c404_yearly.groupby(\"ID\").apply(lambda x: mae(x[\"PREC_ACC_NC_c404\"], x[\"PREC_ACC_NC_hcn\"])) #noqa : E501\n",
    "hcn_c404_mae_PREC_ACC_NC.name = \"PREC_ACC_NC_c404\"\n",
    "hcn_c404_mae_TK = hcn_c404_yearly.groupby(\"ID\").apply(lambda x: mae(x[\"TK_c404\"], x[\"TK_hcn\"])) #noqa : E501\n",
    "hcn_c404_mae_TK.name = \"TK_c404\"\n",
    "\n",
    "hcn_c404_mae = create_stat_df(hcn_c404_mae_PREC_ACC_NC, hcn_c404_mae_TK, \"MAE\") #noqa : E501\n",
    "\n",
    "# RMSE\n",
    "hcn_c404_rmse_PREC_ACC_NC = hcn_c404_yearly.groupby(\"ID\").apply(lambda x: rmse(x[\"PREC_ACC_NC_c404\"], x[\"PREC_ACC_NC_hcn\"])) #noqa : E501\n",
    "hcn_c404_rmse_PREC_ACC_NC.name = \"PREC_ACC_NC_c404\"\n",
    "hcn_c404_rmse_TK = hcn_c404_yearly.groupby(\"ID\").apply(lambda x: rmse(x[\"TK_c404\"], x[\"TK_hcn\"])) #noqa : E501\n",
    "hcn_c404_rmse_TK.name = \"TK_c404\"\n",
    "\n",
    "hcn_c404_rmse = create_stat_df(hcn_c404_rmse_PREC_ACC_NC, hcn_c404_rmse_TK, \"RMSE\") #noqa : E501\n",
    "\n",
    "# Pearson r\n",
    "hcn_c404_pearson_PREC_ACC_NC = hcn_c404_yearly.groupby(\"ID\").apply(lambda x: pearson_r(x[\"PREC_ACC_NC_c404\"], x[\"PREC_ACC_NC_hcn\"])) #noqa : E501\n",
    "hcn_c404_pearson_PREC_ACC_NC.name = \"PREC_ACC_NC_c404\"\n",
    "hcn_c404_pearson_TK = hcn_c404_yearly.groupby(\"ID\").apply(lambda x: pearson_r(x[\"TK_c404\"], x[\"TK_hcn\"])) #noqa : E501\n",
    "hcn_c404_pearson_TK.name = \"TK_c404\"\n",
    "\n",
    "hcn_c404_pearson = create_stat_df(hcn_c404_pearson_PREC_ACC_NC, hcn_c404_pearson_TK, \"pearson\") #noqa : E501\n",
    "\n",
    "# Spearman r\n",
    "hcn_c404_spearman_PREC_ACC_NC = hcn_c404_yearly.groupby(\"ID\").apply(lambda x: spearman_r(x[\"PREC_ACC_NC_c404\"], x[\"PREC_ACC_NC_hcn\"])) #noqa : E501\n",
    "hcn_c404_spearman_PREC_ACC_NC.name = \"PREC_ACC_NC_c404\"\n",
    "hcn_c404_spearman_TK = hcn_c404_yearly.groupby(\"ID\").apply(lambda x: spearman_r(x[\"TK_c404\"], x[\"TK_hcn\"])) #noqa : E501\n",
    "hcn_c404_spearman_TK.name = \"TK_c404\"\n",
    "\n",
    "hcn_c404_spearman = create_stat_df(hcn_c404_spearman_PREC_ACC_NC, hcn_c404_spearman_TK, \"spearman\") #noqa : E501\n",
    "\n",
    "# percent bias\n",
    "hcn_c404_pbias_PREC_ACC_NC = hcn_c404_yearly.groupby(\"ID\").apply(lambda x: pbias(x[\"PREC_ACC_NC_c404\"], x[\"PREC_ACC_NC_hcn\"])) #noqa : E501\n",
    "hcn_c404_pbias_PREC_ACC_NC.name = \"PREC_ACC_NC_c404\"\n",
    "hcn_c404_pbias_TK = hcn_c404_yearly.groupby(\"ID\").apply(lambda x: pbias(x[\"TK_c404\"], x[\"TK_hcn\"])) #noqa : E501\n",
    "hcn_c404_pbias_TK.name = \"TK_c404\"\n",
    "\n",
    "hcn_c404_pbias = create_stat_df(hcn_c404_pbias_PREC_ACC_NC, hcn_c404_pbias_TK, \"pbias\") #noqa : E501\n",
    "\n",
    "# create dataframe of all goodness of fit stats\n",
    "hcn_gof_df = pd.concat([hcn_c404_bias, hcn_c404_mae, hcn_c404_rmse, hcn_c404_pearson, hcn_c404_spearman, hcn_c404_pbias]) #noqa : E501\n",
    "hcn_gof_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat the summary statistics and goodness of fit statistics and sort by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_sort(df_list: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Concat dataframes, fill NaN, and sort final dataframe.\"\"\"\n",
    "    df = pd.concat(df_list)\n",
    "    df = df.sort_values(by=\"ID\")\n",
    "    return df\n",
    "\n",
    "\n",
    "hcn_c404_stats = concat_sort([hcn_c404_summary, hcn_gof_df])\n",
    "hcn_c404_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example export of descriptive stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hcn_c404_stats.to_parquet(\"./file/path/to/c404_hcn_drb_descriptive_stats.parquet\", index=False) #noqa : E501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shut down the client and cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "You have now calculated the statistics needed to start making decisions about CONUS404 and it's ability to be used as a forcing dataset. You have done this by comparing CONUS404 to other gridded datasets (PRISM, CERES-EBAF) and against two sources of ground truthing (HCN, CRN).\n",
    "\n",
    "The next notebook will walk you through different visualizations, including comparing maps of the gridded and point data, boxplots, histograms, and other visualizations of the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda:pangeo",
   "language": "python",
   "name": "conda-pangeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6b8b815d206080047d0881750c260f2e84eb4576ca4137e2355fa3de469693c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
