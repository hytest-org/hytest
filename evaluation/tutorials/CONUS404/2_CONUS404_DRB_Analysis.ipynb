{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create zonal statistics and point extractions for comparing CONUS404 and reference datasets\n",
    "\n",
    "<img src='../../../doc/assets/Eval_Analysis.svg' width=600>\n",
    "\n",
    "Now that the data has been prepared, it is time to compute zonal statistics and perform point extractions. \n",
    "\n",
    "<details>\n",
    "  <summary>Guide to pre-requisites and learning outcomes...&lt;click to expand&gt;</summary>\n",
    "  \n",
    "  <table>\n",
    "    <tr>\n",
    "      <td>Pre-Requisites\n",
    "      <td>To get the most out of this notebook, you should already have an understanding of these topics: \n",
    "        <ul>\n",
    "        <li>the following summary statistics: mean, median, standard deviation, bias, mean absolute error (MAE), root mean squared error (RMSE), Pearson and Spearman correlation coefficients, and percent bias.\n",
    "        <li>the HUC (Hydrologic Unit Code) system of identification of drainage basins used by the USGS, in particular the HUC6 designation used here.\n",
    "        </ul>\n",
    "    <tr>\n",
    "      <td>Expected Results\n",
    "      <td>At the end of this notebook, you will produce: \n",
    "        <ul>\n",
    "        <li>Tables of descriptive statistics for the Delaware River Basin for each HUC6 designation within the basin, for forcing and reference datasets of precipitation, temperature, and net radiation.\n",
    "        <li>Further processed datasets for subsequent use in a separate visualization notebook\n",
    "        </ul>\n",
    "  </table>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The evaluation of forcings data can be performed either directly to some observational or reference data, or indirectly. An example of an indirect evaluation is to use multiple forcing datasets to drive a hydrological model and compare simulated outputs with observations (e.g., streamflow to gaged streamflow) to assess the influence of different forcing data on hydrologic model outputs. Indirect approaches are computationally expensive, thus we demonstrate in this notebook how to perform a direct evaluation of forcing data to multiple reference datasets. The methods for a direct evaluation consist of calculating descriptive and comparative statistics between forcing and reference datasets for several variables (precipitation, temperature, and net radiation).  \n",
    "\n",
    "### How does this notebook evaluate forcing data?\n",
    "First, data from gridded datasets (forcing: CONUS404; reference: PRISM, CERES-EBAF) are spatially summarized by computing area-weighted means over HUC6 spatial units within the Delaware River Basin. Next, annual means are computed over consistent periods of record. From these, some commonly-used descriptive statistics are calculated and tabulated. Similarly, annual means of point (station-based) reference data (GHCN, CRN) and annual means of forcing data extracted from nearest (to station) grid-points are computed, and descriptive statistics calculated and tabulated. For further evaluation and visualization, data processed within the notebook are output for subsequent use by a separate visualization notebook (see graphic above).\n",
    "    \n",
    "The following statistics will be calculated for comparative analysis between CONUS404 and each reference dataset:\n",
    "\n",
    "<ul>\n",
    "    <li>mean</li>\n",
    "    <li>median</li>\n",
    "    <li>standard deviation (stdev)</li>\n",
    "    <li>bias</li>\n",
    "    <li>mean absolute error (mae)</li>\n",
    "    <li>root-mean square error (rmse)</li>\n",
    "    <li>Pearson's correlation</li>\n",
    "    <li>Spearman's correlation</li>\n",
    "    <li>percent bias (pbias)</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "### Data References\n",
    "- [CONUS404 (**CON**tiguous United States for **40** years at **4**-km resolution)](https://doi.org/10.5066/P9PHPK4F)\n",
    "- [PRISM (**P**arameter-elevation **R**egressions on **I**ndependent **S**lopes **M**odel)](https://prism.oregonstate.edu/)\n",
    "- [CERES-EBAF (**C**louds and **E**arth's **R**adiant **E**nergy **S**ystems - **E**nergy **B**alanced **A**nd **F**illed)](https://ceres.larc.nasa.gov/data/)\n",
    "- [GHCN (**G**lobal **H**istorical **C**limate **N**etwork)](https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily)\n",
    "- [CRN (**C**limate **R**eference **N**etwork)](https://www.ncei.noaa.gov/products/land-based-station/us-climate-reference-network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# library imports\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import cf_xarray\n",
    "import dask\n",
    "import fsspec \n",
    "import geopandas as gpd\n",
    "import hvplot.xarray\n",
    "import intake\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pygeohydro import WBD\n",
    "import sparse \n",
    "import warnings\n",
    "import xarray as xr\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the standard statistical metrics for our comparative statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run script for available functions\n",
    "%run ../../../evaluation/Metrics_StdSuite_v1.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the intake catalog, we will load CONUS404 data for the Delaware River Basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "# connect to HyTEST catalog\n",
    "url = 'https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml'\n",
    "cat = intake.open_catalog(url)\n",
    "\n",
    "# access tutorial catalog\n",
    "conus404_drb_cat = cat[\"conus404-drb-eval-tutorial-catalog\"]\n",
    "list(conus404_drb_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Start a Dask client using an appropriate Dask Cluster** \n",
    "This is an optional step, but can speed up data loading significantly, especially when accessing data from the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sydney comment: Do we want to use the start_cluster ipynb here that Gene created?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_cluster(machine):\n",
    "    ''' Helper function to configure cluster\n",
    "    '''\n",
    "    if machine == 'denali':\n",
    "        from dask.distributed import LocalCluster, Client\n",
    "        cluster = LocalCluster(threads_per_worker=1)\n",
    "        client = Client(cluster)\n",
    "    \n",
    "    elif machine == 'tallgrass':\n",
    "        from dask.distributed import Client\n",
    "        from dask_jobqueue import SLURMCluster\n",
    "        cluster = SLURMCluster(queue='cpu', cores=1, interface='ib0',\n",
    "                               job_extra=['--nodes=1', '--ntasks-per-node=1', '--cpus-per-task=1'],\n",
    "                               memory='6GB')\n",
    "        cluster.adapt(maximum_jobs=30)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "    elif machine == 'local':\n",
    "        import os\n",
    "        import warnings\n",
    "        from dask.distributed import LocalCluster, Client\n",
    "        warnings.warn(\"Running locally can result in costly data transfers!\\n\")\n",
    "        n_cores = os.cpu_count() # set to match your machine\n",
    "        cluster = LocalCluster(threads_per_worker=n_cores)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "    ## do we want to change this to \"esip-nebari-gateway-v0.4\"    \n",
    "    elif machine in ['esip-qhub-gateway-v0.4']:   \n",
    "        import sys, os\n",
    "        sys.path.append(os.path.join(os.environ['HOME'],'shared','users','lib'))\n",
    "        import ebdpy as ebd\n",
    "        aws_profile = 'esip-qhub'  \n",
    "        ebd.set_credentials(profile=aws_profile)\n",
    "\n",
    "        aws_region = 'us-west-2'\n",
    "        endpoint = f's3.{aws_region}.amazonaws.com'\n",
    "        ebd.set_credentials(profile=aws_profile, region=aws_region, endpoint=endpoint)\n",
    "        worker_max = 30\n",
    "        client,cluster = ebd.start_dask_cluster(profile=aws_profile, worker_max=worker_max, \n",
    "                                              region=aws_region, use_existing_cluster=True,\n",
    "                                              adaptive_scaling=True, wait_for_cluster=False, \n",
    "                                              worker_profile='Medium Worker', propagate_env=True)\n",
    "        \n",
    "    return client, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup your client and dataset on Nebari or HPC like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'SLURM_CLUSTER_NAME' in os.environ: #USGS HPC use SLURM CLUSTER to handle jobs, otherwise...\n",
    "    machine = os.environ['SLURM_CLUSTER_NAME']\n",
    "    cluster = configure_cluster(machine)\n",
    "else:  # use the Nebari machine\n",
    "    machine = 'esip-qhub-gateway-v0.4'\n",
    "    client, cluster = configure_cluster(machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow outline:\n",
    "1. Read in the prepared dataset\n",
    "2. Read in the HUC6 boundaries and transform to same coordinate reference system as prepared dataset\n",
    "3. Make a data mask with the HUC6 boundaries to calculate zonal statistics\n",
    "4. Compute zonal statistics with data mask and prepared data\n",
    "\n",
    "Once all calculations are done: \n",
    "\n",
    "5. Combine each reference with benchmark into single dataset\n",
    "6. Export gridded data zonal statistics\n",
    "<br>\n",
    "\n",
    "## **Compute zonal statistics for gridded datasets**\n",
    "\n",
    "In the last tutorial, we prepared three gridded datasets: CONUS404 (benchmark), PRISM (reference), and CERES-EBAF (reference). The goal of this section is compute [zonal statistics](https://gisgeography.com/zonal-statistics/) for each HUC6 zone in the Delaware River Basin (DRB) by using the [conservative regridding method put forth by Ryan Abernathy](https://discourse.pangeo.io/t/conservative-region-aggregation-with-xarray-geopandas-and-sparse/2715) to regrid and perform an area-weight analysis.\n",
    "\n",
    "Dataset outline:\n",
    "<ol>\n",
    "    <li>Read in the prepared dataset</li>\n",
    "    <li>Compute bounding bands for grid cells then use these to create polygons in area-preserving CRS</li>\n",
    "    <li>Read in the HUC6 boundaries and transform to same coordinate reference system as prepared dataset</li>\n",
    "    <li>Overlay the dataset polygons over the HUC6 boundaries and create spatial weights matrices</li>\n",
    "    <li>Perform matrix multiplication between The prepared dataset and the spatial weights matrices</li>\n",
    "</ol>\n",
    "\n",
    "The following two functions will be used for regridding each dataset. An explanation of what they do will be provided when they are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounds_to_poly(x_bounds, y_bounds):\n",
    "    \"\"\"Return a polygon based on the x (longitude) and y (longitude) bounding band DataArrays\"\"\"\n",
    "    return Polygon([\n",
    "        (x_bounds[0], y_bounds[0]),\n",
    "        (x_bounds[0], y_bounds[1]),\n",
    "        (x_bounds[1], y_bounds[1]),\n",
    "        (x_bounds[1], y_bounds[0])\n",
    "    ])\n",
    "\n",
    "def apply_weights_matmul_sparse(weights, data):\n",
    "    \"\"\"Apply weights in a sparse matrices to data and regrid\"\"\"\n",
    "    assert isinstance(weights, sparse.SparseArray)\n",
    "    assert isinstance(data, np.ndarray)\n",
    "    data = sparse.COO.from_numpy(data)\n",
    "    data_shape = data.shape\n",
    "    n, k = data_shape[0], data_shape[1] * data_shape[2]\n",
    "    data = data.reshape((n, k))\n",
    "    weights_shape = weights.shape\n",
    "    k_, m = weights_shape[0] * weights_shape[1], weights_shape[2]\n",
    "    assert k == k_\n",
    "    weights_data = weights.reshape((k, m))\n",
    "\n",
    "    regridded = sparse.matmul(data, weights_data)\n",
    "    assert regridded.shape == (n, m)\n",
    "    return regridded.todense()\n",
    "\n",
    "    assert isinstance(weights, sparse.SparseArray)\n",
    "    assert isinstance(data, np.ndarray)\n",
    "    data = sparse.COO.from_numpy(data)\n",
    "    data_shape = data.shape\n",
    "    # k = nlat * nlon\n",
    "    n, k = data_shape[0], data_shape[1] * data_shape[2]\n",
    "    data = data.reshape((n, k))\n",
    "    weights_shape = weights.shape\n",
    "    k_, m = weights_shape[0] * weights_shape[1], weights_shape[2]\n",
    "    assert k == k_\n",
    "    weights_data = weights.reshape((k, m))\n",
    "\n",
    "    regridded = sparse.matmul(data, weights_data)\n",
    "    assert regridded.shape == (n, m)\n",
    "    return regridded.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the following `fsspec.filesystem` will be using to read in each dataset from an [Open Storage Network](https://www.openstoragenetwork.org/) bucket, which is read only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_read = fsspec.filesystem('s3', anon=True, skip_instance_cache=True,\n",
    "                            client_kwargs={'endpoint_url': 'https://renc.osn.xsede.org'})\n",
    "\n",
    "# Sydney comment, what is x and what is y\n",
    "x = \"x\"\n",
    "y = \"y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONUS404 zonal statistics**\n",
    "\n",
    "#### 1. Read in the HUC6 boundaries and transform to same coordinate reference system as prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in HUC6 boundaries found in the DRB\n",
    "drb_wbd = WBD(layer=\"huc6\", outfields=[\"huc6\", \"name\"])\n",
    "drb_gdf = drb_wbd.byids(\"huc6\",  [\"020401\", \"020402\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the dataframe\n",
    "drb_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area preserving coordinate reference system\n",
    "crs_area = \"ESRI:53034\"\n",
    "\n",
    "# set CRS to match c404_drb\n",
    "drb_gdf = drb_gdf.to_crs(crs_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "drb_gdf.plot(edgecolor=\"orange\", facecolor=\"purple\", linewidth=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Read CONUS404 data, previously processed over the DRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# open dataset\n",
    "c404_drb = conus404_drb_cat['conus404-drb-OSN'].to_dask()\n",
    "\n",
    "# crs\n",
    "c404_crs = c404_drb.rio.crs.to_proj4()\n",
    "\n",
    "c404_drb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize (uncomment to view)\n",
    "c404_drb.PREC_ACC_NC.hvplot(x=\"x\", y=\"y\", rasterize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Compute bounding bands for grid cells and use them to create polygons in area-preserving CRS\n",
    "\n",
    "Create the grid of c404_drb using any of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set var\n",
    "c404_var = \"TK\"\n",
    "\n",
    "# drop unneeded variable and coordinates\n",
    "c404_grid = c404_drb[[c404_var]].drop(['time', 'lon', 'lat', c404_var]).reset_coords()\n",
    "c404_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create bounding bands then stack into points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bounds\n",
    "c404_grid = c404_grid.cf.add_bounds(x)\n",
    "c404_grid = c404_grid.cf.add_bounds(y)\n",
    "\n",
    "# stack\n",
    "c404_points = c404_grid.stack(point=(y,x))\n",
    "c404_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the `xarray apply_ufunc` function to apply the `bounds_to_poly` function above to the _c404_points_ DataSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_boxes = xr.apply_ufunc(\n",
    "    bounds_to_poly,\n",
    "    c404_points.x_bounds,\n",
    "    c404_points.y_bounds,\n",
    "    input_core_dims=[(\"bounds\",),  (\"bounds\",)],\n",
    "    output_dtypes=[np.dtype('O')],\n",
    "    vectorize=True\n",
    ")\n",
    "c404_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create geodataframe from boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_grid_df= gpd.GeoDataFrame(\n",
    "    data={\"geometry\": c404_boxes.values, \"y\": c404_boxes[y], \"x\": c404_boxes[x]},\n",
    "    index=c404_boxes.indexes[\"point\"],\n",
    "    crs=c404_crs\n",
    ")\n",
    "c404_grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize (uncomment to view)\n",
    "c404_grid_df.plot(edgecolor=\"red\", facecolor=\"white\", linewidth=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Overlay the dataset polygons we just created over the HUC6 boundaries and create spatial weights matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert DRB to conus404 crs\n",
    "c404_drb_gdf = drb_gdf.to_crs(c404_crs)\n",
    "\n",
    "#perform overlay\n",
    "c404_overlay = c404_grid_df.overlay(c404_drb_gdf, keep_geom_type=True)\n",
    "c404_overlay.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot overlay for single HUC6\n",
    "c404_overlay[c404_overlay.huc6 == \"020402\"].geometry.plot(edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute grid cell fractions. \n",
    "\n",
    "A cell fraction is defined as the area of the grid cell divided by the area of the target polygon (HUC6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_grid_cell_fraction = c404_overlay.geometry.area.groupby(c404_overlay.huc6).transform(lambda x: x / x.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse DataArray (for an in-depth description of sparse data, see [this article](https://www.techopedia.com/definition/9480/sparse-array#:~:text=17%20May%2C%202017-,What%20Does%20Sparse%20Array%20Mean%3F,array%20in%20digital%20data%20handling.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_multi_index = c404_overlay.set_index([y, x, \"huc6\"]).index\n",
    "c404_df_weights = pd.DataFrame({\"weights\": c404_grid_cell_fraction.values}, index=c404_multi_index)\n",
    "\n",
    "# create xarray dataset\n",
    "c404_ds_weights = xr.Dataset(c404_df_weights)\n",
    "\n",
    "# generate sparse data array\n",
    "c404_weights_sparse = c404_ds_weights.unstack(sparse=True, fill_value=0.).weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Perform matrix multiplication between the prepared dataset and the spatial weights matrices\n",
    "\n",
    "Matrix multiplication across each DataArray. We do this for precipitation ('PREC_ACC_NC'), net radiation ('RNET'), and temperature ('TK')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "    \n",
    "    # precipitation\n",
    "    c404_precip_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        c404_weights_sparse,\n",
    "        c404_drb[\"PREC_ACC_NC\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )\n",
    "    \n",
    "    # net radiation\n",
    "    c404_rnet_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        c404_weights_sparse,\n",
    "        c404_drb[\"RNET\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )\n",
    "    \n",
    "    # temperature\n",
    "    c404_tk_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        c404_weights_sparse,\n",
    "        c404_drb[\"TK\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge DataArrays into Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_regridded = xr.Dataset({\"PREC_NC_ACC\":c404_precip_regridded, \"RNET\":c404_rnet_regridded, \"TK\": c404_tk_regridded})\n",
    "c404_regridded = c404_regridded.drop(\"crs\")\n",
    "c404_regridded.attrs = c404_drb.attrs\n",
    "c404_regridded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert to DataFrame. Now we see we have our timeseries for each variable and each HU06 within DRB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c404_df = c404_regridded.load().to_dataframe()\n",
    "c404_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up our data. We will reset our index and modify our dates to represent the year-month instead of reporting on the last day of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "c404_zonal_stats = c404_df.reset_index(drop=False)\n",
    "\n",
    "# convert time to string and remove days\n",
    "c404_zonal_stats[\"time\"] = c404_zonal_stats[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "# drop 1979 as it only had three months of data\n",
    "c404_zonal_stats = c404_zonal_stats[c404_zonal_stats['time'].str.contains(\"1979\") == False]\n",
    "\n",
    "c404_zonal_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRISM zonal statistics**\n",
    "\n",
    "Now, in the next cell, we'll run through the same steps to compute zonal statistics for the two PRISM variables, temperature (TK) and precipitation (PREC_ACC_NC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset\n",
    "prism_drb = conus404_drb_cat['prism-drb-OSN'].to_dask()\n",
    "\n",
    "# prism crs\n",
    "prism_crs = 4269 # EPSG:4269 = NAD83\n",
    "\n",
    "# create the grid of c404_drb using any of the variables\n",
    "prism_var = \"TK\"\n",
    "\n",
    "# drop unneeded variable and coordinates\n",
    "prism_grid = prism_drb[[prism_var]].drop(\n",
    "    ['time', prism_var]).reset_coords().load()\n",
    "\n",
    "\n",
    "# add bounds\n",
    "prism_grid = prism_grid.cf.add_bounds(x)\n",
    "prism_grid = prism_grid.cf.add_bounds(y)\n",
    "\n",
    "# stack\n",
    "prism_points = prism_grid.stack(point=(y,x))\n",
    "\n",
    "# apply bounds_to method\n",
    "prism_boxes = xr.apply_ufunc(\n",
    "    bounds_to_poly,\n",
    "    prism_points.x_bounds,\n",
    "    prism_points.y_bounds,\n",
    "    input_core_dims=[(\"bounds\",),  (\"bounds\",)],\n",
    "    output_dtypes=[np.dtype('O')],\n",
    "    vectorize=True\n",
    ")\n",
    "\n",
    "# create geodataframe from boxes\n",
    "prism_grid_df= gpd.GeoDataFrame(\n",
    "    data={\"geometry\": prism_boxes.values, \"y\": prism_boxes[y], \"x\": prism_boxes[x]},\n",
    "    index=prism_boxes.indexes[\"point\"],\n",
    "    crs=prism_crs\n",
    ")\n",
    "\n",
    "# convert DRB to conus404 crs\n",
    "prism_drb_gdf = drb_gdf.to_crs(epsg=prism_crs)\n",
    "\n",
    "# overlay the two grids\n",
    "prism_overlay = prism_grid_df.overlay(prism_drb_gdf, keep_geom_type=True)\n",
    "\n",
    "# grid cell fractions\n",
    "prism_grid_cell_fraction = prism_overlay.geometry.area.groupby(prism_overlay.huc6).transform(lambda x: x / x.sum())\n",
    "\n",
    "# create sparse dataarray\n",
    "prism_multi_index = prism_overlay.set_index([y, x, \"huc6\"]).index\n",
    "prism_df_weights = pd.DataFrame({\"weights\": prism_grid_cell_fraction.values}, index=prism_multi_index)\n",
    "\n",
    "prism_ds_weights = xr.Dataset(prism_df_weights)\n",
    "\n",
    "prism_weights_sparse = prism_ds_weights.unstack(sparse=True, fill_value=0.).weights\n",
    "\n",
    "# Matrix multiplication across each DataArray\n",
    "with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "    prism_precip_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        prism_weights_sparse,\n",
    "        prism_drb[\"PREC_ACC_NC\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )\n",
    "\n",
    "    prism_tk_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        prism_weights_sparse,\n",
    "        prism_drb[\"TK\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )\n",
    "# merge DataArrays into Dataset\n",
    "prism_regridded = xr.Dataset({\"PREC_NC_ACC\":prism_precip_regridded, \"TK\": prism_tk_regridded})\n",
    "prism_regridded.attrs = prism_drb.attrs\n",
    "\n",
    "# Covert to DataFrame\n",
    "prism_df = prism_regridded.load().to_dataframe()\n",
    "prism_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view our new PRISM data for DRB\n",
    "prism_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up the PRISM data as we have done with the CONUS404 data. We will reset the index and clean up the date formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index and add time back\n",
    "prism_zonal_stats = prism_df.reset_index(drop=False)\n",
    "\n",
    "# convert time to string and remove days\n",
    "prism_zonal_stats[\"time\"] = prism_zonal_stats[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "prism_zonal_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics\n",
    "\n",
    "We will now compute descriptive statistics between the previously computed CONUS404 and PRISM zonal statistics. The statistics that will be calculated are mean, median, standard deviation, bias, MAE, RMSE, Pearson's correlation, Spearman's r, and percent bias. \n",
    "\n",
    "The overall process will look like this:\n",
    "1. Merge zonal stats together on HUC6 and time values\n",
    "2. Resample data to 1 year means of monthly values\n",
    "3. Calculate each statistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the PRISM and CONUS404 zonals stats together based on the HUC6 code and time\n",
    "prism_c404_zonal = prism_zonal_stats.merge(c404_zonal_stats, left_on=['huc6', 'time'], right_on=['huc6', 'time'], suffixes=[\"_prism\", \"_c404\"])\n",
    "\n",
    "#drop RNET\n",
    "prism_c404_zonal.drop(\"RNET\", axis=1, inplace=True)\n",
    "\n",
    "prism_c404_zonal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time column to datetime type\n",
    "prism_c404_zonal[\"time\"] = pd.to_datetime(prism_c404_zonal[\"time\"], format=\"%Y-%m\")\n",
    "prism_c404_zonal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prism_c404_zonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda list env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below cell errored out for Sydney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample done by HUC6 as the index is HUC6 and year\n",
    "prism_c404_yearly = prism_c404_zonal.resample(\"1Y\", on=\"time\").agg(\n",
    "    \"PREC_NC_ACC_c404\": \"sum\",\n",
    "    \"PREC_NC_ACC_prism\": \"sum\",\n",
    "    \"TK_c404\": \"mean\",\n",
    "    \"TK_prism\": \"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_c404_yearly.reset_index(drop=False, inplace=True)\n",
    "prism_c404_yearly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate mean, median, and standard deviation of yearly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mean, median, standard deviation\n",
    "prism_c404_mean = prism_c404_yearly.mean()\n",
    "prism_c404_median = prism_c404_yearly.median()\n",
    "prism_c404_stdev = prism_c404_yearly.std()\n",
    "\n",
    "#create dataframe\n",
    "prism_c404_stats = pd.DataFrame({\"annual_mean\": prism_c404_mean, \"median\": prism_c404_median, \"stdev\": prism_c404_stdev}).T.drop(\"time\", axis=1)\n",
    "#prism_c404_stats = pd.DataFrame({\"annual_mean\": prism_c404_mean, \"annual_median\": prism_c404_median, \"annual_stdev\": prism_c404_stdev}).T.drop(\"time\", axis=1)\n",
    "\n",
    "# reset index and rename\n",
    "prism_c404_stats = prism_c404_stats.reset_index(drop=False).rename({\"index\":\"stat\"}, axis=1)\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias\n",
    "prism_c404_stats_annual_mean = prism_c404_stats.loc[prism_c404_stats['stat'] == \"annual_mean\"]\n",
    "prism_c404_bias_precip = float(prism_c404_stats_annual_mean[\"PREC_NC_ACC_c404\"] - prism_c404_stats_annual_mean[\"PREC_NC_ACC_prism\"])\n",
    "prism_c404_bias_tk = float(prism_c404_stats_annual_mean[\"TK_c404\"] - prism_c404_stats_annual_mean[\"TK_prism\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "prism_c404_stats.loc[len(prism_c404_stats.index)] = [\"bias\", prism_c404_bias_precip, None, prism_c404_bias_tk, None]\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE and RMSE is then calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sydney: we can add an additional stats ipynb to read in like we do for streamflow evaluation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE\n",
    "prism_c404_mae_precip = sum(abs(prism_c404_yearly[\"PREC_NC_ACC_c404\"] - prism_c404_yearly[\"PREC_NC_ACC_prism\"]))/len(prism_c404_yearly)\n",
    "prism_c404_mae_tk = sum(abs(prism_c404_yearly[\"TK_c404\"] - prism_c404_yearly[\"TK_prism\"]))/len(prism_c404_yearly)\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "prism_c404_stats.loc[len(prism_c404_stats.index)] = [\"MAE\", prism_c404_mae_precip, None, prism_c404_mae_tk, None]\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE\n",
    "prism_c404_rmse_precip = math.sqrt(np.square(np.subtract(prism_c404_yearly[\"PREC_NC_ACC_c404\"], prism_c404_yearly[\"PREC_NC_ACC_prism\"])).mean())\n",
    "prism_c404_rmse_tk = math.sqrt(np.square(np.subtract(prism_c404_yearly[\"TK_c404\"], prism_c404_yearly[\"TK_prism\"])).mean())\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "prism_c404_stats.loc[len(prism_c404_stats.index)] = [\"RMSE\", prism_c404_rmse_precip, None, prism_c404_rmse_tk, None]\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../../evaluation/Metrics_StdSuite_v1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearsons correlation\n",
    "prism_c404_pearson_precip = pearson_r(prism_c404_yearly[\"PREC_NC_ACC_c404\"], prism_c404_yearly[\"PREC_NC_ACC_prism\"])\n",
    "prism_c404_pearson_tk = pearson_r(prism_c404_yearly[\"TK_c404\"], prism_c404_yearly[\"TK_prism\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "prism_c404_stats.loc[len(prism_c404_stats.index)] = [\"Pearson\", prism_c404_pearson_precip, None, prism_c404_pearson_tk, None]\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman's correlation\n",
    "prism_c404_spearman_precip = spearman_r(prism_c404_yearly[\"PREC_NC_ACC_c404\"], prism_c404_yearly[\"PREC_NC_ACC_prism\"])\n",
    "prism_c404_spearman_tk = spearman_r(prism_c404_yearly[\"TK_c404\"], prism_c404_yearly[\"TK_prism\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "prism_c404_stats.loc[len(prism_c404_stats.index)] = [\"Spearman\", prism_c404_spearman_precip, None, prism_c404_spearman_tk, None]\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent bias\n",
    "prism_c404_pbias_precip = pbias(prism_c404_yearly[\"PREC_NC_ACC_prism\"], prism_c404_yearly[\"PREC_NC_ACC_c404\"])\n",
    "prism_c404_pbias_tk = pbias(prism_c404_yearly[\"TK_prism\"], prism_c404_yearly[\"TK_c404\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "prism_c404_stats.loc[len(prism_c404_stats.index)] = [\"pbias\", prism_c404_pbias_precip, None, prism_c404_pbias_tk, None]\n",
    "\n",
    "prism_c404_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Net radiation CERES-EBAF zonal statistics**\n",
    "\n",
    "Now, in the next cell, we'll run through the same steps to compute zonal statistics for the single CERES-EBAF variable, net radiation (RNET)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset\n",
    "ceres_drb = conus404_drb_cat['ceres-drb-OSN'].to_dask()\n",
    "\n",
    "# crs\n",
    "ceres_crs = 4326\n",
    "\n",
    "# create the grid of c404_drb using any of the variables\n",
    "ceres_var = \"RNET\"\n",
    "\n",
    "# drop unneeded variable and coordinates\n",
    "ceres_grid = ceres_drb[[ceres_var]].drop(\n",
    "    ['time', ceres_var]).reset_coords().load()\n",
    "\n",
    "\n",
    "# add bounds\n",
    "ceres_grid = ceres_grid.cf.add_bounds(x)\n",
    "ceres_grid = ceres_grid.cf.add_bounds(y)\n",
    "\n",
    "# stack\n",
    "ceres_points = ceres_grid.stack(point=(y, x))\n",
    "\n",
    "# apply bounds_to method\n",
    "ceres_boxes = xr.apply_ufunc(\n",
    "    bounds_to_poly,\n",
    "    ceres_points.x_bounds,\n",
    "    ceres_points.y_bounds,\n",
    "    input_core_dims=[(\"bounds\",),  (\"bounds\",)],\n",
    "    output_dtypes=[np.dtype('O')],\n",
    "    vectorize=True\n",
    ")\n",
    "\n",
    "# create geodataframe from boxes\n",
    "ceres_grid_df = gpd.GeoDataFrame(\n",
    "    data={\"geometry\": ceres_boxes.values,\n",
    "          \"y\": ceres_boxes[y], \"x\": ceres_boxes[x]},\n",
    "    index=ceres_boxes.indexes[\"point\"],\n",
    "    crs=ceres_crs\n",
    ")\n",
    "\n",
    "# convert DRB to conus404 crs\n",
    "ceres_drb_gdf = drb_gdf.to_crs(epsg=ceres_crs)\n",
    "\n",
    "# overlay the two grids\n",
    "ceres_overlay = ceres_grid_df.overlay(ceres_drb_gdf, keep_geom_type=True)\n",
    "\n",
    "# grid cell fractions\n",
    "ceres_grid_cell_fraction = ceres_overlay.geometry.area.groupby(\n",
    "    ceres_overlay.huc6).transform(lambda x: x / x.sum())\n",
    "\n",
    "# create sparse dataarray\n",
    "ceres_multi_index = ceres_overlay.set_index([y, x, \"huc6\"]).index\n",
    "ceres_df_weights = pd.DataFrame(\n",
    "    {\"weights\": ceres_grid_cell_fraction.values}, index=ceres_multi_index)\n",
    "\n",
    "ceres_ds_weights = xr.Dataset(ceres_df_weights)\n",
    "\n",
    "ceres_weights_sparse = ceres_ds_weights.unstack(\n",
    "    sparse=True, fill_value=0.).weights\n",
    "\n",
    "# Matrix multiplication across each DataArray\n",
    "with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "    ceres_rnet_regridded = xr.apply_ufunc(\n",
    "        apply_weights_matmul_sparse,\n",
    "        ceres_weights_sparse,\n",
    "        ceres_drb[\"RNET\"],\n",
    "        join=\"left\",\n",
    "        input_core_dims=[[\"y\", \"x\", \"huc6\"], [\"y\", \"x\"]],\n",
    "        output_core_dims=[[\"huc6\"]],\n",
    "        dask=\"parallelized\",\n",
    "        meta=[np.ndarray((0,))]\n",
    "    )\n",
    "\n",
    "# merge DataArrays into Dataset\n",
    "ceres_regridded = xr.Dataset({\"RNET\": ceres_rnet_regridded})\n",
    "ceres_regridded.attrs = ceres_drb.attrs\n",
    "\n",
    "# Covert to DataFrame\n",
    "ceres_df = ceres_regridded.load().to_dataframe()\n",
    "ceres_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index \n",
    "ceres_zonal_stats = ceres_df.reset_index(drop=False)\n",
    "\n",
    "# convert time to string and drop day\n",
    "ceres_zonal_stats[\"time\"] = ceres_zonal_stats[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "ceres_zonal_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics\n",
    "\n",
    "We will now compute descriptive statistics between the previously computed CONUS404 and CERES-EBAF zonal statistics. This is the same overall process that was done with the precipitation and temperature PRISM zonal stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the PRISM and CONUS404 zonals stats together based on the HUC6 code and time\n",
    "ceres_c404_zonal = ceres_zonal_stats.merge(c404_zonal_stats, left_on=['huc6', 'time'], right_on=['huc6', 'time'], suffixes=[\"_ceres\", \"_c404\"])\n",
    "\n",
    "#drop RNET\n",
    "ceres_c404_zonal.drop([\"PREC_NC_ACC\", \"TK\"], axis=1, inplace=True)\n",
    "\n",
    "ceres_c404_zonal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time column to datetime type\n",
    "ceres_c404_zonal[\"time\"] = pd.to_datetime(ceres_c404_zonal[\"time\"], format=\"%Y-%m\")\n",
    "ceres_c404_zonal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample done by HUC6 as the index is HUC6 and year\n",
    "ceres_c404_yearly = ceres_c404_zonal.resample(\"1Y\", on=\"time\").sum()\n",
    "ceres_c404_yearly.reset_index(drop=False, inplace=True)\n",
    "ceres_c404_yearly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean, median, standard devation\n",
    "ceres_c404_mean = ceres_c404_yearly.mean()\n",
    "ceres_c404_median = ceres_c404_yearly.median()\n",
    "ceres_c404_stdev = ceres_c404_yearly.std()\n",
    "\n",
    "#create dataframe\n",
    "ceres_c404_stats = pd.DataFrame({\"annual_mean\": ceres_c404_mean, \"median\": ceres_c404_median, \"stdev\": ceres_c404_stdev}).T.drop(\"time\", axis=1)\n",
    "\n",
    "# reset index and rename\n",
    "ceres_c404_stats = ceres_c404_stats.reset_index(drop=False).rename({\"index\":\"stat\"}, axis=1)\n",
    "\n",
    "ceres_c404_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Krista: this next cell is no longer working for me...seems to be failing on the \"add stat to bottom of dataframe command\"....\n",
    "\n",
    "Sydney: same 'ValueError: cannot set a row with mismatched columns'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias\n",
    "ceres_c404_stats_annual_mean = ceres_c404_stats.loc[ceres_c404_stats['stat'] == \"annual_mean\"]\n",
    "ceres_c404_bias_rnet = float(ceres_c404_stats_annual_mean[\"RNET_c404\"] - ceres_c404_stats_annual_mean[\"RNET_ceres\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "ceres_c404_stats.loc[len(ceres_c404_stats.index)] = [\"bias\", ceres_c404_bias_rnet, None]\n",
    "\n",
    "# MAE\n",
    "ceres_c404_mae_rnet = sum(abs(ceres_c404_yearly[\"RNET_c404\"] - ceres_c404_yearly[\"RNET_ceres\"]))/len(ceres_c404_yearly)\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "ceres_c404_stats.loc[len(ceres_c404_stats.index)] = [\"MAE\", ceres_c404_mae_rnet, None]\n",
    "\n",
    "# RMSE\n",
    "ceres_c404_rmse_rnet = math.sqrt(np.square(np.subtract(ceres_c404_yearly[\"RNET_c404\"], ceres_c404_yearly[\"RNET_ceres\"])).mean())\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "ceres_c404_stats.loc[len(ceres_c404_stats.index)] = [\"RMSE\", ceres_c404_rmse_rnet, None]\n",
    "\n",
    "# Pearsons correlation\n",
    "ceres_c404_pearson_rnet = pearson_r(ceres_c404_yearly[\"RNET_c404\"], ceres_c404_yearly[\"RNET_ceres\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "ceres_c404_stats.loc[len(ceres_c404_stats.index)] = [\"Pearson\", ceres_c404_pearson_rnet, None]\n",
    "\n",
    "# Spearman's correlation\n",
    "ceres_c404_spearman_rnet = spearman_r(ceres_c404_yearly[\"RNET_c404\"], ceres_c404_yearly[\"RNET_ceres\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "ceres_c404_stats.loc[len(ceres_c404_stats.index)] = [\"Spearman\", ceres_c404_spearman_rnet, None]\n",
    "\n",
    "# percent bias\n",
    "ceres_c404_pbias_rnet = pbias(\n",
    "    ceres_c404_yearly[\"RNET_ceres\"], ceres_c404_yearly[\"RNET_c404\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "ceres_c404_stats.loc[len(ceres_c404_stats.index)] = [\"pbias\", ceres_c404_pbias_rnet, None]\n",
    "\n",
    "ceres_c404_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Extract gridded values to points**\n",
    "\n",
    "The goal of this section is extract values from CONUS404 where they overlay spatially and temporally with station data. This concept is described in an article about the ESRI tool [Extract Values to Points](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-analyst/extract-values-to-points.htm). After extracting the values, the same descriptive statistics used to compare the gridded datasets will be run.\n",
    "\n",
    "Process outline:\n",
    "1. Read in the prepared dataset\n",
    "2. Use latitude and longitude of each point to extract data from matching CONUS404 grid cell\n",
    "<br>\n",
    "\n",
    "**Climate Reference Network point extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb_df = conus404_drb_cat['crn-drb-OSN'].read()\n",
    "\n",
    "# create geodataframe\n",
    "crn_drb = gpd.GeoDataFrame(crn_drb_df, crs=4326,\n",
    "                       geometry=gpd.points_from_xy(crn_drb_df.LONGITUDE, \n",
    "                                                         crn_drb_df.LATITUDE))\n",
    "\n",
    "# convert time to string and drop day\n",
    "crn_drb[\"DATE\"] = crn_drb[\"DATE\"].astype(str).str[:-3]\n",
    "\n",
    "crn_drb.rename({\"DATE\": \"time\",\n",
    "                \"TK\": \"TK_crn\", \n",
    "                \"PREC_ACC_NC\": \"PREC_ACC_NC_crn\"},\n",
    "                  axis=1, inplace=True)\n",
    "\n",
    "crn_drb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get coordinates from CRN stations to index CONUS404 DRB by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# isolate single row and transform to c404_drb crs\n",
    "crn_coords_gdf = crn_drb.iloc[[0]].to_crs(c404_crs)\n",
    "\n",
    "# extract lat/long values\n",
    "crn_lat = crn_coords_gdf.iloc[0][\"geometry\"].y\n",
    "crn_lon = crn_coords_gdf.iloc[0][\"geometry\"].x\n",
    "\n",
    "# get min and max time\n",
    "crn_time_min = crn_drb_df[\"time\"].min()\n",
    "crn_time_max = crn_drb_df[\"time\"].max()\n",
    "\n",
    "# subset c404_drb to lat/long using nearest\n",
    "c404_crn_sub = c404_drb.sel(x=crn_lon, y=crn_lat, method=\"nearest\")\n",
    "\n",
    "# slice to time-steps of crn_drb\n",
    "c404_crn_sub = c404_crn_sub.sel(time=slice(crn_time_min, crn_time_max))\n",
    "\n",
    "c404_crn_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert subset to dataframe and reorganize columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_crn_sub_df = c404_crn_sub.to_dataframe().reset_index(drop=False)\n",
    "\n",
    "# trim columns\n",
    "c404_crn_sub_df = c404_crn_sub_df[[\"time\", \"TK\", \"PREC_ACC_NC\"]]\n",
    "\n",
    "# rename columns\n",
    "c404_crn_sub_df.rename({\"TK\": \"TK_c404\", \n",
    "                    \"PREC_ACC_NC\": \"PREC_ACC_NC_c404\"},\n",
    "                  axis=1, inplace=True)\n",
    "\n",
    "# trim time\n",
    "c404_crn_sub_df[\"time\"] = c404_crn_sub_df[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "c404_crn_sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine CONUS404 subset with CRN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_c404_point = crn_drb.merge(c404_crn_sub_df, on=\"time\").reset_index(drop=False)\n",
    "\n",
    "# drop columns\n",
    "crn_c404_point.drop([\"index\", \"LATITUDE\", \"LONGITUDE\", \"ID\", \"geometry\"], axis=1, inplace=True)\n",
    "\n",
    "crn_c404_point.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time column to datetime type\n",
    "crn_c404_point[\"time\"] = pd.to_datetime(crn_c404_point[\"time\"], format=\"%Y-%m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics\n",
    "\n",
    "We will now compute descriptive statistics between the point extracted CONUS404 and CRN data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample to yearly means\n",
    "crn_c404_yearly = crn_c404_point.resample(\"1Y\", on=\"time\").agg(\n",
    "    \"PREC_NC_ACC_c404\": \"sum\",\n",
    "    \"PREC_NC_ACC_crn\": \"sum\",\n",
    "    \"TK_c404\": \"mean\",\n",
    "    \"TK_crn\": \"mean\"\n",
    ")\n",
    "crn_c404_yearly.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# mean, median, standard devation\n",
    "crn_c404_mean = crn_c404_yearly.mean()\n",
    "crn_c404_median = crn_c404_yearly.median()\n",
    "crn_c404_stdev = crn_c404_yearly.std()\n",
    "\n",
    "#create dataframe\n",
    "crn_c404_stats = pd.DataFrame({\"annual_mean\": crn_c404_mean, \"median\": crn_c404_median, \"stdev\": crn_c404_stdev}).T.drop(\"time\", axis=1)\n",
    "\n",
    "# reset index and rename\n",
    "crn_c404_stats = crn_c404_stats.reset_index(drop=False).rename({\"index\":\"stat\"}, axis=1)\n",
    "\n",
    "# bias\n",
    "crn_c404_stats_annual_mean = crn_c404_stats.loc[crn_c404_stats['stat'] == \"annual_mean\"]\n",
    "crn_c404_bias_precip = float(crn_c404_stats_annual_mean[\"PREC_ACC_NC_c404\"] - crn_c404_stats_annual_mean[\"PREC_ACC_NC_crn\"])\n",
    "crn_c404_bias_tk = float(crn_c404_stats_annual_mean[\"TK_c404\"] - crn_c404_stats_annual_mean[\"TK_crn\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "crn_c404_stats.loc[len(crn_c404_stats.index)] = [\"bias\", crn_c404_bias_precip, None, crn_c404_bias_rnet, None, crn_c404_bias_tk, None]\n",
    "\n",
    "# MAE\n",
    "crn_c404_mae_precip = sum(abs(crn_c404_yearly[\"PREC_ACC_NC_c404\"] - crn_c404_yearly[\"PREC_ACC_NC_crn\"]))/len(crn_c404_yearly)\n",
    "crn_c404_mae_tk = sum(abs(crn_c404_yearly[\"TK_c404\"] - crn_c404_yearly[\"TK_crn\"]))/len(crn_c404_yearly)\n",
    "# add stat to bottom of dataframe\n",
    "crn_c404_stats.loc[len(crn_c404_stats.index)] = [\"MAE\", crn_c404_mae_precip, None, crn_c404_mae_rnet, None, crn_c404_mae_tk, None]\n",
    "\n",
    "# RMSE\n",
    "crn_c404_rmse_precip = math.sqrt(np.square(np.subtract(crn_c404_yearly[\"PREC_ACC_NC_c404\"], crn_c404_yearly[\"PREC_ACC_NC_crn\"])).mean())\n",
    "crn_c404_rmse_tk = math.sqrt(np.square(np.subtract(crn_c404_yearly[\"TK_c404\"], crn_c404_yearly[\"TK_crn\"])).mean())\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "crn_c404_stats.loc[len(crn_c404_stats.index)] = [\"RMSE\", crn_c404_rmse_precip, None, crn_c404_rmse_rnet, None, crn_c404_rmse_tk, None]\n",
    "\n",
    "# Pearsons correlation\n",
    "crn_c404_pearson_precip = pearson_r(crn_c404_yearly[\"PREC_ACC_NC_c404\"], crn_c404_yearly[\"PREC_ACC_NC_crn\"])\n",
    "crn_c404_pearson_tk = pearson_r(crn_c404_yearly[\"TK_c404\"], crn_c404_yearly[\"TK_crn\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "crn_c404_stats.loc[len(crn_c404_stats.index)] = [\"pearson\", crn_c404_pearson_precip, None, crn_c404_pearson_rnet, None, crn_c404_pearson_tk, None]\n",
    "\n",
    "# Spearman's correlation\n",
    "crn_c404_spearman_precip = spearman_r(crn_c404_yearly[\"PREC_ACC_NC_c404\"], crn_c404_yearly[\"PREC_ACC_NC_crn\"])\n",
    "crn_c404_spearman_tk = spearman_r(crn_c404_yearly[\"TK_c404\"], crn_c404_yearly[\"TK_crn\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "crn_c404_stats.loc[len(crn_c404_stats.index)] = [\"spearman\", crn_c404_spearman_precip, None, crn_c404_spearman_rnet, None, crn_c404_spearman_tk, None]\n",
    "\n",
    "# percent bias\n",
    "crn_c404_pbias_precip = pbias(crn_c404_yearly[\"PREC_ACC_NC_c404\"], crn_c404_yearly[\"PREC_ACC_NC_crn\"])\n",
    "crn_c404_pbias_tk = pbias(crn_c404_yearly[\"TK_c404\"], crn_c404_yearly[\"TK_crn\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "crn_c404_stats.loc[len(crn_c404_stats.index)] = [\"pbias\", crn_c404_pbias_precip, None, crn_c404_pbias_rnet, None, crn_c404_pbias_tk, None]\n",
    "\n",
    "crn_c404_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample to yearly means\n",
    "crn_c404_yearly = crn_c404_point.resample(\"1Y\", on=\"time\").agg(\n",
    "    \"PREC_NC_ACC_c404\": \"sum\",\n",
    "    \"PREC_NC_ACC_crn\": \"sum\",\n",
    "    \"TK_c404\": \"mean\",\n",
    "    \"TK_crn\": \"mean\"\n",
    ")\n",
    "crn_c404_yearly.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# mean, median, standard devation\n",
    "crn_c404_mean = crn_c404_yearly.mean()\n",
    "crn_c404_median = crn_c404_yearly.median()\n",
    "crn_c404_stdev = crn_c404_yearly.std()\n",
    "\n",
    "#create dataframe\n",
    "crn_c404_stats = pd.DataFrame({\"annual_mean\": crn_c404_mean, \"median\": crn_c404_median, \"stdev\": crn_c404_stdev}).T.drop(\"time\", axis=1)\n",
    "crn_c404_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crn_c404_point.to_parquet(\"s3://nhgf-development/workspace/tutorial/CONUS404/crn_c404_point.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Historical Climate Network (HCN) point extraction**\n",
    "\n",
    "The HCN data is different than the CRN data as the HCN data comes from multiple stations whereas the CRN data was from a single station. This will involve using multiple sets of geographic coordinates to extract data from CONUS404."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dataset\n",
    "hcn_drb_df = conus404_drb_cat['hcn-drb-OSN'].read()\n",
    "\n",
    "#rename columns\n",
    "hcn_drb_df.rename({\"DATE\": \"time\",\n",
    "                \"TK\": \"TK_hcn\",  \n",
    "                \"PREC_ACC_NC\": \"PREC_ACC_NC_hcn\"},\n",
    "                  axis=1, inplace=True)\n",
    "\n",
    "# change DATE field to string\n",
    "hcn_drb_df[\"time\"] = hcn_drb_df[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "# hcn_drb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a DataFrame of the station IDs, lats, and longs to use for extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hcn_stations = hcn_drb_df.copy().drop([\"time\", \"TK_hcn\", \"PREC_ACC_NC_hcn\"], axis=1)\n",
    "hcn_stations[\"LONGITUDE\"] = pd.to_numeric(hcn_stations[\"LONGITUDE\"])\n",
    "hcn_stations[\"LATITUDE\"] = pd.to_numeric(hcn_stations[\"LATITUDE\"])\n",
    "\n",
    "hcn_stations = hcn_stations.groupby('ID').mean().reset_index(drop=False)\n",
    "# hcn_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a GeoDataFrame to convert the lat and long to the coordinate system of CONUS404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_stations_gdf = gpd.GeoDataFrame(hcn_stations, crs=4326,\n",
    "                       geometry=gpd.points_from_xy(hcn_stations.LONGITUDE, \n",
    "                                                         hcn_stations.LATITUDE))\n",
    "\n",
    "# transform to c404_drb crs\n",
    "hcn_stations_gdf = hcn_stations_gdf.to_crs(c404_crs)\n",
    "\n",
    "# extract lat/long values and give them the ID of the stations\n",
    "target_lon = xr.DataArray(hcn_stations_gdf[\"geometry\"].x.to_numpy(), dims=\"ID\", coords=dict(ID=hcn_stations_gdf.ID))\n",
    "target_lat = xr.DataArray(hcn_stations_gdf[\"geometry\"].y.to_numpy(), dims=\"ID\", coords=dict(ID=hcn_stations_gdf.ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset `c404_drb` to time period of HCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time min/max\n",
    "hcn_time_min = hcn_drb_df[\"time\"].min()\n",
    "hcn_time_max = hcn_drb_df[\"time\"].max()\n",
    "\n",
    "# slice c404 to HCN time and drop unused vars\n",
    "c404_hcn_timesub = c404_drb.sel(time=slice(hcn_time_min, hcn_time_max)).drop_vars(\"RNET\")\n",
    "\n",
    "\n",
    "# rename data_vars names\n",
    "name_dict = dict(TK=\"TK_c404\", PREC_ACC_NC=\"PREC_ACC_NC_c404\")\n",
    "c404_hcn_timesub = c404_hcn_timesub.rename(name_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data from points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to points\n",
    "c404_stations_sub = c404_hcn_timesub.sel(y=target_lat, x=target_lon, method='nearest').compute()\n",
    "\n",
    "# covert to a dataframe and reset the index\n",
    "c404_stations_df = c404_stations_sub.to_dataframe().reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge CONUS404 observations to HCN observations using the station ID and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare df to merge back to hcn_drb_df\n",
    "# start by subsetting columns\n",
    "c404_stations_df = c404_stations_df[[\"ID\", \"time\", \"TK_c404\", \"PREC_ACC_NC_c404\"]]\n",
    "\n",
    "# convert time to string\n",
    "c404_stations_df[\"time\"] = c404_stations_df[\"time\"].dt.strftime(\"%Y-%m\")\n",
    "\n",
    "# merge into hcn_drb_df\n",
    "hcn_c404_point = pd.merge(hcn_drb_df, c404_stations_df, on=[\"ID\", \"time\"])\n",
    "\n",
    "# convert time column to datetime type\n",
    "hcn_c404_point[\"time\"] = pd.to_datetime(hcn_c404_point[\"time\"], format=\"%Y-%m\")\n",
    "\n",
    "hcn_c404_point.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics\n",
    "\n",
    "We will now compute descriptive statistics between the point extracted CONUS404 and HCN data.\n",
    "\n",
    "# TESTING: CODE BELOW UNDERGOING REFACTOR TO MAKE FUNCTIONS RUN BY STATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearly means\n",
    "hcn_c404_yearly = hcn_c404_point.groupby(\"ID\").resample(\"1Y\", on=\"time\").agg(\n",
    "    {\"PREC_ACC_NC_c404\": \"sum\",\n",
    "    \"PREC_ACC_NC_hcn\": \"sum\",\n",
    "    \"TK_c404\": \"mean\",\n",
    "    \"TK_hcn\": \"mean\"}\n",
    ")\n",
    "\n",
    "hcn_c404_yearly = hcn_c404_yearly.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mean\n",
    "hcn_c404_mean = hcn_c404_yearly.mean(level=\"ID\")\n",
    "hcn_c404_mean[\"stat\"] = \"mean\"\n",
    "\n",
    "# mean\n",
    "hcn_c404_median = hcn_c404_yearly.median(level=\"ID\")\n",
    "hcn_c404_median[\"stat\"] = \"median\"\n",
    "\n",
    "#stdev\n",
    "hcn_c404_std = hcn_c404_yearly.std(level=\"ID\")\n",
    "hcn_c404_std[\"stat\"] = \"stdev\"\n",
    "\n",
    "#summary stats\n",
    "hcn_c404_summary = (\n",
    "    pd.concat([\n",
    "    hcn_c404_mean,\n",
    "    hcn_c404_median,\n",
    "    hcn_c404_std\n",
    "    ])\n",
    "    .sort_index()\n",
    "    .reset_index(drop=False)\n",
    ")\n",
    "\n",
    "# reorder columns to make more table more readable\n",
    "hcn_c404_columns = [\"ID\", \"stat\", \"TK_c404\", \"TK_hcn\", \"PREC_ACC_NC_c404\", \"PREC_ACC_NC_hcn\"]\n",
    "hcn_c404_summary = hcn_c404_summary[hcn_c404_columns]\n",
    "\n",
    "hcn_c404_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sydney comment for below cell, we should put this in an additional notebook within the evaluation dir under hytest and just call them in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom stat functions\n",
    "def bias(obs, est):\n",
    "    return obs - est\n",
    "\n",
    "def mae(obs, est, n):\n",
    "    return sum(abs(obs - est))/n\n",
    "\n",
    "def rmse(obs, est):\n",
    "    return math.sqrt(np.square(np.subtract(obs, est)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of comparative stats then transform into pandas dataframe\n",
    "\n",
    "# calculate stats\n",
    "# bias\n",
    "hcn_c404_stats_annual_mean = hcn_c404_summary.loc[hcn_c404_summary['stat'] == \"mean\"]\n",
    "hcn_c404_bias_PREC_ACC_NC = hcn_c404_stats_annual_mean.groupby(\"ID\").apply(lambda x: bias(x[\"PREC_ACC_NC_c404\"], x[\"PREC_ACC_NC_hcn\"]))\n",
    "hcn_c404_bias_TK = hcn_c404_stats_annual_mean.groupby(\"ID\").apply(lambda x: bias(x[\"TK_c404\"], x[\"TK_hcn\"]))\n",
    "# MAE\n",
    "hcn_c404_mae_PREC_ACC_NC = hcn_c404_yearly.groupby(\"ID\").apply(lambda x: mae(x[\"PREC_ACC_NC_c404\"], x[\"PREC_ACC_NC_hcn\"], x[\"PREC_ACC_NC_c404\"].count()))\n",
    "hcn_c404_mae_TK = hcn_c404_yearly.groupby(\"ID\").apply(lambda x: mae(x[\"TK_c404\"], x[\"TK_hcn\"], x[\"TK_c404\"].count()))\n",
    "# RMSE\n",
    "hcn_c404_rmse_PREC_ACC_NC = hcn_c404_yearly.groupby(\"ID\").apply(lambda x: rmse(x[\"PREC_ACC_NC_c404\"], x[\"PREC_ACC_NC_hcn\"]))\n",
    "hcn_c404_rmse_TK = hcn_c404_yearly.groupby(\"ID\").apply(lambda x: rmse(x[\"TK_c404\"], x[\"TK_hcn\"]))\n",
    "# Pearson r\n",
    "hcn_c404_pearson_PREC_ACC_NC = hcn_c404_yearly.groupby(\"ID\").apply(lambda x: pearson_r(x[\"PREC_ACC_NC_c404\"], x[\"PREC_ACC_NC_hcn\"]))\n",
    "hcn_c404_pearson_TK = hcn_c404_yearly.groupby(\"ID\").apply(lambda x: pearson_r(x[\"TK_c404\"], x[\"TK_hcn\"]))\n",
    "# Spearman r\n",
    "hcn_c404_spearman_PREC_ACC_NC = hcn_c404_yearly.groupby(\"ID\").apply(lambda x: spearman_r(x[\"PREC_ACC_NC_c404\"], x[\"PREC_ACC_NC_hcn\"]))\n",
    "hcn_c404_spearman_TK = hcn_c404_yearly.groupby(\"ID\").apply(lambda x: spearman_r(x[\"TK_c404\"], x[\"TK_hcn\"]))\n",
    "# percent bias\n",
    "hcn_c404_pbias_PREC_ACC_NC = hcn_c404_yearly.groupby(\"ID\").apply(lambda x: pbias(x[\"PREC_ACC_NC_c404\"], x[\"PREC_ACC_NC_hcn\"]))\n",
    "hcn_c404_pbias_TK = hcn_c404_yearly.groupby(\"ID\").apply(lambda x: pbias(x[\"TK_c404\"], x[\"TK_hcn\"]))\n",
    "\n",
    "# create dictionary from results\n",
    "hcn_gof_dict = dict(hcn_c404_bias_PREC_ACC_NC=hcn_c404_bias_PREC_ACC_NC.tolist(), \n",
    "                    hcn_c404_mae_PREC_ACC_NC=hcn_c404_mae_PREC_ACC_NC.tolist(), \n",
    "                    hcn_c404_rmse_PREC_ACC_NC=hcn_c404_rmse_PREC_ACC_NC.tolist(),\n",
    "                    hcn_c404_pearson_PREC_ACC_NC=hcn_c404_pearson_PREC_ACC_NC.tolist(), \n",
    "                    hcn_c404_spearman_PREC_ACC_NC=hcn_c404_spearman_PREC_ACC_NC.tolist(), \n",
    "                    hcn_c404_pbias_PREC_ACC_NC=hcn_c404_pbias_PREC_ACC_NC.tolist(),\n",
    "                    hcn_c404_bias_TK=hcn_c404_bias_TK.tolist(), \n",
    "                    hcn_c404_mae_TK=hcn_c404_mae_TK.tolist(), \n",
    "                    hcn_c404_rmse_TK=hcn_c404_rmse_TK.tolist(),\n",
    "                    hcn_c404_pearson_TK=hcn_c404_pearson_TK.tolist(), \n",
    "                    hcn_c404_spearman_TK=hcn_c404_spearman_TK.tolist(), \n",
    "                    hcn_c404_pbias_TK=hcn_c404_pbias_TK.tolist())\n",
    "\n",
    "#create DataFrame from dictionary\n",
    "hcn_gof_df = pd.DataFrame.from_dict(hcn_gof_dict)\n",
    "\n",
    "# set the index to the station IDs\n",
    "hcn_gof_index = pd.Index(hcn_c404_yearly.reset_index()[\"ID\"].unique())\n",
    "hcn_gof_df.set_index(hcn_gof_index, inplace=True)\n",
    "hcn_gof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_c404_yearly[hcn_c404_yearly.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## next step is to solve NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_TK_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old work below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bias\n",
    "hcn_c404_stats_annual_mean = hcn_c404_summary.loc[hcn_c404_summary['stat'] == \"med\"\n",
    "hcn_c404_bias_precip = hcn_c404_stats_annual_mean[\"PREC_ACC_NC_c404\"] - hcn_c404_stats_annual_mean[\"PREC_ACC_NC_hcn\"]\n",
    "hcn_c404_bias_tk = hcn_c404_stats_annual_mean[\"TK_c404\"] - hcn_c404_stats_annual_mean[\"TK_hcn\"]\n",
    "hcn_c404_bias_precip, hcn_c404_bias_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample to yearly means\n",
    "# hcn_c404_yearly = hcn_c404_point.resample(\"1Y\", on=\"time\").mean()\n",
    "# hcn_c404_yearly.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# mean, median, standard devation\n",
    "hcn_c404_mean = hcn_c404_yearly.mean()\n",
    "hcn_c404_median = hcn_c404_yearly.median()\n",
    "hcn_c404_stdev = hcn_c404_yearly.std()\n",
    "\n",
    "#create dataframe\n",
    "hcn_c404_stats = pd.DataFrame({\"annual_mean\": hcn_c404_mean, \"median\": hcn_c404_median, \"stdev\": hcn_c404_stdev}).T.drop(\"time\", axis=1)\n",
    "\n",
    "# reset index and rename\n",
    "hcn_c404_stats = hcn_c404_stats.reset_index(drop=False).rename({\"index\":\"stat\"}, axis=1)\n",
    "\n",
    "# bias\n",
    "hcn_c404_stats_annual_mean = hcn_c404_stats.loc[hcn_c404_stats['stat'] == \"annual_mean\"]\n",
    "hcn_c404_bias_precip = float(hcn_c404_stats_annual_mean[\"PREC_ACC_NC_c404\"] - hcn_c404_stats_annual_mean[\"PREC_ACC_NC_hcn\"])\n",
    "hcn_c404_bias_tk = float(hcn_c404_stats_annual_mean[\"TK_c404\"] - hcn_c404_stats_annual_mean[\"TK_hcn\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "hcn_c404_stats.loc[len(hcn_c404_stats.index)] = [\"bias\", hcn_c404_bias_precip, None, hcn_c404_bias_tk, None]\n",
    "\n",
    "# MAE\n",
    "hcn_c404_mae_precip = sum(abs(hcn_c404_yearly[\"PREC_ACC_NC_c404\"] - hcn_c404_yearly[\"PREC_ACC_NC_hcn\"]))/len(hcn_c404_yearly)\n",
    "hcn_c404_mae_tk = sum(abs(hcn_c404_yearly[\"TK_c404\"] - hcn_c404_yearly[\"TK_hcn\"]))/len(hcn_c404_yearly)\n",
    "# add stat to bottom of dataframe\n",
    "hcn_c404_stats.loc[len(hcn_c404_stats.index)] = [\"MAE\", hcn_c404_mae_precip, None, hcn_c404_mae_tk, None]\n",
    "\n",
    "# RMSE\n",
    "hcn_c404_rmse_precip = math.sqrt(np.square(np.subtract(hcn_c404_yearly[\"PREC_ACC_NC_c404\"], hcn_c404_yearly[\"PREC_ACC_NC_hcn\"])).mean())\n",
    "hcn_c404_rmse_tk = math.sqrt(np.square(np.subtract(hcn_c404_yearly[\"TK_c404\"], hcn_c404_yearly[\"TK_hcn\"])).mean())\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "hcn_c404_stats.loc[len(hcn_c404_stats.index)] = [\"RMSE\", hcn_c404_rmse_precip, None, hcn_c404_rmse_tk, None]\n",
    "\n",
    "# Pearsons correlation\n",
    "hcn_c404_pearson_precip = pearson_r(hcn_c404_yearly[\"PREC_ACC_NC_c404\"], hcn_c404_yearly[\"PREC_ACC_NC_hcn\"])\n",
    "hcn_c404_pearson_tk = pearson_r(hcn_c404_yearly[\"TK_c404\"], hcn_c404_yearly[\"TK_hcn\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "hcn_c404_stats.loc[len(hcn_c404_stats.index)] = [\"pearson\", hcn_c404_pearson_precip, None, hcn_c404_pearson_tk, None]\n",
    "\n",
    "# Spearman's correlation\n",
    "hcn_c404_spearman_precip = spearman_r(hcn_c404_yearly[\"PREC_ACC_NC_c404\"], hcn_c404_yearly[\"PREC_ACC_NC_hcn\"])\n",
    "hcn_c404_spearman_tk = spearman_r(hcn_c404_yearly[\"TK_c404\"], hcn_c404_yearly[\"TK_hcn\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "hcn_c404_stats.loc[len(hcn_c404_stats.index)] = [\"spearman\", hcn_c404_spearman_precip, None, hcn_c404_spearman_tk, None]\n",
    "\n",
    "# percent bias\n",
    "hcn_c404_pbias_precip = pbias(hcn_c404_yearly[\"PREC_ACC_NC_c404\"], hcn_c404_yearly[\"PREC_ACC_NC_hcn\"])\n",
    "hcn_c404_pbias_tk = pbias(hcn_c404_yearly[\"TK_c404\"], hcn_c404_yearly[\"TK_hcn\"])\n",
    "\n",
    "# add stat to bottom of dataframe\n",
    "hcn_c404_stats.loc[len(hcn_c404_stats.index)] = [\"pbias\", hcn_c404_pbias_precip, None, hcn_c404_pbias_tk, None]\n",
    "\n",
    "hcn_c404_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hcn_c404_point.to_parquet(\"s3://nhgf-development/workspace/tutorial/CONUS404/hcn_c404_point.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shut down the client and cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close(); cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: CONUS404 Visualization notebook\n",
    "\n",
    "Now that we have moved through our zonal and point statistics calculations, we can move on to visualizing the results in the CONUS404 Visualization notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Last code cell of the notebook\n",
    "# import watermark.watermark as watermark\n",
    "# print(watermark(iversions=True, python=True, machine=True, globals_=globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-users-pangeo",
   "language": "python",
   "name": "conda-env-users-users-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6b8b815d206080047d0881750c260f2e84eb4576ca4137e2355fa3de469693c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
