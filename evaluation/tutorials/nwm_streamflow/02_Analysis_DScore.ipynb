{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis - Dscore\n",
    "## Streamflow Benchmarking Tutorial\n",
    "\n",
    "<img src='../../../doc/assets/Eval_Analysis.svg' width=600>\n",
    "\n",
    "> This notebook adapted from originals by Timothy Hodson and Rich Signell. See that upstream work at:\n",
    ">* https://github.com/thodson-usgs/dscore\n",
    ">* https://github.com/USGS-python/hytest-evaluation-workflows/\n",
    "\n",
    "## Essential Benchmark Components\n",
    "This benchmark notebook will present a workflow which follows a canonical model for Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations; \n",
    "2) The domain (e.g. space or time) over which to benchmark;\n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "## What is the D-score Suite?\n",
    "The 'D-score suite' follows the mathematical decompostions of the MSLE outlined in [Hodson et al., 2021](https://doi.org/10.1029/2021MS002681) and 'd-score' is a score out of 100 that can be translated to the percentage of the mean squared logarithmic error (MSLE) between the observed and modeled or simulated data for each component of a decomposition. Essentially, d-score is a flexible framework for evaluating models using orthogonal components (or approximately orthogonal in some cases). We can think of a components within a decomposition similarly to ingredients in making a peanut butter and jelly sandwich, where the components are peanut butter, jelly, and bread and the entire item to be decomposed is the sandwich itself (where the sandwich is equivalent to the mean squared log error). This might break down to 20% peanut butter, 60% bread, 15% peanut butter, and 25% jelly/jam that make up the sandwich or error. If this analogy is too confusing, please see [Hodson et al., 2021](https://doi.org/10.1029/2021MS002681) for a more scientific explanation. Essentially, we can think of error in terms being composed of parts, some of the total error being related to timing and some being related to magnitude. Error can be decomposed into many parts, seasons, percentiles, etc. Let's calculate the MSLE between our simulated/modeled and observational streamflow in this notebook to demostrate what is happening.\n",
    "\n",
    "#### Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load libraries needed for the _02_Analysis_Dscore.ipynb_ notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import intake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data\n",
    "\n",
    "<img src='../../../doc/assets/Eval_Analysis_Data.svg' width=360>\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) **A set of predictions and matching observations**,  <span style=\"color:red; font-size:large\"><<--You are here</span>\n",
    "2) The domain over which to benchmark \n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Finding and loading data is made easier for this particular workflow (the _streamflow_ variable), in that most of it has been \n",
    "pre-processed and stored in a cloud-friendly format.  That process is described in the [data preparation](01_Data_Prep.ipynb)\n",
    "notebook. We will proceed here using the already-prepared data for _streamflow_, which is included in the HyTEST **intake catalog**.  \n",
    "\n",
    "> Learn more about `intake` [here](../../../dataset_catalog/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = intake.open_catalog(r'https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml')\n",
    "print(\"Available datasets: \\n\", \"\\n\".join(cat.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above list represents the processed datasets available for benchmarking.  If a dataset\n",
    "you want is not in that list, \n",
    "you can load data directly from S3 or onprem. \n",
    "If you load data from a source other than this list, you can jump to **Step 2: Restrict to a Domain**\n",
    "\n",
    "Note that the interesting datasets in the cataloged dataset above are duplicated: Some are `-onprem` , some are `-cloud`, and some are some are `-osn`. Same data, but the storage location and access protocol will be different. You \n",
    "will definitely want to use the correct copy of the data for your computing environment. The idea is to \"bring the compute to the data\".\n",
    "* `onprem` : Direct access via the USGS internal `caldera` filesystem from _denali_ or _tallgrass_, and now on _hovenweep_.\n",
    "* `osn` : Network access via OSN pod, which uses the S3 API, suitable for consumption on any jupyter server (onprem, cloud, or local)\n",
    "* `cloud` : Network access via S3 bucket, suitable for consumption on cloud-hosted jupyter servers. You could also access using any network-attached computer, but the amount of data will likely saturate your connection.  Use in the cloud is okay (e.g. ESIP Nebari or HyTEST Nebari).\n",
    "\n",
    "So... are you on-prem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "onprem = (platform.node() in ['denali', 'tallgrass','hovenweep'])\n",
    "if onprem:\n",
    "    print(\"Yes : -onprem\")\n",
    "    obs_data_src='nwis-streamflow-usgs-gages-onprem'\n",
    "    mod_data_src='nwm21-streamflow-usgs-gages-onprem'\n",
    "else:\n",
    "    print(\"Not onprem; use '-osn' data source\")\n",
    "    obs_data_src='nwis-streamflow-usgs-gages-osn'\n",
    "    mod_data_src='nwm21-streamflow-usgs-gages-osn'\n",
    "print(\"Observed : \", obs_data_src)\n",
    "print(\"Modeled  : \", mod_data_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_of_interest = 'streamflow'\n",
    "try:\n",
    "    obs = cat[obs_data_src].to_dask()\n",
    "    mod = cat[mod_data_src].to_dask()\n",
    "except KeyError:\n",
    "    print(\"Something wrong with dataset names.\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    obs_data = obs[variable_of_interest]\n",
    "    mod_data = mod[variable_of_interest].astype('float32')\n",
    "except KeyError:\n",
    "    print(f\"{variable_of_interest} was not found in these data.\")\n",
    "\n",
    "obs_data.name = 'observed'\n",
    "mod_data.name = 'simulated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview observational data\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview modeled or simulated data\n",
    "mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Restrict to a Domain\n",
    "\n",
    "<img src='../../../doc/assets/Eval_Analysis_Domain.svg' width=360>\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations,  \n",
    "2) **The domain over which to benchmark** <span style=\"color:red; font-size:large\"><<--You are here</span>\n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Each benchmark domain is defined over specific bounds (typically space and/or time). \n",
    "Benchmark domain definitions can be defined within the notebook, or sourced from\n",
    "elsewhere. For this example, we use the _Cobalt_ gages, avaliable for download on ScienceBase \n",
    "([Foks et al., 2022](https://doi.org/10.5066/P972P42Z)). This is essentially a list of streamflow gages of interest to HyTEST, along with some metadata about that gage (watershed, \n",
    "reachcode, etc).  We will use this as a spatial selector to restrict the modeled and simulated data to only the gages found within this benchmarking domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is coming from ScienceBase, so if there is an error - it could be USGS ScienceBase is offline for repair.\n",
    "cobalt = pd.read_csv(\n",
    "    'https://www.sciencebase.gov/catalog/file/get/6181ac65d34e9f2789e44897?f=__disk__22%2F1a%2Fd2%2F221ad2fe9d95de17731ad35d0fc6b417a4b53ee1',\n",
    "    dtype={'site_no':str, 'huc_cd':str, 'reachcode':str, 'comid':str, 'gagesII_class':str, 'aggecoregion': str}, \n",
    "    index_col='site_no'\n",
    "    )\n",
    "\n",
    "# Re-format the gage_id/site_no string value.  ex:   \"1000000\"  ==> \"USGS-1000000\"\n",
    "cobalt.rename(index=lambda x: f'USGS-{x}', inplace=True)\n",
    "print(f\"{len(cobalt.index)} gages in this benchmark\")\n",
    "cobalt.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a domain dataset representing the stream gages (unique `site_no` values) identifying the locations making up the spatial domain of this benchmark. While we have good meta-data for these gages (lat/lon location, HUC8 code, etc), we really will only use the list of `site_no` values to select locations out of the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Metrics\n",
    "\n",
    "<img src='../../../doc/assets/Eval_Analysis_Metrics.svg' width=360>\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations,  \n",
    "2) The domain over which to benchmark, \n",
    "3) **A set of statistical metrics with which to benchmark.** <span style=\"color:red; font-size:large\"><<--You are here</span>\n",
    "\n",
    "The code to calculate the various metrics has been standardized \n",
    "in {doc}`/evaluation/Metrics_StdSuite_v1`. \n",
    "You can use these or write your own.  To import and use these standard definitions, run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../Metrics_DScore_Suite_v1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you use these functions or your own, we need to put all metric computation into a special all-encompasing \n",
    "benchmarking function--a single call which can be assigned to each gage in our domain list. This sort of arrangement \n",
    "is well-suited to parallelism with `dask`. \n",
    "\n",
    "If this is done well, the process will benefit enormously from task parallelism -- each gage can be given its own \n",
    "CPU to run on.  After all are done, the various results will be collected and assembled into a composite dataset. \n",
    "\n",
    "To achieve this, we need a single 'atomic' function that can execute independently. It will take the gage identifier \n",
    "as input and return a list of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wrapper function -- this func will be called once per gage_id, each call on its own dask worker\n",
    "def compute_benchmark(gage_id):\n",
    "    try:\n",
    "        ## obs_data and mod_data should be globals...\n",
    "        obs = obs_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series()\n",
    "        mod = mod_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series().resample('1D', offset='5h').mean() \n",
    "        \n",
    "        # make sure the indices match\n",
    "        obs.index = obs.index.to_period('D')\n",
    "        mod.index = mod.index.to_period('D')\n",
    "\n",
    "        # merge obs and predictions; drop NaNs.\n",
    "        gage_df = pd.merge(obs, mod, left_index=True, right_index=True).dropna(how='any')\n",
    "\n",
    "        # Log the streamflow data so its approximately normally distributed\n",
    "        obs_log = np.log(gage_df['observed'].clip(lower=0.01)) # clip to remove zeros and negative values\n",
    "        sim_log = np.log(gage_df['simulated'].clip(lower=0.01))\n",
    "\n",
    "        # Add or remove decomposition functions below\n",
    "        scores = pd.concat([\n",
    "                pd.Series([ mse(obs_log, sim_log) ], index=[\"mse\"], dtype='float64'),\n",
    "                bias_distribution_sequence(obs_log, sim_log),\n",
    "                stl(obs_log, sim_log),\n",
    "                seasonal_mse(obs_log, sim_log),\n",
    "                quantile_mse(obs_log, sim_log)\n",
    "                ]\n",
    "            )\n",
    "        scores.name=gage_id\n",
    "        return scores\n",
    "    except Exception as e:#<-- this is an extremely broad way to catch exceptions.  We only do it this way to ensure \n",
    "            #    that a failure on one benchmark (for a single stream gage) will not halt the entire run. \n",
    "        logging.info(\"Benchmark failed for %s\", gage_id)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test to be sure this `compute_benchmark()` function will return data for a single gage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_benchmark('USGS-01030350')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the Analysis \n",
    "We will be doing a lot of work in paralallel, using workers within a 'cluster'.  \n",
    "The details of cluster configuration are handled for us by 'helper' notebooks, below. \n",
    "You can override their function by doing your own cluster configuration if you like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the lines below to read in your AWS credentials if you want to access data from a requester-pays bucket (-cloud)\n",
    "os.environ['AWS_PROFILE'] = 'osn-hytest-scratch'\n",
    "%run ../../../environment_set_up/Help_AWS_Credentials.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start up a distributed cluster of workers\n",
    "## NOTE: This cluster configuration is VERY specific to the JupyterHub cloud environment on USGS internal HyTEST Nebari or ESIP Nebari\n",
    "%run ../../../environment_set_up/Start_Dask_Cluster_Nebari.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verified above that the `compute_benchmark` works on the \"hosted\" server (where this\n",
    "notebook is being executed. As a sanity check before we give the cluster of workers a lot \n",
    "to do, let's verify that we can have a remote worker process a gage by submitting work\n",
    "to one in isolation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.submit(compute_benchmark, 'USGS-01030350').result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got a benchmark function, and can prove that it works in remote workers \n",
    "within the cluster, we can dispatch a fleet of workers to process our data in parallel.\n",
    "We will make use of `dask` to do this using a dask '_bag_'.  \n",
    ">Read more about task parallelism with Dask and how we are using dask bags [here](../../../essential_reading/Parallel_Dask.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dask bag with the contents being a list of the cobalt gages\n",
    "import dask.bag as db\n",
    "\n",
    "# For the first 100 gages within the Cobalt list, try to map the compute benchmark function over them.\n",
    "# Remove '[0:100]' to run full list (may take awhile depending on cores/workers available)\n",
    "bag = db.from_sequence(cobalt.index.tolist()[0:100]).map(compute_benchmark)\n",
    "results = bag.compute() \n",
    "\n",
    "# preview first few gages with results\n",
    "results[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that 100 or so gages ran\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that big task done, we don't need `dask` parallelism any more. Let's shut down the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close(); del client\n",
    "cluster.close(); del cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the results\n",
    "The `bag` now contains a collection of return values (one per call to `compute_benchmark()`).  We can modify that into a table/dataframe for easier processing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [i for i in results if i is not None] # Drop entries where compute_benchmark failed\n",
    "results_df = pd.concat(results, axis=1).T\n",
    "results_df.index.name = 'site_no'\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe/table can be saved to disk as a CSV. It will be used for visualizations in [this notebook](03_Vizualization.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('DScore_streamflow_example.csv') ##<--- change this to a personalized filename"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global-global-pangeo",
   "language": "python",
   "name": "conda-env-global-global-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d7ebce313f85fb1ac8949e834c83f371584cb2422d845bf1570c1220fdedc716"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
