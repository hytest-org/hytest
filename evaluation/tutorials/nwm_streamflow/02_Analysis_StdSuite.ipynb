{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis - Standard\n",
    "## Streamflow Benchmarking Tutorial\n",
    "\n",
    "<img src='../../../doc/assets/Eval_Analysis.svg' width=600>\n",
    "\n",
    "## Essential Benchmark Components\n",
    "This benchmark notebook will present a workflow which follows a canonical model for Essential Benchmark Components: \n",
    "1) A set of predictions and matching observation (the data); \n",
    "2) The domain (e.g. space or time) over which to benchmark;\n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "## What is the \"Standard Suite\"?\n",
    "The 'Standard Suite' is a group of statistical metrics used for benchmarking streamflow from hydrologic models and comes from [Towler et al., 2023](https://doi.org/10.5194/hess-27-1809-2023). These metrics were chosen in order to examine multiple aspects of hydrologic model simulation of streamflow. Though, it is important to note that these metrics are applicable to other water budget components (similar to [d-score](02_Analysis_DScore.ipynb)). Let's calculate these series of statistical metrics between our simulated and observed data in this notebook, and then explore the results in the next.\n",
    "\n",
    "#### Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load libraries and configure Python computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data\n",
    "<img src='../../../doc/assets/Eval_Analysis_Data.svg' width=360>\n",
    "\n",
    "### Essential Benchmark Components: \n",
    "1) **A set of predictions and matching observations** <span style=\"color:red; font-size:large\"><<--You are here</span>\n",
    "2) The domain over which to benchmark \n",
    "3) A set of statistical metrics with which to benchmark.\n",
    "\n",
    "Finding and loading data is made easier for this particular notebook example (using the _streamflow_ variable), in that most of the required data has been pre-processed and stored in a cloud-friendly format. That process is described in the **[data preparation](01_Data_Prep.ipynb)**\n",
    "notebook. We will proceed here using the pre-prepared data for _streamflow_, which is included in the HyTEST **[intake catalog](../../../dataset_catalog/README.md)**.\n",
    "\n",
    "> Learn more about `intake` [here](../../../dataset_catalog/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "cat = intake.open_catalog(r'https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml')\n",
    "print(\"Available datasets: \\n\", \"\\n\".join(cat.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above list represents the processed datasets available for benchmarking.  If a dataset\n",
    "you want is not in that list, \n",
    "you can load data directly from S3 or 'onprem' (on-premises). \n",
    "If you load data from a source other than this list, you can jump to **Step 2: Restrict to a Domain**\n",
    "\n",
    "Note that some of the datasets in the cataloged list of data above are duplicated: Some are `-onprem` , some are `-cloud`, and some are `-osn`. This means it is the same data, but the storage location and access protocol will be different. \n",
    "You will definitely want to use the correct copy of the data for your computing environment. The idea is to \"bring the compute to the data\".\n",
    "* `onprem` : Direct access via the USGS internal `caldera` filesystem from _denali_ or _tallgrass_, and now on _hovenweep_.\n",
    "* `osn` : Network access via OSN pod, which uses the S3 API, suitable for consumption on any jupyter server (onprem, cloud, or local)\n",
    "* `cloud` : Network access via S3 bucket, suitable for consumption on cloud-hosted Jupyter servers. You could also access using any network-attached computer, but the amount of data will likely saturate your connection.  Use in the cloud is okay (e.g. ESIP Nebari or HyTEST Nebari).\n",
    "\n",
    "So... are you on-prem? If so, see and uncomment lines below. If not, don't worry the default is set up to pull data from the OSN. \n",
    ">Note: it is possible to still use datasets hosted on OSN when computing on the supercomputers, though we try to always 'bring the compute to the data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## No, I am on cloud or working locally\n",
    "## Then you'll want to use the following\n",
    "obs_data_src='nwis-streamflow-usgs-gages-osn'\n",
    "mod_data_src='nwm21-streamflow-usgs-gages-osn'\n",
    "print(\"Observed : \", obs_data_src)\n",
    "print(\"Modeled  : \", mod_data_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Yes I am on the supercomputer! \n",
    "## Then you'll want to uncomment and use the below datasets:\n",
    "#obs_data_src='nwis-streamflow-usgs-gages-onprem'\n",
    "#mod_data_src='nwm21-streamflow-usgs-gages-onprem'\n",
    "#print(\"Observed : \", obs_data_src)\n",
    "#print(\"Modeled  : \", mod_data_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_of_interest = 'streamflow'\n",
    "try:\n",
    "    obs = cat[obs_data_src].to_dask()\n",
    "    mod = cat[mod_data_src].to_dask()\n",
    "except KeyError:\n",
    "    print(\"Something wrong with dataset names.\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    obs_data = obs[variable_of_interest]\n",
    "    mod_data = mod[variable_of_interest].astype('float32')\n",
    "except KeyError:\n",
    "    print(f\"{variable_of_interest} was not found in these data.\")\n",
    "\n",
    "obs_data.name = 'observed'\n",
    "mod_data.name = 'simulated'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Restrict to a Domain\n",
    "\n",
    "<img src='../../../doc/assets/Eval_Analysis_Domain.svg' width=360>\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations,  \n",
    "2) **The domain over which to benchmark** <span style=\"color:red; font-size:large\"><<--You are here</span>\n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Each benchmark domain is defined over specific bounds (typically space and/or time). \n",
    "Benchmark domain definitions can be defined within the notebook, or sourced from\n",
    "elsewhere. For this example, we use the _Cobalt_ gages, avaliable for download on ScienceBase \n",
    "([Foks et al., 2022](https://doi.org/10.5066/P972P42Z)).  \n",
    "\n",
    "Each benchmark domain is defined over specific bounds (typically space and/or time). \n",
    "Benchmark domain definitions can be defined within the notebook, or sourced from\n",
    "elsewhere. For this example, we use the _Cobalt_ gages, avaliable for download on ScienceBase \n",
    "([Foks et al., 2022](https://doi.org/10.5066/P972P42Z)). This is essentially a list of streamflow gages of interest to HyTEST, along with some metadata about that gage (watershed, \n",
    "reachcode, etc).  We will use this as a spatial selector to restrict the observed and modeled/simulated data to only the gages found within this benchmarking domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cobalt = pd.read_csv(\n",
    "    'https://www.sciencebase.gov/catalog/file/get/6181ac65d34e9f2789e44897?f=__disk__22%2F1a%2Fd2%2F221ad2fe9d95de17731ad35d0fc6b417a4b53ee1',\n",
    "    dtype={'site_no':str, 'huc_cd':str, 'reachcode':str, 'comid':str, 'gagesII_class':str, 'aggecoregion': str}, \n",
    "    index_col='site_no'\n",
    "    )\n",
    "# Re-format the gage_id/site_no string value.  ex:   \"1000000\"  ==> \"USGS-1000000\"\n",
    "cobalt.rename(index=lambda x: f'USGS-{x}', inplace=True)\n",
    "print(f\"{len(cobalt.index)} gages in this benchmark\")\n",
    "cobalt.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a domain dataset representing the stream gages (unique `site_no` values) identifying the locations making up the spatial domain of this benchmark. While we have good metadata for these gages (lat/lon location, HUC8 code, etc), we will only use the list of `site_no` values to select locations out of the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Metrics\n",
    "\n",
    "<img src='../../../doc/assets/Eval_Analysis_Metrics.svg' width=360>\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations  \n",
    "2) The domain over which to benchmark \n",
    "3) **A set of statistical metrics with which to benchmark** <span style=\"color:red; font-size:large\"><<--You are here</span>\n",
    "\n",
    "The code to calculate the various NWM metrics has been standardized in\n",
    "{doc}`/evaluation/Metrics_StdSuite_v1`\n",
    "with usage examples in \n",
    "{doc}`/evaluation/tutorials/stats/Usage_StdSuite_v1`. \n",
    "You can use these metrics or write your own.  To import and use these standardized definitions, run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../Metrics_StdSuite_v1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you use these functions or your own, we need to put all metric computation into a special all-encompasing \n",
    "benchmarking function--a single call which can be assigned to each gage in our domain list. This sort of arrangement \n",
    "is well-suited to parallelism with `dask`. \n",
    "\n",
    "If this is done well, the process will benefit enormously from task parallelism -- each gage can be given its own \n",
    "CPU worker to run on.  After all are done, the various results will be collected and assembled into a composite dataset. \n",
    "\n",
    "To achieve this, we need a single 'atomic' function that can execute independently. It will take the gage identifier \n",
    "as input and return a list of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wrapper function -- this func will be called once per gage_id, each call on its own dask worker\n",
    "def compute_benchmark(gage_id):\n",
    "    try:\n",
    "        ## obs_data and mod_data should be globals...\n",
    "        obs = obs_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series()\n",
    "        mod = mod_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series().resample('1D', offset='5h').mean() # note the time shift for this\n",
    "    \n",
    "        # make sure the indices match\n",
    "        obs.index = obs.index.to_period('D')\n",
    "        mod.index = mod.index.to_period('D')\n",
    "\n",
    "        # merge obs and predictions; drop NaNs.\n",
    "        gage_df = pd.merge(obs, mod, left_index=True, right_index=True).dropna(how='any')\n",
    "        \n",
    "        scores = pd.Series(\n",
    "            data={\n",
    "                'NSE': NSE(gage_df.observed, gage_df.simulated),\n",
    "                'KGE': KGE(gage_df.observed, gage_df.simulated),\n",
    "                'logNSE': logNSE(gage_df.observed, gage_df.simulated),\n",
    "                'pbias': pbias(gage_df.observed, gage_df.simulated),\n",
    "                'rSD': rSD(gage_df.observed, gage_df.simulated),\n",
    "                'pearson': pearson_r(gage_df.observed, gage_df.simulated),\n",
    "                'spearman': spearman_r(gage_df.observed, gage_df.simulated), \n",
    "                'pBiasFMS': pBiasFMS(gage_df.observed, gage_df.simulated),\n",
    "                'pBiasFLV': pBiasFLV(gage_df.observed, gage_df.simulated),\n",
    "                'pBiasFHV': pBiasFHV(gage_df.observed, gage_df.simulated)\n",
    "            },\n",
    "            name=gage_id,\n",
    "            dtype='float64'\n",
    "        )\n",
    "        return scores\n",
    "    except Exception as e:#<-- this is an extremely broad way to catch exceptions.  We only do it this way to ensure \n",
    "                          #    that a failure on one benchmark (for a single stream gage) will not halt the entire run. \n",
    "        logging.info(\"Benchmark failed for %s\", gage_id)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test to be sure this `compute_benchmark()` function will return data for a single gage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_benchmark('USGS-01030350')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the Analysis \n",
    "We will be doing a lot of work in parallel, using workers within a 'cluster'.  \n",
    "The details of cluster configuration are handled for us by 'helper' notebooks, below. \n",
    "You can override their function by doing your own cluster configuration if you like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the lines below to read in your AWS credentials if you want to access data from a requester-pays bucket (-cloud)\n",
    "# os.environ['AWS_PROFILE'] = 'default'\n",
    "# %run ../../../environment_set_up/Help_AWS_Credentials.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../../environment_set_up/Start_Dask_Cluster_Nebari.ipynb\n",
    "### Executes external 'helper to spin up a cluster of workers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verified above that the `compute_benchmark` works on the \"hosted\" server (where this\n",
    "notebook is being executed. As a sanity check before we give the cluster of workers a lot \n",
    "to do, let's verify that we can have a remote worker process a gage by submitting work\n",
    "to one in isolation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.submit(compute_benchmark, 'USGS-01030350').result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got a benchmark function, and can prove that it works in remote workers \n",
    "within the cluster, we can dispatch a fleet of workers to process our data in parallel.\n",
    "We will make use of `dask` to do this using a dask '_bag_'.  \n",
    "\n",
    ">Read more about task parallelism with Dask and how we are using dask bags [here](../../../essential_reading/Parallel_Dask.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dask bag with the contents being a list of the cobalt gages\n",
    "import dask.bag as db\n",
    "\n",
    "# For the first 100 gages within the Cobalt list, try to map the compute benchmark function over them.\n",
    "# Remove '[0:100]' to run full list (may take awhile depending on cores/workers available)\n",
    "bag = db.from_sequence(cobalt.index.tolist()[0:100]).map(compute_benchmark)\n",
    "\n",
    "results = bag.compute() #<< Dispatch the workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview\n",
    "print(\"Number of gages:\", len(results))\n",
    "results[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that big task done, we don't need `dask` parallelism any more. Let's shut down the cluster. This is important to in order to free up workers for other users on cloud in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close(); del client\n",
    "cluster.close(); del cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the results\n",
    "The `bag` now contains a collection of return values (one per call to `compute_benchmark()`).  We can massage that into a table/dataframe for easier processing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [i for i in results if i is not None] # Drop entries where compute_benchmark failed\n",
    "results_df = pd.concat(r, axis=1).T\n",
    "results_df.index.name = 'site_no'\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe/table can be saved to disk as a CSV. It will be used for visualizations in [other notebooks](03_Vizualization.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('Standard_streamflow_example.csv') ##<--- change this to a personalized filename if desired."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global-global-pangeo",
   "language": "python",
   "name": "conda-env-global-global-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d7ebce313f85fb1ac8949e834c83f371584cb2422d845bf1570c1220fdedc716"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
