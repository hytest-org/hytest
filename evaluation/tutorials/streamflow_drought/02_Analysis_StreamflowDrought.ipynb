{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis - Streamflow Drought\n",
    "## Streamflow Drought Benchmarking Tutorial\n",
    "\n",
    "<img src='../../../doc/assets/Eval_Analysis.svg' width=600>\n",
    "\n",
    "\n",
    "## Essential Benchmark Components\n",
    "This benchmark notebook will present a workflow which follows a canonical model for Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations; \n",
    "2) The domain (e.g. space or time) over which to benchmark;\n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "#### Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import intake\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Streamflow Data\n",
    "\n",
    "Finding and loading data is made easier for this particular workflow (the _streamflow_ variable), in that most of it has been \n",
    "pre-processed and stored in a cloud-friendly format.  That process is described in the [data preparation](01_Data_Prep.ipynb)\n",
    "notebook. We will proceed here using the already-prepared data for _streamflow_, which is included in the HyTEST **intake catalog**. You can learn more about the python library `intake` [here](../../../dataset_catalog/README.md).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview what is in the HyTEST Catalog (if you have a dataset you'd like to add, please reach out on the HyTEST Github Page).\n",
    "hytest_cat = intake.open_catalog(r'https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml')\n",
    "list(hytest_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above list represents the processed datasets available for benchmarking and exploring in general.  \n",
    "If a dataset you want is not in that list, you can load data directly from another source.\n",
    "\n",
    "If you load data from a source other than this list, you can jump to **Step 2: Restrict to a Domain**\n",
    "\n",
    "Note that the datasets in the catalog above are duplicated: Some are `-onprem` , some are `-cloud`, and some are some are `-osn`. Same data, but the storage location and access protocol will be different. You will definitely want to use the correct copy of the data for your computing environment. The idea is to \"bring the compute to the data\" to reduce cost and increase efficiency.\n",
    "* `onprem` : Direct access via the USGS internal `caldera` filesystem from _denali_ or _tallgrass_, and now on _hovenweep_.\n",
    "* `osn` : Network access via OSN pod, which uses the S3 API, suitable for consumption on any jupyter server (onprem, cloud, or local)\n",
    "* `cloud` : Network access via S3 bucket, suitable for consumption on cloud-hosted jupyter servers. You could also access using any network-attached computer, but the amount of data will likely saturate your connection.  Use in the cloud is okay (e.g. ESIP Nebari or HyTEST Nebari).\n",
    "\n",
    "Before we select a dataset from the main catalog, we will explore the subcatalogs for pre-processed streamflow data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing HyTEST subcatalogs to find pre-processed datasets for benchmarking and benchmarking results\n",
    "bench_cat = hytest_cat['benchmarks-catalog']\n",
    "print('Benchmarks Catalog within HyTEST Catalog:')\n",
    "list(bench_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing HyTEST subcatalogs to find pre-processed datasets for streamflow benchmarking and benchmarking results\n",
    "stream_cat = bench_cat['streamflow-benchmarks-catalog']\n",
    "print('Streamflow Catalog within Benchmarks Catalog under the HyTEST Catalog Umbrella:')\n",
    "list(stream_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's explore what this dataset is! Looks like modeled output from NHM-PRMS version 1.0 forced with Daymet (byMuskObs calibration)\n",
    "stream_cat['nhmprms-conus-v1_0-daymetv3-byhrumuskobs-osn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see under the description that this is daily streamflow data from the particular modeling application we suspected. This is a subset of what is available in the model archive (see [Hay and LaFontaine, 2020](https://doi.org/10.5066/P9PGZE0S)) based on the description. \n",
    "\n",
    "##### For now we will work with National Water Model v2.1 data because it exists on OSN and in zarr format (same format as our pre-saved observational data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# National Water Model version 2.1 on OSN\n",
    "hytest_cat['nwm21-streamflow-usgs-gages-osn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observational and modeled datasets of interest exist `-on-prem` and on `-osn` as we can see from the dataset label in the `hytest_cat` above. If on-prem, we will change the variable to `TRUE` to access data close to the compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onprem = False # change to True if on the supercomputer(s)\n",
    "\n",
    "if onprem:\n",
    "    print(\"Yes : -onprem\")\n",
    "    obs_data_src='nwis-streamflow-usgs-gages-onprem'\n",
    "    mod_data_src='nwm21-streamflow-usgs-gages-onprem'\n",
    "else:\n",
    "    print(\"Not onprem; use '-osn' data source\")\n",
    "    obs_data_src='nwis-streamflow-usgs-gages-osn'\n",
    "    mod_data_src='nwm21-streamflow-usgs-gages-osn'\n",
    "print(\"Observed : \", obs_data_src)\n",
    "print(\"Modeled  : \", mod_data_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we identify the variable of interest (`variable_of_interest = 'streamflow'`) from our datasets. Depending on the data you use, you may need to modify this according the variable of interest to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_of_interest = 'streamflow'\n",
    "\n",
    "try:\n",
    "    obs = hytest_cat[obs_data_src].to_dask()\n",
    "    mod = hytest_cat[mod_data_src].to_dask()\n",
    "    \n",
    "except KeyError:\n",
    "    print(\"Something wrong with dataset names.\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    obs_data = obs[variable_of_interest]\n",
    "    mod_data = mod[variable_of_interest].astype('float32')\n",
    "except KeyError:\n",
    "    print(f\"{variable_of_interest} was not found in these data.\")\n",
    "\n",
    "obs_data.name = 'observed'\n",
    "mod_data.name = 'modeled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview observational data, check units on the variable streamflow, check timestep.\n",
    "obs\n",
    "#obs_data\n",
    "\n",
    "## Uncomment below to preview modeled data\n",
    "#mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Restrict to a domain (spatial and temporal)\n",
    "\n",
    "Each benchmark domain is defined over specific bounds (typically space and/or time). \n",
    "Benchmark domain definitions can be defined within the notebook, or sourced from \n",
    "elsewhere. For example, the _Cobalt_ gages, which is available for download on ScienceBase ([Foks et al., 2022](https://doi.org/10.5066/P972P42Z)), is used for the general streamflow benchmarking tutorials also available within the HyTEST repository. \n",
    " \n",
    "For this example, we use the set of gages from a data release [Simeone et al. (2024)](https://doi.org/10.5066/P9P4DHZE), avaliable for download on ScienceBase \n",
    "[here](https://doi.org/10.3390/w16202996). These gages are a subset of the _Cobalt_ gages [(Foks et al., 2022)](https://doi.org/10.5066/P972P42Z), but have been selected \n",
    "for their longer period of record which is necessary for streamflow drought evaluation. We will use this as a spatial selector to restrict the modeled and simulated data to only the gages found within this benchmarking domain.\n",
    "\n",
    "Reference:\n",
    "- Simeone, C.E., Staub, L.E., Kolb, K.R., and Foks, S.S., 2024, Results of benchmarking National Water Model v2.1 simulations of streamflow drought duration, severity, deficit, and occurrence in the conterminous United States: U.S. Geological Survey data release, https://doi.org/10.5066/P9P4DHZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is coming from ScienceBase, so if there is an error - it could be USGS ScienceBase is offline for repair.\n",
    "sites = pd.read_csv(\n",
    "    'https://www.sciencebase.gov/catalog/file/get/6410bbc9d34e22162d3e163e?f=__disk__01%2F51%2F49%2F015149051bb29d52b78cbff550385696bc5bbf31',\n",
    "    dtype={'site':str, 'class':str, 'region':str}, \n",
    "    index_col='site')\n",
    "\n",
    "# Re-format the gage_id/site_no string value.  ex:   \"1000000\"  ==> \"USGS-1000000\"\n",
    "sites.rename(index=lambda x: f'USGS-{x}', inplace=True)\n",
    "print(f\"{len(sites.index)} gages in this benchmark\")\n",
    "sites.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a domain dataset representing the streamgages (unique `site` values) identifying the locations making up the spatial domain of this benchmark. While we have good metadata for these gages (lat/lon location, aridity, etc), we really will only use the list of `site` values to select locations out of the raw data. This select happens in our function `compute_benchmark` below.\n",
    "\n",
    "### Restrict temporal range\n",
    "The temporal range can be restricted using `slice` (example shown below). For this tutorial, we restrict the temporal range in our main function `compute_benchmark` below.\n",
    "\n",
    "```\n",
    "start_date = \"1984-04-01\"\n",
    "end_date = \"2016-03-31\"\n",
    "obs_data = obs_data.sel(time = slice(start_date, end_date))\n",
    "mod_data = mod_data.sel(time = slice(start_date, end_date))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Metrics\n",
    "\n",
    "The code to calculate the various metrics has been standardized \n",
    "in `hytest/evaluation/Metrics_Streamflow_Drought_v1.ipynb`. \n",
    "\n",
    "You can use these functions or write your own.  To import and use these standard definitions, we run the following snippet in the next code cell:\n",
    "\n",
    "```\n",
    "%run ../../Metrics_Streamflow_Drought_v1.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source the ipynb in this one to access functions\n",
    "%run ../../Metrics_Streamflow_Drought_v1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you use these functions or your own, we need to put all metric computation into a special all-encompasing \n",
    "benchmarking function--a single call which can be assigned to each gage in our domain list. This sort of arrangement \n",
    "is well-suited to parallelism, especially with `dask`. \n",
    "\n",
    "If this is done well, the process will benefit enormously from task parallelism -- each gage can be given its own \n",
    "CPU to run on.  After all are done, the various results will be collected and assembled into a composite dataset. To achieve this, we need a single 'atomic' function that can execute independently. It will take the gage identifier as input and return the list of metrics specified in the function below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Experiment For One Gage\n",
    "\n",
    "We will try out some of the functions that are sourced above. We will identify a gage in our dataset, a beginning and end date, and units (very important)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gage id\n",
    "gage_id = \"USGS-01011000\"\n",
    "\n",
    "# Define start and end dates\n",
    "start_date = \"1984-04-01\"\n",
    "end_date = \"2016-03-31\"\n",
    "\n",
    "# Load each timeseries to a pandas series, trim time, resample if necessary\n",
    "obs = (obs_data.sel(gage_id=gage_id, time=slice(start_date, end_date)).load(scheduler='single-threaded').to_series())\n",
    "mod = (mod_data.sel(gage_id=gage_id, time=slice(start_date, end_date)).\n",
    "       load(scheduler='single-threaded').to_series().\n",
    "       resample('1D', offset='5h').mean()) # Note time shift here is specific to National Water Model v2.1 data.\n",
    "\n",
    "# Make sure the indices match\n",
    "obs.index = obs.index.to_period('D')\n",
    "mod.index = mod.index.to_period('D')\n",
    "\n",
    "# Merge obs and predictions; drop NaNs.\n",
    "gage_df = pd.merge(obs, mod, left_index=True, right_index=True).dropna(how='any')\n",
    "gage_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many metrics are sensitive or can be easily influenced by higher streamflows or non-normal distributions. In drought studies, it is common to use evaluation metrics on standardized drought indices or streamflow percentiles instead of directly on streamflow values [(Van Loon, 2015)](https://doi.org/10.1002/wat2.1085). Here we transform the streamflow data into percentiles according to a 'fixed' and 'variable' threshold system for identifying drought periods. We use the 5th, 10th, 20th, and 30th percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentiles\n",
    "df_pct = calculate_percentiles(gage_df, units = \"cms\")\n",
    "df_pct.head()\n",
    "len(df_pct) # length of the timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function expands drought events into a TRUE/FALSE timeseries of drought/non-drought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean threshold - flagging whether or timeseries is below threshold\n",
    "df_bool = calculate_site_boolean_threshold_only(df_pct)\n",
    "df_bool.head()\n",
    "# len(df_bool)  # This is the timeseries * 4 pct_types (fixed and variable, for obs and mod) * 4 threshold percentiles (5, 10, 20, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we calculate Cohen's kappa [(Cohen, 1960)](https://doi.org/10.1177/001316446002000104)  to understand how well our models are picking up periods of drought, it is a way to measure the agreement between categorical variables. [Landis and Koch (1977)](https://doi.org/10.2307/2529310) provide a guide for interpretation of Cohen's kappa values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohen's Kappa\n",
    "df_accuracy = site_cohens_kappa(df_bool, gage_id)\n",
    "df_accuracy\n",
    "\n",
    "# 'site' = fixed\n",
    "# Julian Day ('jd') = variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we calculate the Spearman's correlation, which starts out similarly to the standard Spearman’s r, where the observed and modeled streamflow are each ranked by magnitude across the entire study period. After calculating ranks, however, we subset the data into periods with observed droughts at the various threshold levels of interest. For example, we examine the lowest 20% of flow values for the 20th percentile drought threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Spearmans R\n",
    "df_spearmans = spearman_r(df_pct, gage_id, thresholds = [5,10,20,30])\n",
    "df_spearmans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the bias, percent bias, and ratio of standard deviations (rSD) is calculated (observed flows below the drought threshold versus the modeled flows below the drought threshold (percentiles, not volume).Bias has units of flow, percent bias has units of percent, rSD is unitless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bias \n",
    "df_bias_dist = bias_dist(df_pct, gage_id, thresholds = [5,10,20,30])\n",
    "df_bias_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate site properties\n",
    "properties, annual_stats = calculate_site_properties(df_pct, gage_id, \n",
    "                                                     thresholds = [5, 10, 20 , 30], \n",
    "                                                     percent_type_list = ['weibull_jd_obs', 'weibull_jd_mod', 'weibull_site_obs', 'weibull_site_mod'],\n",
    "                                                     flow_name_list = ['observed', 'modeled', 'observed', 'modeled'], \n",
    "                                                     start_cy = 1985, end_cy = 2016)\n",
    "\n",
    "#annual_stats.to_csv('01011000_annual_stats.csv')\n",
    "#properties.to_csv('01011000_properties.csv')\n",
    "properties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('columns:', annual_stats['measure'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we calculate the NMAE between drought signatures: duration, severity, intensity. \n",
    "\n",
    "For each timeseries, drought characteristics are examined on a Climate Year (Apr 1 - Mar 31) basis for each percentile threshold and fixed/variable method - so each year has a value - then we examine the NMAE between observational values and the modeled values of these drought characteristics (column 'Measures' below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate error on drought signatures\n",
    "sum_df = annual_signatures(annual_stats, gage_id)\n",
    "subset_df = sum_df[sum_df['measure'] == 'maximum_drought_intensity']\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define a function for multiple gages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wrapper function -- this func will be called once per gage_id, each call on its own dask worker\n",
    "def compute_benchmark(gage_id):\n",
    "    try:\n",
    "        # restrict temporal range (if needed) - retain data only between start and end dates\n",
    "        # note that observational data may be more sparse than modeled data and may not span every day\n",
    "        # for streamflow drought, we focus on Climate Years (Apr 1 - Mar 31)\n",
    "        start_date = \"1984-04-01\"\n",
    "        end_date = \"2016-03-31\"\n",
    "\n",
    "        # Daily observational data\n",
    "        obs = (obs_data\n",
    "               .sel(gage_id=gage_id, time=slice(start_date, end_date))\n",
    "               .load(scheduler='single-threaded')\n",
    "               .to_series())\n",
    "        \n",
    "        mod = (mod_data\n",
    "               .sel(gage_id=gage_id, time=slice(start_date, end_date))\n",
    "               .load(scheduler='single-threaded')\n",
    "               .to_series()\n",
    "               .resample('1D', offset='5h').\n",
    "               mean()) # Note time shift here is specific to National Water Model v2.1 data.\n",
    "\n",
    "        # make sure the indices match\n",
    "        obs.index = obs.index.to_period('D')\n",
    "        mod.index = mod.index.to_period('D')\n",
    "\n",
    "        # merge obs and predictions; drop NaNs.\n",
    "        gage_df = pd.merge(obs, mod, left_index=True, right_index=True).dropna(how='any')\n",
    "\n",
    "        # streamflow percentiles are computed with the Weibull plotting position (r/(n + 1), where r is rank and n is the number of data\n",
    "        # see sourced metrics notebook above for further detail\n",
    "        df_pct = calculate_percentiles(gage_df, units = \"cms\")\n",
    "        filename = f'percentiles_{gage_id}.csv'\n",
    "        file_path = os.path.join(output_folder, filename)\n",
    "        df_pct.to_csv(file_path, index=False)\n",
    "\n",
    "        # Boolean threshold\n",
    "        df_bool_threshold_only = calculate_site_boolean_threshold_only(df_pct)\n",
    "        filename = f'drought_boolean_{gage_id}.csv'\n",
    "        file_path = os.path.join(output_folder, filename)\n",
    "        df_pct.to_csv(file_path, index=False)\n",
    "\n",
    "        # Cohen's Kappa\n",
    "        df_accuracy = site_cohens_kappa(df_bool_threshold_only, gage_id)\n",
    "        filename = f'cohens_kappa_{gage_id}.csv'\n",
    "        file_path = os.path.join(output_folder, filename)\n",
    "        df_pct.to_csv(file_path, index=False)\n",
    "\n",
    "        # Calculate Spearmans R\n",
    "        df_spearmans = spearman_r(df_pct, gage_id, thresholds = [5,10,20,30])\n",
    "        filename = f'spearman_{gage_id}.csv'\n",
    "        file_path = os.path.join(output_folder, filename)\n",
    "        df_pct.to_csv(file_path, index=False)\n",
    "\n",
    "        # Calculate bias \n",
    "        df_bias_dist = bias_dist(df_pct, gage_id, thresholds = [5,10,20,30])\n",
    "        filename = f'bias_dist_{gage_id}.csv'\n",
    "        file_path = os.path.join(output_folder, filename)\n",
    "        df_pct.to_csv(file_path, index=False)\n",
    "\n",
    "        # Add or remove decomposition functions below\n",
    "        return gage_id\n",
    "        \n",
    "    except Exception as e:#<-- this is an extremely broad way to catch exceptions.  We only do it this way to ensure \n",
    "            #    that a failure on one benchmark (for a single stream gage) will not halt the entire run. \n",
    "        logging.info(\"Benchmark failed for %s\", gage_id)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test to be sure this `compute_benchmark()` function will return data for a single gage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_folder = 'output'\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_benchmark('USGS-01011000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our dataframe of sites to a list\n",
    "site_list = sites.index.tolist()\n",
    "site_list[1:3]\n",
    "\n",
    "# Run function for several sites\n",
    "for site_id in site_list[1:3]:\n",
    "    print(site_id)\n",
    "    compute_benchmark(site_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: the section below is 'coming soon to a theater near you'\n",
    "Unfortunately, the dask cluster generation isn't working for us on Nebari at the moment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the Analysis \n",
    "We will be doing a lot of work in paralallel, using workers within a 'cluster'.  \n",
    "The details of cluster configuration are handled for us by 'helper' notebooks, below. \n",
    "You can override their function by doing your own cluster configuration if you like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment the lines below to read in your AWS credentials if you want to access data from a requester-pays bucket (-cloud)\n",
    "# os.environ['AWS_PROFILE'] = 'osn-hytest-scratch' # 'default'\n",
    "# %run ../../../environment_set_up/Help_AWS_Credentials.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start up a distributed cluster of workers\n",
    "## NOTE: This cluster configuration is VERY specific to the JupyterHub cloud environment on USGS internal HyTEST Nebari or ESIP Nebari\n",
    "## For other dask cluster configurations, see other 'Start_Dask' notebooks under hytest/environment_set_up\n",
    "#%run ../../../environment_set_up/Start_Dask_Cluster_Nebari.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verified above that the `compute_benchmark` works on the \"hosted\" server (where this\n",
    "notebook is being executed. As a sanity check before we give the cluster of workers a lot \n",
    "to do, let's verify that we can have a remote worker process a gage by submitting work\n",
    "to one in isolation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.submit(compute_benchmark, 'USGS-01030350').result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got a benchmark function, and can prove that it works in remote workers \n",
    "within the cluster, we can dispatch a fleet of workers to process our data in parallel.\n",
    "We will make use of `dask` to do this using a dask '_bag_'.  \n",
    ">Read more about task parallelism with Dask and how we are using dask bags [here](../../../essential_reading/Parallel_Dask.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dask bag with the contents being a list of the streamflow gages used by Simeone et al (2024).\n",
    "#import dask.bag as db\n",
    "\n",
    "# For the first 100 gages within the streamflow gage list, try to map the compute benchmark function over them.\n",
    "# Remove '[0:100]' to run full list (may take awhile depending on cores/workers available)\n",
    "#bag = db.from_sequence(sites.index.tolist()[0:100]).map(compute_benchmark)\n",
    "#results = bag.compute() \n",
    "\n",
    "# preview first few gages with results\n",
    "#print(\"Number of gages:\", len(results))\n",
    "#results[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that big task done, we don't need `dask` parallelism any more. Let's shut down the cluster - this is important to do so others can use the compute and so we don't spend additional funds without a purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.close(); del client\n",
    "#cluster.close(); del cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the results\n",
    "The `bag` now contains a collection of return values (one per call to `compute_benchmark()`).  We can modify that into a table/dataframe for easier processing: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe/table can be saved to disk as a CSV and can be used for visualizations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.to_csv('example_streamflow_drought_benchmarking_results.csv') ##<--- change this to a personalized filename if desired"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global-global-pangeo",
   "language": "python",
   "name": "conda-env-global-global-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d7ebce313f85fb1ac8949e834c83f371584cb2422d845bf1570c1220fdedc716"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
