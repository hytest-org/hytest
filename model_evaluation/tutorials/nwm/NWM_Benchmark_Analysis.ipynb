{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# National Water Model Benchmarking Analysis Workflow\n",
    "\n",
    ":::{note}\n",
    "\n",
    "_This notebook adapted from originals by Timothy Hodson and Rich Signell. See that upstream work at:_\n",
    "* https://github.com/thodson-usgs/dscore\n",
    "* https://github.com/USGS-python/hytest-evaluation-workflows/\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "## Essential Benchmark Components\n",
    "This benchmark notebook will present a workflow which follows a canonical model for Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations; \n",
    "2) The domain (e.g. space or time) over which to benchmark;\n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load libraries and configure Python computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python libraries we will need...\n",
    "import pandas as pd\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will this ananlysis need parallelism?  Likely yes.  \n",
    "\n",
    "The following cell will configure a cluster environment suited to the server hosting this notebook. \n",
    "\n",
    "You may let our `configure_cluster()` helper try to guess the cluster config for you, or you can \n",
    "explicitly name a config matching where you are running this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-west-2\n",
      "No Cluster running.\n",
      "Starting new cluster.\n",
      "{}\n",
      "Setting Cluster Environment Variable AWS_DEFAULT_REGION us-west-2\n",
      "Setting Fixed Scaling workers=30\n",
      "Reconnect client to clear cache\n",
      "client.dashboard_link (for new browser tab/window or dashboard searchbar in Jupyterhub):\n",
      "https://jupyter.qhub.esipfed.org/gateway/clusters/dev.05078e244814404eb476a957d96e41d3/status\n",
      "Propagating environment variables to workers\n",
      "Using environment: users/pangeo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-0edf473a-35d4-11ed-80c7-2a5a400aa7a0</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_gateway.GatewayCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"https://jupyter.qhub.esipfed.org/gateway/clusters/dev.05078e244814404eb476a957d96e41d3/status\" target=\"_blank\">https://jupyter.qhub.esipfed.org/gateway/clusters/dev.05078e244814404eb476a957d96e41d3/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div style='background-color: #f2f2f2; display: inline-block; padding: 10px; border: 1px solid #999999;'>\n",
       "  <h3>GatewayCluster</h3>\n",
       "  <ul>\n",
       "    <li><b>Name: </b>dev.05078e244814404eb476a957d96e41d3\n",
       "    <li><b>Dashboard: </b><a href='https://jupyter.qhub.esipfed.org/gateway/clusters/dev.05078e244814404eb476a957d96e41d3/status' target='_blank'>https://jupyter.qhub.esipfed.org/gateway/clusters/dev.05078e244814404eb476a957d96e41d3/status</a>\n",
       "  </ul>\n",
       "</div>\n",
       "\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tls://10.10.86.170:8786' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r'/shared/users/lib')\n",
    "from HyTest.helpers import configure_cluster\n",
    "(client, cluster) = configure_cluster('cloud')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations,  <span style=\"color:red; font-size:large\"><<--You are here</span>\n",
    "2) The domain over which to benchmark \n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Finding and loading data is made easier for this particular workflow, in tht most of it has been pre-processed and stored in a cloud-friendly format. That data store is indexed by an **intake catalog**.  Learn more about `intake` [here](../L2/xx_Intake.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: \n",
      " conus404-hourly-onprem\n",
      "conus404-hourly-cloud\n",
      "conus404-daily-onprem\n",
      "conus404-daily-cloud\n",
      "nwis-streamflow-usgs-gages-onprem\n",
      "nwis-streamflow-usgs-gages-cloud\n",
      "nwm21-streamflow-usgs-gages-onprem\n",
      "nwm21-streamflow-usgs-gages-cloud\n",
      "nwm21-streamflow-cloud\n",
      "nwm21-scores\n",
      "lcmap-cloud\n",
      "conus404-hourly-cloud-dev\n"
     ]
    }
   ],
   "source": [
    "import intake\n",
    "cat = intake.open_catalog(r'https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml')\n",
    "print(\"Available datasets: \\n\", \"\\n\".join(cat.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above list represents the processed datasets available for benchmarking.  If a dataset\n",
    "you want is not in that list, \n",
    "\n",
    "**DO THIS** TODO: Define this. \n",
    "\n",
    "or load the \n",
    "data manually using [other means](/dev/null). \n",
    "If you load data from a source other than this list, you can jump to [Step 2: Restrict to a Domain](#step-2-restrict-to-a-domain)\n",
    "\n",
    "Note that the interesting datasets in the cataloged dataset above are duplicated: Some are `-onprem` \n",
    "and some are `-cloud`. Same data, but the storage location and access protocol will be different. You \n",
    "will definitely want to use the correct copy of the data for your computing environment.  \n",
    "* `onprem` : Direct access via the `caldera` filesystem from _denali_ or _tallgrass_\n",
    "* `cloud` : Network access via S3 bucket, suitable for consumption on cloud-hosted jupyter servers. You could also access using any network-attached computer, but the amount of data will likely saturate your connection.  Use in the cloud (e.g. ESIP QHub)\n",
    "\n",
    "So... are you on-prem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not onprem; use '-cloud' data source\n",
      "Observed :  nwis-streamflow-usgs-gages-cloud\n",
      "Modeled  :  nwm21-streamflow-usgs-gages-cloud\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "onprem = (platform.node() in ['denali', 'tallgrass'])\n",
    "if onprem:\n",
    "    print(\"Yes : -onprem\")\n",
    "    obs_data_src='nwis-streamflow-usgs-gages-onprem'\n",
    "    mod_data_src='nwm21-streamflow-usgs-gages-onprem'\n",
    "else:\n",
    "    print(\"Not onprem; use '-cloud' data source\")\n",
    "    obs_data_src='nwis-streamflow-usgs-gages-cloud'\n",
    "    mod_data_src='nwm21-streamflow-usgs-gages-cloud'\n",
    "print(\"Observed : \", obs_data_src)\n",
    "print(\"Modeled  : \", mod_data_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_of_interest = 'streamflow'\n",
    "try:\n",
    "    obs = cat[obs_data_src].to_dask()\n",
    "    mod = cat[mod_data_src].to_dask()\n",
    "except KeyError:\n",
    "    print(\"Something wrong with dataset names.\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    obs_data = obs[variable_of_interest]\n",
    "    mod_data = mod[variable_of_interest].astype('float32')\n",
    "except KeyError:\n",
    "    print(f\"{variable_of_interest} was not found in these data.\")\n",
    "\n",
    "obs_data.name = 'observed'\n",
    "mod_data.name = 'predicted'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Restrict to a Domain\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations,  \n",
    "2) The domain over which to benchmark <span style=\"color:red; font-size:large\"><<--You are here</span>\n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Each benchmark domain is defined over specific bounds (typically space and/or time). \n",
    "Benchmark domain definitions are published to Science Base, or they can be defined within the notebook. \n",
    "\n",
    "This notebook will use a benchmark domain definition loaded from ESIP's network file \n",
    "system (S3). It is essentially a list of stream guages in which we are interested, along with some \n",
    "metadata about that gage (watershed, reach code, etc).  We will use this as a spatial selector\n",
    "to restrict the data to only those gages found within this benchmarking domain.\n",
    "\n",
    "Because we are using a standardized list, we need to fetch it from its upstream source, which is\n",
    "an S3 bucket.\n",
    "\n",
    ":::{sidebar}\n",
    "\n",
    "Read more about accessing S3 [here](/dev/null)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5520 gages in this benchmark\n"
     ]
    }
   ],
   "source": [
    "import fsspec\n",
    "fs = fsspec.filesystem('s3', anon=True)\n",
    "try:\n",
    "    domain_data = pd.read_csv(\n",
    "        fs.open('s3://esip-qhub-public/usgs/hytest/streamflow_benchmark_sites_v09.csv'), \n",
    "        dtype={'site_no':str, 'huc_cd':str, 'reachcode':str, 'comid':str },\n",
    "        index_col='site_no'\n",
    "        )\n",
    "except:\n",
    "    print(f\"Could not open the benchmark data ... AWS problem?\")\n",
    "    raise\n",
    "# Re-format the gage_id/site_no string value.  ex:   \"1000000\"  ==> \"USGS-1000000\"\n",
    "domain_data.rename(index=lambda x: f'USGS-{x}', inplace=True)\n",
    "print(f\"{len(domain_data.index)} gages in this benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a domain dataset representing the stream gages (unique `site_no` values) identifying the locations making up the spatial domain of this benchmark. While we have good meta-data for these gages (lat/lon location, HUC8 code, etc), we really will only use the list of `site_no` values to select locations out of the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compute Metrics\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations,  \n",
    "2) The domain over which to benchmark \n",
    "3) A set of statistical metrics with which to benchmark. <span style=\"color:red; font-size:large\"><<--You are here</span>\n",
    "\n",
    "The code to calculate the various metrics has been standardized [here](./NWM_StandardSuite_v1.ipynb). You can \n",
    "use these or write your own.  To import and use these standard definitions, run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./NWM_StandardSuite_v1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you use these functions or your own, we need to put all metric computation into a special all-encompasing \n",
    "benchmarking function--a single call which can be assigned to each gage in our domain list. This sort of arrangement \n",
    "is well-suited to parallelism with `dask`. \n",
    "\n",
    "If this is done well, the process will benefit enormously from task parallelism -- each gage can be given its own \n",
    "CPU to run on.  After all are done, the various results will be collected and assembled into a composite dataset. \n",
    "\n",
    "To achieve this, we need a single 'atomic' function that can execute independently. It will take the gage identifier \n",
    "as input and return a list of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wrapper function -- this func will be called once per gage_id, each call on its own dask worker\n",
    "def compute_benchmark(gage_id):\n",
    "    try:\n",
    "        ## obs_data and mod_data should be globals...\n",
    "        obs = obs_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series()\n",
    "        mod = mod_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series().resample('1D', offset='5h').mean() \n",
    "        \n",
    "        # make sure the indices match\n",
    "        obs.index = obs.index.to_period('D')\n",
    "        mod.index = mod.index.to_period('D')\n",
    "\n",
    "        # merge obs and predictions; drop NaNs.\n",
    "        gage_df = pd.merge(obs, mod, left_index=True, right_index=True).dropna(how='any')\n",
    "        \n",
    "        scores = pd.Series(\n",
    "            data={\n",
    "                'NSE': NSE(gage_df.observed, gage_df.predicted),\n",
    "                'KGE': KGE(gage_df.observed, gage_df.predicted),\n",
    "                'logNSE': logNSE(gage_df.observed, gage_df.predicted),\n",
    "                'pbias': pbias(gage_df.observed, gage_df.predicted),\n",
    "                'rSD': rSD(gage_df.observed, gage_df.predicted),\n",
    "                'pearson': pearson_r(gage_df.observed, gage_df.predicted),\n",
    "                'spearman': spearman_r(gage_df.observed, gage_df.predicted), \n",
    "                'pBiasFMS': pBiasFMS(gage_df.observed, gage_df.predicted),\n",
    "                'pBiasFLV': pBiasFLV(gage_df.observed, gage_df.predicted),\n",
    "                'pBiasFHV': pBiasFHV(gage_df.observed, gage_df.predicted)\n",
    "            },\n",
    "            name=gage_id,\n",
    "            dtype='float64'\n",
    "        )\n",
    "        return scores\n",
    "    except Exception as e:#<-- this is an extremely broad way to catch exceptions.  We only do it this way to ensure \n",
    "            #    that a failure on one benchmark (for a single stream gage) will not halt the entire run. \n",
    "        logging.info(\"Benchmark failed for %s\", gage_id)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test to be sure this `compute_benchmark()` function will return data for a single gage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NSE          0.610186\n",
       "KGE          0.581806\n",
       "logNSE       0.437533\n",
       "pbias      -12.679162\n",
       "rSD          0.655655\n",
       "pearson      0.799410\n",
       "spearman     0.859122\n",
       "pBiasFMS   -34.154380\n",
       "pBiasFLV    90.474838\n",
       "pBiasFHV   -43.865916\n",
       "Name: USGS-01030350, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_benchmark('USGS-01030350')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to set up a way to farm out this function, once per gage ID. `dask` will do this\n",
    "using a dask '_bag_'.  \n",
    "\n",
    ":::{sidebar}\n",
    "\n",
    "Read more about task parallelism with Dask and how we are using dask bags [here](/dev/null)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "bag = db.from_sequence(domain_data.index.tolist()).map(compute_benchmark)\n",
    "results = bag.compute() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that big task done, we don't need `dask` parallelism any more. Let's shut down the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conda/users/76634b8b76503b686ed55f14ef74288ada4ffb7c19282b669f7edfb7fc8aed9d-20220909-192621-731199-163-pangeo/lib/python3.9/site-packages/dask_gateway/client.py:1014: RuntimeWarning: coroutine 'rpc.close_rpc' was never awaited\n",
      "  self.scheduler_comm.close_rpc()\n"
     ]
    }
   ],
   "source": [
    "client.close(); del client\n",
    "cluster.close(); del cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the results\n",
    "The `bag` now contains a collection of return values (one per call to `compute_benchmark()`).  We can massage that into a table/dataframe for easier processing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NSE</th>\n",
       "      <th>KGE</th>\n",
       "      <th>logNSE</th>\n",
       "      <th>pbias</th>\n",
       "      <th>rSD</th>\n",
       "      <th>pearson</th>\n",
       "      <th>spearman</th>\n",
       "      <th>pBiasFMS</th>\n",
       "      <th>pBiasFLV</th>\n",
       "      <th>pBiasFHV</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site_no</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>USGS-01011000</th>\n",
       "      <td>0.689088</td>\n",
       "      <td>0.662586</td>\n",
       "      <td>0.597192</td>\n",
       "      <td>-19.883495</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.846457</td>\n",
       "      <td>0.818826</td>\n",
       "      <td>14.807269</td>\n",
       "      <td>61.287969</td>\n",
       "      <td>-36.838683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-01013500</th>\n",
       "      <td>0.620608</td>\n",
       "      <td>0.480716</td>\n",
       "      <td>0.752852</td>\n",
       "      <td>-22.970420</td>\n",
       "      <td>0.552287</td>\n",
       "      <td>0.871763</td>\n",
       "      <td>0.873238</td>\n",
       "      <td>-14.653922</td>\n",
       "      <td>60.804260</td>\n",
       "      <td>-51.751721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-01015800</th>\n",
       "      <td>0.695682</td>\n",
       "      <td>0.661854</td>\n",
       "      <td>0.764120</td>\n",
       "      <td>-13.975070</td>\n",
       "      <td>0.732493</td>\n",
       "      <td>0.847514</td>\n",
       "      <td>0.862823</td>\n",
       "      <td>-5.586971</td>\n",
       "      <td>43.496275</td>\n",
       "      <td>-40.215325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-01017000</th>\n",
       "      <td>0.676277</td>\n",
       "      <td>0.662182</td>\n",
       "      <td>0.728323</td>\n",
       "      <td>-13.596089</td>\n",
       "      <td>0.739596</td>\n",
       "      <td>0.833192</td>\n",
       "      <td>0.836719</td>\n",
       "      <td>-2.559546</td>\n",
       "      <td>50.629818</td>\n",
       "      <td>-40.769273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-01017550</th>\n",
       "      <td>-0.007024</td>\n",
       "      <td>0.488803</td>\n",
       "      <td>0.187784</td>\n",
       "      <td>25.384879</td>\n",
       "      <td>1.159282</td>\n",
       "      <td>0.585860</td>\n",
       "      <td>0.778948</td>\n",
       "      <td>20.736102</td>\n",
       "      <td>15.696818</td>\n",
       "      <td>-33.564579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-14369500</th>\n",
       "      <td>0.576253</td>\n",
       "      <td>0.614541</td>\n",
       "      <td>0.538312</td>\n",
       "      <td>21.251125</td>\n",
       "      <td>1.297323</td>\n",
       "      <td>0.877457</td>\n",
       "      <td>0.771774</td>\n",
       "      <td>43.800607</td>\n",
       "      <td>112.890399</td>\n",
       "      <td>13.504782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-14372300</th>\n",
       "      <td>0.747763</td>\n",
       "      <td>0.854819</td>\n",
       "      <td>0.462288</td>\n",
       "      <td>5.255767</td>\n",
       "      <td>0.960921</td>\n",
       "      <td>0.870431</td>\n",
       "      <td>0.889055</td>\n",
       "      <td>48.189721</td>\n",
       "      <td>-26.875940</td>\n",
       "      <td>-18.711446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-14375100</th>\n",
       "      <td>-1.256368</td>\n",
       "      <td>-0.270989</td>\n",
       "      <td>-0.519085</td>\n",
       "      <td>96.341803</td>\n",
       "      <td>1.701510</td>\n",
       "      <td>0.558273</td>\n",
       "      <td>0.330279</td>\n",
       "      <td>6.331031</td>\n",
       "      <td>735.452175</td>\n",
       "      <td>8.474573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-14377100</th>\n",
       "      <td>0.751601</td>\n",
       "      <td>0.691753</td>\n",
       "      <td>0.905648</td>\n",
       "      <td>14.723495</td>\n",
       "      <td>1.262068</td>\n",
       "      <td>0.931750</td>\n",
       "      <td>0.955250</td>\n",
       "      <td>-4.176343</td>\n",
       "      <td>22.406697</td>\n",
       "      <td>19.312102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-14400000</th>\n",
       "      <td>0.840578</td>\n",
       "      <td>0.750943</td>\n",
       "      <td>0.904838</td>\n",
       "      <td>-16.752493</td>\n",
       "      <td>0.830591</td>\n",
       "      <td>0.927435</td>\n",
       "      <td>0.957442</td>\n",
       "      <td>-19.263242</td>\n",
       "      <td>2.374988</td>\n",
       "      <td>-23.003937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5494 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    NSE       KGE    logNSE      pbias       rSD   pearson  \\\n",
       "site_no                                                                      \n",
       "USGS-01011000  0.689088  0.662586  0.597192 -19.883495  0.774751  0.846457   \n",
       "USGS-01013500  0.620608  0.480716  0.752852 -22.970420  0.552287  0.871763   \n",
       "USGS-01015800  0.695682  0.661854  0.764120 -13.975070  0.732493  0.847514   \n",
       "USGS-01017000  0.676277  0.662182  0.728323 -13.596089  0.739596  0.833192   \n",
       "USGS-01017550 -0.007024  0.488803  0.187784  25.384879  1.159282  0.585860   \n",
       "...                 ...       ...       ...        ...       ...       ...   \n",
       "USGS-14369500  0.576253  0.614541  0.538312  21.251125  1.297323  0.877457   \n",
       "USGS-14372300  0.747763  0.854819  0.462288   5.255767  0.960921  0.870431   \n",
       "USGS-14375100 -1.256368 -0.270989 -0.519085  96.341803  1.701510  0.558273   \n",
       "USGS-14377100  0.751601  0.691753  0.905648  14.723495  1.262068  0.931750   \n",
       "USGS-14400000  0.840578  0.750943  0.904838 -16.752493  0.830591  0.927435   \n",
       "\n",
       "               spearman   pBiasFMS    pBiasFLV   pBiasFHV  \n",
       "site_no                                                    \n",
       "USGS-01011000  0.818826  14.807269   61.287969 -36.838683  \n",
       "USGS-01013500  0.873238 -14.653922   60.804260 -51.751721  \n",
       "USGS-01015800  0.862823  -5.586971   43.496275 -40.215325  \n",
       "USGS-01017000  0.836719  -2.559546   50.629818 -40.769273  \n",
       "USGS-01017550  0.778948  20.736102   15.696818 -33.564579  \n",
       "...                 ...        ...         ...        ...  \n",
       "USGS-14369500  0.771774  43.800607  112.890399  13.504782  \n",
       "USGS-14372300  0.889055  48.189721  -26.875940 -18.711446  \n",
       "USGS-14375100  0.330279   6.331031  735.452175   8.474573  \n",
       "USGS-14377100  0.955250  -4.176343   22.406697  19.312102  \n",
       "USGS-14400000  0.957442 -19.263242    2.374988 -23.003937  \n",
       "\n",
       "[5494 rows x 10 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = [i for i in results if i is not None] # Drop entries where compute_benchmark failed\n",
    "results_df = pd.concat(results, axis=1).T\n",
    "results_df.index.name = 'site_no'\n",
    "#ds_results = df_results.to_xarray()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe/table can be saved to disk as a CSV. It will be used for visualizations in [other notebooks](NWM_Benchmark_Visualization.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('NWM_v2.1_streamflow_benchmark.csv') ##<--- change this to a personalized filename\n",
    "#ds_results.merge(benchmark_ds, join='inner').to_netcdf('nwm_v2.1_streamflow_benchmark.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-pangeo",
   "language": "python",
   "name": "conda-env-users-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4100cc85ffefb381c538d28dd18cb927e5a99f05bbed6aaad5313d7bb1c2079e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
