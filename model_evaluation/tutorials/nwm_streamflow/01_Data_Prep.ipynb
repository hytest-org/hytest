{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation :: Data Preparation\n",
    "\n",
    "As a part of the generalized evaluation workflow: \n",
    "\n",
    "<img src='../../../doc/assets/Eval_PreProc.svg' width=600>\n",
    "\n",
    "The pre-processing step is needed in order to align the two datasets for analysis.  The specific \n",
    "steps needed to prepare a given dataset may differ, depending on the source and the variable of\n",
    "interest. \n",
    "\n",
    "Some steps might include: \n",
    "\n",
    "* Organizing the time-series index such that the time steps for both simulated and observed are congruent\n",
    "    * This may involve interpolation to estimate a more granular time-step than is found in the source data\n",
    "    * More often, an agregating function is used to 'down-sample' the dataset to a coarser time step (days vs hours).\n",
    "* Coordinate aggregation units between simulated and observed \n",
    "    * Gridded data may be sampled per HUC-12, HUC-6, etc. to match modeled data indexed by these units. \n",
    "    * Index formats may be adjusted (e.g. a 'gage_id' may be 'USGS-01104200' in one data set, vs '01104200' in another)\n",
    "* Re-Chunking the data to make time-series analysis more efficient (see [here](/dev/null) for a primer on re-chunking).\n",
    "\n",
    "## Streamflow Data Prep\n",
    "\n",
    "This document shows one approach to preparing the _streamflow_ data for subsequent analysis (That analysis is outlined [here](02_Analysis_StdSuite.ipynb)).\n",
    "\n",
    "Streamflow analysis will compare time-series of two aligned datasets: \n",
    "* 'observed' data values obtained from [NWIS](https://nwis.waterdata.usgs.gov/nwis) \n",
    "* 'modeled' data extracted from the [NWM](https://registry.opendata.aws/nwm-archive/)\n",
    "\n",
    "These data soruces are accessed using different methods.  We will pull data from their respective sources, reshape and optimize the data structures, then write that data to storage to make later analysis easier. \n",
    "\n",
    "An overview of the steps we will take in this notebook: \n",
    "1) Read Modeled Data\n",
    "2) Establish AWS Credentials\n",
    "3) Source NWIS data via API\n",
    "4) Create a plan to Re-Structure that data for storage as ZARR file on S3. \n",
    "    * Rename variables\n",
    "    * Establish chunking layout\n",
    "    * Establish encoding\n",
    "    * Create a template to formalize this configuration\n",
    "5) In Parallel (one worker per gage_id):\n",
    "    * Fetch data from NWIS\n",
    "    * Write data to ZARR file\n",
    "6) Verify the data is correctly written to ZARR storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Reading the 'modeled' Data\n",
    "Modeled data for this demonstration tutorial will be sourced from the S3 bucket `nhgf-development`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fsspec\n",
    "import dask\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import intake\n",
    "from getpass import getuser\n",
    "username = getuser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeled data is within the HyTEST `intake` catalog, with name \"nwm21-streamflow-usgs-gages-cloud\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = intake.open_catalog(r'https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml')\n",
    "\n",
    "modeled = cat['nwm21-streamflow-usgs-gages-cloud'].to_dask()\n",
    "\n",
    "modeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Source Data as Template**\n",
    "\n",
    "This source data set will establish the indices and boundaries for the data we will eventually pull from the NWIS stream gage network.\n",
    "The two dimensions of this data are the **Gage ID** and **Time**.  We'll use these dimensions to fetch the 'observed' data later. The \n",
    "endpoints and range of these dimensions will establish that future query.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gage IDs\n",
    "import re\n",
    "_gages = [gage_id.lstrip() for gage_id in modeled['gage_id'].values.astype('str')]\n",
    "\n",
    "GAGES = [g for g in _gages if re.search('USGS-\\d+$', g) ][0:100] \n",
    "#                                                        ^^^^^^^ See NOTES:\n",
    "\n",
    "## >> NOTE 1: The regex search pattern ensures we get legal gage names only\n",
    "## >> NOTE 2: We are limiting the GAGES array to the first 100 gages for this \n",
    "##            demo/tutorial. To run this workflow for the entire set of GAGES, \n",
    "##            remove the slice notation [0:100]\n",
    "\n",
    "# Time boundaries for future queries:\n",
    "start_time = modeled.time.values.min()\n",
    "stop_time = modeled.time.values.max()\n",
    "DATE_RANGE=(start_time, stop_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important notes for each of those bounds: \n",
    "* **GAGES** / Gage IDs :: The list of 7994 gage IDs in the model dataset include some values which the NWIS does not \n",
    "  recognize and will not accept. We need to remove them. \n",
    "  * Gage IDs of the form `USGS-\\d+` (A string starting 'USGS-' and ending in an arbitrary number of digits)\n",
    "    are processed by NWIS data requests.\n",
    "  * There are roughly 350 gage IDs in the modeled dataset with letters embedded in the string of digits after the 'USGS-'. These\n",
    "    will be rejected by the API when we try to call NWIS data service to obtain streamflow history for that location.\n",
    "  * This is the reason behind the regular expression search (`re.search`) to select only gage_id of the correct format. \n",
    "  * After selecting the NWIS-compliant gage IDs, the `GAGES` list contains 7647 gages. This tutorial will demonstrate\n",
    "    the workflow using only the first 100 gages on that list. If you want to process the whole list, remove the slice notation\n",
    "    as described in the comments above. \n",
    "* **DATE_RANGE** / Dates :: This defines the temporal range for the historical data will will fetch from NWIS. \n",
    "  * The NWM modeled data includes time values stepped hourly.\n",
    "  * The historical streamflow data is stepped daily.\n",
    "  * We will resample later to make sure the indices match. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Establish AWS Credentials\n",
    "Now that we've got a handle on the 'modeled' data, we can begin to think about the matching 'observed'\n",
    "data.  But before we do that, let's establish credentials for working with our compute environment. Doing \n",
    "this now will streamline future I/O and cluster tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../../environment_set_up/Help_AWS_Credentials.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With credentials established, instantiate a handle for the writable file system\n",
    "fs_write = fsspec.filesystem('s3', \n",
    "                         anon=False, # force a consult to environment vars set above.\n",
    "                         skip_instance_cache=True, \n",
    "                         client_kwargs={'endpoint_url': os.environ['AWS_S3_ENDPOINT']}\n",
    "                        )\n",
    "\n",
    "fname=f's3://rsignellbucket2/testing/{username}/nwis_out_{username}.zarr'  #<<<< THIS will be our output location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Sourcing the 'observed' data\n",
    "\n",
    "Now that we have information about the list of gages and the date range covered by the model, we \n",
    "can use that to query NWIS for matching data points for this same range of dates and station IDs. \n",
    "Because NWIS data is structured a little differently than the modeled streamflow, we'll need to \n",
    "re-arrange the data a little after fetching. \n",
    "\n",
    "In addition, a call to NWIS for historical data can be time consuming -- and we will do it roughly \n",
    "7500 times.  We will eventually set up a mechanism to do these requests in parallel, once we've \n",
    "established how the data restructuring should happen. \n",
    "\n",
    "The first step in that process is to make a NWIS request for just a couple of gages to see how \n",
    "the return data is structured.  We'll use that information to create the plan by which the full \n",
    "dataset is to be fetched and reorganized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygeohydro import NWIS\n",
    "nwis = NWIS()\n",
    "## Fetch data for a couple of gages to see how NWIS formats a response\n",
    "observed = nwis.get_streamflow(GAGES[0:2], DATE_RANGE, to_xarray=True) \n",
    "    ## get_streamflow() is an API call to a data server via internet.\n",
    "observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Examine the Response Data -- Make a plan\n",
    "Our goal is to use the NWIS service to fetch data for a large number of gages, then assemble that\n",
    "into a dataset that is structured similarly to our modeled data. \n",
    "\n",
    "We requested two stream gages from NWIS rather than just one, to ensure that the dataset is multi-dimensional \n",
    "(as the final dataset will be). Using what we learned from the above NWIS call, we can make a plan\n",
    "for how to match it to the existing modeled data.  A few considerations: \n",
    "\n",
    "* We'll need to rename some variables (i.e. '_discharge_' --> '_streamflow_', etc).  \n",
    "* We also need\n",
    "  to make note of which data variables are strings, but perhaps stored with different encodings.\n",
    "* Because the composite dataset will be quite large, some care should be taken to chunk it such that it performs well\n",
    "  for time-series analysis. \n",
    "  \n",
    ":::{margin}\n",
    "Learn more about chunking [here](/dev/null)\n",
    ":::\n",
    "\n",
    "Lastly, we note the time range returned by NWIS.  It covers the time range we requested in \n",
    "DATE_RAGE, but is stepped daily. We will use this information to interpolate future results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4a :  rename variables\n",
    "observed = (observed\n",
    "            .rename_dims({'station_id':'gage_id'})\n",
    "            .rename({'discharge':'streamflow', 'station_id':'gage_id'})\n",
    "           )\n",
    "observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4b : define chunking in a 'template' dataset\n",
    "source_dataset = observed\n",
    "template = (xr.zeros_like(source_dataset)  # DataSet just like 'observed'\n",
    "             .chunk()           \n",
    "             .isel(gage_id=0, drop=True)      # temporarily remove gage_id as a dimension and coordinate\n",
    "             .expand_dims(gage_id=len(GAGES), axis=-1) # add it back, reserving space for the full size of GAGES\n",
    "             .assign_coords({'gage_id': GAGES}) # add coordinate to match dimension\n",
    "             .chunk({                         # define chunk sizes\n",
    "                 'time': len(observed.time),  # all time vals in one chunk\n",
    "                 'gage_id': 1}                # one gage_id per chunk\n",
    "             )\n",
    "           )\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4c :  Commit template to permanent storage: \n",
    "if fs_write.exists(fname):\n",
    "    print(\"Removing old copy of tutorial/demo output...\", end=\"\")\n",
    "    fs_write.rm(fname, recursive=True)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4c (continued): write the template with specific encodings\n",
    "outfile = fs_write.get_mapper(fname)\n",
    "template.to_zarr(\n",
    "    outfile,\n",
    "    compute=False,\n",
    "    encoding =  {                                  # encodings sets data types for the disk store\n",
    "        'station_nm':  dict( _FillValue=None,        dtype='<U64'), \n",
    "        'alt_datum_cd':dict( _FillValue=None,        dtype='<U6'),\n",
    "        'alt_acy_va':  dict( _FillValue=-2147483647, dtype=np.int32),\n",
    "        'alt_va':      dict( _FillValue=9.96921e+36, dtype=np.float32),\n",
    "        'dec_lat_va':  dict( _FillValue=None,        dtype=np.float32),\n",
    "        'dec_long_va': dict( _FillValue=None,        dtype=np.float32),\n",
    "        'streamflow':  dict( _FillValue=9.96921e+36, dtype=np.float32)\n",
    "    },\n",
    "    consolidated=True,                             # Consolidate metadata\n",
    "    mode='w'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Parallel Processing \n",
    "The above steps were necessary to establish a permanent disk storage space for the output\n",
    "dataset.  We've established its structure (variables, chunking plan, encodings) and also\n",
    "given a hint as to its size (by asserting the length of the `gage_id` index to be `len(GAGES)`\n",
    "items).  \n",
    "\n",
    "With that all established, we can now execute a job where each gage's data is fetched from \n",
    "NWIS and inserted into the permanent store. This demo is limited (by default) to only 100\n",
    "gages, so could in theory be executed serially.  We want to do it in parallel so as to model\n",
    "the process for an arbitrary number of gages.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5a) : write the 'worker' function -- this will be called once per gage\n",
    "# Globals: \n",
    "n_timesteps = len(observed.time)\n",
    "time_steps = observed.time.values\n",
    "\n",
    "def write_one_gage(n):\n",
    "    \"\"\" \n",
    "    Writes one gage's data to the existing zarr file.  Uses the NWIS API call to fetch data.\n",
    "    \n",
    "    Arguments: \n",
    "    n   : integer\n",
    "       the index into the GAGES array identifying which gage to fetch and write. \n",
    "    \"\"\"\n",
    "    site_id = GAGES[n]\n",
    "    try:\n",
    "        _obs = nwis.get_streamflow(site_id, DATE_RANGE, to_xarray=True).interp(time=time_steps)\n",
    "        _obs = _obs.rename_dims({'station_id':'gage_id'}).rename({'station_id':'gage_id','discharge':'streamflow'})\n",
    "        ## We must force the returned data into the datatype that we stored to disk. \n",
    "        _obs['station_nm'] = xr.DataArray(data=_obs['station_nm'].values.astype('<U64'), dims='gage_id')\n",
    "        _obs['alt_datum_cd'] = xr.DataArray(data=_obs['alt_datum_cd'].values.astype('<U6'), dims='gage_id')\n",
    " \n",
    "        _obs.to_zarr(\n",
    "            outfile, \n",
    "            region={ #<<< Specifying a region lets us 'insert' data to a specific place in the dataset. \n",
    "                'time': slice(0, n_timesteps), \n",
    "                'gage_id': slice(n,n+1)\n",
    "                }\n",
    "            )\n",
    "        return n # If success, returns the index into GAGES. \n",
    "    except Exception as e: \n",
    "        pass\n",
    "        #return e  # if failure, return the exception thrown.\n",
    "        # This is an extremely broad way to catch exceptions... and in general is to be avoided. \n",
    "        # We do it this way in this case to protect the parallel run. it allows a single write_one_gage() \n",
    "        # to fail silently without affecting the rest of the run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5b) Start up a distributed cluster of workers\n",
    "# NOTE: This cluster configuration is VERY specific to the JupyterHub cloud environment on ESIP/QHUB\n",
    "%run ../../../environment_set_up/Start_Dask_Cluster_Nebari.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Run the list of tasks: \n",
    "results = dask.compute(*[dask.delayed(write_one_gage)(i) for i in range(len(GAGES))], retries=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Consolidate metadata, to make future reads easier/faster\n",
    "from zarr.convenience import consolidate_metadata\n",
    "_ = consolidate_metadata(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shut down the cluster\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Verify\n",
    "We can now read the dataset that we just wrote to disk.  Does it have the dimensions, chunking, and encoding that we want? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fname is already set from above...\n",
    "outfile=fs_write.get_mapper(fname)\n",
    "dst = xr.open_dataset(outfile, engine='zarr', chunks={}, backend_kwargs=dict(consolidated=True))\n",
    "## NOTE: xarray will employ a 'lazy' loader; only metadata will be loaded initially.  Will only \n",
    "## read real data when it is actually needed for computation. \n",
    "dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "\n",
    "dst.sel(gage_id='USGS-07241800').hvplot(x='time',y='streamflow', grid=True)\n",
    "## This select operator is a specific call to read data -- so may take a \n",
    "## moment to fetch the full time series for the specified gage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DONE\n",
    "That dataset is now available for future analysis in which we need a consolidated NWIS\n",
    "dataset, chunked to optimize time-series analysis. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "d7ebce313f85fb1ac8949e834c83f371584cb2422d845bf1570c1220fdedc716"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
