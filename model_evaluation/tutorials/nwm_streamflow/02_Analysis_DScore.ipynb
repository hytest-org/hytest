{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D-Score Steamflow Analysis Workflow\n",
    "\n",
    "<img src='./Eval_Analysis.svg' width=600>\n",
    "\n",
    ":::{note}\n",
    "\n",
    "_This notebook adapted from originals by Timothy Hodson and Rich Signell. See that upstream work at:_\n",
    "* https://github.com/thodson-usgs/dscore\n",
    "* https://github.com/USGS-python/hytest-evaluation-workflows/\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "## Essential Benchmark Components\n",
    "This benchmark notebook will present a workflow which follows a canonical model for Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations; \n",
    "2) The domain (e.g. space or time) over which to benchmark;\n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load libraries and configure Python computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data\n",
    "\n",
    "<img src='./Eval_Analysis_Data.svg' width=360>\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations,  <span style=\"color:red; font-size:large\"><<--You are here</span>\n",
    "2) The domain over which to benchmark \n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Finding and loading data is made easier for this particular workflow (the _streamflow_ variable), in that most of it has been \n",
    "pre-processed and stored in a cloud-friendly format.  That process is described in the [data preparation](./01_Data_Prep.ipynb)\n",
    "notebook. We will proceed here using the already-prepared data for _streamflow_, which is included in the HyTEST **intake catalog**.  \n",
    "\n",
    ":::{sidebar}\n",
    "Learn more about `intake` [here](/dev/null)\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: \n",
      " conus404-hourly-onprem\n",
      "conus404-hourly-cloud\n",
      "conus404-daily-onprem\n",
      "conus404-daily-cloud\n",
      "nwis-streamflow-usgs-gages-onprem\n",
      "nwis-streamflow-usgs-gages-cloud\n",
      "nwm21-streamflow-usgs-gages-onprem\n",
      "nwm21-streamflow-usgs-gages-cloud\n",
      "nwm21-streamflow-cloud\n",
      "nwm21-scores\n",
      "lcmap-cloud\n",
      "conus404-hourly-cloud-dev\n"
     ]
    }
   ],
   "source": [
    "import intake\n",
    "cat = intake.open_catalog(r'https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml')\n",
    "print(\"Available datasets: \\n\", \"\\n\".join(cat.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above list represents the processed datasets available for benchmarking.  If a dataset\n",
    "you want is not in that list, \n",
    "you can load data directly from S3 or onprem. \n",
    "If you load data from a source other than this list, you can jump to [Step 2: Restrict to a Domain](#step-2-restrict-to-a-domain)\n",
    "\n",
    "Note that the interesting datasets in the cataloged dataset above are duplicated: Some are `-onprem` \n",
    "and some are `-cloud`. Same data, but the storage location and access protocol will be different. You \n",
    "will definitely want to use the correct copy of the data for your computing environment.  \n",
    "* `onprem` : Direct access via the `caldera` filesystem from _denali_ or _tallgrass_\n",
    "* `cloud` : Network access via S3 bucket, suitable for consumption on cloud-hosted jupyter servers. You could also access using any network-attached computer, but the amount of data will likely saturate your connection.  Use in the cloud (e.g. ESIP QHub)\n",
    "\n",
    "So... are you on-prem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not onprem; use '-cloud' data source\n",
      "Observed :  nwis-streamflow-usgs-gages-cloud\n",
      "Modeled  :  nwm21-streamflow-usgs-gages-cloud\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "onprem = (platform.node() in ['denali', 'tallgrass'])\n",
    "if onprem:\n",
    "    print(\"Yes : -onprem\")\n",
    "    obs_data_src='nwis-streamflow-usgs-gages-onprem'\n",
    "    mod_data_src='nwm21-streamflow-usgs-gages-onprem'\n",
    "else:\n",
    "    print(\"Not onprem; use '-cloud' data source\")\n",
    "    obs_data_src='nwis-streamflow-usgs-gages-cloud'\n",
    "    mod_data_src='nwm21-streamflow-usgs-gages-cloud'\n",
    "print(\"Observed : \", obs_data_src)\n",
    "print(\"Modeled  : \", mod_data_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_of_interest = 'streamflow'\n",
    "try:\n",
    "    obs = cat[obs_data_src].to_dask()\n",
    "    mod = cat[mod_data_src].to_dask()\n",
    "except KeyError:\n",
    "    print(\"Something wrong with dataset names.\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    obs_data = obs[variable_of_interest]\n",
    "    mod_data = mod[variable_of_interest].astype('float32')\n",
    "except KeyError:\n",
    "    print(f\"{variable_of_interest} was not found in these data.\")\n",
    "\n",
    "obs_data.name = 'observed'\n",
    "mod_data.name = 'predicted'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Restrict to a Domain\n",
    "\n",
    "<img src='./Eval_Analysis_Domain.svg' width=360>\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations,  \n",
    "2) The domain over which to benchmark <span style=\"color:red; font-size:large\"><<--You are here</span>\n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Each benchmark domain is defined over specific bounds (typically space and/or time). \n",
    "Benchmark domain definitions can be defined within the notebook, or sourced from\n",
    "elsewhere. For this example, we use the _Cobalt_ gages, avaliable for download on ScienceBase \n",
    "([Foks et al., 2022](https://doi.org/10.5066/P972P42Z)).  \n",
    "\n",
    "This is essentially a list of \n",
    "stream guages in which we are interested, along with some  metadata about that gage (watershed, \n",
    "reach code, etc).  We will use this as a spatial selector to restrict the original data to only \n",
    "those gages found within this benchmarking domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5390 gages in this benchmark\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dec_lat_va</th>\n",
       "      <th>dec_long_va</th>\n",
       "      <th>comid</th>\n",
       "      <th>reachcode</th>\n",
       "      <th>reach_meas</th>\n",
       "      <th>drain_sqkm</th>\n",
       "      <th>huc02</th>\n",
       "      <th>gagesII_class</th>\n",
       "      <th>aggecoregion</th>\n",
       "      <th>complete_yrs</th>\n",
       "      <th>n_days</th>\n",
       "      <th>nldi</th>\n",
       "      <th>swim</th>\n",
       "      <th>gfv1d1</th>\n",
       "      <th>camels</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site_no</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>USGS-01011000</th>\n",
       "      <td>47.069722</td>\n",
       "      <td>-69.079444</td>\n",
       "      <td>721640</td>\n",
       "      <td>01010002000001</td>\n",
       "      <td>53.747189</td>\n",
       "      <td>3186.8</td>\n",
       "      <td>01</td>\n",
       "      <td>Non-ref</td>\n",
       "      <td>NorthEast</td>\n",
       "      <td>33</td>\n",
       "      <td>12146</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-01013500</th>\n",
       "      <td>47.237500</td>\n",
       "      <td>-68.582778</td>\n",
       "      <td>724696</td>\n",
       "      <td>01010003000003</td>\n",
       "      <td>7.660419</td>\n",
       "      <td>2252.7</td>\n",
       "      <td>01</td>\n",
       "      <td>Ref</td>\n",
       "      <td>NorthEast</td>\n",
       "      <td>33</td>\n",
       "      <td>12146</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dec_lat_va  dec_long_va   comid       reachcode  reach_meas  \\\n",
       "site_no                                                                      \n",
       "USGS-01011000   47.069722   -69.079444  721640  01010002000001   53.747189   \n",
       "USGS-01013500   47.237500   -68.582778  724696  01010003000003    7.660419   \n",
       "\n",
       "               drain_sqkm huc02 gagesII_class aggecoregion  complete_yrs  \\\n",
       "site_no                                                                    \n",
       "USGS-01011000      3186.8    01       Non-ref    NorthEast            33   \n",
       "USGS-01013500      2252.7    01           Ref    NorthEast            33   \n",
       "\n",
       "               n_days  nldi  swim  gfv1d1  camels  \n",
       "site_no                                            \n",
       "USGS-01011000   12146     1     1       1       0  \n",
       "USGS-01013500   12146     1     1       1       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cobalt = pd.read_csv(\n",
    "    'https://www.sciencebase.gov/catalog/file/get/6181ac65d34e9f2789e44897?f=__disk__22%2F1a%2Fd2%2F221ad2fe9d95de17731ad35d0fc6b417a4b53ee1',\n",
    "    dtype={'site_no':str, 'huc_cd':str, 'reachcode':str, 'comid':str, 'gagesII_class':str, 'aggecoregion': str}, \n",
    "    index_col='site_no'\n",
    "    )\n",
    "# Re-format the gage_id/site_no string value.  ex:   \"1000000\"  ==> \"USGS-1000000\"\n",
    "cobalt.rename(index=lambda x: f'USGS-{x}', inplace=True)\n",
    "print(f\"{len(cobalt.index)} gages in this benchmark\")\n",
    "cobalt.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a domain dataset representing the stream gages (unique `site_no` values) identifying the locations making up the spatial domain of this benchmark. While we have good meta-data for these gages (lat/lon location, HUC8 code, etc), we really will only use the list of `site_no` values to select locations out of the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Metrics\n",
    "\n",
    "<img src='./Eval_Analysis_Metrics.svg' width=360>\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations,  \n",
    "2) The domain over which to benchmark \n",
    "3) A set of statistical metrics with which to benchmark. <span style=\"color:red; font-size:large\"><<--You are here</span>\n",
    "\n",
    "The code to calculate the various metrics has been standardized [here](./NWM_StandardSuite_v1.ipynb). You can \n",
    "use these or write your own.  To import and use these standard definitions, run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../Metrics_DScore_Suite_v1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you use these functions or your own, we need to put all metric computation into a special all-encompasing \n",
    "benchmarking function--a single call which can be assigned to each gage in our domain list. This sort of arrangement \n",
    "is well-suited to parallelism with `dask`. \n",
    "\n",
    "If this is done well, the process will benefit enormously from task parallelism -- each gage can be given its own \n",
    "CPU to run on.  After all are done, the various results will be collected and assembled into a composite dataset. \n",
    "\n",
    "To achieve this, we need a single 'atomic' function that can execute independently. It will take the gage identifier \n",
    "as input and return a list of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wrapper function -- this func will be called once per gage_id, each call on its own dask worker\n",
    "def compute_benchmark(gage_id):\n",
    "    try:\n",
    "        ## obs_data and mod_data should be globals...\n",
    "        obs = obs_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series()\n",
    "        mod = mod_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series().resample('1D', offset='5h').mean() \n",
    "        \n",
    "        # make sure the indices match\n",
    "        obs.index = obs.index.to_period('D')\n",
    "        mod.index = mod.index.to_period('D')\n",
    "\n",
    "        # merge obs and predictions; drop NaNs.\n",
    "        gage_df = pd.merge(obs, mod, left_index=True, right_index=True).dropna(how='any')\n",
    "        \n",
    "        obs_log = np.log(gage_df['observed'].clip(lower=0.01)) # clip to remove zeros and negative values\n",
    "        sim_log = np.log(gage_df['predicted'].clip(lower=0.01))\n",
    "        \n",
    "        scores = pd.concat([\n",
    "                pd.Series([ mse(obs_log, sim_log) ], index=[\"mse\"], dtype='float64'),\n",
    "                bias_distribution_sequence(obs_log, sim_log), \n",
    "                seasonal_mse(obs_log, sim_log),\n",
    "                quantile_mse(obs_log, sim_log)\n",
    "                ]\n",
    "            )\n",
    "        scores.name=gage_id\n",
    "        return scores\n",
    "    except Exception as e:#<-- this is an extremely broad way to catch exceptions.  We only do it this way to ensure \n",
    "            #    that a failure on one benchmark (for a single stream gage) will not halt the entire run. \n",
    "        logging.info(\"Benchmark failed for %s\", gage_id)\n",
    "        raise\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test to be sure this `compute_benchmark()` function will return data for a single gage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conda/users/76634b8b76503b686ed55f14ef74288ada4ffb7c19282b669f7edfb7fc8aed9d-20220909-192621-731199-163-pangeo/lib/python3.9/site-packages/dask/base.py:1283: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>observed</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-10-01</th>\n",
       "      <td>2.018991</td>\n",
       "      <td>7.970833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-10-02</th>\n",
       "      <td>2.973269</td>\n",
       "      <td>12.272500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-10-03</th>\n",
       "      <td>5.521785</td>\n",
       "      <td>9.787916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-10-04</th>\n",
       "      <td>5.323567</td>\n",
       "      <td>8.144583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-10-05</th>\n",
       "      <td>4.728913</td>\n",
       "      <td>5.756250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-27</th>\n",
       "      <td>4.643963</td>\n",
       "      <td>8.657499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-28</th>\n",
       "      <td>3.596240</td>\n",
       "      <td>9.261250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-29</th>\n",
       "      <td>2.831685</td>\n",
       "      <td>6.902916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30</th>\n",
       "      <td>2.392774</td>\n",
       "      <td>4.537083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31</th>\n",
       "      <td>2.092615</td>\n",
       "      <td>3.518421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4475 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            observed  predicted\n",
       "time                           \n",
       "2008-10-01  2.018991   7.970833\n",
       "2008-10-02  2.973269  12.272500\n",
       "2008-10-03  5.521785   9.787916\n",
       "2008-10-04  5.323567   8.144583\n",
       "2008-10-05  4.728913   5.756250\n",
       "...              ...        ...\n",
       "2020-12-27  4.643963   8.657499\n",
       "2020-12-28  3.596240   9.261250\n",
       "2020-12-29  2.831685   6.902916\n",
       "2020-12-30  2.392774   4.537083\n",
       "2020-12-31  2.092615   3.518421\n",
       "\n",
       "[4475 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gage_id='USGS-01030350'\n",
    "obs = obs_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series()\n",
    "mod = mod_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series().resample('1D', offset='5h').mean() \n",
    "\n",
    "# make sure the indices match\n",
    "obs.index = obs.index.to_period('D')\n",
    "mod.index = mod.index.to_period('D')\n",
    "\n",
    "# merge obs and predictions; drop NaNs.\n",
    "gage_df = pd.merge(obs, mod, left_index=True, right_index=True).dropna(how='any')\n",
    "gage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conda/users/76634b8b76503b686ed55f14ef74288ada4ffb7c19282b669f7edfb7fc8aed9d-20220909-192621-731199-163-pangeo/lib/python3.9/site-packages/dask/base.py:1283: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mse          1.356541\n",
       "e_bias       0.181100\n",
       "e_dist       0.914028\n",
       "e_seq        0.261676\n",
       "winter       0.087221\n",
       "spring       0.101688\n",
       "summer       0.549519\n",
       "fall         0.618114\n",
       "low          1.110189\n",
       "below_avg    0.076378\n",
       "above_avg    0.055341\n",
       "high         0.114634\n",
       "Name: USGS-01030350, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_benchmark('USGS-01030350')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the Analysis \n",
    "We now need to set up a way to farm out this function, once per gage ID, to workers in a parallel cluster. `dask` will do this\n",
    "using a dask '_bag_'.  \n",
    "\n",
    ":::{sidebar}\n",
    "\n",
    "Read more about task parallelism with Dask and how we are using dask bags [here](/dev/null)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ### ### CONFIG CUSTOM CLUSTER HERE ### ### ###\n",
    "aws_profile = 'esip-qhub'\n",
    "aws_region = 'us-west-2'\n",
    "aws_endpoint = f's3.{aws_region}.amazonaws.com'\n",
    "  \n",
    "import configparser\n",
    "awsconfig = configparser.ConfigParser()\n",
    "awsconfig.read(\n",
    "    os.path.expanduser(r'~/.aws/credentials') # default location... if yours is elsewhere, change this.\n",
    ")\n",
    "# Set environment vars based on parsed awsconfig\n",
    "os.environ['AWS_ACCESS_KEY_ID']     = awsconfig[aws_profile]['aws_access_key_id']    \n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = awsconfig[aws_profile]['aws_secret_access_key']    \n",
    "os.environ['AWS_S3_ENDPOINT']       = aws_endpoint\n",
    "os.environ['AWS_DEFAULT_REGION']    = aws_region\n",
    "\n",
    "try: \n",
    "    del os.environ['AWS_PROFILE']\n",
    "except KeyError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "gateway = Gateway()\n",
    "options = gateway.cluster_options()\n",
    "options.conda_environment='users/users-pangeo'\n",
    "options.profile = 'Medium Worker'\n",
    "options.environment_vars = dict(\n",
    "    DASK_DISTRIBUTED__SCHEDULER__WORKER_SATURATION=\"1.0\"\n",
    ")\n",
    "options.environment_vars.update(dict(os.environ))\n",
    "cluster = gateway.new_cluster(options)\n",
    "cluster.adapt(minimum=10, maximum=30)\n",
    "\n",
    "# get the client for the cluster\n",
    "client = cluster.get_client()\n",
    "client.dashboard_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dask bag with the contents beging a list of the cobalt gages\n",
    "import dask.bag as db\n",
    "bag = db.from_sequence( cobalt.index.tolist() ).map(compute_benchmark)\n",
    "results = bag.compute() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that big task done, we don't need `dask` parallelism any more. Let's shut down the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close(); del client\n",
    "cluster.close(); del cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the results\n",
    "The `bag` now contains a collection of return values (one per call to `compute_benchmark()`).  We can massage that into a table/dataframe for easier processing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [i for i in results if i is not None] # Drop entries where compute_benchmark failed\n",
    "results_df = pd.concat(results, axis=1).T\n",
    "results_df.index.name = 'site_no'\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe/table can be saved to disk as a CSV. It will be used for visualizations in [other notebooks](NWM_Benchmark_Visualization.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('DScore_streamflow_example.csv') ##<--- change this to a personalized filename"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "d7ebce313f85fb1ac8949e834c83f371584cb2422d845bf1570c1220fdedc716"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
