{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D-Score Steamflow Analysis Workflow\n",
    "\n",
    "<img src='./Eval_Analysis.svg' width=600>\n",
    "\n",
    ":::{note}\n",
    "\n",
    "_This notebook adapted from originals by Timothy Hodson and Rich Signell. See that upstream work at:_\n",
    "* https://github.com/thodson-usgs/dscore\n",
    "* https://github.com/USGS-python/hytest-evaluation-workflows/\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "## Essential Benchmark Components\n",
    "This benchmark notebook will present a workflow which follows a canonical model for Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations; \n",
    "2) The domain (e.g. space or time) over which to benchmark;\n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load libraries and configure Python computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data\n",
    "\n",
    "<img src='./Eval_Analysis_Data.svg' width=360>\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations,  <span style=\"color:red; font-size:large\"><<--You are here</span>\n",
    "2) The domain over which to benchmark \n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Finding and loading data is made easier for this particular workflow (the _streamflow_ variable), in that most of it has been \n",
    "pre-processed and stored in a cloud-friendly format.  That process is described in the [data preparation](./01_Data_Prep.ipynb)\n",
    "notebook. We will proceed here using the already-prepared data for _streamflow_, which is included in the HyTEST **intake catalog**.  \n",
    "\n",
    ":::{sidebar}\n",
    "Learn more about `intake` [here](/dev/null)\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: \n",
      " conus404-hourly-onprem\n",
      "conus404-hourly-cloud\n",
      "conus404-daily-onprem\n",
      "conus404-daily-diagnostic-onprem\n",
      "conus404-daily-cloud\n",
      "conus404-daily-diagnostic-cloud\n",
      "conus404-monthly-onprem\n",
      "conus404-monthly-cloud\n",
      "nwis-streamflow-usgs-gages-onprem\n",
      "nwis-streamflow-usgs-gages-cloud\n",
      "nwm21-streamflow-usgs-gages-onprem\n",
      "nwm21-streamflow-usgs-gages-cloud\n",
      "nwm21-streamflow-cloud\n",
      "nwm21-scores\n",
      "lcmap-cloud\n",
      "conus404-hourly-cloud-dev\n",
      "nhm-v1.0-daymet-byHRU-onprem\n",
      "nhm-v1.0-daymet-byHW-musk-onprem\n",
      "nhm-v1.0-daymet-byHW-musk-obs-onprem\n",
      "nhm-v1.0-daymet-byHW-noroute-onprem\n",
      "nhm-v1.0-daymet-byHW-noroute_obs-onprem\n",
      "nhm-v1.1-gridmet-byHRU-onprem\n",
      "nhm-v1.1-gridmet-byHW-onprem\n",
      "nhm-v1.1-gridmet-byHWobs-onprem\n"
     ]
    }
   ],
   "source": [
    "import intake\n",
    "cat = intake.open_catalog(r'https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml')\n",
    "print(\"Available datasets: \\n\", \"\\n\".join(cat.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The above list represents the processed datasets available for benchmarking.  If a dataset\n",
    "you want is not in that list, \n",
    "you can load data directly from S3 or onprem. \n",
    "If you load data from a source other than this list, you can jump to [Step 2: Restrict to a Domain](#step-2-restrict-to-a-domain)\n",
    "\n",
    "Note that the interesting datasets in the cataloged dataset above are duplicated: Some are `-onprem` \n",
    "and some are `-cloud`. Same data, but the storage location and access protocol will be different. You \n",
    "will definitely want to use the correct copy of the data for your computing environment.  \n",
    "* `onprem` : Direct access via the `caldera` filesystem from _denali_ or _tallgrass_\n",
    "* `cloud` : Network access via S3 bucket, suitable for consumption on cloud-hosted jupyter servers. You could also access using any network-attached computer, but the amount of data will likely saturate your connection.  Use in the cloud (e.g. ESIP QHub)\n",
    "\n",
    "So... are you on-prem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not onprem; use '-cloud' data source\n",
      "Observed :  nwis-streamflow-usgs-gages-cloud\n",
      "Modeled  :  nwm21-streamflow-usgs-gages-cloud\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "onprem = (platform.node() in ['denali', 'tallgrass'])\n",
    "if onprem:\n",
    "    print(\"Yes : -onprem\")\n",
    "    obs_data_src='nwis-streamflow-usgs-gages-onprem'\n",
    "    mod_data_src='nwm21-streamflow-usgs-gages-onprem'\n",
    "else:\n",
    "    print(\"Not onprem; use '-cloud' data source\")\n",
    "    obs_data_src='nwis-streamflow-usgs-gages-cloud'\n",
    "    mod_data_src='nwm21-streamflow-usgs-gages-cloud'\n",
    "print(\"Observed : \", obs_data_src)\n",
    "print(\"Modeled  : \", mod_data_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_of_interest = 'streamflow'\n",
    "try:\n",
    "    obs = cat[obs_data_src].to_dask()\n",
    "    mod = cat[mod_data_src].to_dask()\n",
    "except KeyError:\n",
    "    print(\"Something wrong with dataset names.\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    obs_data = obs[variable_of_interest]\n",
    "    mod_data = mod[variable_of_interest].astype('float32')\n",
    "except KeyError:\n",
    "    print(f\"{variable_of_interest} was not found in these data.\")\n",
    "\n",
    "obs_data.name = 'observed'\n",
    "mod_data.name = 'predicted'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Restrict to a Domain\n",
    "\n",
    "<img src='./Eval_Analysis_Domain.svg' width=360>\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations,  \n",
    "2) The domain over which to benchmark <span style=\"color:red; font-size:large\"><<--You are here</span>\n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Each benchmark domain is defined over specific bounds (typically space and/or time). \n",
    "Benchmark domain definitions can be defined within the notebook, or sourced from\n",
    "elsewhere. For this example, we use the _Cobalt_ gages, avaliable for download on ScienceBase \n",
    "([Foks et al., 2022](https://doi.org/10.5066/P972P42Z)).  \n",
    "\n",
    "This is essentially a list of \n",
    "stream guages in which we are interested, along with some  metadata about that gage (watershed, \n",
    "reach code, etc).  We will use this as a spatial selector to restrict the original data to only \n",
    "those gages found within this benchmarking domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5390 gages in this benchmark\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dec_lat_va</th>\n",
       "      <th>dec_long_va</th>\n",
       "      <th>comid</th>\n",
       "      <th>reachcode</th>\n",
       "      <th>reach_meas</th>\n",
       "      <th>drain_sqkm</th>\n",
       "      <th>huc02</th>\n",
       "      <th>gagesII_class</th>\n",
       "      <th>aggecoregion</th>\n",
       "      <th>complete_yrs</th>\n",
       "      <th>n_days</th>\n",
       "      <th>nldi</th>\n",
       "      <th>swim</th>\n",
       "      <th>gfv1d1</th>\n",
       "      <th>camels</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site_no</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>USGS-01011000</th>\n",
       "      <td>47.069722</td>\n",
       "      <td>-69.079444</td>\n",
       "      <td>721640</td>\n",
       "      <td>01010002000001</td>\n",
       "      <td>53.747189</td>\n",
       "      <td>3186.8</td>\n",
       "      <td>01</td>\n",
       "      <td>Non-ref</td>\n",
       "      <td>NorthEast</td>\n",
       "      <td>33</td>\n",
       "      <td>12146</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-01013500</th>\n",
       "      <td>47.237500</td>\n",
       "      <td>-68.582778</td>\n",
       "      <td>724696</td>\n",
       "      <td>01010003000003</td>\n",
       "      <td>7.660419</td>\n",
       "      <td>2252.7</td>\n",
       "      <td>01</td>\n",
       "      <td>Ref</td>\n",
       "      <td>NorthEast</td>\n",
       "      <td>33</td>\n",
       "      <td>12146</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dec_lat_va  dec_long_va   comid       reachcode  reach_meas  \\\n",
       "site_no                                                                      \n",
       "USGS-01011000   47.069722   -69.079444  721640  01010002000001   53.747189   \n",
       "USGS-01013500   47.237500   -68.582778  724696  01010003000003    7.660419   \n",
       "\n",
       "               drain_sqkm huc02 gagesII_class aggecoregion  complete_yrs  \\\n",
       "site_no                                                                    \n",
       "USGS-01011000      3186.8    01       Non-ref    NorthEast            33   \n",
       "USGS-01013500      2252.7    01           Ref    NorthEast            33   \n",
       "\n",
       "               n_days  nldi  swim  gfv1d1  camels  \n",
       "site_no                                            \n",
       "USGS-01011000   12146     1     1       1       0  \n",
       "USGS-01013500   12146     1     1       1       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cobalt = pd.read_csv(\n",
    "    'https://www.sciencebase.gov/catalog/file/get/6181ac65d34e9f2789e44897?f=__disk__22%2F1a%2Fd2%2F221ad2fe9d95de17731ad35d0fc6b417a4b53ee1',\n",
    "    dtype={'site_no':str, 'huc_cd':str, 'reachcode':str, 'comid':str, 'gagesII_class':str, 'aggecoregion': str}, \n",
    "    index_col='site_no'\n",
    "    )\n",
    "# Re-format the gage_id/site_no string value.  ex:   \"1000000\"  ==> \"USGS-1000000\"\n",
    "cobalt.rename(index=lambda x: f'USGS-{x}', inplace=True)\n",
    "print(f\"{len(cobalt.index)} gages in this benchmark\")\n",
    "cobalt.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a domain dataset representing the stream gages (unique `site_no` values) identifying the locations making up the spatial domain of this benchmark. While we have good meta-data for these gages (lat/lon location, HUC8 code, etc), we really will only use the list of `site_no` values to select locations out of the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Metrics\n",
    "\n",
    "<img src='./Eval_Analysis_Metrics.svg' width=360>\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations,  \n",
    "2) The domain over which to benchmark \n",
    "3) A set of statistical metrics with which to benchmark. <span style=\"color:red; font-size:large\"><<--You are here</span>\n",
    "\n",
    "The code to calculate the various metrics has been standardized [here](./NWM_StandardSuite_v1.ipynb). You can \n",
    "use these or write your own.  To import and use these standard definitions, run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../Metrics_DScore_Suite_v1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you use these functions or your own, we need to put all metric computation into a special all-encompasing \n",
    "benchmarking function--a single call which can be assigned to each gage in our domain list. This sort of arrangement \n",
    "is well-suited to parallelism with `dask`. \n",
    "\n",
    "If this is done well, the process will benefit enormously from task parallelism -- each gage can be given its own \n",
    "CPU to run on.  After all are done, the various results will be collected and assembled into a composite dataset. \n",
    "\n",
    "To achieve this, we need a single 'atomic' function that can execute independently. It will take the gage identifier \n",
    "as input and return a list of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wrapper function -- this func will be called once per gage_id, each call on its own dask worker\n",
    "def compute_benchmark(gage_id):\n",
    "    try:\n",
    "        ## obs_data and mod_data should be globals...\n",
    "        obs = obs_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series()\n",
    "        mod = mod_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series().resample('1D', offset='5h').mean() \n",
    "        \n",
    "        # make sure the indices match\n",
    "        obs.index = obs.index.to_period('D')\n",
    "        mod.index = mod.index.to_period('D')\n",
    "\n",
    "        # merge obs and predictions; drop NaNs.\n",
    "        gage_df = pd.merge(obs, mod, left_index=True, right_index=True).dropna(how='any')\n",
    "        \n",
    "        obs_log = np.log(gage_df['observed'].clip(lower=0.01)) # clip to remove zeros and negative values\n",
    "        sim_log = np.log(gage_df['predicted'].clip(lower=0.01))\n",
    "        \n",
    "        scores = pd.concat([\n",
    "                pd.Series([ mse(obs_log, sim_log) ], index=[\"mse\"], dtype='float64'),\n",
    "                bias_distribution_sequence(obs_log, sim_log), \n",
    "                seasonal_mse(obs_log, sim_log),\n",
    "                quantile_mse(obs_log, sim_log)\n",
    "                ]\n",
    "            )\n",
    "        scores.name=gage_id\n",
    "        return scores\n",
    "    except Exception as e:#<-- this is an extremely broad way to catch exceptions.  We only do it this way to ensure \n",
    "            #    that a failure on one benchmark (for a single stream gage) will not halt the entire run. \n",
    "        logging.info(\"Benchmark failed for %s\", gage_id)\n",
    "        #raise\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test to be sure this `compute_benchmark()` function will return data for a single gage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conda/users/4a0797bbd1f8291f89a5724f0c4063e777248062b823ac5906f8dede9cb4192f-20230118-174644-766648-50-pangeo/lib/python3.9/site-packages/dask/base.py:1367: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>observed</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-10-01</th>\n",
       "      <td>2.018991</td>\n",
       "      <td>7.970833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-10-02</th>\n",
       "      <td>2.973269</td>\n",
       "      <td>12.272500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-10-03</th>\n",
       "      <td>5.521785</td>\n",
       "      <td>9.787916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-10-04</th>\n",
       "      <td>5.323567</td>\n",
       "      <td>8.144583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-10-05</th>\n",
       "      <td>4.728913</td>\n",
       "      <td>5.756250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-27</th>\n",
       "      <td>4.643963</td>\n",
       "      <td>8.657499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-28</th>\n",
       "      <td>3.596240</td>\n",
       "      <td>9.261250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-29</th>\n",
       "      <td>2.831685</td>\n",
       "      <td>6.902916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30</th>\n",
       "      <td>2.392774</td>\n",
       "      <td>4.537083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31</th>\n",
       "      <td>2.092615</td>\n",
       "      <td>3.518421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4475 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            observed  predicted\n",
       "time                           \n",
       "2008-10-01  2.018991   7.970833\n",
       "2008-10-02  2.973269  12.272500\n",
       "2008-10-03  5.521785   9.787916\n",
       "2008-10-04  5.323567   8.144583\n",
       "2008-10-05  4.728913   5.756250\n",
       "...              ...        ...\n",
       "2020-12-27  4.643963   8.657499\n",
       "2020-12-28  3.596240   9.261250\n",
       "2020-12-29  2.831685   6.902916\n",
       "2020-12-30  2.392774   4.537083\n",
       "2020-12-31  2.092615   3.518421\n",
       "\n",
       "[4475 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gage_id='USGS-01030350'\n",
    "obs = obs_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series()\n",
    "mod = mod_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series().resample('1D', offset='5h').mean() \n",
    "\n",
    "# make sure the indices match\n",
    "obs.index = obs.index.to_period('D')\n",
    "mod.index = mod.index.to_period('D')\n",
    "\n",
    "# merge obs and predictions; drop NaNs.\n",
    "gage_df = pd.merge(obs, mod, left_index=True, right_index=True).dropna(how='any')\n",
    "gage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mse          1.356541\n",
       "e_bias       0.181100\n",
       "e_dist       0.914028\n",
       "e_seq        0.261676\n",
       "winter       0.087221\n",
       "spring       0.101688\n",
       "summer       0.549519\n",
       "fall         0.618114\n",
       "low          1.110189\n",
       "below_avg    0.076378\n",
       "above_avg    0.055341\n",
       "high         0.114634\n",
       "Name: USGS-01030350, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_benchmark('USGS-01030350')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the Analysis \n",
    "We now need to set up a way to farm out this function, once per gage ID, to workers in a parallel cluster. `dask` will do this\n",
    "using a dask '_bag_'.  \n",
    "\n",
    ":::{sidebar}\n",
    "\n",
    "Read more about task parallelism with Dask and how we are using dask bags [here](/dev/null)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "### ### ### CONFIG CUSTOM CLUSTER HERE ### ### ###\n",
    "aws_profile = 'esip-qhub'\n",
    "aws_region = 'us-west-2'\n",
    "aws_endpoint = f's3.{aws_region}.amazonaws.com'\n",
    "  \n",
    "import configparser\n",
    "awsconfig = configparser.ConfigParser()\n",
    "awsconfig.read(\n",
    "    os.path.expanduser(r'~/.aws/credentials') # default location... if yours is elsewhere, change this.\n",
    ")\n",
    "# Set environment vars based on parsed awsconfig\n",
    "os.environ['AWS_ACCESS_KEY_ID']     = awsconfig[aws_profile]['aws_access_key_id']    \n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = awsconfig[aws_profile]['aws_secret_access_key']    \n",
    "os.environ['AWS_S3_ENDPOINT']       = aws_endpoint\n",
    "os.environ['AWS_DEFAULT_REGION']    = aws_region\n",
    "\n",
    "try: \n",
    "    del os.environ['AWS_PROFILE']\n",
    "except KeyError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://nebari.esipfed.org/gateway/clusters/dev.308b1bbbeddc49cb9c930757e79a7ffc/status'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask_gateway import Gateway\n",
    "gateway = Gateway()\n",
    "options = gateway.cluster_options()\n",
    "options.conda_environment='users/users-pangeo'\n",
    "options.profile = 'Medium Worker'\n",
    "options.environment_vars = dict(\n",
    "    DASK_DISTRIBUTED__SCHEDULER__WORKER_SATURATION=\"1.0\"\n",
    ")\n",
    "options.environment_vars.update(dict(os.environ))\n",
    "cluster = gateway.new_cluster(options)\n",
    "cluster.adapt(minimum=10, maximum=30)\n",
    "\n",
    "# get the client for the cluster\n",
    "client = cluster.get_client()\n",
    "client.dashboard_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dask bag with the contents beging a list of the cobalt gages\n",
    "import dask.bag as db\n",
    "bag = db.from_sequence( \n",
    "    cobalt.index.tolist() \n",
    ").map(compute_benchmark)\n",
    "results = bag.compute() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that big task done, we don't need `dask` parallelism any more. Let's shut down the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conda/users/4a0797bbd1f8291f89a5724f0c4063e777248062b823ac5906f8dede9cb4192f-20230118-174644-766648-50-pangeo/lib/python3.9/site-packages/dask_gateway/client.py:1014: RuntimeWarning: coroutine 'rpc.close_rpc' was never awaited\n",
      "  self.scheduler_comm.close_rpc()\n"
     ]
    }
   ],
   "source": [
    "client.close(); del client\n",
    "cluster.close(); del cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the results\n",
    "The `bag` now contains a collection of return values (one per call to `compute_benchmark()`).  We can massage that into a table/dataframe for easier processing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>e_bias</th>\n",
       "      <th>e_dist</th>\n",
       "      <th>e_seq</th>\n",
       "      <th>winter</th>\n",
       "      <th>spring</th>\n",
       "      <th>summer</th>\n",
       "      <th>fall</th>\n",
       "      <th>low</th>\n",
       "      <th>below_avg</th>\n",
       "      <th>above_avg</th>\n",
       "      <th>high</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site_no</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>USGS-01011000</th>\n",
       "      <td>0.349280</td>\n",
       "      <td>0.062854</td>\n",
       "      <td>0.008747</td>\n",
       "      <td>0.277697</td>\n",
       "      <td>0.077164</td>\n",
       "      <td>0.108429</td>\n",
       "      <td>0.108192</td>\n",
       "      <td>0.055495</td>\n",
       "      <td>0.052829</td>\n",
       "      <td>0.092242</td>\n",
       "      <td>0.104895</td>\n",
       "      <td>0.099314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-01013500</th>\n",
       "      <td>0.231121</td>\n",
       "      <td>0.002947</td>\n",
       "      <td>0.070471</td>\n",
       "      <td>0.157718</td>\n",
       "      <td>0.024475</td>\n",
       "      <td>0.088122</td>\n",
       "      <td>0.049798</td>\n",
       "      <td>0.068726</td>\n",
       "      <td>0.072867</td>\n",
       "      <td>0.038483</td>\n",
       "      <td>0.036119</td>\n",
       "      <td>0.083652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-01015800</th>\n",
       "      <td>0.283531</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.014034</td>\n",
       "      <td>0.268733</td>\n",
       "      <td>0.039518</td>\n",
       "      <td>0.110405</td>\n",
       "      <td>0.073111</td>\n",
       "      <td>0.060497</td>\n",
       "      <td>0.059348</td>\n",
       "      <td>0.060264</td>\n",
       "      <td>0.074423</td>\n",
       "      <td>0.089495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-01017000</th>\n",
       "      <td>0.316433</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.012394</td>\n",
       "      <td>0.302951</td>\n",
       "      <td>0.046091</td>\n",
       "      <td>0.109606</td>\n",
       "      <td>0.083467</td>\n",
       "      <td>0.077269</td>\n",
       "      <td>0.062643</td>\n",
       "      <td>0.064209</td>\n",
       "      <td>0.086774</td>\n",
       "      <td>0.102808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-01017550</th>\n",
       "      <td>0.667978</td>\n",
       "      <td>0.020339</td>\n",
       "      <td>0.046881</td>\n",
       "      <td>0.600846</td>\n",
       "      <td>0.125872</td>\n",
       "      <td>0.201270</td>\n",
       "      <td>0.146279</td>\n",
       "      <td>0.194557</td>\n",
       "      <td>0.128817</td>\n",
       "      <td>0.154305</td>\n",
       "      <td>0.165163</td>\n",
       "      <td>0.219694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-14369500</th>\n",
       "      <td>0.458882</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>0.152934</td>\n",
       "      <td>0.297617</td>\n",
       "      <td>0.061887</td>\n",
       "      <td>0.084826</td>\n",
       "      <td>0.153114</td>\n",
       "      <td>0.159056</td>\n",
       "      <td>0.194792</td>\n",
       "      <td>0.084246</td>\n",
       "      <td>0.119609</td>\n",
       "      <td>0.060235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-14372300</th>\n",
       "      <td>0.306279</td>\n",
       "      <td>0.010342</td>\n",
       "      <td>0.148491</td>\n",
       "      <td>0.147466</td>\n",
       "      <td>0.047533</td>\n",
       "      <td>0.045161</td>\n",
       "      <td>0.043905</td>\n",
       "      <td>0.169680</td>\n",
       "      <td>0.105135</td>\n",
       "      <td>0.101732</td>\n",
       "      <td>0.062807</td>\n",
       "      <td>0.036606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-14375100</th>\n",
       "      <td>2.365585</td>\n",
       "      <td>0.529877</td>\n",
       "      <td>0.202781</td>\n",
       "      <td>1.633049</td>\n",
       "      <td>1.084581</td>\n",
       "      <td>0.929287</td>\n",
       "      <td>0.222655</td>\n",
       "      <td>0.129067</td>\n",
       "      <td>1.310581</td>\n",
       "      <td>0.822220</td>\n",
       "      <td>0.133049</td>\n",
       "      <td>0.099743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-14377100</th>\n",
       "      <td>0.277418</td>\n",
       "      <td>0.033382</td>\n",
       "      <td>0.020332</td>\n",
       "      <td>0.223719</td>\n",
       "      <td>0.042932</td>\n",
       "      <td>0.022683</td>\n",
       "      <td>0.108892</td>\n",
       "      <td>0.102911</td>\n",
       "      <td>0.133749</td>\n",
       "      <td>0.067866</td>\n",
       "      <td>0.045944</td>\n",
       "      <td>0.029858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-14400000</th>\n",
       "      <td>0.242510</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.054383</td>\n",
       "      <td>0.188010</td>\n",
       "      <td>0.048414</td>\n",
       "      <td>0.048052</td>\n",
       "      <td>0.044409</td>\n",
       "      <td>0.101635</td>\n",
       "      <td>0.067314</td>\n",
       "      <td>0.056164</td>\n",
       "      <td>0.064461</td>\n",
       "      <td>0.054571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5380 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mse    e_bias    e_dist     e_seq    winter    spring  \\\n",
       "site_no                                                                     \n",
       "USGS-01011000  0.349280  0.062854  0.008747  0.277697  0.077164  0.108429   \n",
       "USGS-01013500  0.231121  0.002947  0.070471  0.157718  0.024475  0.088122   \n",
       "USGS-01015800  0.283531  0.000781  0.014034  0.268733  0.039518  0.110405   \n",
       "USGS-01017000  0.316433  0.001108  0.012394  0.302951  0.046091  0.109606   \n",
       "USGS-01017550  0.667978  0.020339  0.046881  0.600846  0.125872  0.201270   \n",
       "...                 ...       ...       ...       ...       ...       ...   \n",
       "USGS-14369500  0.458882  0.008361  0.152934  0.297617  0.061887  0.084826   \n",
       "USGS-14372300  0.306279  0.010342  0.148491  0.147466  0.047533  0.045161   \n",
       "USGS-14375100  2.365585  0.529877  0.202781  1.633049  1.084581  0.929287   \n",
       "USGS-14377100  0.277418  0.033382  0.020332  0.223719  0.042932  0.022683   \n",
       "USGS-14400000  0.242510  0.000133  0.054383  0.188010  0.048414  0.048052   \n",
       "\n",
       "                 summer      fall       low  below_avg  above_avg      high  \n",
       "site_no                                                                      \n",
       "USGS-01011000  0.108192  0.055495  0.052829   0.092242   0.104895  0.099314  \n",
       "USGS-01013500  0.049798  0.068726  0.072867   0.038483   0.036119  0.083652  \n",
       "USGS-01015800  0.073111  0.060497  0.059348   0.060264   0.074423  0.089495  \n",
       "USGS-01017000  0.083467  0.077269  0.062643   0.064209   0.086774  0.102808  \n",
       "USGS-01017550  0.146279  0.194557  0.128817   0.154305   0.165163  0.219694  \n",
       "...                 ...       ...       ...        ...        ...       ...  \n",
       "USGS-14369500  0.153114  0.159056  0.194792   0.084246   0.119609  0.060235  \n",
       "USGS-14372300  0.043905  0.169680  0.105135   0.101732   0.062807  0.036606  \n",
       "USGS-14375100  0.222655  0.129067  1.310581   0.822220   0.133049  0.099743  \n",
       "USGS-14377100  0.108892  0.102911  0.133749   0.067866   0.045944  0.029858  \n",
       "USGS-14400000  0.044409  0.101635  0.067314   0.056164   0.064461  0.054571  \n",
       "\n",
       "[5380 rows x 12 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = [i for i in results if i is not None] # Drop entries where compute_benchmark failed\n",
    "results_df = pd.concat(results, axis=1).T\n",
    "results_df.index.name = 'site_no'\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe/table can be saved to disk as a CSV. It will be used for visualizations in [other notebooks](NWM_Benchmark_Visualization.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('DScore_streamflow_example.csv') ##<--- change this to a personalized filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-users-pangeo",
   "language": "python",
   "name": "conda-env-users-users-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "d7ebce313f85fb1ac8949e834c83f371584cb2422d845bf1570c1220fdedc716"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
