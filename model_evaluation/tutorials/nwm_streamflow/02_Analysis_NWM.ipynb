{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Workflow :: NWM streamflow Analysis\n",
    "\n",
    "<img src='./Eval_Analysis.svg' width=600>\n",
    "\n",
    "## Essential Benchmark Components\n",
    "This benchmark notebook will present a workflow which follows a canonical model for Essential Benchmark Components: \n",
    "1) A set of predictions and matching observation (the data); \n",
    "2) The domain (e.g. space or time) over which to benchmark;\n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load libraries and configure Python computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data\n",
    "<img src='./Eval_Analysis_Data.svg' width=360>\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) <span style=\"color:green; font-size:large\">A set of predictions and matching observations</span>\n",
    "2) The domain over which to benchmark \n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Finding and loading data is made easier for this particular workflow (the _streamflow_ variable), in that most of it has been \n",
    "pre-processed and stored in a cloud-friendly format.  That process is described in the [data preparation](./01_Data_Prep.ipynb)\n",
    "notebook. We will proceed here using the already-prepared data for _streamflow_, which is included in the HyTEST **intake catalog**.  \n",
    "\n",
    ":::{sidebar}\n",
    "Learn more about `intake` [here](/dev/null)\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: \n",
      " conus404-hourly-onprem\n",
      "conus404-hourly-cloud\n",
      "conus404-daily-onprem\n",
      "conus404-daily-diagnostic-onprem\n",
      "conus404-daily-cloud\n",
      "conus404-daily-diagnostic-cloud\n",
      "conus404-monthly-onprem\n",
      "conus404-monthly-cloud\n",
      "nwis-streamflow-usgs-gages-onprem\n",
      "nwis-streamflow-usgs-gages-cloud\n",
      "nwm21-streamflow-usgs-gages-onprem\n",
      "nwm21-streamflow-usgs-gages-cloud\n",
      "nwm21-streamflow-cloud\n",
      "nwm21-scores\n",
      "lcmap-cloud\n",
      "conus404-hourly-cloud-dev\n",
      "nhm-v1.0-daymet-byHRU-onprem\n",
      "nhm-v1.0-daymet-byHW-musk-onprem\n",
      "nhm-v1.0-daymet-byHW-musk-obs-onprem\n",
      "nhm-v1.0-daymet-byHW-noroute-onprem\n",
      "nhm-v1.0-daymet-byHW-noroute_obs-onprem\n",
      "nhm-v1.1-gridmet-byHRU-onprem\n",
      "nhm-v1.1-gridmet-byHW-onprem\n",
      "nhm-v1.1-gridmet-byHWobs-onprem\n"
     ]
    }
   ],
   "source": [
    "import intake\n",
    "cat = intake.open_catalog(r'https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml')\n",
    "print(\"Available datasets: \\n\", \"\\n\".join(cat.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above list represents the processed datasets available for benchmarking.  If a dataset\n",
    "you want is not in that list, \n",
    "you can load the data manually via direct connection to 'on-prem' or S3 object storage. \n",
    "If you load data from a source other than this list, you can jump to [Step 2: Restrict to a Domain](#step-2-restrict-to-a-domain)\n",
    "\n",
    "Note that the interesting datasets in the cataloged dataset above are duplicated: Some are `-onprem` \n",
    "and some are `-cloud`. Same data, but the storage location and access protocol will be different. You \n",
    "will definitely want to use the correct copy of the data for your computing environment.  \n",
    "* `onprem` : Direct access via the `caldera` filesystem from _denali_ or _tallgrass_\n",
    "* `cloud` : Network access via S3 bucket, suitable for consumption on cloud-hosted jupyter servers. You could also access using any network-attached computer, but the amount of data will likely saturate your connection.  Use in the cloud (e.g. ESIP QHub)\n",
    "\n",
    "So... are you on-prem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not onprem; use '-cloud' data source\n",
      "Observed :  nwis-streamflow-usgs-gages-cloud\n",
      "Modeled  :  nwm21-streamflow-usgs-gages-cloud\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "onprem = (platform.node() in ['denali', 'tallgrass'])  \n",
    "# NOTE: these hostnames are not quite correct... this will always return not onprem.\n",
    "if onprem:\n",
    "    print(\"Yes : -onprem\")\n",
    "    obs_data_src='nwis-streamflow-usgs-gages-onprem'\n",
    "    mod_data_src='nwm21-streamflow-usgs-gages-onprem'\n",
    "else:\n",
    "    print(\"Not onprem; use '-cloud' data source\")\n",
    "    obs_data_src='nwis-streamflow-usgs-gages-cloud'\n",
    "    mod_data_src='nwm21-streamflow-usgs-gages-cloud'\n",
    "print(\"Observed : \", obs_data_src)\n",
    "print(\"Modeled  : \", mod_data_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_of_interest = 'streamflow'\n",
    "try:\n",
    "    obs = cat[obs_data_src].to_dask()\n",
    "    mod = cat[mod_data_src].to_dask()\n",
    "except KeyError:\n",
    "    print(\"Something wrong with dataset names.\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    obs_data = obs[variable_of_interest]\n",
    "    mod_data = mod[variable_of_interest].astype('float32')\n",
    "except KeyError:\n",
    "    print(f\"{variable_of_interest} was not found in these data.\")\n",
    "\n",
    "obs_data.name = 'observed'\n",
    "mod_data.name = 'predicted'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Restrict to a Domain\n",
    "\n",
    "<img src='./Eval_Analysis_Domain.svg' width=360>\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations,  \n",
    "2) <span style=\"color:green; font-size:large\">The domain over which to benchmark </span>\n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Each benchmark domain is defined over specific bounds (typically space and/or time). \n",
    "Benchmark domain definitions can be defined within the notebook, or sourced from\n",
    "elsewhere. For this example, we use the _Cobalt_ gages, avaliable for download on ScienceBase \n",
    "([Foks et al., 2022](https://doi.org/10.5066/P972P42Z)).  \n",
    "\n",
    "This is essentially a list of \n",
    "stream guages in which we are interested, along with some  metadata about that gage (watershed, \n",
    "reach code, etc).  We will use this as a spatial selector to restrict the original data to only \n",
    "those gages found within this benchmarking domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5390 gages in this benchmark\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dec_lat_va</th>\n",
       "      <th>dec_long_va</th>\n",
       "      <th>comid</th>\n",
       "      <th>reachcode</th>\n",
       "      <th>reach_meas</th>\n",
       "      <th>drain_sqkm</th>\n",
       "      <th>huc02</th>\n",
       "      <th>gagesII_class</th>\n",
       "      <th>aggecoregion</th>\n",
       "      <th>complete_yrs</th>\n",
       "      <th>n_days</th>\n",
       "      <th>nldi</th>\n",
       "      <th>swim</th>\n",
       "      <th>gfv1d1</th>\n",
       "      <th>camels</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site_no</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>USGS-01011000</th>\n",
       "      <td>47.069722</td>\n",
       "      <td>-69.079444</td>\n",
       "      <td>721640</td>\n",
       "      <td>01010002000001</td>\n",
       "      <td>53.747189</td>\n",
       "      <td>3186.8</td>\n",
       "      <td>01</td>\n",
       "      <td>Non-ref</td>\n",
       "      <td>NorthEast</td>\n",
       "      <td>33</td>\n",
       "      <td>12146</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-01013500</th>\n",
       "      <td>47.237500</td>\n",
       "      <td>-68.582778</td>\n",
       "      <td>724696</td>\n",
       "      <td>01010003000003</td>\n",
       "      <td>7.660419</td>\n",
       "      <td>2252.7</td>\n",
       "      <td>01</td>\n",
       "      <td>Ref</td>\n",
       "      <td>NorthEast</td>\n",
       "      <td>33</td>\n",
       "      <td>12146</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dec_lat_va  dec_long_va   comid       reachcode  reach_meas  \\\n",
       "site_no                                                                      \n",
       "USGS-01011000   47.069722   -69.079444  721640  01010002000001   53.747189   \n",
       "USGS-01013500   47.237500   -68.582778  724696  01010003000003    7.660419   \n",
       "\n",
       "               drain_sqkm huc02 gagesII_class aggecoregion  complete_yrs  \\\n",
       "site_no                                                                    \n",
       "USGS-01011000      3186.8    01       Non-ref    NorthEast            33   \n",
       "USGS-01013500      2252.7    01           Ref    NorthEast            33   \n",
       "\n",
       "               n_days  nldi  swim  gfv1d1  camels  \n",
       "site_no                                            \n",
       "USGS-01011000   12146     1     1       1       0  \n",
       "USGS-01013500   12146     1     1       1       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cobalt = pd.read_csv(\n",
    "    'https://www.sciencebase.gov/catalog/file/get/6181ac65d34e9f2789e44897?f=__disk__22%2F1a%2Fd2%2F221ad2fe9d95de17731ad35d0fc6b417a4b53ee1',\n",
    "    dtype={'site_no':str, 'huc_cd':str, 'reachcode':str, 'comid':str, 'gagesII_class':str, 'aggecoregion': str}, \n",
    "    index_col='site_no'\n",
    "    )\n",
    "# Re-format the gage_id/site_no string value.  ex:   \"1000000\"  ==> \"USGS-1000000\"\n",
    "cobalt.rename(index=lambda x: f'USGS-{x}', inplace=True)\n",
    "print(f\"{len(cobalt.index)} gages in this benchmark\")\n",
    "cobalt.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a domain dataset representing the stream gages (unique `site_no` values) identifying the locations making up the spatial domain of this benchmark. While we have good meta-data for these gages (lat/lon location, HUC8 code, etc), we really will only use the list of `site_no` values to select locations out of the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Metrics\n",
    "\n",
    "<img src='./Eval_Analysis_Metrics.svg' width=360>\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations,  \n",
    "2) The domain over which to benchmark \n",
    "3) <span style=\"color:green; font-size:large\">A set of statistical metrics with which to benchmark.</span>\n",
    "\n",
    "The specific metrics for the NWWM \"Standard Suite\" have been codified for use in Python Jupyter notebooks\n",
    "[here](../../Metrics_NWM_StdSuite_v1.ipynb).  You can see examples of how these might be used in \n",
    "[this notebook](../../Usage_NWM_StdSuite_v1.ipynb). \n",
    "\n",
    "You can use these pre-coded metrics or write your own. To use the Standard Suite, we can run their\n",
    "definition notebook in the following cell. This will make its contents available to us in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../Metrics_NWM_StdSuite_v1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Whether you use these functions or your own, we need to put all metric computation into a single all-encompasing \n",
    "benchmarking function--a single call which can be assigned to each gage in our domain list. This sort of arrangement \n",
    "is well-suited to parallelism with `dask`. \n",
    "\n",
    "If this is done well, the process will benefit enormously from task parallelism -- each gage can be given its own \n",
    "CPU on which to run. After all are done, the various results will be collected and assembled into a composite dataset. \n",
    "\n",
    "To achieve this, we need a single 'atomic' function that can execute independently. It will take the gage identifier \n",
    "as input and return a list of metrics from the Standard Suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wrapper function -- this func will be called once per gage_id, each call on its own dask worker\n",
    "def compute_benchmark(gage_id):\n",
    "    try:\n",
    "        ## obs_data and mod_data should be globals...\n",
    "        obs = obs_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series()\n",
    "        mod = mod_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series().resample('1D', offset='5h').mean() \n",
    "        \n",
    "        # make sure the indices match\n",
    "        obs.index = obs.index.to_period('D')\n",
    "        mod.index = mod.index.to_period('D')\n",
    "\n",
    "        # merge obs and predictions; drop NaNs.\n",
    "        gage_df = pd.merge(obs, mod, left_index=True, right_index=True).dropna(how='any')\n",
    "        \n",
    "        scores = pd.Series(\n",
    "            data={\n",
    "                'NSE': NSE(gage_df.observed, gage_df.predicted),\n",
    "                'KGE': KGE(gage_df.observed, gage_df.predicted),\n",
    "                'logNSE': logNSE(gage_df.observed, gage_df.predicted),\n",
    "                'pbias': pbias(gage_df.observed, gage_df.predicted),\n",
    "                'rSD': rSD(gage_df.observed, gage_df.predicted),\n",
    "                'pearson': pearson_r(gage_df.observed, gage_df.predicted),\n",
    "                'spearman': spearman_r(gage_df.observed, gage_df.predicted), \n",
    "                'pBiasFMS': pBiasFMS(gage_df.observed, gage_df.predicted),\n",
    "                'pBiasFLV': pBiasFLV(gage_df.observed, gage_df.predicted),\n",
    "                'pBiasFHV': pBiasFHV(gage_df.observed, gage_df.predicted)\n",
    "            },\n",
    "            name=gage_id,\n",
    "            dtype='float64'\n",
    "        )\n",
    "        return scores\n",
    "    except Exception as e:#<-- this is an extremely broad way to catch exceptions.  We only do it this way to ensure \n",
    "                          #    that a failure on one benchmark (for a single stream gage) will not halt the entire run. \n",
    "        logging.info(\"Benchmark failed for %s\", gage_id)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test to be sure this `compute_benchmark()` function will return data for a single gage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NSE          0.610186\n",
       "KGE          0.581806\n",
       "logNSE       0.437533\n",
       "pbias      -12.679162\n",
       "rSD          0.655655\n",
       "pearson      0.799410\n",
       "spearman     0.859122\n",
       "pBiasFMS   -34.154380\n",
       "pBiasFLV    90.474838\n",
       "pBiasFHV   -43.865916\n",
       "Name: USGS-01030350, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_benchmark('USGS-01030350')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the Analysis \n",
    "\n",
    "We now need to set up a way to farm out this function, called once per gage ID, to workers in a \n",
    "parallel cluster. `dask` will do this using a dask '_bag_'.  \n",
    "\n",
    ":::{sidebar}\n",
    "\n",
    "Read more about task parallelism with Dask and how we are using dask bags [here](/dev/null)\n",
    "\n",
    ":::\n",
    "\n",
    "The cluster configuration is dependent on your AWS profile and credentials.  First, establish AWS identity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ### ### CONFIG CUSTOM CLUSTER HERE ### ### ###\n",
    "aws_profile = 'esip-qhub'\n",
    "aws_region = 'us-west-2'\n",
    "aws_endpoint = f's3.{aws_region}.amazonaws.com'\n",
    "  \n",
    "import configparser\n",
    "awsconfig = configparser.ConfigParser()\n",
    "awsconfig.read(\n",
    "    os.path.expanduser(r'~/.aws/credentials') # default location... if yours is elsewhere, change this.\n",
    ")\n",
    "# Set environment vars based on parsed awsconfig\n",
    "os.environ['AWS_ACCESS_KEY_ID']     = awsconfig[aws_profile]['aws_access_key_id']    \n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = awsconfig[aws_profile]['aws_secret_access_key']    \n",
    "os.environ['AWS_S3_ENDPOINT']       = aws_endpoint\n",
    "os.environ['AWS_DEFAULT_REGION']    = aws_region\n",
    "\n",
    "try: \n",
    "    del os.environ['AWS_PROFILE']\n",
    "except KeyError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With identity established, we can spin up a cluster of workers, accessed via a Dask `gateway`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://nebari.esipfed.org/gateway/clusters/dev.9b48b85e7a22461e932d49f5fc4f9d52/status'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask_gateway import Gateway\n",
    "gateway = Gateway()\n",
    "options = gateway.cluster_options()\n",
    "options.conda_environment='users/users-pangeo'\n",
    "options.profile = 'Medium Worker'\n",
    "options.environment_vars = dict(\n",
    "    DASK_DISTRIBUTED__SCHEDULER__WORKER_SATURATION=\"1.0\"\n",
    ")\n",
    "options.environment_vars.update(dict(os.environ))\n",
    "cluster = gateway.new_cluster(options)\n",
    "cluster.adapt(minimum=10, maximum=30)\n",
    "\n",
    "# get the client for the cluster\n",
    "client = cluster.get_client()\n",
    "client.dashboard_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dask bag with the contents being a list of the cobalt gages\n",
    "import dask.bag as db\n",
    "bag = db.from_sequence( cobalt.index.tolist() ).map(compute_benchmark)\n",
    "\n",
    "results = bag.compute() #<< Dispatch the workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that big task done, we don't need `dask` parallelism any more. Let's shut down the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conda/users/f98ef943295f27473771c0bee116b403b0a4d29b0d81105cc1c19fce9f8b4050-20230112-200052-865543-19-pangeo/lib/python3.9/site-packages/dask_gateway/client.py:1014: RuntimeWarning: coroutine 'rpc.close_rpc' was never awaited\n",
      "  self.scheduler_comm.close_rpc()\n"
     ]
    }
   ],
   "source": [
    "client.close(); del client\n",
    "cluster.close(); del cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the results\n",
    "The `bag` now contains a collection of return values (one per call to `compute_benchmark()`).  We can massage that into a table/dataframe for easier processing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NSE</th>\n",
       "      <th>KGE</th>\n",
       "      <th>logNSE</th>\n",
       "      <th>pbias</th>\n",
       "      <th>rSD</th>\n",
       "      <th>pearson</th>\n",
       "      <th>spearman</th>\n",
       "      <th>pBiasFMS</th>\n",
       "      <th>pBiasFLV</th>\n",
       "      <th>pBiasFHV</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site_no</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>USGS-01011000</th>\n",
       "      <td>0.689088</td>\n",
       "      <td>0.662586</td>\n",
       "      <td>0.597192</td>\n",
       "      <td>-19.883495</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.846457</td>\n",
       "      <td>0.818826</td>\n",
       "      <td>14.807269</td>\n",
       "      <td>61.287969</td>\n",
       "      <td>-36.838683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-01013500</th>\n",
       "      <td>0.620608</td>\n",
       "      <td>0.480716</td>\n",
       "      <td>0.752852</td>\n",
       "      <td>-22.970420</td>\n",
       "      <td>0.552287</td>\n",
       "      <td>0.871763</td>\n",
       "      <td>0.873238</td>\n",
       "      <td>-14.653922</td>\n",
       "      <td>60.804260</td>\n",
       "      <td>-51.751721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-01015800</th>\n",
       "      <td>0.695682</td>\n",
       "      <td>0.661854</td>\n",
       "      <td>0.764120</td>\n",
       "      <td>-13.975070</td>\n",
       "      <td>0.732493</td>\n",
       "      <td>0.847514</td>\n",
       "      <td>0.862823</td>\n",
       "      <td>-5.586971</td>\n",
       "      <td>43.496275</td>\n",
       "      <td>-40.215325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-01017000</th>\n",
       "      <td>0.676277</td>\n",
       "      <td>0.662182</td>\n",
       "      <td>0.728323</td>\n",
       "      <td>-13.596089</td>\n",
       "      <td>0.739596</td>\n",
       "      <td>0.833192</td>\n",
       "      <td>0.836719</td>\n",
       "      <td>-2.559546</td>\n",
       "      <td>50.629818</td>\n",
       "      <td>-40.769273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-01017550</th>\n",
       "      <td>-0.007024</td>\n",
       "      <td>0.488803</td>\n",
       "      <td>0.187784</td>\n",
       "      <td>25.384879</td>\n",
       "      <td>1.159282</td>\n",
       "      <td>0.585860</td>\n",
       "      <td>0.778948</td>\n",
       "      <td>20.736102</td>\n",
       "      <td>15.696818</td>\n",
       "      <td>-33.564579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-14369500</th>\n",
       "      <td>0.576253</td>\n",
       "      <td>0.614541</td>\n",
       "      <td>0.538312</td>\n",
       "      <td>21.251125</td>\n",
       "      <td>1.297323</td>\n",
       "      <td>0.877457</td>\n",
       "      <td>0.771774</td>\n",
       "      <td>43.800607</td>\n",
       "      <td>112.890399</td>\n",
       "      <td>13.504782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-14372300</th>\n",
       "      <td>0.747763</td>\n",
       "      <td>0.854819</td>\n",
       "      <td>0.462288</td>\n",
       "      <td>5.255767</td>\n",
       "      <td>0.960921</td>\n",
       "      <td>0.870431</td>\n",
       "      <td>0.889055</td>\n",
       "      <td>48.189721</td>\n",
       "      <td>-26.875940</td>\n",
       "      <td>-18.711446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-14375100</th>\n",
       "      <td>-1.256368</td>\n",
       "      <td>-0.270989</td>\n",
       "      <td>-0.519085</td>\n",
       "      <td>96.341803</td>\n",
       "      <td>1.701510</td>\n",
       "      <td>0.558273</td>\n",
       "      <td>0.330279</td>\n",
       "      <td>6.331031</td>\n",
       "      <td>735.452175</td>\n",
       "      <td>8.474573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-14377100</th>\n",
       "      <td>0.751601</td>\n",
       "      <td>0.691753</td>\n",
       "      <td>0.905648</td>\n",
       "      <td>14.723495</td>\n",
       "      <td>1.262068</td>\n",
       "      <td>0.931750</td>\n",
       "      <td>0.955250</td>\n",
       "      <td>-4.176343</td>\n",
       "      <td>22.406697</td>\n",
       "      <td>19.312102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-14400000</th>\n",
       "      <td>0.840578</td>\n",
       "      <td>0.750943</td>\n",
       "      <td>0.904838</td>\n",
       "      <td>-16.752493</td>\n",
       "      <td>0.830591</td>\n",
       "      <td>0.927435</td>\n",
       "      <td>0.957442</td>\n",
       "      <td>-19.263242</td>\n",
       "      <td>2.374988</td>\n",
       "      <td>-23.003937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5380 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    NSE       KGE    logNSE      pbias       rSD   pearson  \\\n",
       "site_no                                                                      \n",
       "USGS-01011000  0.689088  0.662586  0.597192 -19.883495  0.774751  0.846457   \n",
       "USGS-01013500  0.620608  0.480716  0.752852 -22.970420  0.552287  0.871763   \n",
       "USGS-01015800  0.695682  0.661854  0.764120 -13.975070  0.732493  0.847514   \n",
       "USGS-01017000  0.676277  0.662182  0.728323 -13.596089  0.739596  0.833192   \n",
       "USGS-01017550 -0.007024  0.488803  0.187784  25.384879  1.159282  0.585860   \n",
       "...                 ...       ...       ...        ...       ...       ...   \n",
       "USGS-14369500  0.576253  0.614541  0.538312  21.251125  1.297323  0.877457   \n",
       "USGS-14372300  0.747763  0.854819  0.462288   5.255767  0.960921  0.870431   \n",
       "USGS-14375100 -1.256368 -0.270989 -0.519085  96.341803  1.701510  0.558273   \n",
       "USGS-14377100  0.751601  0.691753  0.905648  14.723495  1.262068  0.931750   \n",
       "USGS-14400000  0.840578  0.750943  0.904838 -16.752493  0.830591  0.927435   \n",
       "\n",
       "               spearman   pBiasFMS    pBiasFLV   pBiasFHV  \n",
       "site_no                                                    \n",
       "USGS-01011000  0.818826  14.807269   61.287969 -36.838683  \n",
       "USGS-01013500  0.873238 -14.653922   60.804260 -51.751721  \n",
       "USGS-01015800  0.862823  -5.586971   43.496275 -40.215325  \n",
       "USGS-01017000  0.836719  -2.559546   50.629818 -40.769273  \n",
       "USGS-01017550  0.778948  20.736102   15.696818 -33.564579  \n",
       "...                 ...        ...         ...        ...  \n",
       "USGS-14369500  0.771774  43.800607  112.890399  13.504782  \n",
       "USGS-14372300  0.889055  48.189721  -26.875940 -18.711446  \n",
       "USGS-14375100  0.330279   6.331031  735.452175   8.474573  \n",
       "USGS-14377100  0.955250  -4.176343   22.406697  19.312102  \n",
       "USGS-14400000  0.957442 -19.263242    2.374988 -23.003937  \n",
       "\n",
       "[5380 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = [i for i in results if i is not None] # Drop entries where compute_benchmark failed\n",
    "results_df = pd.concat(r, axis=1).T\n",
    "results_df.index.name = 'site_no'\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe/table can be saved to disk as a CSV. It will be used for visualizations in [other notebooks](NWM_Benchmark_Viz.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('NWM_v2.1_streamflow_example.csv') ##<--- change this to a personalized filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-users-pangeo",
   "language": "python",
   "name": "conda-env-users-users-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "d7ebce313f85fb1ac8949e834c83f371584cb2422d845bf1570c1220fdedc716"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
