{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Workflow :: NWM streamflow Analysis\n",
    "\n",
    "<img src='./Eval_Analysis.svg' width=600>\n",
    "\n",
    "## Essential Benchmark Components\n",
    "This benchmark notebook will present a workflow which follows a canonical model for Essential Benchmark Components: \n",
    "1) A set of predictions and matching observation (the data); \n",
    "2) The domain (e.g. space or time) over which to benchmark;\n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load libraries and configure Python computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data\n",
    "<img src='./Eval_Analysis_Data.svg' width=360>\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) <span style=\"color:green; font-size:large\">A set of predictions and matching observations</span>\n",
    "2) The domain over which to benchmark \n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Finding and loading data is made easier for this particular workflow (the _streamflow_ variable), in that most of it has been \n",
    "pre-processed and stored in a cloud-friendly format.  That process is described in the [data preparation](./01_Data_Prep.ipynb)\n",
    "notebook. We will proceed here using the already-prepared data for _streamflow_, which is included in the HyTEST **intake catalog**.  \n",
    "\n",
    ":::{sidebar}\n",
    "Learn more about `intake` [here](/dev/null)\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: \n",
      " conus404-hourly-onprem\n",
      "conus404-hourly-cloud\n",
      "conus404-daily-onprem\n",
      "conus404-daily-cloud\n",
      "nwis-streamflow-usgs-gages-onprem\n",
      "nwis-streamflow-usgs-gages-cloud\n",
      "nwm21-streamflow-usgs-gages-onprem\n",
      "nwm21-streamflow-usgs-gages-cloud\n",
      "nwm21-streamflow-cloud\n",
      "nwm21-scores\n",
      "lcmap-cloud\n",
      "conus404-hourly-cloud-dev\n"
     ]
    }
   ],
   "source": [
    "import intake\n",
    "cat = intake.open_catalog(r'https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml')\n",
    "print(\"Available datasets: \\n\", \"\\n\".join(cat.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above list represents the processed datasets available for benchmarking.  If a dataset\n",
    "you want is not in that list, \n",
    "you can load the data manually via direct connection to 'on-prem' or S3 object storage. \n",
    "If you load data from a source other than this list, you can jump to [Step 2: Restrict to a Domain](#step-2-restrict-to-a-domain)\n",
    "\n",
    "Note that the interesting datasets in the cataloged dataset above are duplicated: Some are `-onprem` \n",
    "and some are `-cloud`. Same data, but the storage location and access protocol will be different. You \n",
    "will definitely want to use the correct copy of the data for your computing environment.  \n",
    "* `onprem` : Direct access via the `caldera` filesystem from _denali_ or _tallgrass_\n",
    "* `cloud` : Network access via S3 bucket, suitable for consumption on cloud-hosted jupyter servers. You could also access using any network-attached computer, but the amount of data will likely saturate your connection.  Use in the cloud (e.g. ESIP QHub)\n",
    "\n",
    "So... are you on-prem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not onprem; use '-cloud' data source\n",
      "Observed :  nwis-streamflow-usgs-gages-cloud\n",
      "Modeled  :  nwm21-streamflow-usgs-gages-cloud\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "onprem = (platform.node() in ['denali', 'tallgrass'])  # NOTE: these hostnames are not quite correct... this will always return not onprem.\n",
    "if onprem:\n",
    "    print(\"Yes : -onprem\")\n",
    "    obs_data_src='nwis-streamflow-usgs-gages-onprem'\n",
    "    mod_data_src='nwm21-streamflow-usgs-gages-onprem'\n",
    "else:\n",
    "    print(\"Not onprem; use '-cloud' data source\")\n",
    "    obs_data_src='nwis-streamflow-usgs-gages-cloud'\n",
    "    mod_data_src='nwm21-streamflow-usgs-gages-cloud'\n",
    "print(\"Observed : \", obs_data_src)\n",
    "print(\"Modeled  : \", mod_data_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_of_interest = 'streamflow'\n",
    "try:\n",
    "    obs = cat[obs_data_src].to_dask()\n",
    "    mod = cat[mod_data_src].to_dask()\n",
    "except KeyError:\n",
    "    print(\"Something wrong with dataset names.\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    obs_data = obs[variable_of_interest]\n",
    "    mod_data = mod[variable_of_interest].astype('float32')\n",
    "except KeyError:\n",
    "    print(f\"{variable_of_interest} was not found in these data.\")\n",
    "\n",
    "obs_data.name = 'observed'\n",
    "mod_data.name = 'predicted'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Restrict to a Domain\n",
    "\n",
    "<img src='./Eval_Analysis_Domain.svg' width=360>\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations,  \n",
    "2) <span style=\"color:green; font-size:large\">The domain over which to benchmark </span>\n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Each benchmark domain is defined over specific bounds (typically space and/or time). \n",
    "Benchmark domain definitions can be defined within the notebook, or sourced from\n",
    "elsewhere. For this example, we use the _Cobalt_ gages, avaliable for download on ScienceBase \n",
    "([Foks et al., 2022](https://doi.org/10.5066/P972P42Z)).  \n",
    "\n",
    "This is essentially a list of \n",
    "stream guages in which we are interested, along with some  metadata about that gage (watershed, \n",
    "reach code, etc).  We will use this as a spatial selector to restrict the original data to only \n",
    "those gages found within this benchmarking domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5390 gages in this benchmark\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dec_lat_va</th>\n",
       "      <th>dec_long_va</th>\n",
       "      <th>comid</th>\n",
       "      <th>reachcode</th>\n",
       "      <th>reach_meas</th>\n",
       "      <th>drain_sqkm</th>\n",
       "      <th>huc02</th>\n",
       "      <th>gagesII_class</th>\n",
       "      <th>aggecoregion</th>\n",
       "      <th>complete_yrs</th>\n",
       "      <th>n_days</th>\n",
       "      <th>nldi</th>\n",
       "      <th>swim</th>\n",
       "      <th>gfv1d1</th>\n",
       "      <th>camels</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site_no</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>USGS-01011000</th>\n",
       "      <td>47.069722</td>\n",
       "      <td>-69.079444</td>\n",
       "      <td>721640</td>\n",
       "      <td>01010002000001</td>\n",
       "      <td>53.747189</td>\n",
       "      <td>3186.8</td>\n",
       "      <td>01</td>\n",
       "      <td>Non-ref</td>\n",
       "      <td>NorthEast</td>\n",
       "      <td>33</td>\n",
       "      <td>12146</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-01013500</th>\n",
       "      <td>47.237500</td>\n",
       "      <td>-68.582778</td>\n",
       "      <td>724696</td>\n",
       "      <td>01010003000003</td>\n",
       "      <td>7.660419</td>\n",
       "      <td>2252.7</td>\n",
       "      <td>01</td>\n",
       "      <td>Ref</td>\n",
       "      <td>NorthEast</td>\n",
       "      <td>33</td>\n",
       "      <td>12146</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dec_lat_va  dec_long_va   comid       reachcode  reach_meas  \\\n",
       "site_no                                                                      \n",
       "USGS-01011000   47.069722   -69.079444  721640  01010002000001   53.747189   \n",
       "USGS-01013500   47.237500   -68.582778  724696  01010003000003    7.660419   \n",
       "\n",
       "               drain_sqkm huc02 gagesII_class aggecoregion  complete_yrs  \\\n",
       "site_no                                                                    \n",
       "USGS-01011000      3186.8    01       Non-ref    NorthEast            33   \n",
       "USGS-01013500      2252.7    01           Ref    NorthEast            33   \n",
       "\n",
       "               n_days  nldi  swim  gfv1d1  camels  \n",
       "site_no                                            \n",
       "USGS-01011000   12146     1     1       1       0  \n",
       "USGS-01013500   12146     1     1       1       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cobalt = pd.read_csv(\n",
    "    'https://www.sciencebase.gov/catalog/file/get/6181ac65d34e9f2789e44897?f=__disk__22%2F1a%2Fd2%2F221ad2fe9d95de17731ad35d0fc6b417a4b53ee1',\n",
    "    dtype={'site_no':str, 'huc_cd':str, 'reachcode':str, 'comid':str, 'gagesII_class':str, 'aggecoregion': str}, \n",
    "    index_col='site_no'\n",
    "    )\n",
    "# Re-format the gage_id/site_no string value.  ex:   \"1000000\"  ==> \"USGS-1000000\"\n",
    "cobalt.rename(index=lambda x: f'USGS-{x}', inplace=True)\n",
    "print(f\"{len(cobalt.index)} gages in this benchmark\")\n",
    "cobalt.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a domain dataset representing the stream gages (unique `site_no` values) identifying the locations making up the spatial domain of this benchmark. While we have good meta-data for these gages (lat/lon location, HUC8 code, etc), we really will only use the list of `site_no` values to select locations out of the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Metrics\n",
    "\n",
    "<img src='./Eval_Analysis_Metrics.svg' width=360>\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations,  \n",
    "2) The domain over which to benchmark \n",
    "3) <span style=\"color:green; font-size:large\">A set of statistical metrics with which to benchmark.</span>\n",
    "\n",
    "The code to calculate the various NWM metrics has been standardized [here](../../Metrics_NWM_StdSuite_v1.ipynb), \n",
    "with usage examples [here](../../Usage_NWM_StdSuite_v1.ipynb). \n",
    "You can use these metrics or write your own.  To import and use these standardized definitions, run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../Metrics_NWM_StdSuite_v1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you use these functions or your own, we need to put all metric computation into a special all-encompasing \n",
    "benchmarking function--a single call which can be assigned to each gage in our domain list. This sort of arrangement \n",
    "is well-suited to parallelism with `dask`. \n",
    "\n",
    "If this is done well, the process will benefit enormously from task parallelism -- each gage can be given its own \n",
    "CPU to run on.  After all are done, the various results will be collected and assembled into a composite dataset. \n",
    "\n",
    "To achieve this, we need a single 'atomic' function that can execute independently. It will take the gage identifier \n",
    "as input and return a list of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wrapper function -- this func will be called once per gage_id, each call on its own dask worker\n",
    "def compute_benchmark(gage_id):\n",
    "    try:\n",
    "        ## obs_data and mod_data should be globals...\n",
    "        obs = obs_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series()\n",
    "        mod = mod_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series().resample('1D', offset='5h').mean() \n",
    "        \n",
    "        # make sure the indices match\n",
    "        obs.index = obs.index.to_period('D')\n",
    "        mod.index = mod.index.to_period('D')\n",
    "\n",
    "        # merge obs and predictions; drop NaNs.\n",
    "        gage_df = pd.merge(obs, mod, left_index=True, right_index=True).dropna(how='any')\n",
    "        \n",
    "        scores = pd.Series(\n",
    "            data={\n",
    "                'NSE': NSE(gage_df.observed, gage_df.predicted),\n",
    "                'KGE': KGE(gage_df.observed, gage_df.predicted),\n",
    "                'logNSE': logNSE(gage_df.observed, gage_df.predicted),\n",
    "                'pbias': pbias(gage_df.observed, gage_df.predicted),\n",
    "                'rSD': rSD(gage_df.observed, gage_df.predicted),\n",
    "                'pearson': pearson_r(gage_df.observed, gage_df.predicted),\n",
    "                'spearman': spearman_r(gage_df.observed, gage_df.predicted), \n",
    "                'pBiasFMS': pBiasFMS(gage_df.observed, gage_df.predicted),\n",
    "                'pBiasFLV': pBiasFLV(gage_df.observed, gage_df.predicted),\n",
    "                'pBiasFHV': pBiasFHV(gage_df.observed, gage_df.predicted)\n",
    "            },\n",
    "            name=gage_id,\n",
    "            dtype='float64'\n",
    "        )\n",
    "        return scores\n",
    "    except Exception as e:#<-- this is an extremely broad way to catch exceptions.  We only do it this way to ensure \n",
    "        #raise e    #    that a failure on one benchmark (for a single stream gage) will not halt the entire run. \n",
    "        logging.info(\"Benchmark failed for %s\", gage_id)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test to be sure this `compute_benchmark()` function will return data for a single gage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conda/users/266bc1f046057293eb778c8b95277aae62235b8cac1c12e1334002c7a7656436-20221021-195048-630919-211-pangeo/lib/python3.9/site-packages/dask/base.py:1283: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NSE          0.610186\n",
       "KGE          0.581806\n",
       "logNSE       0.437533\n",
       "pbias      -12.679162\n",
       "rSD          0.655655\n",
       "pearson      0.799410\n",
       "spearman     0.859122\n",
       "pBiasFMS   -34.154380\n",
       "pBiasFLV    90.474838\n",
       "pBiasFHV   -43.865916\n",
       "Name: USGS-01030350, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_benchmark('USGS-01030350')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the Analysis \n",
    "We now need to set up a way to farm out this function, once per gage ID, to workers in a parallel cluster. `dask` will do this\n",
    "using a dask '_bag_'.  \n",
    "\n",
    ":::{sidebar}\n",
    "\n",
    "Read more about task parallelism with Dask and how we are using dask bags [here](/dev/null)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-west-2\n",
      "Existing Dask clusters:\n",
      "Cluster Index c_idx: 0 / Name: dev.1ae9080066f9445e957caa7b22c507b2 ClusterStatus.RUNNING\n",
      "Using existing cluster [0].\n",
      "Setting Fixed Scaling workers=30\n",
      "Reconnect client to clear cache\n",
      "client.dashboard_link (for new browser tab/window or dashboard searchbar in Jupyterhub):\n",
      "https://jupyter.qhub.esipfed.org/gateway/clusters/dev.1ae9080066f9445e957caa7b22c507b2/status\n",
      "Propagating environment variables to workers\n",
      "Using environment: users/pangeo\n"
     ]
    }
   ],
   "source": [
    "%run /shared/users/gzt5142/cluster_conf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dask bag with the contents beging a list of the cobalt gages\n",
    "import dask.bag as db\n",
    "bag = db.from_sequence( cobalt.index.tolist() ).map(compute_benchmark)\n",
    "\n",
    "results = bag.compute() #<< Dispatch the workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that big task done, we don't need `dask` parallelism any more. Let's shut down the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conda/users/266bc1f046057293eb778c8b95277aae62235b8cac1c12e1334002c7a7656436-20221021-195048-630919-211-pangeo/lib/python3.9/site-packages/dask_gateway/client.py:1014: RuntimeWarning: coroutine 'rpc.close_rpc' was never awaited\n",
      "  self.scheduler_comm.close_rpc()\n"
     ]
    }
   ],
   "source": [
    "client.close(); del client\n",
    "cluster.close(); del cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the results\n",
    "The `bag` now contains a collection of return values (one per call to `compute_benchmark()`).  We can massage that into a table/dataframe for easier processing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NSE</th>\n",
       "      <th>KGE</th>\n",
       "      <th>logNSE</th>\n",
       "      <th>pbias</th>\n",
       "      <th>rSD</th>\n",
       "      <th>pearson</th>\n",
       "      <th>spearman</th>\n",
       "      <th>pBiasFMS</th>\n",
       "      <th>pBiasFLV</th>\n",
       "      <th>pBiasFHV</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site_no</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>USGS-01011000</th>\n",
       "      <td>0.689088</td>\n",
       "      <td>0.662586</td>\n",
       "      <td>0.597192</td>\n",
       "      <td>-19.883495</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.846457</td>\n",
       "      <td>0.818826</td>\n",
       "      <td>14.807269</td>\n",
       "      <td>61.287969</td>\n",
       "      <td>-36.838683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-01013500</th>\n",
       "      <td>0.620608</td>\n",
       "      <td>0.480716</td>\n",
       "      <td>0.752852</td>\n",
       "      <td>-22.970420</td>\n",
       "      <td>0.552287</td>\n",
       "      <td>0.871763</td>\n",
       "      <td>0.873238</td>\n",
       "      <td>-14.653922</td>\n",
       "      <td>60.804260</td>\n",
       "      <td>-51.751721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-01015800</th>\n",
       "      <td>0.695682</td>\n",
       "      <td>0.661854</td>\n",
       "      <td>0.764120</td>\n",
       "      <td>-13.975070</td>\n",
       "      <td>0.732493</td>\n",
       "      <td>0.847514</td>\n",
       "      <td>0.862823</td>\n",
       "      <td>-5.586971</td>\n",
       "      <td>43.496275</td>\n",
       "      <td>-40.215325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-01017000</th>\n",
       "      <td>0.676277</td>\n",
       "      <td>0.662182</td>\n",
       "      <td>0.728323</td>\n",
       "      <td>-13.596089</td>\n",
       "      <td>0.739596</td>\n",
       "      <td>0.833192</td>\n",
       "      <td>0.836719</td>\n",
       "      <td>-2.559546</td>\n",
       "      <td>50.629818</td>\n",
       "      <td>-40.769273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-01017550</th>\n",
       "      <td>-0.007024</td>\n",
       "      <td>0.488803</td>\n",
       "      <td>0.187784</td>\n",
       "      <td>25.384879</td>\n",
       "      <td>1.159282</td>\n",
       "      <td>0.585860</td>\n",
       "      <td>0.778948</td>\n",
       "      <td>20.736102</td>\n",
       "      <td>15.696818</td>\n",
       "      <td>-33.564579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-14369500</th>\n",
       "      <td>0.576253</td>\n",
       "      <td>0.614541</td>\n",
       "      <td>0.538312</td>\n",
       "      <td>21.251125</td>\n",
       "      <td>1.297323</td>\n",
       "      <td>0.877457</td>\n",
       "      <td>0.771774</td>\n",
       "      <td>43.800607</td>\n",
       "      <td>112.890399</td>\n",
       "      <td>13.504782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-14372300</th>\n",
       "      <td>0.747763</td>\n",
       "      <td>0.854819</td>\n",
       "      <td>0.462288</td>\n",
       "      <td>5.255767</td>\n",
       "      <td>0.960921</td>\n",
       "      <td>0.870431</td>\n",
       "      <td>0.889055</td>\n",
       "      <td>48.189721</td>\n",
       "      <td>-26.875940</td>\n",
       "      <td>-18.711446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-14375100</th>\n",
       "      <td>-1.256368</td>\n",
       "      <td>-0.270989</td>\n",
       "      <td>-0.519085</td>\n",
       "      <td>96.341803</td>\n",
       "      <td>1.701510</td>\n",
       "      <td>0.558273</td>\n",
       "      <td>0.330279</td>\n",
       "      <td>6.331031</td>\n",
       "      <td>735.452175</td>\n",
       "      <td>8.474573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-14377100</th>\n",
       "      <td>0.751601</td>\n",
       "      <td>0.691753</td>\n",
       "      <td>0.905648</td>\n",
       "      <td>14.723495</td>\n",
       "      <td>1.262068</td>\n",
       "      <td>0.931750</td>\n",
       "      <td>0.955250</td>\n",
       "      <td>-4.176343</td>\n",
       "      <td>22.406697</td>\n",
       "      <td>19.312102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGS-14400000</th>\n",
       "      <td>0.840578</td>\n",
       "      <td>0.750943</td>\n",
       "      <td>0.904838</td>\n",
       "      <td>-16.752493</td>\n",
       "      <td>0.830591</td>\n",
       "      <td>0.927435</td>\n",
       "      <td>0.957442</td>\n",
       "      <td>-19.263242</td>\n",
       "      <td>2.374988</td>\n",
       "      <td>-23.003937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5380 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    NSE       KGE    logNSE      pbias       rSD   pearson  \\\n",
       "site_no                                                                      \n",
       "USGS-01011000  0.689088  0.662586  0.597192 -19.883495  0.774751  0.846457   \n",
       "USGS-01013500  0.620608  0.480716  0.752852 -22.970420  0.552287  0.871763   \n",
       "USGS-01015800  0.695682  0.661854  0.764120 -13.975070  0.732493  0.847514   \n",
       "USGS-01017000  0.676277  0.662182  0.728323 -13.596089  0.739596  0.833192   \n",
       "USGS-01017550 -0.007024  0.488803  0.187784  25.384879  1.159282  0.585860   \n",
       "...                 ...       ...       ...        ...       ...       ...   \n",
       "USGS-14369500  0.576253  0.614541  0.538312  21.251125  1.297323  0.877457   \n",
       "USGS-14372300  0.747763  0.854819  0.462288   5.255767  0.960921  0.870431   \n",
       "USGS-14375100 -1.256368 -0.270989 -0.519085  96.341803  1.701510  0.558273   \n",
       "USGS-14377100  0.751601  0.691753  0.905648  14.723495  1.262068  0.931750   \n",
       "USGS-14400000  0.840578  0.750943  0.904838 -16.752493  0.830591  0.927435   \n",
       "\n",
       "               spearman   pBiasFMS    pBiasFLV   pBiasFHV  \n",
       "site_no                                                    \n",
       "USGS-01011000  0.818826  14.807269   61.287969 -36.838683  \n",
       "USGS-01013500  0.873238 -14.653922   60.804260 -51.751721  \n",
       "USGS-01015800  0.862823  -5.586971   43.496275 -40.215325  \n",
       "USGS-01017000  0.836719  -2.559546   50.629818 -40.769273  \n",
       "USGS-01017550  0.778948  20.736102   15.696818 -33.564579  \n",
       "...                 ...        ...         ...        ...  \n",
       "USGS-14369500  0.771774  43.800607  112.890399  13.504782  \n",
       "USGS-14372300  0.889055  48.189721  -26.875940 -18.711446  \n",
       "USGS-14375100  0.330279   6.331031  735.452175   8.474573  \n",
       "USGS-14377100  0.955250  -4.176343   22.406697  19.312102  \n",
       "USGS-14400000  0.957442 -19.263242    2.374988 -23.003937  \n",
       "\n",
       "[5380 rows x 10 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = [i for i in results if i is not None] # Drop entries where compute_benchmark failed\n",
    "results_df = pd.concat(r, axis=1).T\n",
    "results_df.index.name = 'site_no'\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe/table can be saved to disk as a CSV. It will be used for visualizations in [other notebooks](NWM_Benchmark_Viz.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('NWM_v2.1_streamflow_example.csv') ##<--- change this to a personalized filename"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-pangeo",
   "language": "python",
   "name": "conda-env-users-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d7ebce313f85fb1ac8949e834c83f371584cb2422d845bf1570c1220fdedc716"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
