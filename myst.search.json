{"version":"1","records":[{"hierarchy":{"lvl1":"Contributing"},"type":"lvl1","url":"/contributing","position":0},{"hierarchy":{"lvl1":"Contributing"},"content":"Contributions are welcome from the community.\n\nProblems with code or requests for changes to the repo can be made by opening an issue on the \n\nissues page.\n\nQuestions can be asked on the \n\ndiscussions page.\n\nSubmissions of new content to our JupyterBook can be added on our dedicated \n\nJupyterBook Submissions Discussion Board.\n\nBefore creating a new issue or discussion, please take a moment to search and make sure a similar one does not already exist. If one does exist, you can comment (most simply even withjust a :+1:) to show your support for that issue/discussion.\n\nIf you have direct contributions you would like considered for incorporation into the project you can \n\nfork this repository and \n\nsubmit a pull request for review.\n\nUSGS Staff members who make contributions to this repository should make sure their contributions comply with \n\nUSGS’s Open Source Software Requirements. Any custom-developed code developed by USGS staff must go through the USGS software release process on government-owned repositories.","type":"content","url":"/contributing","position":1},{"hierarchy":{"lvl1":"License"},"type":"lvl1","url":"/license","position":0},{"hierarchy":{"lvl1":"License"},"content":"Creative Commons Legal Code\n\nCC0 1.0 UniversalCREATIVE COMMONS CORPORATION IS NOT A LAW FIRM AND DOES NOT PROVIDE\nLEGAL SERVICES. DISTRIBUTION OF THIS DOCUMENT DOES NOT CREATE AN\nATTORNEY-CLIENT RELATIONSHIP. CREATIVE COMMONS PROVIDES THIS\nINFORMATION ON AN \"AS-IS\" BASIS. CREATIVE COMMONS MAKES NO WARRANTIES\nREGARDING THE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS\nPROVIDED HEREUNDER, AND DISCLAIMS LIABILITY FOR DAMAGES RESULTING FROM\nTHE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS PROVIDED\nHEREUNDER.\n\nStatement of Purpose\n\nThe laws of most jurisdictions throughout the world automatically confer\nexclusive Copyright and Related Rights (defined below) upon the creator and\nsubsequent owner(s) (each and all, an “owner”) of an original work of\nauthorship and/or a database (each, a “Work”).\n\nCertain owners wish to permanently relinquish those rights to a Work for the\npurpose of contributing to a commons of creative, cultural and scientific\nworks (“Commons”) that the public can reliably and without fear of later\nclaims of infringement build upon, modify, incorporate in other works, reuse\nand redistribute as freely as possible in any form whatsoever and for any\npurposes, including without limitation commercial purposes. These owners may\ncontribute to the Commons to promote the ideal of a free culture and the\nfurther production of creative, cultural and scientific works, or to gain\nreputation or greater distribution for their Work in part through the use and\nefforts of others.\n\nFor these and/or other purposes and motivations, and without any expectation\nof additional consideration or compensation, the person associating CC0 with a\nWork (the “Affirmer”), to the extent that he or she is an owner of Copyright\nand Related Rights in the Work, voluntarily elects to apply CC0 to the Work\nand publicly distribute the Work under its terms, with knowledge of his or her\nCopyright and Related Rights in the Work and the meaning and intended legal\neffect of CC0 on those rights.\n\nCopyright and Related Rights. A Work made available under CC0 may be\nprotected by copyright and related or neighboring rights (“Copyright and\nRelated Rights”). Copyright and Related Rights include, but are not limited\nto, the following:\n\ni. the right to reproduce, adapt, distribute, perform, display, communicate,\nand translate a Work;\n\nii. moral rights retained by the original author(s) and/or performer(s);\n\niii. publicity and privacy rights pertaining to a person’s image or likeness\ndepicted in a Work;\n\niv. rights protecting against unfair competition in regards to a Work,\nsubject to the limitations in paragraph 4(a), below;\n\nv. rights protecting the extraction, dissemination, use and reuse of data in\na Work;\n\nvi. database rights (such as those arising under Directive 96/9/EC of the\nEuropean Parliament and of the Council of 11 March 1996 on the legal\nprotection of databases, and under any national implementation thereof,\nincluding any amended or successor version of such directive); and\n\nvii. other similar, equivalent or corresponding rights throughout the world\nbased on applicable law or treaty, and any national implementations thereof.\n\nWaiver. To the greatest extent permitted by, but not in contravention of,\napplicable law, Affirmer hereby overtly, fully, permanently, irrevocably and\nunconditionally waives, abandons, and surrenders all of Affirmer’s Copyright\nand Related Rights and associated claims and causes of action, whether now\nknown or unknown (including existing as well as future claims and causes of\naction), in the Work (i) in all territories worldwide, (ii) for the maximum\nduration provided by applicable law or treaty (including future time\nextensions), (iii) in any current or future medium and for any number of\ncopies, and (iv) for any purpose whatsoever, including without limitation\ncommercial, advertising or promotional purposes (the “Waiver”). Affirmer makes\nthe Waiver for the benefit of each member of the public at large and to the\ndetriment of Affirmer’s heirs and successors, fully intending that such Waiver\nshall not be subject to revocation, rescission, cancellation, termination, or\nany other legal or equitable action to disrupt the quiet enjoyment of the Work\nby the public as contemplated by Affirmer’s express Statement of Purpose.\n\nPublic License Fallback. Should any part of the Waiver for any reason be\njudged legally invalid or ineffective under applicable law, then the Waiver\nshall be preserved to the maximum extent permitted taking into account\nAffirmer’s express Statement of Purpose. In addition, to the extent the Waiver\nis so judged Affirmer hereby grants to each affected person a royalty-free,\nnon transferable, non sublicensable, non exclusive, irrevocable and\nunconditional license to exercise Affirmer’s Copyright and Related Rights in\nthe Work (i) in all territories worldwide, (ii) for the maximum duration\nprovided by applicable law or treaty (including future time extensions), (iii)\nin any current or future medium and for any number of copies, and (iv) for any\npurpose whatsoever, including without limitation commercial, advertising or\npromotional purposes (the “License”). The License shall be deemed effective as\nof the date CC0 was applied by Affirmer to the Work. Should any part of the\nLicense for any reason be judged legally invalid or ineffective under\napplicable law, such partial invalidity or ineffectiveness shall not\ninvalidate the remainder of the License, and in such case Affirmer hereby\naffirms that he or she will not (i) exercise any of his or her remaining\nCopyright and Related Rights in the Work or (ii) assert any associated claims\nand causes of action with respect to the Work, in either case contrary to\nAffirmer’s express Statement of Purpose.\n\nLimitations and Disclaimers.\n\na. No trademark or patent rights held by Affirmer are waived, abandoned,\nsurrendered, licensed or otherwise affected by this document.\n\nb. Affirmer offers the Work as-is and makes no representations or warranties\nof any kind concerning the Work, express, implied, statutory or otherwise,\nincluding without limitation warranties of title, merchantability, fitness\nfor a particular purpose, non infringement, or the absence of latent or\nother defects, accuracy, or the present or absence of errors, whether or not\ndiscoverable, all to the greatest extent permissible under applicable law.\n\nc. Affirmer disclaims responsibility for clearing rights of other persons\nthat may apply to the Work or any use thereof, including without limitation\nany person’s Copyright and Related Rights in the Work. Further, Affirmer\ndisclaims responsibility for obtaining any necessary consents, permissions\nor other rights required for any use of the Work.\n\nd. Affirmer understands and acknowledges that Creative Commons is not a\nparty to this document and has no duty or obligation with respect to this\nCC0 or use of the Work.\n\nFor more information, please see\n\n\nhttp://​creativecommons​.org​/publicdomain​/zero​/1​.0/","type":"content","url":"/license","position":1},{"hierarchy":{"lvl1":"CONUS404 Products Data Access"},"type":"lvl1","url":"/dataset-access/conus404-access","position":0},{"hierarchy":{"lvl1":"CONUS404 Products Data Access"},"content":"Before working with the CONUS404 data, you may want to consider reviewing \n\nNCAR’s Climate Primer for Water Availability Assessments to learn more about how to apply this dataset in studies focused on water availability.\n\nThis section of our JupyterBook contains notebooks that demonstrate how to access and perform basic data manipulation for the \n\nCONUS404 dataset. The examples can also be applied to the \n\nCONUS404 bias-adjusted dataset and the \n\nCONUS404 psuedo global warming (PGW) dataset.\n\nIn the CONUS404 intake sub-catalog (see \n\nhere for an explainer of our intake data catalog), you will see entries for:\n\nfour CONUS404 datasets: conus404-hourly, conus404-daily, conus404-monthly, and conus404-daily-diagnostic data\n\ntwo CONUS404 bias-adjusted datasets: conus404-hourly-ba, conus404-daily-ba\n\ntwo CONUS404 PGW datasets: conus404-pgw-hourly and conus404-pgw-daily-diagnostic\n\nEach of these datasets is duplicated in up to three different storage locations (as the \n\nintake catalog section also describes).\n\nWe recommend that you regularly check our \n\nCONUS404 changelog to see any updates that have been made to the zarr stores. We do not anticipate regular changes to the dataset, but we may need to fix an occasional bug or update the dataset with additional years of data.","type":"content","url":"/dataset-access/conus404-access","position":1},{"hierarchy":{"lvl1":"CONUS404 Products Data Access","lvl2":"CONUS404 Data"},"type":"lvl2","url":"/dataset-access/conus404-access#conus404-data","position":2},{"hierarchy":{"lvl1":"CONUS404 Products Data Access","lvl2":"CONUS404 Data"},"content":"CONUS404 is a unique, high-resolution hydro-climate dataset appropriate for forcing hydrological models and conducting meteorological analysis over the contiguous United States. Users should review the official \n\nCONUS404 data release to understand the dataset before working with the zarr stores provided in our intake catalog.\n\nThe conus404-hourly data is a subset of the wrfout model output and conus404-daily-diagnostic is a subset from the wrfxtrm model output, both of which are described in the official data release. We also provide conus404-daily and conus404-monthly files, which are just resampled from the conus404-hourly data.\n\nPlease note that the values in the ACLWDNB, ACLWUPB, ACSWDNB, ACSWDNT, and ACSWUPB variables available in the zarr store differ from the original model output. These variables have been re-calculated to reflect the accumulated value since the model start, as directed in the WRF manual. An attribute has been added to each of these variables in the zarr store to denote the accumulation period for the variable.","type":"content","url":"/dataset-access/conus404-access#conus404-data","position":3},{"hierarchy":{"lvl1":"CONUS404 Products Data Access","lvl2":"CONUS404 Bias-Adjusted Data"},"type":"lvl2","url":"/dataset-access/conus404-access#conus404-bias-adjusted-data","position":4},{"hierarchy":{"lvl1":"CONUS404 Products Data Access","lvl2":"CONUS404 Bias-Adjusted Data"},"content":"The conus404-hourly-ba data contains bias-adjusted temperature and precipitation data from the CONUS404 dataset, which is described in the official \n\nCONUS404 bias adjusted data release. Users should review the official data release to understand the dataset before working with the zarr stores provided in our intake catalog.\n\nThe conus404-daily-ba files are resampled from the conus404-hourly-ba data.\n\nPlease note that missing values have been identified in the U2D and V2D variables. This issue was raised in our \n\ndiscussion boards, and we hope to find a resolution to this issue.","type":"content","url":"/dataset-access/conus404-access#conus404-bias-adjusted-data","position":5},{"hierarchy":{"lvl1":"CONUS404 Products Data Access","lvl2":"CONUS404 PGW Data"},"type":"lvl2","url":"/dataset-access/conus404-access#conus404-pgw-data","position":6},{"hierarchy":{"lvl1":"CONUS404 Products Data Access","lvl2":"CONUS404 PGW Data"},"content":"The CONUS404 pseudo-global warming (PGW) dataset is a future-perturbed hydro-climate dataset, created as a follow on to the CONUS404 dataset. The CONUS404 PGW dataset represents the weather from 1980 to 2021 under a warmer and wetter climate environment and provides an opportunity to explore the event-based climate change impacts when used with the CONUS404 historical data. Users should review the official \n\nCONUS404 PGW data release to understand the dataset before working with the zarr stores provided in our intake catalog.\n\nThe conus404-pgw-hourly data is a subset of the wrfout model output and conus404-pgw-daily-diagnostic is a subset from the wrfxtrm model output, both of which are described in the official data release.\n\nPlease note that the values in the ACLWDNB, ACLWUPB, ACSWDNB, ACSWDNT, and ACSWUPB variables available in the zarr store differ from the original model output. These variables have been re-calculated to reflect the accumulated value since the model start, as directed in the WRF manual. An attribute has been added to each of these variables in the zarr store to denote the accumulation period for the variable.","type":"content","url":"/dataset-access/conus404-access#conus404-pgw-data","position":7},{"hierarchy":{"lvl1":"CONUS404 Products Data Access","lvl2":"Example Notebooks"},"type":"lvl2","url":"/dataset-access/conus404-access#example-notebooks","position":8},{"hierarchy":{"lvl1":"CONUS404 Products Data Access","lvl2":"Example Notebooks"},"content":"We currently have several notebooks to help demonstrate how to work with these datasets in a python workflow:\n\nExplore CONUS404 Dataset: opens the CONUS404 dataset, loads and plots the entire spatial\ndomain of a specified variable at a specfic time step, and loads and plots a time series of a variable at a specified coordinate pair.\n\nCONUS404 Temporal Aggregation: calculates a daily average of the CONUS404 hourly data.\n\nCONUS404 Spatial Aggregation: calculates the area-weighted mean of the CONUS404 data for a particular basin (several methods are demoed in this section)\n\nCONUS404 Point Selection: samples the CONUS404 data at a selection of gage locations using their lat/lon point coordinates.\n\nCONUS404 Regridding (Curvilinear => Rectilinear): regrids a subset of the CONUS404 dataset from a curvilinear grid to a rectilinear grid and saves the output to a netcdf file. The package used in this demo is not compatible with Windows. We hope to improve upon this methodology, and will likely update the package/technique used in the future.\n\nThese methods are likely applicable to many of the other key HyTEST datasets that can be opened with xarray.\n\nNote: If you need help setting up a computing environment where you can run these notebooks, you should review the \n\nComputing Environments section of the documentation.","type":"content","url":"/dataset-access/conus404-access#example-notebooks","position":9},{"hierarchy":{"lvl1":"CONUS404 Zarr Changelog"},"type":"lvl1","url":"/dataset-access/conus404-changelog","position":0},{"hierarchy":{"lvl1":"CONUS404 Zarr Changelog"},"content":"This changelog documents major changes to the \n\nCONUS404 zarr datasets. We do not anticipate regular changes to the dataset, but we may need to fix an occasional bug or update the dataset with additional years of data. Therefore, we recommend that users of the CONUS404 zarr data check this changelog regularly.","type":"content","url":"/dataset-access/conus404-changelog","position":1},{"hierarchy":{"lvl1":"CONUS404 Zarr Changelog","lvl2":"2025-04"},"type":"lvl2","url":"/dataset-access/conus404-changelog#id-2025-04","position":2},{"hierarchy":{"lvl1":"CONUS404 Zarr Changelog","lvl2":"2025-04"},"content":"We have move the CONUS404 zarr dataset in S3 storage to Glacier storage, so it is no longer accessible to be read into workflows. We recommend you use the copy of the zarr on the OSN pod, which can be accessed from any computing location.","type":"content","url":"/dataset-access/conus404-changelog#id-2025-04","position":3},{"hierarchy":{"lvl1":"CONUS404 Zarr Changelog","lvl2":"2024-02"},"type":"lvl2","url":"/dataset-access/conus404-changelog#id-2024-02","position":4},{"hierarchy":{"lvl1":"CONUS404 Zarr Changelog","lvl2":"2024-02"},"content":"Water year 2022 data (October 1, 2021 - September 30, 2022) was added to all zarr stores (conus404-hourly-*, conus404-daily-*, conus404-monthly-*).\n\nCoordinate x and y values were updated to fix an issue with how they were generated that resulted in small location errors (lat and lon were not changed).","type":"content","url":"/dataset-access/conus404-changelog#id-2024-02","position":5},{"hierarchy":{"lvl1":"CONUS404 Zarr Changelog","lvl2":"2023-11"},"type":"lvl2","url":"/dataset-access/conus404-changelog#id-2023-11","position":6},{"hierarchy":{"lvl1":"CONUS404 Zarr Changelog","lvl2":"2023-11"},"content":"Removed derived variables (E2, ES2, RH2, SH2) that were not part of original CONUS404 model output from CONUS404 zarr stores conus404-hourly-*, conus404-daily-*, conus404-monthly-*.","type":"content","url":"/dataset-access/conus404-changelog#id-2023-11","position":7},{"hierarchy":{"lvl1":"Explore CONUS404 Dataset"},"type":"lvl1","url":"/dataset-access/conus404-explore","position":0},{"hierarchy":{"lvl1":"Explore CONUS404 Dataset"},"content":"This dataset was created by extracting specified variables from a collection of wrf2d output files, rechunking to better facilitate data extraction for a variety of use cases, and adding \n\nCF conventions to allow easier analysis, visualization and data extraction using Xarray and Holoviz.\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\n\nimport fsspec\nimport xarray as xr\nimport hvplot.xarray\nimport zarr\nimport pystac\nfrom packaging.version import Version\nimport metpy\nimport cartopy.crs as ccrs\n\n\n\n","type":"content","url":"/dataset-access/conus404-explore","position":1},{"hierarchy":{"lvl1":"Explore CONUS404 Dataset","lvl2":"1) Select the Dataset from the WMA STAC Catalog"},"type":"lvl2","url":"/dataset-access/conus404-explore#id-1-select-the-dataset-from-the-wma-stac-catalog","position":2},{"hierarchy":{"lvl1":"Explore CONUS404 Dataset","lvl2":"1) Select the Dataset from the WMA STAC Catalog"},"content":"\n\ndef get_children(catalog, collection_id=None):\n    \"\"\"\n    This function retrieves a specified collection from a STAC catalog/collection and prints key metadata \n    for exploring/accessing the datasets contained within it.\n    If there is no collection ID provided, the collections in the top level of the catalog will be printed.\n    If a collection ID is provided, it will retrieve the collection with that ID from the input catalog/collection.\n    If the collection ID points to a dataset, it will print the assets available for the dataset.\n    If the collection ID points to another collection, it will list the child collections in the IDed collection.\n\n    Args:\n        catalog (pystac.Catalog | pystac.Collection): The STAC catalog/collection object.\n        collection_id (str): The ID of the collection or dataset to retrieve from catalog.\n    \n    Returns:\n        collection (pystac.Catalog | pystac.Collection): The collection object corresponding to the provided ID\n                                                         or the top-level catalog if no ID is provided.\n    \"\"\"\n    dataset = False\n    if collection_id:\n        collection = catalog.get_child(collection_id)\n        if collection.assets:\n            dataset = True\n            print(f\"{collection_id} is a dataset. Please review the assets below and select one to open.\")\n\n        else:\n            print(f\"{collection_id} is a collection. Please review the child items and select one to open in the next cell.\")\n    else:\n        collection = catalog\n    if dataset==True:\n        # List the assets\n        for asset in collection.assets:\n            print(f\"Asset ID: {asset}\")\n            print(f\"    Title: {collection.assets[asset].title}\")\n            print(f\"    Description: {collection.assets[asset].description}\")\n    else:\n        collections = list(collection.get_collections())\n        print(f\"Number of collections: {len(collections)}\")\n        print(\"Collections IDs:\")\n        for child_collection in collections:\n            id = child_collection.id\n            cite_as = \"Not available\"\n            for link in child_collection.links:\n                if link.rel == \"cite-as\":\n                    cite_as = link.target\n            print(f\"- {id}, Source: {cite_as}\")\n    return collection\n\n\n\n# url for the WMA STAC Catalog\ncatalog_url = \"https://api.water.usgs.gov/gdp/pygeoapi/stac/stac-collection/\"\n\n# use pystac to read the catalog\ncatalog = pystac.Catalog.from_file(catalog_url)\n\n# list the collections in the catalog\ncatalog = get_children(catalog)\n\n\n\n# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(catalog, collection_id=\"conus404\")\n\n\n\n# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(collection, collection_id=\"conus404_hourly\")\n\n\n\n# replace with the asset ID you want to use:\nselected_asset_id = \"zarr-s3-osn\"\n\n# read the asset metadata\nasset = collection.assets[selected_asset_id]\n\n\n\n","type":"content","url":"/dataset-access/conus404-explore#id-1-select-the-dataset-from-the-wma-stac-catalog","position":3},{"hierarchy":{"lvl1":"Explore CONUS404 Dataset","lvl2":"2) Set Up AWS Credentials (Optional)"},"type":"lvl2","url":"/dataset-access/conus404-explore#id-2-set-up-aws-credentials-optional","position":4},{"hierarchy":{"lvl1":"Explore CONUS404 Dataset","lvl2":"2) Set Up AWS Credentials (Optional)"},"content":"This notebook reads data from the OSN pod. The OSN pod is object store data on a high speed internet connection with free access from any computing environment. If you change this notebook to use one of the CONUS404 datasets stored on S3 (options ending in -cloud), you will be pulling data from a requester-pays S3 bucket. This means you have to set up your AWS credentials before you are able to load the data. Please note that reading the -cloud data from S3 may incur charges if you are reading data outside of AWS’s us-west-2 region or running the notebook outside of the cloud altogether. If you would like to access one of the -cloud options, uncomment and run the following code snippet to set up your AWS credentials. You can find more info about this AWS helper function \n\nhere.\n\n# uncomment the lines below to read in your AWS credentials if you want to access data from a requester-pays bucket (-cloud)\n# os.environ['AWS_PROFILE'] = 'default'\n# %run ../environment_set_up/Help_AWS_Credentials.ipynb\n\n\n\n","type":"content","url":"/dataset-access/conus404-explore#id-2-set-up-aws-credentials-optional","position":5},{"hierarchy":{"lvl1":"Explore CONUS404 Dataset","lvl2":"3) Parallelize with Dask (Optional, but recommended)"},"type":"lvl2","url":"/dataset-access/conus404-explore#id-3-parallelize-with-dask-optional-but-recommended","position":6},{"hierarchy":{"lvl1":"Explore CONUS404 Dataset","lvl2":"3) Parallelize with Dask (Optional, but recommended)"},"content":"Some of the steps we will take are aware of parallel clustered compute environments\nusing dask. We will start a cluster so that future steps can take advantage\nof this ability.\n\nThis is an optional step, but speed ups data loading significantly, especially\nwhen accessing data from the cloud.\n\nWe have documentation on how to start a Dask Cluster in different computing environments \n\nhere.\n\n#%run ../environment_set_up/Start_Dask_Cluster_Nebari.ipynb\n## If this notebook is not being run on Nebari, replace the above \n## path name with a helper appropriate to your compute environment.  Examples:\n# %run ../environment_set_up/Start_Dask_Cluster_Denali.ipynb\n# %run ../environment_set_up/Start_Dask_Cluster_Tallgrass.ipynb\n# %run ../environment_set_up/Start_Dask_Cluster_Desktop.ipynb\n\n\n\n","type":"content","url":"/dataset-access/conus404-explore#id-3-parallelize-with-dask-optional-but-recommended","position":7},{"hierarchy":{"lvl1":"Explore CONUS404 Dataset","lvl2":"4) Explore the dataset"},"type":"lvl2","url":"/dataset-access/conus404-explore#id-4-explore-the-dataset","position":8},{"hierarchy":{"lvl1":"Explore CONUS404 Dataset","lvl2":"4) Explore the dataset"},"content":"\n\n# read in the dataset and use metpy to parse the crs information on the dataset\nif Version(zarr.__version__) < Version(\"3.0.0\"):\n    ds = xr.open_dataset(\n        asset.href,\n        storage_options=asset.extra_fields['xarray:storage_options'],\n        **asset.extra_fields['xarray:open_kwargs']\n    )\nelse:\n    ds = xr.open_dataset(\n    asset.href,\n    storage_options=asset.extra_fields['xarray:storage_options'],\n    **asset.extra_fields['xarray:open_kwargs'],\n    zarr_format=2\n    )\nds = ds.metpy.parse_cf()\nds\n\n\n\n# Examine the grid data structure for SNOW: \nds.SNOW\n\n\n\nLooks like this dataset is organized in three coordinates (x, y, and time), and we have used the metpy package to pase the crs information into the metpy_crs variable:\n\ncrs = ds['SNOW'].metpy.cartopy_crs\ncrs\n\n\n\n","type":"content","url":"/dataset-access/conus404-explore#id-4-explore-the-dataset","position":9},{"hierarchy":{"lvl1":"Explore CONUS404 Dataset","lvl2":"Example A: Load the entire spatial domain for a variable at a specific time step"},"type":"lvl2","url":"/dataset-access/conus404-explore#example-a-load-the-entire-spatial-domain-for-a-variable-at-a-specific-time-step","position":10},{"hierarchy":{"lvl1":"Explore CONUS404 Dataset","lvl2":"Example A: Load the entire spatial domain for a variable at a specific time step"},"content":"\n\n%%time\nda = ds.SNOW_ACC_NC.sel(time='2009-12-24 00:00').load()\n### NOTE: the `load()` is dask-aware, so will operate in parallel if\n### a cluster has been started. \n\n\n\nda.hvplot.quadmesh(x='lon', y='lat', rasterize=True, geo=True, tiles='OSM', cmap='viridis').opts('Image', alpha=0.5)\n\n\n\n","type":"content","url":"/dataset-access/conus404-explore#example-a-load-the-entire-spatial-domain-for-a-variable-at-a-specific-time-step","position":11},{"hierarchy":{"lvl1":"Explore CONUS404 Dataset","lvl2":"Example B: Load a time series for a variable at a specific grid cell for a specified time range"},"type":"lvl2","url":"/dataset-access/conus404-explore#example-b-load-a-time-series-for-a-variable-at-a-specific-grid-cell-for-a-specified-time-range","position":12},{"hierarchy":{"lvl1":"Explore CONUS404 Dataset","lvl2":"Example B: Load a time series for a variable at a specific grid cell for a specified time range"},"content":"\n\nWe will identify a point that we want to pull data for using lat/lon coordinates.\n\nThe CONUS404 data is in a Lambert Conformal Conic projection, so we need to re-project/transform using the\nbuilt-in crs we examined earlier.\n\nlat,lon = 39.978322,-105.2772194    \nx, y = crs.transform_point(lon, lat, src_crs=ccrs.PlateCarree())   \nprint(x,y) # these vals are in LCC\n\n\n\n%%time\n# pull out a particulat time slice at the specified coordinates\nda = ds.PREC_ACC_NC.sel(x=x, y=y, method='nearest').sel(time=slice('2013-01-01 00:00','2013-12-31 00:00')).load()\n\n\n\n# plot your time series\nda.hvplot(x='time', grid=True)\n\n\n\n","type":"content","url":"/dataset-access/conus404-explore#example-b-load-a-time-series-for-a-variable-at-a-specific-grid-cell-for-a-specified-time-range","position":13},{"hierarchy":{"lvl1":"Explore CONUS404 Dataset","lvl2":"Stop cluster"},"type":"lvl2","url":"/dataset-access/conus404-explore#stop-cluster","position":14},{"hierarchy":{"lvl1":"Explore CONUS404 Dataset","lvl2":"Stop cluster"},"content":"Uncomment the line below if you started a dask cluster to shut it down.\n\n#client.close(); cluster.shutdown()\n\n","type":"content","url":"/dataset-access/conus404-explore#stop-cluster","position":15},{"hierarchy":{"lvl1":"Data Catalogs"},"type":"lvl1","url":"/dataset-catalog/readme","position":0},{"hierarchy":{"lvl1":"Data Catalogs"},"content":"The HyTEST community currently makes use of several public data catalogs to read data into our modeling workflows. Two of these public data catalogs are maintained paritally or fully by the HyTEST community and contain a majority of the datasets you see used in our example workflows:\n\nUSGS Water Mission Area (WMA) STAC catalog\n\nHyTEST intake catalog\n\nWe are also likely to reference other public data catalogs which point to analysis-ready data assets of relevance to our community. Some of the key data catalogs that we are not involved in the maintenance of, but reference regularly when looking for data assets include the following:\n\nRegistry of Open Data on AWS\n\nMicrosoft Planetary Computer Data Catalog\n\nClimateR Catalog","type":"content","url":"/dataset-catalog/readme","position":1},{"hierarchy":{"lvl1":"Data Catalogs","lvl2":"HyTEST Intake vs WMA STAC Catalogs"},"type":"lvl2","url":"/dataset-catalog/readme#hytest-intake-vs-wma-stac-catalogs","position":2},{"hierarchy":{"lvl1":"Data Catalogs","lvl2":"HyTEST Intake vs WMA STAC Catalogs"},"content":"The HyTEST community started by catalogging the key data assets we needed for our workflows in an \n\nintake catalog. We are now moving towards combining our data catalog with the USGS WMA \n\nSTAC Catalog. At the present moment, the WMA STAC Catalog contains gridded datasets related to:\n\ncommon hydrologic model inputs such as climate or other forcing datasets\n\nhydrologic model outputs\n\nobservational datasets related to hydrology and/or water budgets\n\nThe gridded datasets used by the HyTEST community are also still contained in our intake catalog, but we will move towards using the STAC Catalog exclusively for these assets in time.\n\nThe tabular and vector datasets used by the HyTEST community are currently cataloged only in our intake catalog; however, this may change with time, and we encourage our users to visit this page to get the latest information about recommended data catalogs and their maintenance status.\n\nCurrently you are likely to need to use a combination of these data catalogs to open all the data you need for our tutorials. We recommend prioritizing the use of the WMA STAC Catalog when possible, and supplementing with the HyTEST intake catalog for data assets that are not yet incorporated into the STAC Catalog. To learn more about each of theses catalogs, please review our tutorials of each:\n\nWMA STAC Catalog\n\nHyTEST intake catalog\n\nIn order to effectively understand which data assets are best for your usage, you may also need to read more about the different \n\nstorage locations that we use to store our data assets.","type":"content","url":"/dataset-catalog/readme#hytest-intake-vs-wma-stac-catalogs","position":3},{"hierarchy":{"lvl1":"Water Mission Area STAC Catalog"},"type":"lvl1","url":"/dataset-catalog/stac","position":0},{"hierarchy":{"lvl1":"Water Mission Area STAC Catalog"},"content":"The \n\nUSGS Water Resources Mission Area (WMA) has created a \n\nSTAC Catalog to help users find and access data related to our water modeling projects. STAC Catalogs provide a mechanism to expose spatiotemporal datasets using a machine-readable format, allowing users to discover and access datasets in a standardized way. The WMA Catalog hosts a variety of datasets related to hydro-terrestrial modeling. The scope of the WMA STAC Catalog datasets may include:\n\ncommon hydrologic model inputs such as climate or other forcing datasets\n\nhydrologic model outputs\n\nobservational datasets related to hydrology and/or water budgets\n\nWe have exposed our STAC Catalog through a \n\npygeoapi endpoint that is compliant with the \n\nOGC API suite of standards. This endpoint allows both API access, as well as a user interface for browsing our data assets.\n\nThis STAC Catalog is part of a \n\nmodernized replacement for the legacy Geo Data Portal.\n\n","type":"content","url":"/dataset-catalog/stac","position":1},{"hierarchy":{"lvl1":"Water Mission Area STAC Catalog","lvl2":"Finding the data you need and reading it into your workflow"},"type":"lvl2","url":"/dataset-catalog/stac#finding-the-data-you-need-and-reading-it-into-your-workflow","position":2},{"hierarchy":{"lvl1":"Water Mission Area STAC Catalog","lvl2":"Finding the data you need and reading it into your workflow"},"content":"You have two ways to discover the data assets in the WMA STAC Catalog:\n\nUsing the \n\npygeoapi user interface to browse our datasets through a user interface.\n\nReading the pygeoapi API endpoint into a workflow using a library designed for reading STAC catalogs, such as \n\nPySTAC.\n\nWe will describe/demonstrate both methods below.\n\n# first import the packages we will need\nimport pystac\nimport xarray as xr\nimport zarr\nfrom packaging.version import Version\n\n\n\n","type":"content","url":"/dataset-catalog/stac#finding-the-data-you-need-and-reading-it-into-your-workflow","position":3},{"hierarchy":{"lvl1":"Water Mission Area STAC Catalog","lvl3":"Option 1: Find and access a dataset using the PyGEOAPI or Radiant Earth user interface","lvl2":"Finding the data you need and reading it into your workflow"},"type":"lvl3","url":"/dataset-catalog/stac#option-1-find-and-access-a-dataset-using-the-pygeoapi-or-radiant-earth-user-interface","position":4},{"hierarchy":{"lvl1":"Water Mission Area STAC Catalog","lvl3":"Option 1: Find and access a dataset using the PyGEOAPI or Radiant Earth user interface","lvl2":"Finding the data you need and reading it into your workflow"},"content":"Step 1: Explore the catalog through either our \n\npygeoapi user interface or the \n\nRadiant Earth STAC Browser and find the dataset you want to use.\n\nThis catalog contains both datasets and collections of datasets in the top level of the catalog. Therefore, some assets that you click may open up directly to a dataset explore page (with a map viewer and metadata about the dataset), while others will open up to another catalog listing. As you drill down through the collections, you will eventually land on a dataset explore page.\n\nYou can learn more about each dataset at its source publication point, which is linked in the cite-as property displayed on each dataset page (if available).\n\nClick through the catalog until you find a dataset that you want to use. For this example, we will use the gridMET dataset.\n\nStep 2: Identify which asset you want to use to access the dataset you’ve chosen.\n\nReview the Title and Description of each asset listed in the Assets section. Each asset is a different copy of the dataset that may be stored in a different format or location.\n\nTypically, we recommend using the asset with the title “Free access to zarr via S3 API”.\n\nCopy the URL, Open Keywords, and Storage Options for the asset you have chosen, and store them as python variables, demonstrated below:\n\n# URL\nzarr_url = 's3://hytest/conus404/conus404_hourly.zarr/'\n\n# Open Keywords\n# note that you will need to capitalize the True or False in \"consolidated\" - these keys are stored as lowercase boolens in the STAC catalog json\n# but should be capitalized for python\nopen_keywords = {\n  \"chunks\": {},\n  \"consolidated\": True,\n  \"engine\": \"zarr\"\n}\n\n# Storage Options\n# note that you will need to capitalize the True or False in \"anon\" - these keys are stored as lowercase boolens in the STAC catalog json\n# but should be capitalized for python\nstorage_options = {\n  \"anon\": True,\n  \"client_kwargs\": {\n    \"endpoint_url\": \"https://usgs.osn.mghpcc.org/\"\n  }\n}\n\n\n\n","type":"content","url":"/dataset-catalog/stac#option-1-find-and-access-a-dataset-using-the-pygeoapi-or-radiant-earth-user-interface","position":5},{"hierarchy":{"lvl1":"Water Mission Area STAC Catalog","lvl4":"Open the dataset using xarray","lvl3":"Option 1: Find and access a dataset using the PyGEOAPI or Radiant Earth user interface","lvl2":"Finding the data you need and reading it into your workflow"},"type":"lvl4","url":"/dataset-catalog/stac#open-the-dataset-using-xarray","position":6},{"hierarchy":{"lvl1":"Water Mission Area STAC Catalog","lvl4":"Open the dataset using xarray","lvl3":"Option 1: Find and access a dataset using the PyGEOAPI or Radiant Earth user interface","lvl2":"Finding the data you need and reading it into your workflow"},"content":"Now we will open and view the dataset using xarray.\n\nPlease note that there are two major zarr format specifications: 2 and 3. If you are using the python package zarr>=3.0.0, you must specify the format of the zarr store you are trying to open in the xarray.open_dataset function. If you are using zarr<3.0.0, you do not need to specify the format, as it will default to version 2.\n\nif Version(zarr.__version__) < Version(\"3.0.0\"):\n    ds = xr.open_dataset(\n        zarr_url,\n        storage_options=storage_options,\n        **open_keywords\n    )\nelse:\n    ds = xr.open_dataset(\n    zarr_url,\n    storage_options=storage_options,\n    **open_keywords,\n    zarr_format=2\n    )\nds\n\n\n\n","type":"content","url":"/dataset-catalog/stac#open-the-dataset-using-xarray","position":7},{"hierarchy":{"lvl1":"Water Mission Area STAC Catalog","lvl3":"Option 2: Find and access a dataset using the PySTAC python library","lvl2":"Finding the data you need and reading it into your workflow"},"type":"lvl3","url":"/dataset-catalog/stac#option-2-find-and-access-a-dataset-using-the-pystac-python-library","position":8},{"hierarchy":{"lvl1":"Water Mission Area STAC Catalog","lvl3":"Option 2: Find and access a dataset using the PySTAC python library","lvl2":"Finding the data you need and reading it into your workflow"},"content":"Before we begin, we will define a helper function that can be used to drill down through the STAC Catalog and extract key metadata.\n\ndef get_children(catalog, collection_id=None):\n    \"\"\"\n    This function retrieves a specified collection from a STAC catalog/collection and prints key metadata \n    for exploring/accessing the datasets contained within it.\n    If there is no collection ID provided, the collections in the top level of the catalog will be printed.\n    If a collection ID is provided, it will retrieve the collection with that ID from the input catalog/collection.\n    If the collection ID points to a dataset, it will print the assets available for the dataset.\n    If the collection ID points to another collection, it will list the child collections in the IDed collection.\n\n    Args:\n        catalog (pystac.Catalog | pystac.Collection): The STAC catalog/collection object.\n        collection_id (str): The ID of the collection or dataset to retrieve from catalog.\n    \n    Returns:\n        collection (pystac.Catalog | pystac.Collection): The collection object corresponding to the provided ID\n                                                         or the top-level catalog if no ID is provided.\n    \"\"\"\n    dataset = False\n    if collection_id:\n        collection = catalog.get_child(collection_id)\n        if collection.assets:\n            dataset = True\n            print(f\"{collection_id} is a dataset. Please review the assets below and select one to open.\")\n\n        else:\n            print(f\"{collection_id} is a collection. Please review the child items and select one to open in the next cell.\")\n    else:\n        collection = catalog\n    if dataset==True:\n        # List the assets\n        for asset in collection.assets:\n            print(f\"Asset ID: {asset}\")\n            print(f\"    Title: {collection.assets[asset].title}\")\n            print(f\"    Description: {collection.assets[asset].description}\")\n    else:\n        collections = list(collection.get_collections())\n        print(f\"Number of collections: {len(collections)}\")\n        print(\"Collections IDs:\")\n        for child_collection in collections:\n            id = child_collection.id\n            cite_as = \"Not available\"\n            for link in child_collection.links:\n                if link.rel == \"cite-as\":\n                    cite_as = link.target\n            print(f\"- {id}, Source: {cite_as}\")\n    return collection\n\n\n\nStep 1: Explore the catalog using the \n\nPySTAC library and find the dataset you want to use.\n\nThe following code will read the WMA STAC Catalog and print the items in the top level of the catalog.\n\nThis catalog contains both datasets and collections of datasets in the top level of the catalog. Therefore, some collections may point to a dataset endpoint, while others will point to another collection of several datasets. As you drill down through the collections, you will eventually land on a dataset.\n\nYou can learn more about each dataset at its source publication point, which the helper function defined above prints out (if available).\n\nIdentify a dataset that you want to use. For this example, we will use the gridMET dataset.\n\nNote: We are working on enabling stac-search to our pygeoapi endpoint, which will allow you to search for datasets using keywords and other metadata which will help facilitate this exploration process.\n\n# url for the WMA STAC Catalog\ncatalog_url = \"https://api.water.usgs.gov/gdp/pygeoapi/stac/stac-collection/\"\n\n# use pystac to read the catalog\ncatalog = pystac.Catalog.from_file(catalog_url)\n\n# list the collections in the catalog\ncatalog = get_children(catalog)\n\n\n\n# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(catalog, collection_id=\"conus404\")\n\n\n\n# if the above was a collection, uncomment the line below and enter the collection ID \n# you want to use from the parent collection you selected:\ncollection = get_children(collection, collection_id=\"conus404_hourly\")\n\n\n\nStep 2: Identify which asset you want to use to access the dataset you’ve chosen.\n\nReview the Title and Description of each asset printed above. Each asset is a different copy of the dataset that may be stored in a different format or location.\n\nTypically, we recommend people use the zarr-s3-osn asset unless they have a reason to use a different asset.\n\nCopy the Asset ID for the asset you have chosen, and paste in the code below to read the asset metadata.\n\n# replace with the asset ID you want to use:\nselected_asset_id = \"zarr-s3-osn\"\n\n# read the asset metadata\nasset = collection.assets[selected_asset_id]\n\n\n\n","type":"content","url":"/dataset-catalog/stac#option-2-find-and-access-a-dataset-using-the-pystac-python-library","position":9},{"hierarchy":{"lvl1":"Water Mission Area STAC Catalog","lvl4":"Open the dataset using xarray","lvl3":"Option 2: Find and access a dataset using the PySTAC python library","lvl2":"Finding the data you need and reading it into your workflow"},"type":"lvl4","url":"/dataset-catalog/stac#open-the-dataset-using-xarray-1","position":10},{"hierarchy":{"lvl1":"Water Mission Area STAC Catalog","lvl4":"Open the dataset using xarray","lvl3":"Option 2: Find and access a dataset using the PySTAC python library","lvl2":"Finding the data you need and reading it into your workflow"},"content":"Now we will open and view the dataset using xarray.\n\nPlease note that there are two major zarr format specifications: 2 and 3. If you are using the python package zarr>=3.0.0, you must specify the format of the zarr store you are trying to open in the xarray.open_dataset function. If you are using zarr<3.0.0, you do not need to specify the format, as it will default to version 2.\n\nif Version(zarr.__version__) < Version(\"3.0.0\"):\n    ds = xr.open_dataset(\n        asset.href,\n        storage_options=asset.extra_fields['xarray:storage_options'],\n        **asset.extra_fields['xarray:open_kwargs']\n    )\nelse:\n    ds = xr.open_dataset(\n    asset.href,\n    storage_options=asset.extra_fields['xarray:storage_options'],\n    **asset.extra_fields['xarray:open_kwargs'],\n    zarr_format=2\n    )\nds\n\n\n\nYou can use whichever of the two methods described above to find and access datasets from our STAC Catalog. Now that you have the dataset open in xarray, you can build on the workflow to visualize, analyze, or do any other data processing you might need to do. You don’t need to download your own copy of the dataset - you can perform your analysis directly on the dataset from the source we provide. If your workflow exceeds your computer’s memory, you can use the dask library to parallelize your analysis and take advantage of HPC or scalable cloud computing resources.\n\n","type":"content","url":"/dataset-catalog/stac#open-the-dataset-using-xarray-1","position":11},{"hierarchy":{"lvl1":"Water Mission Area STAC Catalog","lvl2":"Reporting Issues with the Catalog"},"type":"lvl2","url":"/dataset-catalog/stac#reporting-issues-with-the-catalog","position":12},{"hierarchy":{"lvl1":"Water Mission Area STAC Catalog","lvl2":"Reporting Issues with the Catalog"},"content":"If you find any issues with our STAC catalog or the datasets it contains, please reach out to us in one of the following ways:\n\nIf you have a code.usgs.gov account, you can \n\nopen an issue\n\nE-mail us at mdmf@usgs.gov\n\nWe will do our best to address any issues you find in a timely manner.\n\n","type":"content","url":"/dataset-catalog/stac#reporting-issues-with-the-catalog","position":13},{"hierarchy":{"lvl1":"Water Mission Area STAC Catalog","lvl2":"Contributing to the Catalog"},"type":"lvl2","url":"/dataset-catalog/stac#contributing-to-the-catalog","position":14},{"hierarchy":{"lvl1":"Water Mission Area STAC Catalog","lvl2":"Contributing to the Catalog"},"content":"We are very interested in expanding the datasets available in our STAC Catalog to include more datasets that are relevant to water modeling. If you have a dataset that you think should be included, we would love to hear from you! We are particularly interested in datasets that are very large or difficult to access. Please \n\nopen an issue if you think you have a dataset that should be included in the catalog and tell us a bit about the dataset, who is using it, and why you would like to add it to our STAC Catalog.","type":"content","url":"/dataset-catalog/stac#contributing-to-the-catalog","position":15},{"hierarchy":{"lvl1":"HyTEST Data Catalog (Intake)"},"type":"lvl1","url":"/dataset-catalog/intake","position":0},{"hierarchy":{"lvl1":"HyTEST Data Catalog (Intake)"},"content":"This section describes how to use HyTEST’s \n\nintake catalog. Intake catalogs help reduce or remove the burden of handling different file formats and storage locations, making it easier to read data into your workflow. They also allow data providers to update the filepath/storage location of a dataset without breaking the workflows that were built on top of the intake catalog.\n\nOur catalog facilitates this access for HyTEST’s key data offerings and is used to read the data into the notebooks contained in this repository. While intake catalogs are Python-centric, they are stored as a yaml file, which should also be easy to parse using other programming languages, even if there is no equivalent package in that programming language.\n\nPlease note that we are trying to move towards using the \n\nUSGS Water Mission Area STAC Catalog to catalog our data holdings. This transition may take time, and we may end up using a combination of the STAC Catalog and our HyTEST intake catalog, but we encourage users to prioritize reading data from STAC when possible, as we plan to move in that direction.","type":"content","url":"/dataset-catalog/intake","position":1},{"hierarchy":{"lvl1":"HyTEST Data Catalog (Intake)","lvl2":"Example Intake Catalog Usage"},"type":"lvl2","url":"/dataset-catalog/intake#example-intake-catalog-usage","position":2},{"hierarchy":{"lvl1":"HyTEST Data Catalog (Intake)","lvl2":"Example Intake Catalog Usage"},"content":"Now that you have an understanding of the \n\ndifferent storage systems HyTEST uses, you will be able to navigate the HyTEST intake catalog and make a selection that is appropriate for your computing environment. Below is a demonstration of how to use HyTEST’s intake catalog to select and open a dataset in your python workflow.import intake\nurl = 'https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml'\ncat = intake.open_catalog(url)\nlist(cat)\n\nproduces a list of datasets, for example:['conus404-catalog',\n 'conus404-drb-eval-tutorial-catalog',\n 'nhm-v1.0-daymet-catalog',\n 'nhm-v1.1-c404-bc-catalog',\n 'nhm-v1.1-gridmet-catalog',\n 'trends-and-drivers-catalog',\n 'nhm-prms-v1.1-gridmet-format-testing-catalog',\n 'nwis-streamflow-usgs-gages-onprem',\n 'nwis-streamflow-usgs-gages-osn',\n 'nwm21-streamflow-usgs-gages-onprem',\n 'nwm21-streamflow-usgs-gages-osn',\n 'nwm21-streamflow-cloud',\n 'geofabric_v1_1-zip-osn',\n 'geofabric_v1_1_POIs_v1_1-osn',\n 'geofabric_v1_1_TBtoGFv1_POIs-osn',\n 'geofabric_v1_1_nhru_v1_1-osn',\n 'geofabric_v1_1_nhru_v1_1_simp-osn',\n 'geofabric_v1_1_nsegment_v1_1-osn',\n 'gages2_nndar-osn',\n 'wbd-zip-osn',\n 'huc12-geoparquet-osn',\n 'huc12-gpkg-osn',\n 'nwm21-scores',\n 'lcmap-cloud',\n 'rechunking-tutorial-osn',\n 'pointsample-tutorial-sites-osn',\n 'pointsample-tutorial-output-osn']\n\nThe characteristics of indivdual datasets can be explored:cat['lcmap-cloud']\n\nproducinglcmap-cloud:\n  args:\n    consolidated: false\n    storage_options:\n      fo: s3://nhgf-development/lcmap/lcmap.json\n      remote_options:\n        requester_pays: true\n      remote_protocol: s3\n      target_options:\n        requester_pays: true\n    urlpath: reference://\n  description: LCMAP, all 36 years\n  driver: intake_xarray.xzarr.ZarrSource\n  metadata:\n    catalog_dir: https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog\n\nOnce you have identified the dataset you would like to work with, xarray-type datasets can be loaded with to_dask(), while panda-type datasets can be loaded with .read():ds = cat['lcmap-cloud'].to_dask()\ndf = cat['nwm21-scores'].read()","type":"content","url":"/dataset-catalog/intake#example-intake-catalog-usage","position":3},{"hierarchy":{"lvl1":"HyTEST Data Catalog (Intake)","lvl2":"Subcatalogs"},"type":"lvl2","url":"/dataset-catalog/intake#subcatalogs","position":4},{"hierarchy":{"lvl1":"HyTEST Data Catalog (Intake)","lvl2":"Subcatalogs"},"content":"The main HyTEST intake catalog includes the use of sub-catalogs. A sub-catalog may contain a set of datasets for a particular use case (like a specific tutorial) or groupings of related datasets. For example, the CONUS404 datasets (at different time steps and storage locations) are stored in their own sub-catalog. An example of calling these catalogs in can be found \n\nhere.","type":"content","url":"/dataset-catalog/intake#subcatalogs","position":5},{"hierarchy":{"lvl1":"HyTEST Data Catalog (Intake)","lvl2":"Demos"},"type":"lvl2","url":"/dataset-catalog/intake#demos","position":6},{"hierarchy":{"lvl1":"HyTEST Data Catalog (Intake)","lvl2":"Demos"},"content":"You will see use of the intake catalog in many of the example workflows in the JupyterBook. Additional demos for working with the data catalogs can be found in the \n\ndemos folder of our repository. These are not as fully documented as the tutorials found in this JupyterBook.","type":"content","url":"/dataset-catalog/intake#demos","position":7},{"hierarchy":{"lvl1":"Data Storage Locations"},"type":"lvl1","url":"/dataset-catalog/storage-locations","position":0},{"hierarchy":{"lvl1":"Data Storage Locations"},"content":"Before getting into the details of how to use our \n\nSTAC or intake catalogs, it will be helpful to have some background on the various data storage systems HyTEST uses to host data assets. Many of the datasets in our intake catalog have been duplicated in multiple storage locations, so you will need to have a basic understanding of these systems to navigate the data catalog. For datasets that are duplicated in multiple locations, the data on all storage systems will be identical; however, the details and costs associated with accessing them may differ.\n\nThe four locations we store data currently are: AWS S3 buckets, Open Storage Network (OSN) pods, and \n\nUSGS on-premises supercomputer storage systems (one storage system for the Tallgrass/Denali supercomputers and another for the Hovenweep supercomputer).","type":"content","url":"/dataset-catalog/storage-locations","position":1},{"hierarchy":{"lvl1":"Data Storage Locations","lvl2":"Identifying Storage Location from a Data Catalog Entry"},"type":"lvl2","url":"/dataset-catalog/storage-locations#identifying-storage-location-from-a-data-catalog-entry","position":2},{"hierarchy":{"lvl1":"Data Storage Locations","lvl2":"Identifying Storage Location from a Data Catalog Entry"},"content":"","type":"content","url":"/dataset-catalog/storage-locations#identifying-storage-location-from-a-data-catalog-entry","position":3},{"hierarchy":{"lvl1":"Data Storage Locations","lvl3":"intake","lvl2":"Identifying Storage Location from a Data Catalog Entry"},"type":"lvl3","url":"/dataset-catalog/storage-locations#intake","position":4},{"hierarchy":{"lvl1":"Data Storage Locations","lvl3":"intake","lvl2":"Identifying Storage Location from a Data Catalog Entry"},"content":"In the intake catalog, datasets that are duplicated in multiple locations will have an intake catalog entry for each storage location. These entries will have identical names, up until the last hypenated part of the name, which will indicate the storage location; for example, conus404-hourly-s3, conus404-hourly-osn, and conus404-hourly-onprem-hw are all identical datasets stored in different places (s3, osn, and onprem-hw).","type":"content","url":"/dataset-catalog/storage-locations#intake","position":5},{"hierarchy":{"lvl1":"Data Storage Locations","lvl3":"STAC","lvl2":"Identifying Storage Location from a Data Catalog Entry"},"type":"lvl3","url":"/dataset-catalog/storage-locations#stac","position":6},{"hierarchy":{"lvl1":"Data Storage Locations","lvl3":"STAC","lvl2":"Identifying Storage Location from a Data Catalog Entry"},"content":"In the STAC Catalog, datasets that are duplicated in multiple locations will have multiple Assets attached to the singular STAC entry for that dataset. Information about the storage location for each data asset is included in the the asset’s title, description, and roles.","type":"content","url":"/dataset-catalog/storage-locations#stac","position":7},{"hierarchy":{"lvl1":"Data Storage Locations","lvl2":"Description of Storage Locations"},"type":"lvl2","url":"/dataset-catalog/storage-locations#description-of-storage-locations","position":8},{"hierarchy":{"lvl1":"Data Storage Locations","lvl2":"Description of Storage Locations"},"content":"Each of the storage locations used by the HyTEST community to host data assets is described in more detail below.","type":"content","url":"/dataset-catalog/storage-locations#description-of-storage-locations","position":9},{"hierarchy":{"lvl1":"Data Storage Locations","lvl3":"Open Storage Network (OSN) Pod","lvl2":"Description of Storage Locations"},"type":"lvl3","url":"/dataset-catalog/storage-locations#open-storage-network-osn-pod","position":10},{"hierarchy":{"lvl1":"Data Storage Locations","lvl3":"Open Storage Network (OSN) Pod","lvl2":"Description of Storage Locations"},"content":"This location provides object storage through Woods Hole Oceanographic Institute’s \n\nOpen Storage Network (OSN) storage pod. This OSN pod that HyTEST uses is housed at the Massachusetts Green High Performance Computing Center on a high-speed (100+ GbE) network. This copy of the data is free to access from any computing environment and does not require any credentials to access.\n\nThe OSN pod storage can be accessed through an API that is compatible with the basic data access model of the S3 API. The only major difference is that the user needs to specify the appropriate endpoint url for the OSN pod when making the request. However, a user accessing data on the OSN pod through HyTEST’s intake catalog or the WMA STAC Catalog will not have to worry about these details because the python packages used to open the data from these catalogs will ingest that information directly from our catalog. If you would like to access the data on the OSN pod through a mechanism other than intake or STAC, you may want to review the \n\nData/Cloud Storage section of this book.\n\nDatasets in the intake catalog that are stored on the OSN pod have a name ending in “-osn”.","type":"content","url":"/dataset-catalog/storage-locations#open-storage-network-osn-pod","position":11},{"hierarchy":{"lvl1":"Data Storage Locations","lvl3":"AWS S3","lvl2":"Description of Storage Locations"},"type":"lvl3","url":"/dataset-catalog/storage-locations#aws-s3","position":12},{"hierarchy":{"lvl1":"Data Storage Locations","lvl3":"AWS S3","lvl2":"Description of Storage Locations"},"content":"This location provides object storage through an Amazon Web Services (AWS) Simple Storage Service (S3) bucket. This data is free to access for workflows that are running in the AWS us-west-2 region. However, if you would like to pull the data out of the AWS cloud (to your local computer, a supercomputer, or another cloud provider) or into another \n\nAWS cloud region, you will incur fees. This is because the bucket that we use to store the data is a “requester pays” bucket. The costs associated with reading the data to other computing environments or AWS regions is documented \n\nhere (on the “Requests and Data Retrievals” tab). If you do need to read this data into a computing environment outside the AWS us-west-2 region, you will need to make sure you have an \n\nAWS account set up. You will need credentials from this account to read in the data, and your account will be billed. Please refer to the \n\nAWS Credentials section of this book for more details on handling AWS credentials.\n\nDatasets in the intake catalog that are stored in an S3 bucket have a name ending in “-s3”.","type":"content","url":"/dataset-catalog/storage-locations#aws-s3","position":13},{"hierarchy":{"lvl1":"Data Storage Locations","lvl3":"USGS On-premises Supercomputer Storage (Caldera for Tallgrass/Denali and Hovenweep)","lvl2":"Description of Storage Locations"},"type":"lvl3","url":"/dataset-catalog/storage-locations#usgs-on-premises-supercomputer-storage-caldera-for-tallgrass-denali-and-hovenweep","position":14},{"hierarchy":{"lvl1":"Data Storage Locations","lvl3":"USGS On-premises Supercomputer Storage (Caldera for Tallgrass/Denali and Hovenweep)","lvl2":"Description of Storage Locations"},"content":"The last storage location is the USGS on-premises disk storage that is attached to the USGS supercomputers (often referred to as Caldera). This location is only accessible to USGS employees or collaborators who have been granted access to \n\nUSGS supercomputers. This is the preferred data storage to use if you are working on the USGS supercomputers as it will give you the fastest data reads.\n\nThe Tallgrass and Denali supercomputers share on filesystem, and the Hovenweep supercomputer has a different filesystem. These supercomputers can only read data from their own filesystems (you cannot read data from the filesystem attached to Denali/Tallgrass into Hovenweep and vice versa). You also cannot read data from an on-premises storage system into any computing environment outside of the USGS supercomputers (like your local computer or the cloud). More information about this storage system can be found in the \n\nHPC User Docs (which are also only accessible through the internal USGS network).\n\nDatasets in the intake catalog that are stored on the filesystem attached to Denali/Tallgrass have a name ending in “-onprem”, while datasets stored on the filesystem attached to Hovenweep have a name ending in “-onprem-hw”.","type":"content","url":"/dataset-catalog/storage-locations#usgs-on-premises-supercomputer-storage-caldera-for-tallgrass-denali-and-hovenweep","position":15},{"hierarchy":{"lvl1":"Data Storage Locations","lvl3":"Storage Location Summary","lvl2":"Description of Storage Locations"},"type":"lvl3","url":"/dataset-catalog/storage-locations#storage-location-summary","position":16},{"hierarchy":{"lvl1":"Data Storage Locations","lvl3":"Storage Location Summary","lvl2":"Description of Storage Locations"},"content":"The following table is a quick summarization of the storage locations described above in a more quickly referenceable format.\n\nStorage Location\n\nWho Can Access?\n\nWhat computing environments can you read the data into?\n\nCost to Access\n\nIntake Catalog Naming\n\nOpen Storage Network (OSN) Pod\n\nPublic, no credentials required\n\nany computing location\n\nFree\n\n-osn\n\nAWS S3 Storage\n\nPublic, AWS credentials required\n\nany computing location\n\nFree for workflows running in AWS us-west-2 region, otherwise see \n\nAWS pricing\n\n-s3\n\nHovenweep (USGS Supercomputer)\n\nUSGS employees, collaborators with access to USGS supercomputers\n\nUSGS Hovenweep supercomputer\n\nFree\n\n-onprem-hw\n\nTallgrass/Denali (USGS Supercomputers)\n\nUSGS employees,s collaborators with access to USGS supercomputers\n\nUSGS Tallgrass and Denali supercomputers\n\nFree\n\n-onprem","type":"content","url":"/dataset-catalog/storage-locations#storage-location-summary","position":17},{"hierarchy":{"lvl1":"HyTEST Intake Sub-Catalogs"},"type":"lvl1","url":"/dataset-catalog/subcatalogs/readme","position":0},{"hierarchy":{"lvl1":"HyTEST Intake Sub-Catalogs"},"content":"This section describes how to use the subcatalogs contained in HyTEST’s main data catalog (hytest_intake_catalog.yml). Example usage of the CONUS404 sub-catalog is shown below.import intake\nhytest_cat = intake.open_catalog(\"https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml\")\nlist(hytest_cat)\n\nproduces a list of datasets and sub-catalogs in the main HyTEST data catalog, for example:['conus404-catalog',\n 'conus404-drb-eval-tutorial-catalog',\n 'nhm-v1.0-daymet-catalog',\n 'nhm-v1.1-c404-bc-catalog',\n 'nhm-v1.1-gridmet-catalog',\n 'trends-and-drivers-catalog',\n 'nhm-prms-v1.1-gridmet-format-testing-catalog',\n 'nwis-streamflow-usgs-gages-onprem',\n 'nwis-streamflow-usgs-gages-osn',\n 'nwm21-streamflow-usgs-gages-onprem',\n 'nwm21-streamflow-usgs-gages-osn',\n 'nwm21-streamflow-cloud',\n 'geofabric_v1_1-zip-osn',\n 'geofabric_v1_1_POIs_v1_1-osn',\n 'geofabric_v1_1_TBtoGFv1_POIs-osn',\n 'geofabric_v1_1_nhru_v1_1-osn',\n 'geofabric_v1_1_nhru_v1_1_simp-osn',\n 'geofabric_v1_1_nsegment_v1_1-osn',\n 'gages2_nndar-osn',\n 'wbd-zip-osn',\n 'huc12-geoparquet-osn',\n 'huc12-gpkg-osn',\n 'nwm21-scores',\n 'lcmap-cloud',\n 'rechunking-tutorial-osn',\n 'pointsample-tutorial-sites-osn',\n 'pointsample-tutorial-output-osn']\n\n\nWe can then open the CONUS404 sub-catalog with:cat = hytest_cat['conus404-catalog']\nlist(cat)\n\nproducing a list of all the CONUS404 dataset versions:['conus404-hourly-onprem-hw',\n 'conus404-hourly-cloud',\n 'conus404-hourly-osn',\n 'conus404-daily-diagnostic-onprem-hw',\n 'conus404-daily-diagnostic-cloud',\n 'conus404-daily-diagnostic-osn',\n 'conus404-daily-onprem-hw',\n 'conus404-daily-cloud',\n 'conus404-daily-osn',\n 'conus404-monthly-onprem-hw',\n 'conus404-monthly-cloud',\n 'conus404-monthly-osn',\n 'conus404-hourly-ba-onprem-hw',\n 'conus404-hourly-ba-osn',\n 'conus404-daily-ba-onprem',\n 'conus404-daily-ba-osn',\n 'conus404-pgw-hourly-onprem-hw',\n 'conus404-pgw-hourly-osn',\n 'conus404-pgw-daily-diagnostic-onprem-hw',\n 'conus404-pgw-daily-diagnostic-osn']\n\nThe characteristics of indivdual datasets can be explored:cat['conus404-hourly-osn']\n\nproducingconus404-hourly-osn:\n  args:\n    consolidated: true\n    storage_options:\n      anon: true\n      client_kwargs:\n        endpoint_url: https://usgs.osn.mghpcc.org/\n      requester_pays: false\n    urlpath: s3://hytest/conus404/conus404_hourly.zarr\n  description: \"CONUS404 Hydro Variable subset, 43 years of hourly values. These files\\\n    \\ were created wrfout model output files (see ScienceBase data release for more\\\n    \\ details: https://doi.org/10.5066/P9PHPK4F). This data is stored on HyTEST\\u2019\\\n    s Open Storage Network (OSN) pod. This data can be read with the S3 API and is\\\n    \\ free to work with in any computing environment (there are no egress fees).\"\n  driver: intake_xarray.xzarr.ZarrSource\n  metadata:\n    catalog_dir: https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/subcatalogs","type":"content","url":"/dataset-catalog/subcatalogs/readme","position":1},{"hierarchy":{"lvl1":"Spatial Aggregation"},"type":"lvl1","url":"/dataset-processing/spatial-aggregation","position":0},{"hierarchy":{"lvl1":"Spatial Aggregation"},"content":"Spatial aggregation of a gridded dataset to a polygon area is a common processing method in the geospatial world, and there are a variety of packages and methods that can be used to perform this task. Our team has tested two methods: one using the python package \n\ngdptools, and the other using conservative regional methods with xarray and geopandas natively, as described in \n\nthis Pangeo Discourse.\n\nWe developed both of these methods over the Delaware River Basin to test how their speed and accuracy would compare. You can see notebooks demonstrating each method over this spatially subsetted area in the following notebooks:\n\ngdptools method\n\nPangeo method\n\nAs well as a notebook directly comparing the two methods’ speed and accuracy:\n\ngdptools-Pangeo comparison\n\nWhile both methods will produce the same result in similar amounts of time, we recommend using the gdptools method as it set up with a more user-friendly interface.\n\nIf you plan to use gdptools at spatial scales larger than the Delaware River Basin, we have made a few adaptations to the workflows above to mitigate the likelihood of crashing the compute instance you are working on. We have two workflows demonstrating this larger-scale spatial processing of the CONUS404 data over a set of polygons covering the full CONUS extent. Each workflow processes the data to a different set of polygons (\n\nNHDPlusV2 snapshot of the Watershed Boundary Dataset HUC12 boundaries or \n\nGeoSpatialFabric v1.1), both of which are commonly used in hydrologic modeling:\n\nGeospatial Fabric v1.1\n\nNHDPlusV2 snapshot of the Watershed Boundary Dataset HUC12 boundaries\n\nPlease note that these last two notebooks have not been executed in our JupyterBook because they are configured to run a large computation on a specific computing environment (USGS on-prem Hovenweep supercomputer), so they cannot be executed within the context that builds this book.","type":"content","url":"/dataset-processing/spatial-aggregation","position":1},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size"},"type":"lvl1","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize","position":0},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size"},"content":"The goal of this notebook is to learn the basics about chunk shape and size.\nWe will discuss several factors to think about when deciding on chunk shape and size for datasets being written to storage.\nThese factors can affect the read pattern from storage and subsequently the computations.\n\nimport xarray as xr\nimport fsspec\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize","position":1},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size","lvl2":"Accessing the Example Dataset"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize#accessing-the-example-dataset","position":2},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size","lvl2":"Accessing the Example Dataset"},"content":"In this notebook, we will use the monthly PRISM v2 dataset as an example for understanding the effects of chunk shape and size.\nLet’s go ahead and read in the file using xarray.\nTo do this, we will use \n\nfsspec to get a mapper to the Zarr file on the HyTEST OSN.\n\nNote\n\nThe xarray loader is “lazy”, meaning it will read just enough of the data to make decisions about its shape, structure, etc.\nIt will pretend like the whole dataset is in memory (and we can treat it that way), but it will only load data as required.\n\nfs = fsspec.filesystem(\n    's3',\n    anon=True,   # anonymous = does not require credentials\n    client_kwargs={'endpoint_url': 'https://usgs.osn.mghpcc.org/'}\n)\nds = xr.open_dataset(\n    fs.get_mapper('s3://mdmf/gdp/PRISM_v2.zarr/'),\n    engine='zarr'\n)\nds\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize#accessing-the-example-dataset","position":3},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size","lvl2":"Chunk Shape and Size"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize#chunk-shape-and-size","position":4},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size","lvl2":"Chunk Shape and Size"},"content":"Given what we know about this data, we can apply some storage principles to form a strategy for how best to chunk the data if we were to write it to storage (assuming it isn’t already).\nBroadly, we need to specify chunk shape and size.","type":"content","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize#chunk-shape-and-size","position":5},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size","lvl3":"Shape Considerations","lvl2":"Chunk Shape and Size"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize#shape-considerations","position":6},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size","lvl3":"Shape Considerations","lvl2":"Chunk Shape and Size"},"content":"“\n\nChunk shape” is the shape of a chunk, which specifies the number of elements in each dimension.\nSo, we will need to decide on the size of each of the dimensions of the chunks.\nThe preferred shape of each chunk will depend on the read pattern for future analyses.\nOur goal is to chunk the data so that future reads will be performant, and that depends on whether the analyses favor one dimension or another.\nFor some datasets, this will be very apparent.\nFor example, streamflow gage data is very likely to be consumed along the time dimension.\nSo, a collection of data from multiple gages is more likely to have the individual time series analyzed as opposed to analyzing all gages at a given time.\nTherefore, we would want a chunk shape that is larger along the time dimension.\nFor datasets where there is no clear preference, we can try to chunk based on likely read patterns, but allow for other patterns without too much of a performance penalty.\n\nLet’s see how we might do this for our example dataset.\nBeing this dataset spans space and time, it will likely be used in one of two dominant read patterns:\n\nTime series for a given location (or small spatial extent)\n\nSpecial case: Is it likely that the time series will be subset by a logical unit (e.g., will this monthly data be consumed in blocks of 12 (i.e., yearly))?\n\nFull spatial extent for a given point in time.\n\nSpecial case: Are specific spatial regions more used than others?\n\nLet’s look at a couple of options for space and time chunking:\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize#shape-considerations","position":7},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size","lvl4":"Time Dimension","lvl3":"Shape Considerations","lvl2":"Chunk Shape and Size"},"type":"lvl4","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize#time-dimension","position":8},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size","lvl4":"Time Dimension","lvl3":"Shape Considerations","lvl2":"Chunk Shape and Size"},"content":"As we can see above, the example dataset has 1555 monthly time steps.\nHow many chunks would we have if we chunked in groups of twelve (i.e., a year at a time)?\n\nprint(f\"Number of chunks: {len(ds.time) / 12:0.2f}\")\n\n\n\nIn this case, a user could get a single year of monthly data as a single chunk.\nIt is important to note that we have just over a round number of chunks. Having 129.58 time chunks means we will have 130 chunks in practice, but the last one is not full-sized. The last chunk would be a \n\n“partial chunk” because we do not have a full year of data for 2024.\n\nSo this is where the judgement call gets made: Which is the more likely read pattern for time: year-by-year, or the whole time series (or some sequence of a few years)?\nFor PRISM, it is more likely that someone will want more than just one year of data.\nA happy medium for chunk shape along the time dimension could be 6 years of data per chunk.\n\ntime_chunk_shape = 12 * 6\nprint(f\"Number of chunks: {len(ds.time) / time_chunk_shape:0.2f}; Chunk of shape: {time_chunk_shape}\")\n\n\n\nThis pattern means only 22 chunks (instead of the 126 chunks we were considering a moment ago) are needed for a full time series in a given location.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize#time-dimension","position":9},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size","lvl4":"Spatial Dimension","lvl3":"Shape Considerations","lvl2":"Chunk Shape and Size"},"type":"lvl4","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize#spatial-dimension","position":10},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size","lvl4":"Spatial Dimension","lvl3":"Shape Considerations","lvl2":"Chunk Shape and Size"},"content":"As we can see in our example dataset, it technically contains two spatial dimensions: lat and lon.\nSo, we’re really chunking both of these dimensions when we talk about chunking with respect to space.\nWhile we will consider them both together here, it is important to point out that they can have separate chunk shapes.\nThis leads to the the question of whether future users of this data will want strips of latitude or longitude, square “tiles” in space, or some proportionally-sized tiles of latitude and longitude?\nThat is, is it important that the North-South extent be broken into the same number of chunks as the East-West extent?\nLet’s start by chunking this into square tiles.\nBeing that there are more lon elements than lat elements, this means there will be more lon chunks than lat chunks.\n\nnlon = len(ds.lon)\nnlat = len(ds.lat)\nspace_chunk_size = nlat // 4 # split the smaller of the two dimensions into 4 chunks\nprint(f\"Number of 'lon' chunks: {nlon / space_chunk_size:0.3f}; Chunk of shape {space_chunk_size}; Size of last chunk: {nlon % space_chunk_size}\")\nprint(f\"Number of 'lat' chunks: {nlat / space_chunk_size:0.3f}; Chunk of shape {space_chunk_size}; Size of last chunk: {nlat % space_chunk_size}\")\n\n\n\nHaving 9.06 longitude chunks means we will have 10 chunks in practice, but that last one is not full-sized.\nIn this case, this means that the last chunk in the given dimension will be extremely thin.\n\nIn the case of the latitude chunks, the extra 0.006 of a chunk means that the last, fractional chunk (or \n\n“partial chunk”) is only one lat observation.\n\nTip\n\nIdeally, we would want partial chunks to be at least half the size of the standard chunk.\nThe bigger that “remainder” fraction, the better.\n\nLet’s adjust the chunk shape a little so that we don’t have that sliver.\nWe’re still committed to square tiles, so let’s try a larger chunk shape to change the size of that last fraction.\nIncreasing the chunk size a little should get us bigger “remainders”.\n\nspace_chunk_size = 157\nprint(f\"Number of 'lon' chunks: {nlon / space_chunk_size:0.2f}; Chunk of shape {space_chunk_size}; Size of last chunk: {nlon % space_chunk_size}\")\nprint(f\"Number of 'lat' chunks: {nlat / space_chunk_size:0.2f}; Chunk of shape {space_chunk_size}; Size of last chunk: {nlat % space_chunk_size}\")\n\n\n\nWith this pattern, the “remainder” latitude chunk will have a shape of 150 in the lat dimension, and the “remainder” longitude chunk will have a shape of 149 in the lon dimension.\nAll others will be a square 157 observations in both dimensions.\nThis amounts to a 9x4 chunk grid, with the last chunk in each dimension being partial.\n\nThe entire spatial extent for a single time step can be read in 36 chunks with this chunk shape.\nThat seems a little high, given that this dataset will likely be taken at full spatial extent for a typical analysis.\nLet’s go a little bigger to see what that gets us:\n\nspace_chunk_size = 354 # 157 * 2\nprint(f\"Number of 'lon' chunks: {nlon / space_chunk_size:0.2f}; Chunk of shape {space_chunk_size}; Size of last chunk: {nlon % space_chunk_size}\")\nprint(f\"Number of 'lat' chunks: {nlat / space_chunk_size:0.2f}; Chunk of shape {space_chunk_size}; Size of last chunk: {nlat % space_chunk_size}\")\n\n\n\nThis is just as good in terms of full-chunk remainders, and the whole extent can be read in with only 8 chunks.\nThe smallest remainder is still >75% of a full-sized square tile, which is acceptable.\n\nAlternatively, we could stop being committed to square tiles and try and split the spatial regions more evenly.\nFor example, we could get as close to a 4x2 split as possible:\n\n# Add one to do a ceil divide\nlon_space_chunk_size = nlon // 4 + 1\nlat_space_chunk_size = nlat // 2 + 1\nprint(f\"Number of 'lon' chunks: {nlon / lon_space_chunk_size:0.3f}; Chunk of shape {lon_space_chunk_size}; Size of last chunk: {nlon % lon_space_chunk_size}\")\nprint(f\"Number of 'lat' chunks: {nlat / lat_space_chunk_size:0.3f}; Chunk of shape {lat_space_chunk_size}; Size of last chunk: {nlat % lat_space_chunk_size}\")\n\n\n\nOr we could aim for a 3x3 split:\n\n# Add one to do a ceil divide\nlon_space_chunk_size = nlon // 3 + 1\nlat_space_chunk_size = nlat // 3 + 1\nprint(f\"Number of 'lon' chunks: {nlon / lon_space_chunk_size:0.3f}; Chunk of shape {lon_space_chunk_size}; Size of last chunk: {nlon % lon_space_chunk_size}\")\nprint(f\"Number of 'lat' chunks: {nlat / lat_space_chunk_size:0.3f}; Chunk of shape {lat_space_chunk_size}; Size of last chunk: {nlat % lat_space_chunk_size}\")\n\n\n\nAs you might be getting, the chunking proportion between latitude and longitude is not super important.\nWhat is important for basic chunk shape is the total number of chunks between the two and the minimization of the remainder in the final chunk of each dimension.\n\nNote\n\nIf we were really confident that most analyses wanted the full extent, we might be better off to just put the whole lat/lon dimensions into single chunks each.\nThis would ensure (and require) that we read the entire spatial extent.\nHowever, our poor time-series analysis would then be stuck reading the entire dataset to get all time values for a single location.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize#spatial-dimension","position":11},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size","lvl3":"Size Considerations","lvl2":"Chunk Shape and Size"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize#size-considerations","position":12},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size","lvl3":"Size Considerations","lvl2":"Chunk Shape and Size"},"content":"Shape is only part of the equation.\nTotal “\n\nchunk size” also matters.\nSize considerations come into play mostly as a consideration of how the chunks are stored on disk.\nThe retrieval time is influenced by the size of each chunk.\nHere are some constraints:\n\nFiles Too Big:\nIn a Zarr dataset, each chunk is stored as a separate binary file (and the entire zarr dataset is a directory grouping these many “chunk” files).\nIf we need data from a particular chunk, no matter how little or how much, that file gets opened, decompressed, and the whole thing read into memory.\nA large chunk size means that there may be a lot of data transferred in situations when only a small subset of that chunk’s data is actually needed.\nIt also means there might not be enough chunks to allow the dask workers to stay busy loading data in parallel.\n\nFiles Too Small:\nIf the chunk size is too small, the time it takes to read and decompress the data for each chunk can become comparable to the latency of S3 (typically 10-100ms).\nWe want the reads to take at least a second or so, so that the latency is not a significant part of the overall timing.\n\nTip\n\nAs a general rule, aim for chunk sizes between 10 and 200 MB, depending on shape and expected read pattern of a user.","type":"content","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize#size-considerations","position":13},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size","lvl4":"Total Chunk Size","lvl3":"Size Considerations","lvl2":"Chunk Shape and Size"},"type":"lvl4","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize#total-chunk-size","position":14},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size","lvl4":"Total Chunk Size","lvl3":"Size Considerations","lvl2":"Chunk Shape and Size"},"content":"To esimate the total chunk size, all we need is the expected chunk shape and data type to know how many bytes a value takes up.\nAs an example, let’s use a chunk shape of {'time': 72, 'lat': 354, 'lon': 354}\nThis will tell us if we’ve hit our target of between 10 and 200 MB per chunk.\n\nchunks = {'time': 72, 'lat': 354, 'lon': 354}\nbytes_per_value = ds.tmn.dtype.itemsize\ntotal_bytes = chunks['time'] * chunks['lat'] * chunks['lon'] * bytes_per_value\nkiB = total_bytes / (2 ** 10)\nMiB = kiB / (2 ** 10)\nprint(f\"TMN chunk size: {total_bytes} ({kiB=:.2f}) ({MiB=:.2f})\")\n\n\n\nWe’re looking really good for size: about 69 MiB.\nThis maybe even a bit low.\nBut we’re in the (admittedly broad) range of 10-200 MiB of uncompressed data (i.e., in-memory) per chunk.\nTherefore, this seems like it would be a reasonable chunk shape and size for our dataset.\nIf we were curious about other chunk shapes, like a non-square lat and lon chunk, we could repeat this computation to estimate its size and determine if it is reasonable.\nHowever, we aren’t going to do that here, but it is something you could try on your own if you are curious.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize#total-chunk-size","position":15},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size","lvl2":"Review and Final Considerations"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize#review-and-final-considerations","position":16},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size","lvl2":"Review and Final Considerations"},"content":"Now that you have a general idea on how to pick chunk shape and size, let’s review and add a few final considerations.","type":"content","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize#review-and-final-considerations","position":17},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size","lvl3":"Basic Chunking Recommendations","lvl2":"Review and Final Considerations"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize#basic-chunking-recommendations","position":18},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size","lvl3":"Basic Chunking Recommendations","lvl2":"Review and Final Considerations"},"content":"When determining the basic chunk shape and size, the choice will depend on the future read pattern and analysis.\nIf this pattern is unknown, then it is important to take a balanced chunking approach that does not favor one dimension over the others (i.e., larger overall shape in a given dimension).\nNext, choosing a chunk shape should try to prevent partial chunks if possible.\nOtherwise, partial chunks should be at least half the size of the standard chunk.\nFinally, the total chunk size should be between 10 and 200 MiB for optimal performance.","type":"content","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize#basic-chunking-recommendations","position":19},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size","lvl3":"Final Considerations","lvl2":"Review and Final Considerations"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize#final-considerations","position":20},{"hierarchy":{"lvl1":"Basics of Chunk Shape and Size","lvl3":"Final Considerations","lvl2":"Review and Final Considerations"},"content":"One final thing to consider is that these basic recommendations assume that your chunked data will be static and not updated.\nHowever, some datasets, especially climate related ones, are periodically updated in their time dimension.\nThese datasets are commonly updated at regular intervals (e.g., every year with the previous years data).\nThis can change the choice of chunk shape such that adding the next year’s worth of data does not require rechunking the whole data set or result in small partial chunks.\nFor our PRISM example, if we chose a temporal chunk shape of length 72 (i.e., six years per chunk), adding a year worth of data would require appending the partial chunk until it becomes full.\nThen, further new data would require starting a new partial chunk.\nThis could be prevented if we chose a chunk size of 12 (i.e., one year per chunk).\nThen, additional data would only require making new chunks versus editing existing chunks.\nTherefore, considering updates to the dataset when deciding the chunking plan can save a lot of time when appending the dataset in the future.\n\nAdditionally, all of the information provided here does not discuss proper optimization of chunk shape and size.\nProper optimization would attempt to select chunk sizes that are near powers of two (i.e., 2^N) to facilitate optimal storage and disk retrieval.\nDetails on this topic can be found in the advanced topic notebook of \n\nChoosing an Optimal Chunk Size and Shape.","type":"content","url":"/dataset-processing/tutorials/chunking/101/basicsshapesize#final-considerations","position":21},{"hierarchy":{"lvl1":"How to Examine a Stored Dataset’s Chunk Shape"},"type":"lvl1","url":"/dataset-processing/tutorials/chunking/101/examinedatachunking","position":0},{"hierarchy":{"lvl1":"How to Examine a Stored Dataset’s Chunk Shape"},"content":"The objective of this notebook is to learn how to examine a stored dataset and understand if it is chunked and, if so, what its “\n\nchunk shape” is.\nTo do this, we will utilize an existing dataset from the HyTEST OSN, take a guided tour of the data, and show how to figure out its chunk shape.\n\nimport xarray as xr\nimport fsspec\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/examinedatachunking","position":1},{"hierarchy":{"lvl1":"How to Examine a Stored Dataset’s Chunk Shape","lvl2":"Accessing the Dataset"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/examinedatachunking#accessing-the-dataset","position":2},{"hierarchy":{"lvl1":"How to Examine a Stored Dataset’s Chunk Shape","lvl2":"Accessing the Dataset"},"content":"Before we can open the dataset, we must first get a mapper that will easily allow for \n\nxarray to open the dataset.\nTo do this, we will use \n\nfsspec to perform an anonymous read from an endpoint outside of S3, that uses the S3 API (i.e., the HyTEST OSN).\nThis requires us to set up an S3 file system and feed it the endpoint URL.\nWe can then point the file system to our dataset (in this case the PRISM V2 Zarr store) and get a mapper to the file.\n\nfs = fsspec.filesystem(\n    's3',\n    anon=True,   # anonymous = does not require credentials\n    client_kwargs={'endpoint_url': 'https://usgs.osn.mghpcc.org/'}\n)\nfile = fs.get_mapper('s3://mdmf/gdp/PRISM_v2.zarr/')\n\n\n\nNow that we have our file mapper, we can open the dataset using \n\nxarray.open_dataset() with zarr specified as our engine.\n\nNote\n\nThe xarray loader is “lazy”, meaning it will read just enough of the data to make decisions about its shape, structure, etc.\nIt will pretend like the whole dataset is in memory (and we can treat it that way), but it will only load data as required.\n\nds = xr.open_dataset(file, engine='zarr')\nds\n\n\n\nThe HTML output for the \n\nxarray.Dataset includes a lot of information, some of which is hidden behind toggles.\nClick on the icons to the right to expand and see all the metadata available for the dataset.\nThe page icon will display attributes attached to the data, while the database icon will display information about the dataset.\n\nNotable observations:\n\nDimensions: This dataset is 3D, with data being indexed by lat, lon, and time (setting  aside tbnd for the moment; it is a special case).\nLooking at the “Dimensions” line, you can see that each of these dimensions is quantified (i.e., the size of each dimension).\n\nlat = 621\n\nlon = 1405\n\ntime = 1555\n\nCoordinates: These are the convenient handles by which dimensions can be referenced.\nIn this dataset, a coordinate can be used to pick out a particular cell of the array.\nSelecting cells where say lat=49.9 is possible because these coordinates map the meaningful values of latitude to the behind-the-scenes cell index needed to fetch the value.\n\nData Variables: The primary variables are ppt, tmn, and tmx, which each have three dimensions (time, lat, lon) by which data values are located in space and time.\n\nIndexes: This is an internal data structure to help xarray quickly find items in the array.\n\nAttributes: Arbitrary metadata that has been given to the dataset.\n\nLet’s look at one of the data variables to learn more about it.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/examinedatachunking#accessing-the-dataset","position":3},{"hierarchy":{"lvl1":"How to Examine a Stored Dataset’s Chunk Shape","lvl3":"Variable = xarray.DataArray","lvl2":"Accessing the Dataset"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/101/examinedatachunking#variable-xarray-dataarray","position":4},{"hierarchy":{"lvl1":"How to Examine a Stored Dataset’s Chunk Shape","lvl3":"Variable = xarray.DataArray","lvl2":"Accessing the Dataset"},"content":"Each data variable is its own N-dimensional array (in this case, 3-dimensional, indexed by lat, lon, and time).\nWe can look at the individual variables by examining its array separately from the dataset:\n\nds.tmn\n\n\n\nNote from the top line that this variable is indexed as a tuple in (time, lat, lon).\nSo, behind the scenes, there is an array whose first index (for time) is a value between 0 and 1555.\nBut how do we know the time value of index 0 (or any index, really)?\nThe “Coordinates” are the lookup table to say what “real” time value is associated with each index address.\n\nYou’ll notice the data description in this case is merely “1356745275 values with dtype=float64” with no indication as to how it is chunked.\nAssuming our 3-D array is fully populated, this value makes sense:\n\n# time  lat  lon\n1555 * 621 * 1405\n\n\n\nIn terms of chunking, this is where it gets interesting.\nIf you thoroughly examined the HTML output, you may have noticed that there is no reference to chunking anywhere.\nTherefore, we need to directly access the data in a way that returns the true chunk shape of the stored dataset.\n\nTo do this, we can simply check a variable’s “encoding”.\nThis returns metadata that was used by xarray when reading the data.\n\nds.tmn.encoding\n\n\n\nFrom here we can see two keys dealing with chunks: 'chunks' and 'preferred_chunks' - in this case both contain the same information of how the data is chunked.\nThe difference between the two is 'chunks' is the chunk shape of the chunks stored on disk (what is commonly termed “\n\nstored chunks”), while 'preferred_chunks' is the chunk shape that the engine chose to open the dataset with.\nGenerally, these are the same, but they may be different if the engine you use has not been set to equate them or if a different chunk shape is specified when opening the dataset.\nTherefore, our data has a stored chunk shape of {'time': 68, 'lat': 131, 'lon': 294}.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/examinedatachunking#variable-xarray-dataarray","position":5},{"hierarchy":{"lvl1":"How to Examine a Stored Dataset’s Chunk Shape","lvl2":"Getting the Chunking When Reading Data"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/examinedatachunking#getting-the-chunking-when-reading-data","position":6},{"hierarchy":{"lvl1":"How to Examine a Stored Dataset’s Chunk Shape","lvl2":"Getting the Chunking When Reading Data"},"content":"While checking the “encoding” of the variable can tell you what the dataset’s stored chunk shape is, it is typically easier to do this in one step when you open the dataset.\nTo do this, all we need is to add a another keyword when we open the dataset with xarray: chunks={}.\nAs per the \n\nxarray.open_dataset documentation:\n\nchunks={} loads the data with dask using the engine’s preferred chunk size, generally identical to the format’s chunk size.\n\nIn other words, using chunks={} will load the data with chunk shape equal to 'preferred_chunks'.\nLet’s check this out and see how our data looks when we include this keyword when opening.\n\nds = xr.open_dataset(file, engine='zarr', chunks={})\nds\n\n\n\nNow when we inspect the data variables metadata, we will see the data is now read in as a dask array.\nLet’s look at the tmn variable again to simplify this.\n\nds.tmn\n\n\n\nAs we can see the data is chunked into chunks of shape (68, 131, 294) with a \n\nchunk size of ~20 MiB.\nThis is exactly what we saw when looking at the encoding.\nSo, this additional keyword worked as expected and gives us a standard way to open chunked datasets using the stored chunk shape as our chunk shape!\n\nNote that the coordinate variables themselves (lat, lon, and time) are stored as single unchunked arrays of data.\nRecall that these are used to translate a coordinate value into the index of the corresponding array.\nTherefore, these coordinate arrays will always be needed in their entirity.\nSo, they are included in each chunk such that they read whenever a chunk is read, and they do not affect how the data representing the data variables is chunked.","type":"content","url":"/dataset-processing/tutorials/chunking/101/examinedatachunking#getting-the-chunking-when-reading-data","position":7},{"hierarchy":{"lvl1":"How to Examine a Stored Dataset’s Chunk Shape","lvl2":"Changing the Chunk Shape and Size"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/examinedatachunking#changing-the-chunk-shape-and-size","position":8},{"hierarchy":{"lvl1":"How to Examine a Stored Dataset’s Chunk Shape","lvl2":"Changing the Chunk Shape and Size"},"content":"Now we can identify the stored chunk shape and size, but these settings may not always be ideal for performing analysis.\nFor example, \n\nZarr recommends a stored chunk size of at least 1 MB uncompressed as they give better performance.\nHowever, \n\ndask recommends chunk sizes between 10 MB and 1 GB for computations, depending on the availability of RAM and the duration of computations.\nTherefore, our stored chunk size may not be large enough for optimal computations.\nThankfully, stored chunks do not need to be the same size as those we use for our computations.\nIn other words, we can group multiple smaller stored chunks together when performing our computations.\nXarray makes this easy by allowing us to adjust the chunk shape and size, either as we load the data or after.\n\nLet’s show how this works by increasing our chunks of the minimum monthly temperature to a size of ~500 MiB.\nTo do so when reading in the data, all we need to do is specify the chunk shape with the chunks argument.\nFor our example, let’s do chunks of shape: {'time': 150, 'lat': 310, 'lon': 1405}.\n\n# Note we drop the other variables and select tmn when reading the data\nds_tmn = xr.open_dataset(file, engine='zarr',\n                         chunks={'time': 150, 'lat': 310, 'lon': 1405},\n                         drop_variables=['ppt', 'time_bnds', 'tmx', 'crs']).tmn\nds_tmn\n\n\n\nNice!\nAs we can see, the chunk shape is now displayed in the DataArray description with the chunk size we requested.\nHowever, we did get a warning indicating that:UserWarning: The specified chunks separate the stored chunks along dimension X starting at index i. This could degrade performance. Instead, consider rechunking after loading.\n\nImportant\n\nThis warning is telling us the chunk shape we have chosen is not a multiple (or grouping) of the stored chunks, and if we really want this chunk shape, we should rechunk the data.\n\nOops, as we are not attached to this chunk shape nor wanting to rechunk the data (see \n\nWhy (re)Chunk Data? notebook for reasons why you might), we need to select a chunk shape that is a multiple of the stored chunks.\nThis time, let’s try: {'time': 68*3, 'lat': 131*3, 'lon': 294*3}.\nThis should increase our original chunk size (~20 MiB) by a factor of 27 (3^3 = 27) - close to the ~500 MiB we are wanting.\n\nds_tmn = xr.open_dataset(file, engine='zarr',\n                         chunks={'time': 68 * 3, 'lat': 131 * 3, 'lon': 294 * 3},\n                         drop_variables=['ppt', 'time_bnds', 'tmx', 'crs']).tmn\nds_tmn\n\n\n\nLook at that, no warning and close to the chunk size we wanted!\n\nAs a final note, we selected our chunk shape while reading in the data.\nHowever, we could change them after using \n\nxarray.Dataset.chunk().\n\nds.tmn.chunk({'time': 68 * 4, 'lat': 131 * 4, 'lon': 294 * 4})\n\n\n\nNot Recommended\n\nWarning: We do not recommend using this method as you will not get the same warning notifying you that the chosen chunk shape does not match a multiple of the stored chunk shape. If you choose a non-multiple chunk shape you could slow down your whole workflow as the data will have to be rechunked to meet your requested chunk shape.","type":"content","url":"/dataset-processing/tutorials/chunking/101/examinedatachunking#changing-the-chunk-shape-and-size","position":9},{"hierarchy":{"lvl1":"Rechunking"},"type":"lvl1","url":"/dataset-processing/tutorials/chunking/101/rechunking","position":0},{"hierarchy":{"lvl1":"Rechunking"},"content":"Note\n\nThis notebook was inspired by the material in \n\nthis notebook by James McCreight, and \n\nthe Rechunker Tutorial.\n\nThe goal of this notebook is to learn how to “\n\nrechunk” data.\nThis will be a culmination of all the \n\nprevious introductory material where we will:\n\nRead in a Zarr store\n\nCheck the current chunking\n\nChoose a new chunk shape\n\nRechunk using \n\nRechunker\n\nConfirm the proper creation of the Zarr store by Rechunker\n\n%xmode Minimal\nimport xarray as xr\nimport fsspec\nfrom rechunker import rechunk\nimport zarr\nimport shutil\nimport numpy as np\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/rechunking","position":1},{"hierarchy":{"lvl1":"Rechunking","lvl2":"Read in a Zarr Store"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/rechunking#read-in-a-zarr-store","position":2},{"hierarchy":{"lvl1":"Rechunking","lvl2":"Read in a Zarr Store"},"content":"For the dataset in this tutorial, we will use the data from the National Water Model Reanalysis Version 2.1.\nThe full dataset is part of the \n\nAWS Open Data Program, available via the S3 bucket at: s3://noaa-nwm-retro-v2-zarr-pds/.\n\nAs this is a Zarr store, we can easily read it in directly with \n\nxarray.open_dataset(), including the keyword chunks={} to make sure dask loads the data using the stored chunks’ shape and size.\n\nfile = fsspec.get_mapper('s3://noaa-nwm-retro-v2-zarr-pds', anon=True)\nds = xr.open_dataset(file, chunks={}, engine='zarr')\nds\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/rechunking#read-in-a-zarr-store","position":3},{"hierarchy":{"lvl1":"Rechunking","lvl2":"Check the Current Chunk Shape and Size"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/rechunking#check-the-current-chunk-shape-and-size","position":4},{"hierarchy":{"lvl1":"Rechunking","lvl2":"Check the Current Chunk Shape and Size"},"content":"From the output, we can see there are two dimensions, time of length 227,904 and feature_id of length 2,729,077.\nBoth time and feature_id are coordinates, along with two extra coordinates of latitude and longitude which are tied to the feature_id.\nInspecting these extra coordinates in detail, we notice they are “chunked”, but only to a single chunk of size 10.41 MiB.\nThis is good, as we will want them in a single chunk for writing, such that they are fully read in with each data variable chunk.\nFinally, there are eight data variables, all having both of the dimensions, meaning they are 2D data.\nExamining the variables in detail, they all have chunks of shape {'time': 672, 'feature_id': 30000} and size of 153.81 MiB or 76.9 MiB (depending if the variable is 64- or 32-bit, respectively).\nAdditionally, we can see that each variable is a whopping 4.53 or 2.26 TiB in memory, which means the whole dataset is almost 32 TiB!\n\nThis is a bit more than we want to work with in our example.\nSo, let’s go ahead and select a subset of the data that is about 1 GiB in memory.\nWhile this is not a larger-than-memory dataset anymore, it is still reasonably large and it will work well for this example without taking forever to get data and rechunk.\nFor the subset, let’s only look at water year 2018 (i.e., October 2017 to September 2018) for the variables streamflow and velocity for the first 15,000 feature_ids.\n\nNote\n\nAs we are not working on AWS, we have selected this subset to minimize the number of chunks that need to be downloaded from the S3 bucket.\n\nds = ds[['streamflow', 'velocity']]\nds = ds.isel(feature_id=slice(0, 15000))\nds = ds.sel(time=slice('2017-10-01', '2018-09-30'))\nds\n\n\n\nNow, our subset of data is only about 1 GiB per data variable and has a chunk shape of {'time': 672, 'feature_id': 15000} with size of 76.9 MiB.\nThis is a good chunk size and between the optimal range of 10 to 200 MiB.\nHowever, the chunk shape may not be an optimal choice for our analysis as it is chunked completely by feature_id (i.e., all feature IDs for a given time are read in a single chunk).","type":"content","url":"/dataset-processing/tutorials/chunking/101/rechunking#check-the-current-chunk-shape-and-size","position":5},{"hierarchy":{"lvl1":"Rechunking","lvl2":"Choose a New Chunk Shape and Size"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/rechunking#choose-a-new-chunk-shape-and-size","position":6},{"hierarchy":{"lvl1":"Rechunking","lvl2":"Choose a New Chunk Shape and Size"},"content":"To decide on a new chunk shape and size, we need to determine how we will use the data.\nAs we just discussed, an analysis which samples all locations for a single point in time would need to fetch only a single chunk, which is perfect for that analysis.\nHowever, a time-series analysis (i.e. sampling all time-step values for a single feature_id) would require 13 chunks to be read for one feature.\nThis means many more chunks must be fetched for this read pattern.\nThinking of data usage, the preferred format for the streamflow data variable is likely time-series wise chunking as this variable is more often used as full time series at a single location.\nThe same goes for velocity.\nHowever, for the purpose of this example, let’s assume that we don’t know how velocity will be used and give it a different chunking pattern, one where we balance the number of chunks between each dimension.\n\nnfeature = len(ds.feature_id)\nntime = len(ds.time)\n\nstreamflow_chunk_plan = {'time': ntime, 'feature_id': 1}\nbytes_per_value = ds.streamflow.dtype.itemsize\ntotal_bytes = streamflow_chunk_plan['time'] * streamflow_chunk_plan['feature_id'] * bytes_per_value\nstreamflow_MiB = total_bytes / (2 ** 10) ** 2\nprint(\"STREAMFLOW \\n\"\n      f\"Shape of chunk: {streamflow_chunk_plan} \\n\"\n      f\"Partial 'time' chunk remainder: {(ntime % streamflow_chunk_plan['time']) / streamflow_chunk_plan['time']} \\n\"\n      f\"Partial 'feature_id' chunk remainder: {(nfeature % streamflow_chunk_plan['feature_id']) / streamflow_chunk_plan['feature_id']} \\n\"\n      f\"Chunk size: {streamflow_MiB:.2f} [MiB] \\n\")\n\nchunks_per_dim = 3\nvelocity_chunk_plan = {'time': ntime // chunks_per_dim, 'feature_id': nfeature // chunks_per_dim}\nbytes_per_value = ds.velocity.dtype.itemsize\ntotal_bytes = velocity_chunk_plan['time'] * velocity_chunk_plan['feature_id'] * bytes_per_value\nvelocity_MiB = total_bytes / (2 ** 10) ** 2\nprint(\"VELOCITY \\n\"\n      f\"Shape of chunk: {velocity_chunk_plan} \\n\"\n      f\"Partial 'time' chunk remainder: {(ntime % velocity_chunk_plan['time']) / velocity_chunk_plan['time']} \\n\"\n      f\"Partial 'feature_id' chunk remainder: {(nfeature % velocity_chunk_plan['feature_id']) / velocity_chunk_plan['feature_id']} \\n\"\n      f\"Chunk size: {velocity_MiB:.2f} [MiB]\")\n\n\n\nThe time-series streamflow chunking for a single feature ID has way too small of a chunk size.\nIf we increase the number of feature IDs per chunk by a factor of 1000, we should have the right size for that chunk\nAs for velocity, the split of the data into three chunks along both dimensions resulted in no partial chunks and a chunk size of 111 MiB.\nThis is within our optimal chunk size range.\nSo, let’s stick with the velocity chunks and recheck the streamflow chunking with 1000 features per time series chunk.\n\nstreamflow_chunk_plan = {'time': ntime, 'feature_id': 1000}\nbytes_per_value = ds.streamflow.dtype.itemsize\ntotal_bytes = streamflow_chunk_plan['time'] * streamflow_chunk_plan['feature_id'] * bytes_per_value\nstreamflow_MiB = total_bytes / (2 ** 10) ** 2\nprint(\"STREAMFLOW \\n\"\n      f\"Chunk of shape {streamflow_chunk_plan} \\n\"\n      f\"Partial 'time' chunk remainder: {(ntime % streamflow_chunk_plan['time']) / streamflow_chunk_plan['time']} \\n\"\n      f\"Partial 'feature_id' chunk remainder: {(nfeature % streamflow_chunk_plan['feature_id']) / streamflow_chunk_plan['feature_id']} \\n\"\n      f\"Chunk size: {streamflow_MiB:.2f} [MiB] \\n\")\n\n\n\nAlright, no remainders and a reasonable chunk size.\nTime to get to rechunking!\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/rechunking#choose-a-new-chunk-shape-and-size","position":7},{"hierarchy":{"lvl1":"Rechunking","lvl2":"Rechunk with Rechunker"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/rechunking#rechunk-with-rechunker","position":8},{"hierarchy":{"lvl1":"Rechunking","lvl2":"Rechunk with Rechunker"},"content":"This is a relatively trivial example, due to the smaller size of the subset of the dataset.\nAs the whole subset can fit into memory easily, chunking in general is largely unnecesary in terms of optimizing I/O (however, parallelism is still a consideration).\nBut it is worth doing, as the concepts will apply if we take this to the full dataset.\n\nFirst thing we need to determine is a chunk plan to describe the chunk layout we want.\nWe already created a basic version of this above that describes the chunk shapes of streamflow and velocity.\nWe just need to place it into a more cohesive list along with chunk shapes for the coordinates.\n\nchunk_plan = {\n    'streamflow': streamflow_chunk_plan,\n    'velocity': velocity_chunk_plan,\n     # We don't want any of the coordinates chunked\n    'latitude': (nfeature,),\n    'longitude': (nfeature,),    \n    'time': (ntime,),\n    'feature_id': (nfeature,)\n}\nchunk_plan\n\n\n\nWith this plan, we can ask \n\nRechunker to re-write the data using the prescribed chunking pattern.\nRechunker will read the data and rechunk it using an intermediate Zarr store for efficiency.\nThis will produce a new Zarr store with the chunk shapes we specified in chunk_plan.\n\noutfile = \"rechunked_nwm.zarr\"\ntemp_store = \"/tmp/scratch.zarr\"\ntry:\n    result = rechunk(\n        ds,\n        target_chunks=chunk_plan,\n        max_mem=\"2GB\",\n        target_store=outfile ,\n        temp_store=temp_store\n    )\n    display(result)\nexcept Exception as e:\n    print(e)\n\n\n\nOh, that is not what we wanted!\nWe seem to have gotten an error indicating overlap in chunks between the read and write.\nLooking at the error, it is saying that the first time chunk we are reading is a partial chunk and not a full chunk.\nSo, when Rechunker tries to read the data and then write the first rechunk, it is having to read two chunks to write to the one chunk.\nThis is a one-to-many write, which can corrupt our file when done in parallel with dask.\nThank goodness Rechunker caught this for us!\nReading the recommended fix, it seems the only way to go about this is to call chunk() and reset the chunking on the original data.\nIn other words, after we select the subset from the dataset, we need to realign the chunks such that the first chunk is not a partial chunk.\nThis is simple enough to do.\nSo much so, we can just do it when passing the dataset subset to Rechunker.\n\nNote\n\nrechunker.rechunk does not overwrite any data.\nIf it sees that rechunked_nwm.zarr or /tmp/scratch.zarr already exist, it will raise an exception.\nBe sure that these locations do not exist before calling Rechunker.\n\n# We must delete the started rechunked zarr stores\nshutil.rmtree(outfile)\nshutil.rmtree(temp_store)\n\nresult = rechunk(\n    ds.chunk({'time': 672, 'feature_id': 15000}),\n    target_chunks=chunk_plan,\n    max_mem=\"2GB\",\n    target_store=outfile ,\n    temp_store=temp_store\n)\nresult\n\n\n\nAlright, that worked with no problems!\nNow, we must specifically direct rechunk to calculate.\nTo do this, we can call execute() on our result Rechunked object.\nWithout the call to execute(), the Zarr dataset will be empty, and result will only hold a ‘task graph’ outlining the calculation steps.\n\nTip\n\nRechunker also writes a minimalist data group, meaning that variable metadata is not consolidated.\nThis is not a required step, but it will really spead up future workflows when the data is read back in using xarray.\n\n_ = result.execute()\n_ = zarr.consolidate_metadata(outfile)\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/rechunking#rechunk-with-rechunker","position":9},{"hierarchy":{"lvl1":"Rechunking","lvl2":"Confirm the Creation of the Zarr Store by Rechunker"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/rechunking#confirm-the-creation-of-the-zarr-store-by-rechunker","position":10},{"hierarchy":{"lvl1":"Rechunking","lvl2":"Confirm the Creation of the Zarr Store by Rechunker"},"content":"Let’s read in the resulting re-chunked dataset to confirm it turned out how we intended.\n\nds_rechunked = xr.open_zarr(outfile)\nds_rechunked\n\n\n\nGreat, our chunk shapes are exactly what we specified!\nLet’s do one final check that will compare our original subset of the dataset with our new rechunked dataset, that way we can confirm nothing unexpected happened during rechunking.\n\nNote\n\nFor our small example dataset this is easy as the data will fit into memory.\nMore efficient and better ways should be used for larger datasets.\n\nnp.abs(ds - ds_rechunked).compute().max()\n\n\n\nPerfect!\nThe maximum absolute difference between each both the streamflow and velocity variables is 0.\nIn other words, they are exactly the same, and Rechunker worked as expect.\n\nNow that you know how to rechunk a Zarr store using Rechunker, you should know all of the basics there are in terms of chunking.\nYou are now ready to explore more \n\nadvanced chunking topics in chunking if you are interested!","type":"content","url":"/dataset-processing/tutorials/chunking/101/rechunking#confirm-the-creation-of-the-zarr-store-by-rechunker","position":11},{"hierarchy":{"lvl1":"Rechunking","lvl2":"Clean Up"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/rechunking#clean-up","position":12},{"hierarchy":{"lvl1":"Rechunking","lvl2":"Clean Up"},"content":"As we don’t want to keep this rechunked Zarr on our local machine, let’s go ahead and delete it.\n\nshutil.rmtree(outfile)\nshutil.rmtree(temp_store)\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/rechunking#clean-up","position":13},{"hierarchy":{"lvl1":"Why (re)Chunk Data?"},"type":"lvl1","url":"/dataset-processing/tutorials/chunking/101/whychunk","position":0},{"hierarchy":{"lvl1":"Why (re)Chunk Data?"},"content":"If you are completely new to chunking, then you are probably interested in learning “what is data chunking?” and “why should I care?”.\nThe goal of this notebook is to answer these two basic questions and give you the understanding of what it means for data to be chunked and why you would want to do it.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/whychunk","position":1},{"hierarchy":{"lvl1":"Why (re)Chunk Data?","lvl2":"What is chunking?"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/whychunk#what-is-chunking","position":2},{"hierarchy":{"lvl1":"Why (re)Chunk Data?","lvl2":"What is chunking?"},"content":"Since modern computers were invented, there have existed datasets that were too large to fully read into computer memory.\nThese datasets have come to be known as “\n\nlarger-than-memory” datasets.\nWhile these datasets may be larger than memory, we will still want to access them and perform analysis on the data.\nThis is where chunking comes in.\n“\n\nChunking” is the process of breaking down large amounts of data into smaller, more manageable pieces.\nBy breaking the data down into “\n\nchunks”, it allows for us to work with the chunks of the larger overall dataset using a structured approach without exceeding our machine’s available memory.\nAdditionally, proper chunking can allow for faster retrieval and analysis when we only need to work with part of the dataset.\n\nNote\n\nChunks are not another dimension to your data, but merely a map to how the dataset is partitioned into more palatable sized units for manipulation in memory.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/whychunk#what-is-chunking","position":3},{"hierarchy":{"lvl1":"Why (re)Chunk Data?","lvl2":"Why should I care?"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/whychunk#why-should-i-care","position":4},{"hierarchy":{"lvl1":"Why (re)Chunk Data?","lvl2":"Why should I care?"},"content":"The simple reason you should care is that you are working with a dataset that is larger-than-memory.\nThis dataset has to be divided in some way so that only those parts of the data being actively worked on are loaded into memory at a given time; otherwise, your machine would crash.\nThis has benefits when it comes to parallel algorithms - if work can be performed on independent chunks, it is easy to set up your algorithm such that separate parallel workers each work on a chunk of the data simultaneously.\nTherefore, proper chunking can allow for faster retrieval and analysis of the dataset.\nEven datasets that are small enough to fit into memory can still technically be chunked, and proper chunking of these datasets can potentially speed up retrieval and analysis.\nTo help you understand this, let’s begin with a simple example.","type":"content","url":"/dataset-processing/tutorials/chunking/101/whychunk#why-should-i-care","position":5},{"hierarchy":{"lvl1":"Why (re)Chunk Data?","lvl3":"Example - First Principles","lvl2":"Why should I care?"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/101/whychunk#example-first-principles","position":6},{"hierarchy":{"lvl1":"Why (re)Chunk Data?","lvl3":"Example - First Principles","lvl2":"Why should I care?"},"content":"In this example, we will illustrate two common memory organization strategies (analagous to chunking) that computers use when handling basic multidimensional data.\nTo simplify this, let’s consider a small 10x10 array of integer values.\\def\\arraystretch{2.0}\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|}\n   \\hline\n   0,0 & 0,1 & 0,2 & 0,3 & 0,4 & 0,5 & 0,6 & 0,7 & 0,8 & 0,9\\\\\n   \\hline\n   1,0 & 1,1 & 1,2 & 1,3 & 1,4 & 1,5 & 1,6 & 1,7 & 1,8 & 1,9\\\\\n   \\hline\n   2,0 & 2,1 & 2,2 & 2,3 & 2,4 & 2,5 & 2,6 & 2,7 & 2,8 & 2,9\\\\\n   \\hline\n    3,0 & 3,1 & 3,2 & 3,3 & 3,4 & 3,5 & 3,6 & 3,7 & 3,8 & 3,9\\\\\n   \\hline\n    4,0 & 4,1 & 4,2 & 4,3 & 4,4 & 4,5 & 4,6 & 4,7 & 4,8 & 4,9\\\\\n   \\hline\n    5,0 & 5,1 & 5,2 & 5,3 & 5,4 & 5,5 & 5,6 & 5,7 & 5,8 & 5,9\\\\\n    \\hline\n   6,0 & 6,1 & 6,2 & 6,3 & 6,4 & 6,5 & 6,6 & 6,7 & 6,8 & 6,9\\\\\n    \\hline\n   7,0 & 7,1 & 7,2 & 7,3 & 7,4 & 7,5 & 7,6 & 7,7 & 7,8 & 7,9\\\\\n    \\hline\n   8,0 & 8,1 & 8,2 & 8,3 & 8,4 & 8,5 & 8,6 & 8,7 & 8,8 & 8,9\\\\\n    \\hline\n   9,0 & 9,1 & 9,2 & 9,3 & 9,4 & 9,5 & 9,6 & 9,7 & 9,8 & 9,9\\\\\n\\hline\n\\end{array}\n\nWhile this is easy for us humans to visualize, computer memory is not addressed in grids.\nInstead, it is organized as a linear address space.\nSo, the 2D matrix has to be organized in memory such that it presents as 2D, while being stored as 1D.\nTwo common options are row-major order, and column-major order:\n\nRow-Major: A row of data occupies a contiguous block of memory.\nThis implies that cells which are logically adjacent vertically are not physically near one another in memory.\nThe “distance” from r0c0 to r0c1 (a one-cell logical move within the row) is short, while the “distance” to r1c0 (a one-cell logical move within the column) is long.\\def\\arraystretch{2.0}\n\\begin{array}{|c|c|c|c|c c|c|c|c|c| c}\n   \\hline\n   0,0 & 0,1 & 0,2 & 0,3 & ... & ... & 1,0  & 1,1 & 1,2 & 1,3 & ... \\\\\n   \\hline\n\\end{array}\n\nColumn-Major: A column of the array occupies a contiguous block of memory.\nThis implies that cells which are adjacent horizontally are not near one another physically in memory.\\def\\arraystretch{2.0}\n\\begin{array}{|c|c|c|c|c c|c|c|c|c| c}\n   \\hline\n   0,0 & 1,0 & 2,0 & 3,0 & ... & ... & 0,1  & 1,1 & 2,1 & 3,1 & ... \\\\\n   \\hline\n\\end{array}\n\nIn either mapping, r3c5 (for example) still fetches the same value.\nFor a single value, this is not a problem.\nThe array is still indexed/addressed in the same way as far as the user is concerned, but the memory organization strategy determines how nearby an “adjacent” index is.\nThis becomes important when trying to get a subsection of the data.\nFor example, if the array is in row-major order and we select say r0, this is fast for the computer as all the data is adjacent.\nHowever, if we wanted c0, then the computer has to access every 10th value in memory, which as you can imagine is not as efficient.","type":"content","url":"/dataset-processing/tutorials/chunking/101/whychunk#example-first-principles","position":7},{"hierarchy":{"lvl1":"Why (re)Chunk Data?","lvl3":"Extend to Chunking","lvl2":"Why should I care?"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/101/whychunk#extend-to-chunking","position":8},{"hierarchy":{"lvl1":"Why (re)Chunk Data?","lvl3":"Extend to Chunking","lvl2":"Why should I care?"},"content":"The basic idea behind chunking is an extension of this memory organization principle.\nAs the size of the array increases, the chunk shape becomes more relevant.\nNow suppose the square array is now larger-than-memory and stored on disk such that only a single row or column can fit into memory at a time.\nIf your data is chunked by row, and you need to process the i^{th} column, you will have to read one row at a time into memory, skip to the i^{th} column value in each row, and extract that value.\nFor this type of analysis, you can see why this would be slow due to the massive amount of I/O and it would be better if the array could instead be chunked in column-major order.\nJust to make this clear, if your data was now chunked by columns, all you would have to do is read the i^{th} column into memory, and you would be good to go.\nMeaning you would just need a single read from disk versus reading however many rows your data has.\nWhile handling chunks may seem like it would become complicated, array-handling libraries (\n\nnumpy, \n\nxarray, \n\npandas, \n\ndask, and others) will handle all of the record-keeping to know which chunk holds what data within the dataset.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/whychunk#extend-to-chunking","position":9},{"hierarchy":{"lvl1":"Why (re)Chunk Data?","lvl2":"Toy Example"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/whychunk#toy-example","position":10},{"hierarchy":{"lvl1":"Why (re)Chunk Data?","lvl2":"Toy Example"},"content":"By now, we have hopefully answered both of the questions about “what is data chunking?” and “why should I care?”.\nTo really drive home the idea, let’s apply the above theoretical example using \n\ndask.\nIn this case, we will generate a square array of ones to test how different “\n\nchunk shapes” compare.\n\nimport dask.array as da\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/whychunk#toy-example","position":11},{"hierarchy":{"lvl1":"Why (re)Chunk Data?","lvl3":"Chunk by Rows","lvl2":"Toy Example"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/101/whychunk#chunk-by-rows","position":12},{"hierarchy":{"lvl1":"Why (re)Chunk Data?","lvl3":"Chunk by Rows","lvl2":"Toy Example"},"content":"First, let’s start with the square array chunked by rows.\nWe’ll do a 50625x50625 array as this is about 19 GiB, which is larger than the typical memory availability of a laptop.\nThe nice thing about dask is that we can see how big our array and chunks are in the output.\n\nvals = da.ones(shape=(50625, 50625), chunks=(1, 50625))\nvals\n\n\n\nNow, let’s see how long on average it takes to get the first column.\n\nNote\n\nWe use the .compute() method on our slice to ensure its extraction is not lazily performed.\n\n%%timeit\nvals[:, 0].compute()\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/whychunk#chunk-by-rows","position":13},{"hierarchy":{"lvl1":"Why (re)Chunk Data?","lvl3":"Chunk by Columns","lvl2":"Toy Example"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/101/whychunk#chunk-by-columns","position":14},{"hierarchy":{"lvl1":"Why (re)Chunk Data?","lvl3":"Chunk by Columns","lvl2":"Toy Example"},"content":"Switching the array to be chunked by columns.\n\nvals = da.ones(shape=(50625, 50625), chunks=(50625, 1))\nvals\n\n\n\nTime to see how much faster this is.\n\n%%timeit\nvals[:, 0].compute()\n\n\n\nAs expected, the time difference is massive when properly chunked.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/whychunk#chunk-by-columns","position":15},{"hierarchy":{"lvl1":"Why (re)Chunk Data?","lvl3":"Balanced Chunks","lvl2":"Toy Example"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/101/whychunk#balanced-chunks","position":16},{"hierarchy":{"lvl1":"Why (re)Chunk Data?","lvl3":"Balanced Chunks","lvl2":"Toy Example"},"content":"As a final example, let’s check a square chunk shape that keeps about the same number of elements as the pure row and column chunking.\n\nvals = da.ones(shape=(50625, 50625), chunks=(225, 225))\nvals\n\n\n\n%%timeit\nvals[:, 0].compute()\n\n\n\nAs we can see, this is only slightly slower when accessing the first column compared to the column chunking.\nHowever, let’s time how long it takes to access a single row.\n\n%%timeit\nvals[0, :].compute()\n\n\n\nAs expected, it is about the same as accessing a single column.\nHowever, that means it is drastically faster than the column chunking when accessing rows.\nTherefore, a chunk shape that balances the dimensions is more generally applicable when both dimensions are needed for analysis.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/whychunk#balanced-chunks","position":17},{"hierarchy":{"lvl1":"Why (re)Chunk Data?","lvl2":"Pros & Cons to Chunking"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/whychunk#pros-cons-to-chunking","position":18},{"hierarchy":{"lvl1":"Why (re)Chunk Data?","lvl2":"Pros & Cons to Chunking"},"content":"As a wrap up, let’s review some of the pros and cons to chunking.\nSome we have clearly discussed while others may be more subtle.\nThe primary pro, as we hopefully conveyed with our previous example, is that well chunked data substantially speeds up any analysis that favors that chunk shape.\nHowever, this becomes a con when you change your analysis to one that favors a new chunk shape.\nIn other words, data that is well-organized to optimize one kind of analysis may not suit another kind of analysis on the same data.\nWhile not a problem for our example here, changing the chunk shape (known as “\n\nrechunking”) on an established dataset is time-consuming, and it produces a separate copy of the dataset, increasing storage requirements.\nThe space commitment can be substantial if a complex dataset needs to be organized for many different analyses.\nIf our example above used unique values that we wanted to keep as we changed chunking, this would have meant that rather than having a single ~19 GiB dataset, we would have needed to keep all three, tripling our storage to almost 60 GiB.\nTherefore, selecting an appropriate chunk shape is critical when generating widely used datasets.","type":"content","url":"/dataset-processing/tutorials/chunking/101/whychunk#pros-cons-to-chunking","position":19},{"hierarchy":{"lvl1":"Writing Chunked Files"},"type":"lvl1","url":"/dataset-processing/tutorials/chunking/101/writechunkedfiles","position":0},{"hierarchy":{"lvl1":"Writing Chunked Files"},"content":"Note\n\nThis notebook is based on the \n\nXarray User-guide for reading and writing Zarr.\n\nThe goal of this notebook is to learn how to load a collection of NetCDF files, chunk the data, write the data in Zarr format, and confirm the proper creation of the Zarr store.\nWe will be writing to our local storage for simplicity (as this is just a tutorial notebook), but you can easily change the output path to be anywhere including cloud storage.\n\nimport fsspec\nimport xarray as xr\nimport numpy as np\nimport hvplot.xarray\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/writechunkedfiles","position":1},{"hierarchy":{"lvl1":"Writing Chunked Files","lvl2":"Example Dataset"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/writechunkedfiles#example-dataset","position":2},{"hierarchy":{"lvl1":"Writing Chunked Files","lvl2":"Example Dataset"},"content":"In this notebook, we will use the daily gridMET precipitation dataset as an example for reading data and writing to Zarr.\nThe data is currently hosted on the HyTEST OSN as a collection of NetCDF files.\nTo get the files, we will use \n\nfsspec to open each year of precipitation data to a list.\nThen, we can read in the all the files at once using \n\nxarray.open_mfdataset.\n\nfs = fsspec.filesystem(\n    's3',\n    anon=True,   # anonymous = does not require credentials\n    client_kwargs={'endpoint_url': 'https://usgs.osn.mghpcc.org/'}\n)\nprecip_files = [fs.open(file) for file in fs.glob('s3://mdmf/gdp/netcdf/gridmet/gridmet/pr_*.nc')]\nds = xr.open_mfdataset(\n    precip_files,\n    chunks={},\n    engine='h5netcdf'\n)\nds\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/writechunkedfiles#example-dataset","position":3},{"hierarchy":{"lvl1":"Writing Chunked Files","lvl2":"Selecting Chunk Shape and Size"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/writechunkedfiles#selecting-chunk-shape-and-size","position":4},{"hierarchy":{"lvl1":"Writing Chunked Files","lvl2":"Selecting Chunk Shape and Size"},"content":"As we can see in the rich HTML output of our dataset, the NetCDF files were already chunked with pattern of {'day': 61, 'lat': 98, 'lon': 231}.\nHowever, the size of these chunks are relatively small and near the lower limit of an acceptable chunk size of 10 MiB.\nSo, it would be better if we could increase our chunk sizes to say 70-110 MiB.\nTo do this, we will simply use multiples of the current chunks so we don’t have to completely rechunk the dataset (grouping chunks is way faster than completely changing chunk shape).\nOur goal will be to try and evenly distribute the number of chunks between the time and space dimensions.\n\nFirst, let’s check how many chunks are in the temporal and spatial dimensions with the current chunk shape.\n\nds.precipitation_amount.encoding\n\n\n\ntime_chunk_shape, lat_chunk_shape, lon_chunk_shape = ds.precipitation_amount.encoding['chunksizes']\n\nprint(f\"Number of temporal chunks: {np.ceil(len(ds.day) / time_chunk_shape)}\")\nprint(f\"Number of spatial chunks: {np.ceil(len(ds.lat) / lat_chunk_shape) * np.ceil(len(ds.lon) / lon_chunk_shape)}\")\n\n\n\nOkay, so there are about 7 times as many time chunks as there are spatial chunks.\nLet’s try to balance this better so there are an approximately equal amount.\nWe will do this by simply increasing the time chunk shape by a factor of 7.\nThen, we can double check our chunk size to see if it aligns with our chunk size goal.\n\nNote\n\nAlso by increasing our temporal chunk shape by a factor of seven, it is now larger than a whole year.\nThis means if we only wanted a single year, we would at most need to read two chunks.\n\nchunk_plan = {\n    \"day\": time_chunk_shape * 7, \n    \"lat\" : lat_chunk_shape,\n    \"lon\" : lon_chunk_shape\n}\n\nbytes_per_value = ds.precipitation_amount.dtype.itemsize\n\nprint(f\"Chunk Plan:{chunk_plan}\")\nprint(f\"Number of temporal chunks: {np.ceil(len(ds.day) / chunk_plan['day'])}\")\nprint(f\"Number of spatial chunks: {np.ceil(len(ds.lat) / chunk_plan['lat']) * np.ceil(len(ds.lon) / chunk_plan['lon'])}\")\nprint(f\"Chunk size in MiB: {(bytes_per_value * chunk_plan['day'] * chunk_plan['lat'] * chunk_plan['lon']) / 2 ** 10 / 2 ** 10:0.2f}\")\n\n\n\nNice!\nThat got us the exact same number of chunks and the chunk size we were looking for.\nAs a final check, let’s make sure none of the ending chunks contain <50% data.\n\nprint(f\"Number of day chunks: {(len(ds.day) / chunk_plan['day'])}\")\nprint(f\"Number of lat chunks: {(len(ds.lat) / chunk_plan['lat'])}\")\nprint(f\"Number of lon chunks: {(len(ds.lon) / chunk_plan['lon'])}\")\n\n\n\nPerfect!\nSo the final day chunk is only 77% full, but that is good enough.\nLet’s now chunk the dataset to our chunking plan.\n\nds = ds.chunk(chunk_plan)\nds\n\n\n\nNow, let’s save this data to a Zarr!\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/writechunkedfiles#selecting-chunk-shape-and-size","position":5},{"hierarchy":{"lvl1":"Writing Chunked Files","lvl2":"Writing Chunked Zarr"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/writechunkedfiles#writing-chunked-zarr","position":6},{"hierarchy":{"lvl1":"Writing Chunked Files","lvl2":"Writing Chunked Zarr"},"content":"\n\nAs discussed in the \n\nXarray User-guide for reading and writing Zarr, chunks are specified to our Zarr store in one of three ways in the preferential order of:\n\nManual chunk sizing through the use of the encoding argument\n\nAutomatic chunking based on chunks of the dask arrays\n\nDefault chunk behavior determined by the Zarr library\n\nIn our case, we updated the dask array chunks by calling ds.chunk().\nTherefore, we have the correct chunks and should be good to go.\n\nTip\n\nThis is our preferred method over using the encoding argument, as the positional ordering of the chunk shape in the encoding argument must match the positional ordering of the dimensions in each array.\nIf they do not match you can get incorrect chunk shapes in the Zarr store.\n\nIf you have multiple variables, using encoding could allow for specifying individual chunking shapes for each variable.\nHowever, if this is the case, we recommend updating each variable individually using, for example, ds.precipitation_amount.chunk() to change the individual variable chunk shape.\n\n# Let's actually use only one time chunk as this is an\n# example and we don't want to write 90 GiB\nds_write = ds.isel(day=slice(0, chunk_plan['day']))\n\noutfile = \"gridmet_precip.zarr\"\n_ = ds_write.to_zarr(outfile)\n\n\n\nNow, let’s read in the just-written dataset to verify its integrity and chunks.\n\nds_read = xr.open_dataset(outfile, engine='zarr', chunks={})\nds_read\n\n\n\nGreat! Everything looks good!\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/writechunkedfiles#writing-chunked-zarr","position":7},{"hierarchy":{"lvl1":"Writing Chunked Files","lvl2":"Assessing Compression"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/writechunkedfiles#assessing-compression","position":8},{"hierarchy":{"lvl1":"Writing Chunked Files","lvl2":"Assessing Compression"},"content":"Now that our Zarr store is made, let’s check how much the data was compressed.\nBy default, \n\nZarr uses the Blosc compressor when calling \n\nxarray.Dataset.to_zarr() if we don’t specify a compressor in the encoding.\nSo, our data should be compressed by default, and we can examine each chunk on disk to confirm their compression factor.\n\nTo examine this, let’s create a new local filesystem to get the file info.\n\nfs_local = fsspec.filesystem('local')\n\n\n\nNow, estimate the file sizes.\n\n# Exclude any hidden files (i.e., starts with .)\nchunkfiles = fs_local.glob(outfile + \"/precipitation_amount/[!.]*\")\nfilesize = np.array([fs_local.du(file) for file in chunkfiles])\nfilesize_MiB = xr.DataArray(data=filesize / 2**10 / 2**10, name='Size', attrs={'units': 'MiB'})\n\nprint(\"Stored Chunk Size Summary Statistics [MiB]:\\n\"\n      f\"Total: {filesize_MiB.sum():.4f}\\n\"\n      f\"Minimum: {filesize_MiB.min():.4f}\\n\"\n      f\"Mean:  {filesize_MiB.mean():.4f}\\n\"\n      f\"Maximum: {filesize_MiB.max():.4f}\")\nfilesize_MiB.hvplot.hist(ylabel='Counts')\n\n\n\nAs we can see, the total dataset (excluding coordinates) is only 85 MiB on disk, with chunk sizes varying from 75 KiB to 4.9 MiB.\nThis size is drastically smaller than the quoted total size for the xarray output, which said 2.58 GiB.\nSame for the individual chunks, which were quoted at 73.75 MiB.\nLet’s get an exact comparison and compression ratio for the data we read in.\n\nbytes_per_value = ds_read.precipitation_amount.dtype.itemsize\ntotal_size = ds_read['precipitation_amount'].size * bytes_per_value\nchunk_size = np.array(ds_read['precipitation_amount'].encoding['chunks']).prod() * bytes_per_value\nprint(\"Read-in Chunk Size Summary Statistics:\\n\"\n      f\"Total: {total_size / (2**10)**3:.4f} [GiB]\\n\"\n      f\"Chunks: {chunk_size / (2**10)**2:.4f} [MiB]\")\n\nprint(\"\\n\"\n      \"Compression Ratio Summary Statistics:\\n\"\n      f\"Total: {total_size / filesize.sum():.3f}\\n\"\n      f\"Minimum: {(chunk_size / filesize).min():.4f}\\n\"\n      f\"Mean:  {(chunk_size / filesize).mean():.4f}\\n\"\n      f\"Maximum: {(chunk_size / filesize).max():.4f}\")\n\n\n\nOoph! This tells us that we get an astonishing average compression ratio of 30:1 on this particular data.\nPretty good results.","type":"content","url":"/dataset-processing/tutorials/chunking/101/writechunkedfiles#assessing-compression","position":9},{"hierarchy":{"lvl1":"Writing Chunked Files","lvl2":"Appending New Chunk"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/writechunkedfiles#appending-new-chunk","position":10},{"hierarchy":{"lvl1":"Writing Chunked Files","lvl2":"Appending New Chunk"},"content":"Since this compression is so good, let’s go ahead and add another time chunk onto our existing Zarr store.\nThis is simple in xarray, especially since we are just appending another time chunk.\nAll we have to do is \n\nadd append_dim to our .to_zarr() call to append to the time dimension.\n\nds_write = ds.isel(day=slice(chunk_plan['day'], chunk_plan['day']* 2))\n_ = ds_write.to_zarr(outfile, append_dim='day')\n\n\n\nNow, let’s read in the appended dataset to verify everything worked.\n\nds_read = xr.open_dataset(outfile, engine='zarr', chunks={})\nds_read\n\n\n\nThat looks like it worked as expected!\nLet’s now double check the compression on these files.\n\n# Exclude any hidden files (i.e., starts with .)\nchunkfiles = fs_local.glob(outfile + \"/precipitation_amount/[!.]*\")\nfilesize = np.array([fs_local.du(file) for file in chunkfiles])\nfilesize_MiB = xr.DataArray(data=filesize / 2**10 / 2**10, name='Size', attrs={'units': 'MiB'})\n\nprint(\"Stored Chunk Size Summary Statistics [MiB]:\\n\"\n      f\"Total: {filesize_MiB.sum():.4f}\\n\"\n      f\"Minimum: {filesize_MiB.min():.4f}\\n\"\n      f\"Mean:  {filesize_MiB.mean():.4f}\\n\"\n      f\"Maximum: {filesize_MiB.max():.4f}\")\nfilesize_MiB.hvplot.hist(ylabel='Counts')\n\n\n\nLooks like we stayed at the same levels of compression which is great.","type":"content","url":"/dataset-processing/tutorials/chunking/101/writechunkedfiles#appending-new-chunk","position":11},{"hierarchy":{"lvl1":"Writing Chunked Files","lvl2":"Clean Up"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/101/writechunkedfiles#clean-up","position":12},{"hierarchy":{"lvl1":"Writing Chunked Files","lvl2":"Clean Up"},"content":"So, hopefully now you know the basics of how to create a Zarr store from some NetCDF files and set its chunks’ shape.\nThe same methods would apply when rechunking a dataset, which we will get into next.\n\nAs we don’t want to keep this Zarr on our local machine, let’s go ahead and delete it.\n\n# Clean up by deleting the zarr store\nfs_local.rm(outfile, recursive=True)\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/101/writechunkedfiles#clean-up","position":13},{"hierarchy":{"lvl1":"Introduction to Chunking"},"type":"lvl1","url":"/dataset-processing/tutorials/chunking/101","position":0},{"hierarchy":{"lvl1":"Introduction to Chunking"},"content":"In this first series of notebooks, we will go over basic introductory topics associated with chunking.\nAs you will soon learn, “chunking” is an an essential part of the data preparation workflow, particularly for large datasets.\nThe key concepts you should understand have after this series include:\n\nHyTEST Project\n\nEnvironment Set-Up\n\nCompute Environments\n\nGeneral QuickStart\n\nCloud: Nebari Quick-Start\n\nHPC Server: Open OnDemand Quick-Start\n\nHPC Server: Jupyter Forward Quick-Start\n\nHPC Server: Start Script Quick-Start\n\nAWS Credentials\n\nStarting a Dask Cluster\n\nNebari Dask Cluster\n\nDenali HPC Dask Cluster\n\nHovenweep/Tallgrass HPC Dask Cluster\n\nLocal Desktop Dask Cluster\n\nGetting Started\n\nEssential Reading\n\nFoundations\n\nData Sources\n\nData/APIs\n\nData/Cloud Storage\n\nParallelism with Dask\n\nDataset Access\n\nData Catalogs\n\nData Storage Locations\n\nWater Mission Area STAC Catalog\n\nHyTEST Data Catalog (Intake)\n\nHyTEST Intake Sub-Catalogs\n\nCONUS404 Products Data Access\n\nCONUS404 Zarr Changelog\n\nExplore CONUS404 Dataset\n\nZarr Creation\n\nData Chunking Tutorial Overview\n\nIntroduction to Chunking\n\nWhy (re)Chunk Data?\n\nHow to Examine a Stored Dataset’s Chunk Shape\n\nBasics of Chunk Shape and Size\n\nWriting Chunked Files\n\nRechunking\n\nAdvanced Topics in Chunking\n\nRechunking Larger Datasets with Dask\n\nGenerating a Virtual Zarr Store\n\nAdding Coordinate Reference Systems (CRS) to Zarr Datasets\n\nChoosing an Optimal Chunk Size and Shape\n\nChunking References\n\nGlossary\n\nAdditional Resources\n\nDataset Processing\n\nCONUS404 Temporal Aggregation\n\nSpatial Aggregation\n\ngdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s\n\ngdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1\n\ngdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s\n\nPangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s\n\ngdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation\n\nCONUS404 Site Data Selection\n\nCONUS404 Regridding (Curvilinear => Rectilinear)\n\nModel Evaluation\n\nEvaluation Metrics\n\nD-Score Suite (v1) Benchmark\n\nD-Score Suite (v1) Benchmark -- Usage Examples\n\nStandard Suite (v1) Metrics\n\nStandard Suite (v1) Metrics -- Usage Examples\n\nStreamflow Model Evaluation\n\nStreamflow Eval :: Data Preparation\n\nStreamflow Eval :: StdSuite Analysis\n\nSteamflow Eval :: DScore Analysis\n\nStreamflow Eval :: Visualization\n\nBack Matter\n\nContributing\n\nLicense\n\nBuckle up... we will get up to speed fast.","type":"content","url":"/dataset-processing/tutorials/chunking/101","position":1},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store"},"type":"lvl1","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr","position":0},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store"},"content":"Note\n\nThis notebook builds off the \n\nKerchunk and \n\nVirtualiZarr docs.\n\nThe objective of this notebook is to learn how to create a virtual Zarr store for a collection of NetCDF files that together make up a complete dataset.\nTo do this, we will use \n\nKerchunk and \n\nVirtualiZarr.\nAs these two packages can both create virtual Zarr stores but do it in different ways, we will utilize them both to show how they compare in combination with \n\ndask for parallel execution.\n\nimport os\n# Needed when boto3 >= 1.36.0 or the rechunking process will fail\n# This needs to be set before the boto3 library gets loaded\n# See: https://github.com/aws/aws-cli/issues/9214#issuecomment-2606619168\nos.environ['AWS_REQUEST_CHECKSUM_CALCULATION'] = 'when_required'\nos.environ['AWS_RESPONSE_CHECKSUM_VALIDATION'] = 'when_required'\nimport fsspec\nimport xarray as xr\nimport ujson\nimport time\nimport kerchunk.hdf\nimport kerchunk.combine\nfrom virtualizarr import open_virtual_dataset\nimport logging\nimport dask\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr","position":1},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl2":"Kerchunk vs VirtualiZarr"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#kerchunk-vs-virtualizarr","position":2},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl2":"Kerchunk vs VirtualiZarr"},"content":"To begin, let’s explain what a virtual Zarr store even is.\nA “\n\nvirtual Zarr store” is a virtual representation of a Zarr store generated by mapping any number of real datasets in individual files (e.g., NetCDF/HDF5, GRIB2, TIFF) together into a single, sliceable dataset via an interface layer.\nThis interface layer, which Kerchunk and VirtualiZarr generate, contains information about the original files (e.g., chunking, compression, data byte location, etc.) needed to efficiently access the data.\nWhile this could be done with \n\nxarray.open_mfdataset, we don’t want to run this command every time we open the dataset as it can be a slow and expensive process.\nThe reason for this is that xarray.open_mfdataset performs many consistency checks as it runs, and it requires partially opening all of the datasets to get general metadata information on each of the individual files.\nTherefore, for numerous files, this can have significant overhead, and it would be preferable to just cache these checks and metadata for more performant future reads.\nThis cache (specifically in Zarr format) is what a virtual Zarr store is.\nOnce we have the virtual Zarr store, we can open the combined xarray dataset using \n\nxarray.open_dataset for an almost instantaneous read.\n\nNow that we know what a virtual Zarr store is, let’s discuss the differences between Kerchunk and VirtualiZarr and their virtual Zarr stores.\nAt a top level, VirtualiZarr provides almost all of the same features as Kerchunk.\nThe primary difference is that Kerchunk supports non-Zarr-like virtual format, while VirtualiZarr is specifically focused on the Zarr format.\nAdditionally, Kerchunk creates the virtual Zarr store and represents it in memory using json formatting (the format used for Zarr metadata).\nAlternatively, VirtualiZarr represents the store as array-level abstractions (which can be converted to json format).\nThese abstractions can be cleanly wrapped by xarray for easy use of xarray.concat and xarray.merge commands to combine virtual Zarr stores.\nA nice table comparing the two packages can be found in the \n\nVirtualiZarr FAQs, which shows how the two packages represent virtual Zarr stores and their comparative syntax.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#kerchunk-vs-virtualizarr","position":3},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl2":"Spin up Dask Cluster"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#spin-up-dask-cluster","position":4},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl2":"Spin up Dask Cluster"},"content":"To run the virtual Zarr creation in parallel, we need to spin up a Dask cluster to schedule the various workers.\n\n%run ../../../../environment_set_up/Start_Dask_Cluster_Nebari.ipynb\n## If this notebook is not being run on Nebari, replace the above \n## path name with a helper appropriate to your compute environment.  Examples:\n# %run ../../../../environment_set_up/Start_Dask_Cluster_Denali.ipynb\n# %run ../../../../environment_set_up/Start_Dask_Cluster_Tallgrass.ipynb\n# %run ../../../../environment_set_up/Start_Dask_Cluster_Desktop.ipynb\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#spin-up-dask-cluster","position":5},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl2":"Example Comparison"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#example-comparison","position":6},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl2":"Example Comparison"},"content":"With our Dask cluster ready, let’s see how Kerchunk and VirtualiZarr can be utilized to generate a vitrual Zarr store.\nFor this example, we will use the same daily gridMET NetCDF data as used in the \n\nWriting Chunked File tutorial.\nOnly this time we will use all of the variables not just precipitation.\nThese include:\n\nprecipitation,\n\nmaximum relative humidity,\n\nminimum relative humidity,\n\nspecific humidity,\n\ndownward shortwave radiation,\n\nminimum air temperature,\n\nmaximum air temperature,\n\nwind direction, and\n\nwind speed.\n\nThe data is currently hosted on the HyTEST OSN as a collection NetCDF files.\nTo access the data with both Kerchunk and VirtualiZarr, we will use \n\nfsspec to get the list of files that we are wanting to combine into a virtual Zarr store.\n\nFirst we need to create the file system for accessing the files, and a second one for outputting the virtual Zarr store.\n\nNote\n\nWe will exclude the year 2019 for now and use it later to show how to append virtual Zarr stores.\nAlso, we will not use 2020 as it is a partial year with different chunking than the other 40 years, which is currently incompatible with Kerchunk and Virtualizarr.\n\n# These reader options will be needed for VirtualiZarr\n# We created them here to show how they fold into fsspec\nreader_options = {\n    'storage_options': {\n        'anon': True, \n        'client_kwargs': {\n            'endpoint_url': 'https://usgs.osn.mghpcc.org/'\n        }\n    }\n}\n\nfs = fsspec.filesystem(\n    protocol='s3',\n    **reader_options['storage_options']\n)\n\nfs_local = fsspec.filesystem('')\n# Make directories to save the virtual zarr stores\nfs_local.mkdirs('virtual_zarr/kerchunk', exist_ok=True)\nfs_local.mkdirs('virtual_zarr/virtualizarr', exist_ok=True)\n\nfile_glob = fs.glob('s3://mdmf/gdp/netcdf/gridmet/gridmet/*198*.nc')\nfile_glob = [file for file in file_glob if (('2020' not in file) and ('2019' not in file))]\n\n\n\nNow, we are ready to generate the virtual Zarr stores.\nFor both Kerchunk and VirtualiZarr (\n\nfor now), this consists of two steps:\n\nConvert single original data files into individual virtual Zarr stores,\n\nCombine the individual virtual Zarr stores into a single combined virtual Zarr store.\n\nWe will show these two steps seperately and how they are done for each package.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#example-comparison","position":7},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl3":"Generate Individual Virtual Zarr Stores","lvl2":"Example Comparison"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#generate-individual-virtual-zarr-stores","position":8},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl3":"Generate Individual Virtual Zarr Stores","lvl2":"Example Comparison"},"content":"","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#generate-individual-virtual-zarr-stores","position":9},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl4":"Kerchunk","lvl3":"Generate Individual Virtual Zarr Stores","lvl2":"Example Comparison"},"type":"lvl4","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#kerchunk","position":10},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl4":"Kerchunk","lvl3":"Generate Individual Virtual Zarr Stores","lvl2":"Example Comparison"},"content":"To generate the individual virtual Zarr stores with Kerchunk, we will use \n\nkerchunk.hdf.SingleHdf5ToZarr, which translates the content of one HDF5 file into Zarr metadata.\nOther translators exist in Kerchunk that can convert GeoTiffs and NetCDF3 files.\nHowever, as we are looking at NetCDF4 files (a specific version of a HDF5 file), we will use the HDF5 translator.\nAs this only translates one file, we can make a collection of \n\ndask.delayed objects that wrap the SingleHdf5ToZarr call to run it for all files in parallel.\n\n# Make a function to run in parallel with dask\n@dask.delayed\ndef generate_single_virtual_zarr(file):\n    with fs.open(file) as hdf:\n        h5chunks = kerchunk.hdf.SingleHdf5ToZarr(hdf, file, inline_threshold=0)\n        return h5chunks.translate()\n\n# Time the duration for later comparison\nt0 = time.time()\n\n# Generate Dask Delayed objects\ntasks = [generate_single_virtual_zarr(file) for file in file_glob]\n# Compute the delayed object\nsingle_virtual_zarrs = dask.compute(*tasks)\n\nkerchunk_time = time.time() - t0\n\nsingle_virtual_zarrs[0]\n\n\n\nNotice that the output for a virtualization of a single NetCDF is a json style dictionary, where the coordinate data is actually kept in the dictionary, while the data is a file pointer and the byte range for each chunk.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#kerchunk","position":11},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl4":"VirtualiZarr","lvl3":"Generate Individual Virtual Zarr Stores","lvl2":"Example Comparison"},"type":"lvl4","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#virtualizarr","position":12},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl4":"VirtualiZarr","lvl3":"Generate Individual Virtual Zarr Stores","lvl2":"Example Comparison"},"content":"To generate the individual virtual Zarr stores with VirtualiZarr, we will use \n\nvirtualizarr.open_virtual_dataset, which can infer what type of file we are reading instead of us having to specify.\nLike Kerchunk, this only translates one file at a time.\nSo, we can make a collection of \n\ndask.delayed objects that wraps open_virtual_dataset to run it for all files in parallel.\n\nImportant\n\nWhen reading in the individual files as virtual datasets, it is critical to include the loadable_variables keyword.\nThe keyword should be set to a list of the coordinate names.\nBy adding this keyword, the coordinates are read into memory rather than being loaded as virtual data.\nThis can make a massive difference in the next steps of (1) concatenation as it gives the coordinates indexes and (2) the serialization of the virtual Zarr store as it saves the in-memory coordinates directly to the store rather than a pointer.\nAlso, if this is not included, coordinates of different sizes will not be able to be concatenated due to potential chunking differences.\nThe only downside is that it can slightly increase the time it takes to initially read the virtual datasets.\nHowever, this slowdown is more than worth the future convenience of having the coords in-memory when reading in the virtual Zarr store.\n\nt0 = time.time()\n\ntasks = [\n    dask.delayed(open_virtual_dataset)(\n        f's3://{file}',\n        indexes={},\n        loadable_variables=['day', 'lat', 'lon', 'crs'],\n        decode_times=True,\n        reader_options=reader_options\n    )\n    for file in file_glob\n]\n\nvirtual_datasets = dask.compute(*tasks)\n\nvirtualizarr_time = time.time() - t0\n\nvirtual_datasets[0]\n\n\n\nNotice that the output for a virtualization of a single NetCDF is now an xarray.Dataset, where the data is a \n\nManifestArray object.\nThis ManifestArray contains \n\nChunkManifest objects that hold the same info as the Kerchunk json format (i.e., a file pointer and the byte range for each chunk), but allows for it to be nicely wrapped by xarray.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#virtualizarr","position":13},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl3":"Combine Individual Virtual Zarr Stores","lvl2":"Example Comparison"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#combine-individual-virtual-zarr-stores","position":14},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl3":"Combine Individual Virtual Zarr Stores","lvl2":"Example Comparison"},"content":"","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#combine-individual-virtual-zarr-stores","position":15},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl4":"Kerchunk","lvl3":"Combine Individual Virtual Zarr Stores","lvl2":"Example Comparison"},"type":"lvl4","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#kerchunk-1","position":16},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl4":"Kerchunk","lvl3":"Combine Individual Virtual Zarr Stores","lvl2":"Example Comparison"},"content":"To combine the individual virtual Zarr stores into one virtual Zarr store with Kerchunk, we will use \n\nkerchunk.combine.MultiZarrToZarr, which combines the content of multiple virtual Zarr stores into a single virtual Zarr store.\nThis call requires feeding MultiZarrToZarr the remote access info that we needed for our file system, along with the dimension we want to combine.\n\nt0 = time.time()\n\nmzz = kerchunk.combine.MultiZarrToZarr(\n    single_virtual_zarrs,\n    remote_protocol='s3',\n    remote_options=reader_options['storage_options'],\n    concat_dims=[\"day\"]\n)\n\nout = mzz.translate()\n\n# Save the virtual Zarr store, serialized as json\nwith fs_local.open('virtual_zarr/kerchunk/gridmet.json', 'wb') as f:\n    f.write(ujson.dumps(out).encode())\n\nkerchunk_time += time.time() - t0\n\nout\n\n\n\nAgain, notice the output type is in a json format with the coords in the dictionary and data chunks having pointers, but this time all chunks are in the one dictionary.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#kerchunk-1","position":17},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl4":"VirtualiZarr","lvl3":"Combine Individual Virtual Zarr Stores","lvl2":"Example Comparison"},"type":"lvl4","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#virtualizarr-1","position":18},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl4":"VirtualiZarr","lvl3":"Combine Individual Virtual Zarr Stores","lvl2":"Example Comparison"},"content":"To combine the virtual datasets from VirtualiZarr, we can just use \n\nxarray.combine_by_coords which will auto-magically combine the virtual datasets together.\n\nt0 = time.time()\n\nvirtual_ds = xr.combine_by_coords(virtual_datasets, coords='minimal', compat='override', combine_attrs='override')\n\n# Save the virtual Zarr store, serialized as json\nvirtual_ds.virtualize.to_kerchunk('virtual_zarr/virtualizarr/gridmet.json', format='json')\n\nvirtualizarr_time += time.time() - t0\n\nvirtual_ds\n\n\n\nNotice that when we saved the virtual dataset that we converted it to a Kerchunk format for saving.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#virtualizarr-1","position":19},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl3":"Opening the Virtual Zarr Stores","lvl2":"Example Comparison"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#opening-the-virtual-zarr-stores","position":20},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl3":"Opening the Virtual Zarr Stores","lvl2":"Example Comparison"},"content":"To open the virtual Zarr stores, we can use the same method for both stores as we converted to Kerchunk format when saving from VirtualiZarr.","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#opening-the-virtual-zarr-stores","position":21},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl4":"Kerchunk","lvl3":"Opening the Virtual Zarr Stores","lvl2":"Example Comparison"},"type":"lvl4","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#kerchunk-2","position":22},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl4":"Kerchunk","lvl3":"Opening the Virtual Zarr Stores","lvl2":"Example Comparison"},"content":"\n\nt0 = time.time()\n\nds = xr.open_dataset(\n    'virtual_zarr/kerchunk/gridmet.json',\n    chunks={},\n    engine=\"kerchunk\",\n    backend_kwargs={\n        \"storage_options\": {\n            \"remote_protocol\": \"s3\",\n            \"remote_options\": reader_options['storage_options']\n        },\n    }\n)\n\nkerchunk_read_time = time.time() - t0\n\nds\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#kerchunk-2","position":23},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl4":"VirtualiZarr","lvl3":"Opening the Virtual Zarr Stores","lvl2":"Example Comparison"},"type":"lvl4","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#virtualizarr-2","position":24},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl4":"VirtualiZarr","lvl3":"Opening the Virtual Zarr Stores","lvl2":"Example Comparison"},"content":"\n\nt0 = time.time()\n\nds = xr.open_dataset(\n    'virtual_zarr/virtualizarr/gridmet.json',\n    chunks={},\n    engine=\"kerchunk\",\n    backend_kwargs={\n        \"storage_options\": {\n            \"remote_protocol\": \"s3\",\n            \"remote_options\": reader_options['storage_options']\n        },\n    }\n)\n\nvirtualizarr_read_time = time.time() - t0\n\nds\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#virtualizarr-2","position":25},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl3":"Reading with xarray.open_mfdataset","lvl2":"Example Comparison"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#reading-with-xarray-open-mfdataset","position":26},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl3":"Reading with xarray.open_mfdataset","lvl2":"Example Comparison"},"content":"As a comparison of read times, let’s also compile the dataset using \n\nxarray.open_mfdataset in parallel with Dask.\nThis way we can see if we will be saving time in the future by having the compiled virtual Zarr for faster reads.\n\n%%time \nt0 = time.time()\n\nds = xr.open_mfdataset(\n    [fs.open(file) for file in file_glob],\n    chunks={},\n    parallel=True,\n    engine='h5netcdf'\n)\n\nopen_mfdataset_time = time.time() - t0\n\nds\n\n\n\nNow, let’s compare the computational times!\n\nprint(\"Kerchunk virtual Zarr creation time: \"\n      f\"{kerchunk_time:.0f}s ({kerchunk_time/60:.1f} min)\")\nprint(\"VirtualiZarr virtual Zarr creation time: \"\n      f\"{virtualizarr_time:.0f}s ({virtualizarr_time/60:.1f} min)\")\nprint(\"open_mfdataset dataset creation time: \"\n      f\"{open_mfdataset_time:.0f}s ({open_mfdataset_time/60:.1f} min)\")\nprint(f\"Time ratio: Kerchunk to open_mfdataset = {kerchunk_time/open_mfdataset_time:.2f}\\n\"\n      f\"            VirtualiZarr to open_mfdataset = {virtualizarr_time/open_mfdataset_time:.2f}\\n\"\n      f\"            Kerchunk to VirtualiZarr = {kerchunk_time/virtualizarr_time:.2f}\")\n\n\n\nAs we can see the Kerchunk and VirtualiZarr take about the same amount of time to create a virtual zarr store. open_mfdataset can create the Dataset on the fly in slightly less time than it takes to create the virtual zarr stores (about 30 seconds less, in this case). However, if you use open_mfdataset, you will have to create this dataset each time you run your code, whereas Kerchunk and Virtualizarr can create the dataset once and then read it much faster on future uses. Looking at read speed after the virtual Zarr store creation:\n\nprint(\"Kerchunk virtual Zarr read time: \"\n      f\"{kerchunk_read_time:.2f}s\")\nprint(\"VirtualiZarr virtual Zarr read time: \"\n      f\"{virtualizarr_read_time:.2f}s\")\nprint(\"open_mfdataset dataset read/creation time: \"\n      f\"{open_mfdataset_time:.0f}s ({open_mfdataset_time/60:.1f} min)\")\nprint(f\"Time ratio: Kerchunk to open_mfdataset = {kerchunk_read_time/open_mfdataset_time:.3f}\\n\"\n      f\"            VirtualiZarr to open_mfdataset = {virtualizarr_read_time/open_mfdataset_time:.3f}\\n\"\n      f\"            Kerchunk to VirtualiZarr = {kerchunk_read_time/virtualizarr_read_time:.3f}\")\n\n\n\nFrom this, it is very clear that performing more than one read using either the Kerchunk or VirtualiZarr virtual Zarr store is more efficient that reading with open_mfdataset.\nAdditionally, the differences in read times between Kerchunk and Virtualizarr, while appearing drastic, is likely not going to be significant in any workflow.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#reading-with-xarray-open-mfdataset","position":27},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl2":"Appending to Existing Virtual Zarr Store"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#appending-to-existing-virtual-zarr-store","position":28},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl2":"Appending to Existing Virtual Zarr Store"},"content":"As noted when \n\nintroducing the gridMET data, we did not utilize the 2019 data in order to show how to append it to a virtual Zarr store.\nThe ability to append more data to the virtual Zarr store is highly convienient, as plenty of datasets are continuously updated as new data becomes available.\nSo, let’s appends some data to our virtual Zarr stores we just made.\n\nFirst, we create the 2019 file glob.\n\nfile_glob_2019 = fs.glob('s3://mdmf/gdp/netcdf/gridmet/gridmet/*_2019.nc')\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#appending-to-existing-virtual-zarr-store","position":29},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl3":"Create New Virtual Zarr for New File","lvl2":"Appending to Existing Virtual Zarr Store"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#create-new-virtual-zarr-for-new-file","position":30},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl3":"Create New Virtual Zarr for New File","lvl2":"Appending to Existing Virtual Zarr Store"},"content":"Next, we need to get our 2019 NetCDFs into a virtual Zarr store.","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#create-new-virtual-zarr-for-new-file","position":31},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl4":"Kerchunk","lvl3":"Create New Virtual Zarr for New File","lvl2":"Appending to Existing Virtual Zarr Store"},"type":"lvl4","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#kerchunk-3","position":32},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl4":"Kerchunk","lvl3":"Create New Virtual Zarr for New File","lvl2":"Appending to Existing Virtual Zarr Store"},"content":"We will do this for Kerchunk the same way we did before, by using \n\nkerchunk.hdf.SingleHdf5ToZarr, which translates the content of one HDF5 (NetCDF4) file into Zarr metadata.\n\ntasks = [generate_single_virtual_zarr(file) for file in file_glob_2019]\nsingle_virtual_zarrs_2019 = dask.compute(*tasks)\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#kerchunk-3","position":33},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl4":"VirtualiZarr","lvl3":"Create New Virtual Zarr for New File","lvl2":"Appending to Existing Virtual Zarr Store"},"type":"lvl4","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#virtualizarr-3","position":34},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl4":"VirtualiZarr","lvl3":"Create New Virtual Zarr for New File","lvl2":"Appending to Existing Virtual Zarr Store"},"content":"And for VirtualiZarr, we will use \n\nvirtualizarr.open_virtual_dataset.\n\ntasks = [\n    dask.delayed(open_virtual_dataset)(\n        f's3://{file}',\n        indexes={},\n        loadable_variables=['day', 'lat', 'lon', 'crs'],\n        decode_times=True,\n        reader_options=reader_options\n    )\n    for file in file_glob_2019\n]\n\nvirtual_datasets_2019 = dask.compute(*tasks)\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#virtualizarr-3","position":35},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl3":"Append to Existing Store","lvl2":"Appending to Existing Virtual Zarr Store"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#append-to-existing-store","position":36},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl3":"Append to Existing Store","lvl2":"Appending to Existing Virtual Zarr Store"},"content":"Now, we can append the virtualized NetCDFs to our existing stores.","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#append-to-existing-store","position":37},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl4":"Kerchunk","lvl3":"Append to Existing Store","lvl2":"Appending to Existing Virtual Zarr Store"},"type":"lvl4","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#kerchunk-4","position":38},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl4":"Kerchunk","lvl3":"Append to Existing Store","lvl2":"Appending to Existing Virtual Zarr Store"},"content":"For Kerchunk, we will use still \n\nkerchunk.combine.MultiZarrToZarr.\nHowever, this time we will need to use the append method to append our new data.\n\n# Append to the existing reference file\nmzz = kerchunk.combine.MultiZarrToZarr.append(\n    single_virtual_zarrs_2019,\n    original_refs=out,\n    concat_dims=[\"day\"],\n    remote_protocol='s3',\n    remote_options=reader_options['storage_options'],\n)\n\nout_2019 = mzz.translate()\n\n# Save the virtual Zarr store, serialized as json\nwith fs_local.open('virtual_zarr/kerchunk/gridmet_appended.json', 'wb') as f:\n    f.write(ujson.dumps(out_2019).encode())\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#kerchunk-4","position":39},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl4":"VirtualiZarr","lvl3":"Append to Existing Store","lvl2":"Appending to Existing Virtual Zarr Store"},"type":"lvl4","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#virtualizarr-4","position":40},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl4":"VirtualiZarr","lvl3":"Append to Existing Store","lvl2":"Appending to Existing Virtual Zarr Store"},"content":"For VirtualiZarr, we can just use xarray.concat and xarray.merge like would to combine any xarray.Dataset.\n\nvirtual_ds_2019 = xr.merge(virtual_datasets_2019, compat='override', combine_attrs='override')\nvirtual_ds = xr.concat([virtual_ds, virtual_ds_2019], dim='day', coords='minimal', compat='override', combine_attrs='override')\nvirtual_ds\n\n\n\nThis simple xarray.merge and concat is the major advantage of VirtualiZarr.\nRather than having to figure out Kerchunk’s syntax and commands, we can keep using xarray as we already do.\nTherefore, the increase in time to create the virtual Zarr store compared to Kerchunk is likely worth it due to its native compatibility with xarray.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#virtualizarr-4","position":41},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl3":"Double Check New Stores","lvl2":"Appending to Existing Virtual Zarr Store"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#double-check-new-stores","position":42},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl3":"Double Check New Stores","lvl2":"Appending to Existing Virtual Zarr Store"},"content":"Finally, let’s read in the appended stores to make sure that we correctly appended the 2019 data.\n\nds = xr.open_dataset(\n    'virtual_zarr/kerchunk/gridmet_appended.json',\n    engine=\"kerchunk\",\n    chunks={},\n    backend_kwargs={\n        \"storage_options\": {\n            \"remote_protocol\": \"s3\",\n            \"remote_options\": reader_options['storage_options']\n        },\n    }\n)\nds\n\n\n\nNice!\nThe 2019 data is now appended and showing on the day coordinate.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#double-check-new-stores","position":43},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl2":"Clean Up"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#clean-up","position":44},{"hierarchy":{"lvl1":"Generating a Virtual Zarr Store","lvl2":"Clean Up"},"content":"Rather than deleting the virtual Zarr stores that we created, we will actually keep them for use in future tutorials.\nHowever, we will do want to conform with best practices and close our Dask client and cluster.\n\nclient.close()\ncluster.close()\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/createvirtualzarr#clean-up","position":45},{"hierarchy":{"lvl1":"Adding Coordinate Reference Systems (CRS) to Zarr Datasets"},"type":"lvl1","url":"/dataset-processing/tutorials/chunking/201/includecrsinzarr","position":0},{"hierarchy":{"lvl1":"Adding Coordinate Reference Systems (CRS) to Zarr Datasets"},"content":"Note\n\nThis notebook is a pared down version of the material in \n\nthis notebook by Kieran Bartels and Sarah Jordan.\n\nThe goal of this notebook is to learn how to add and verify the addition of coordinate reference systems (CRS) to Zarr datasets.\nWe will be utilizing \n\nthe Climate and Forecast (CF) Metadata Conventions for our CRS format and to determine where we include the CRS in the Zarr store.\nWhile these conventions were \n\noriginally designed for NetCDF files, Zarr has become the cloud optimized alternative to NetCDF, and it retains the same general metadata and data representation standards.\nTherefore, we can easily apply the CF conventions to promote the standardization and interoperability of climate data even within Zarr stores.\n\nNote\n\nSee the \n\nblog post by Thomas Martin and Ward Fisher for some details on the differences between Zarr and NetCDF.\n\nimport fsspec\nimport ujson\nimport xarray as xr\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/includecrsinzarr","position":1},{"hierarchy":{"lvl1":"Adding Coordinate Reference Systems (CRS) to Zarr Datasets","lvl2":"Example Dataset"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/201/includecrsinzarr#example-dataset","position":2},{"hierarchy":{"lvl1":"Adding Coordinate Reference Systems (CRS) to Zarr Datasets","lvl2":"Example Dataset"},"content":"In this notebook, we will use the virtual Zarr store of the daily gridMET dataset that we created in \n\nthe notebook on Generating a Virtual Zarr Store.\nNote that this means you will need to run that notebook to run this one.\nHowever, the contents of this example can easily be used for any Zarr store, virtual or not.\nThe only difference will be how the data is read in.\n\nOn that note, let’s read in the data using \n\nxarray.open_dataset and Kerchunk as the engine (i.e., engine='kerchunk').\n\nds = xr.open_dataset(\n    'virtual_zarr/kerchunk/gridmet.json',\n    engine='kerchunk',\n    chunks={},\n    backend_kwargs={\n        'storage_options': {\n            'remote_protocol': 's3',\n            'remote_options': {\n                'anon': True,\n                'endpoint_url': 'https://usgs.osn.mghpcc.org/'}\n        },\n    }\n)\nds\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/includecrsinzarr#example-dataset","position":3},{"hierarchy":{"lvl1":"Adding Coordinate Reference Systems (CRS) to Zarr Datasets","lvl2":"Identify the CRS"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/201/includecrsinzarr#identify-the-crs","position":4},{"hierarchy":{"lvl1":"Adding Coordinate Reference Systems (CRS) to Zarr Datasets","lvl2":"Identify the CRS"},"content":"A “\n\ncoordinate reference system” (CRS) is a framework used to precisely measure locations on the surface of Earth as coordinates (\n\nwikipedia.org, 2024).\nTo be CF-Compliant, a dataset must contain a “grid mapping” variable that is used to explicitly declare the CRS for any variables with spatial dimensions.\nSpecifically, the grid mapping variable provides the description of the CRS via a collection of attached attributes.\nCommon grid mapping variable names include:\n\ncrs,\n\npolar_stereographic,\n\nalbers_conical_equal_area,\n\nrcm_map.\n\nSpatial variables that need the CRS to define their positions are then linked to the grid mapping variable by having a grid_mapping attribute, which is set to the name of the grid mapping variable.\n\nTo locate the grid mapping variable, the first thing is to check for a variable (or coordinate) with one of the common grid mapping variable names.\nIf one exists, it can be confirmed to be the grid mapping by checking a spatial variable for the grid_mapping attribute and making sure it points to the grid mapping variable name.\nIf so, you are good to go.\nIf no obvious grid mapping variable is present, you can still check a spatial variable for its grid_mapping attribute to see what grid mapping variable it points to.\nIf there is a grid_mapping attribute, but no variable present with the name it points to, or if there is no grid_mapping attribute at all, you will need to create the CRS given what you know about the data.\nHowever, creating a CRS is beyond this notebook and we refer readers to \n\nthis notebook by Kieran Bartels and Sarah Jordan on how to add a missing CRS.\n\nOkay, now that we know how to identify the CRS, let’s look at our data again and check for it.\n\nds\n\n\n\nRight away, we can see that the dataset has a crs coordinate, and a check if a spatial variable, say precipitation_amount, shows a grid_mapping attribute of crs.\nSo, this dataset already has the CRS info, and it is stored in the crs coordinate with a dimension of crs.","type":"content","url":"/dataset-processing/tutorials/chunking/201/includecrsinzarr#identify-the-crs","position":5},{"hierarchy":{"lvl1":"Adding Coordinate Reference Systems (CRS) to Zarr Datasets","lvl2":"Making the CRS CF compliant"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/201/includecrsinzarr#making-the-crs-cf-compliant","position":6},{"hierarchy":{"lvl1":"Adding Coordinate Reference Systems (CRS) to Zarr Datasets","lvl2":"Making the CRS CF compliant"},"content":"While we have the CRS info, to be CF compliant, the CRS needs to be a dimensionless data variable.\nSince it is currently a coordinate with a dimension, we can convert it to a variable by squeezing (\n\nsqueeze) out the crs dimension and reseting the crs coordinate (\n\nreset_coords).\n\nds.squeeze().reset_coords('crs')\n\n\n\nNice, that did it!\nHowever, this actually gets us a CF compliant CRS for the in-memory dataset, and it does not actually adjust the Zarr store to be CF compliant.\nAs we don’t want to rewrite the whole Zarr store, we can fix this by directly adjusting some of the keywords in the json serialized Zarr store.\n\nNote: The following code is for our virtual Zarr store, which is a single json file with nested keys that make the Zarr store.\nSee \n\nbelow for how to adjust the CRS in a regular, non-virtual Zarr store.\n\nfs_local = fsspec.filesystem('')\n\nvirtual_zarr_store = ujson.load(fs_local.open('virtual_zarr/kerchunk/gridmet.json'))\n\nzattrs = ujson.loads(virtual_zarr_store['refs']['crs/.zattrs'])\nzattrs['_ARRAY_DIMENSIONS'] = []\nvirtual_zarr_store['refs']['crs/.zattrs'] = ujson.dumps(zattrs)\n\nzarray = ujson.loads(virtual_zarr_store['refs']['crs/.zarray'])\nzarray['chunks'] = []\nzarray['shape'] = []\nvirtual_zarr_store['refs']['crs/.zarray'] = ujson.dumps(zarray)\n\nwith fs_local.open('virtual_zarr/kerchunk/gridmet_cf_crs.json', 'wb') as f:\n    f.write(ujson.dumps(virtual_zarr_store).encode())\n\n\n\nNow that we have updated our virtual Zarr store, let’s read it in to make sure everything looks right.\n\nds = xr.open_dataset(\n    'virtual_zarr/kerchunk/gridmet_cf_crs.json',\n    engine='kerchunk',\n    chunks={},\n    backend_kwargs={\n        'storage_options': {\n            'remote_protocol': 's3',\n            'remote_options': {\n                'anon': True,\n                'endpoint_url': 'https://usgs.osn.mghpcc.org/'}\n        },\n    }\n)\nds\n\n\n\nLook at that, easy as can be, and we now have a CF compliant virtual Zarr store!","type":"content","url":"/dataset-processing/tutorials/chunking/201/includecrsinzarr#making-the-crs-cf-compliant","position":7},{"hierarchy":{"lvl1":"Adding Coordinate Reference Systems (CRS) to Zarr Datasets","lvl2":"Adding CRS to a Regular Zarr Store"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/201/includecrsinzarr#adding-crs-to-a-regular-zarr-store","position":8},{"hierarchy":{"lvl1":"Adding Coordinate Reference Systems (CRS) to Zarr Datasets","lvl2":"Adding CRS to a Regular Zarr Store"},"content":"As you may want to apply this process to a regular Zarr store, we have included some code that does the same adjustments as above, but for a regular Zarr store.zarr_store_path = 'gridment.zarr'\n\nwith fs_local.open(f'{zarr_store_path}/crs/.zattrs', 'wb') as f:\n    orig_metadata = ujson.load(f)        \n    orig_metadata['_ARRAY_DIMENSIONS'] = []\n    f.write(ujson.dumps(orig_metadata).encode())\n\nwith fs_local.open(f'{zarr_store_path}/crs/.zarray', 'wb') as f:\n    orig_metadata = ujson.load(f)        \n    orig_metadata['shape'] = []\n    orig_metadata['chunks'] = []\n    f.write(ujson.dumps(orig_metadata).encode())","type":"content","url":"/dataset-processing/tutorials/chunking/201/includecrsinzarr#adding-crs-to-a-regular-zarr-store","position":9},{"hierarchy":{"lvl1":"Choosing an Optimal Chunk Size and Shape"},"type":"lvl1","url":"/dataset-processing/tutorials/chunking/201/optimalchunkselection","position":0},{"hierarchy":{"lvl1":"Choosing an Optimal Chunk Size and Shape"},"content":"Note\n\nThis notebook is based on a 2013 blog post by Russ Rew at Unidata, \n\nChunking Data: Choosing Shapes.\nThe algorithm utilized in this notebook is based on the algorithm presented in that blog post.\n\nThe objective of this notebook is to learn and discuss how to select the optimal chunk size and shape for a given dataset.\nIn the \n\nBasics of Chunk Shape and Size notebook, we discussed the general considerations for chunking, but not how to apply these consisderations for selecting chunk shape and size.\nThese considerations included:\n\nchunk size between 10-200 MiB,\n\nhaving partial final chunks at least half the size of the standard chunk,\n\nperformant future reads across all dimensions (or optimized for the dominant read pattern), and\n\nchunk shape that is optimized for future data additions (if they occur).\n\nHere, we build upon those considerations and present a basic algorithm for automatically selecting the “optimal” chunk shape and size.\nTherefore, giving us a method for selecting chunk shape and size without much trial and error.\n\nimport fsspec\nimport xarray as xr\nimport numpy as np\nimport itertools\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/optimalchunkselection","position":1},{"hierarchy":{"lvl1":"Choosing an Optimal Chunk Size and Shape","lvl2":"The Many Considerations for Optimal Chunk Shape and Size Selection"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/201/optimalchunkselection#the-many-considerations-for-optimal-chunk-shape-and-size-selection","position":2},{"hierarchy":{"lvl1":"Choosing an Optimal Chunk Size and Shape","lvl2":"The Many Considerations for Optimal Chunk Shape and Size Selection"},"content":"As we just listed, there are several things to consider when selecting a chunk size.\nCreating an “optimal” chunk shape and size requires balancing all of these things to make our dataset perform as we want.\nFor example, the “perfect” chunking would have:\n\nchunk size around 10 MiB (to accomodate different hardware),\n\nno partial final chunks (i.e., chunk shape is integer divisor of the full data shape),\n\nperformant future reads across all dimensions and groups of dimensions, and\n\na shape that is an integer divisor of future data additions (if they occur).\n\nIn addition to these, if we can select a chunk size that is optimal for our disk storage (e.g., disk block size), we should further improve read times.\nHowever, in practice, there is no way to get the “perfect” chunk.\nWe will almost always have to compromise on one or more of these criteria to stay within the constraints created by another.\nThe criteria we compromise on is up to us, which makes determining the “optimal” chunk relatively subjective.\nTherefore, chunk shape and size selection is just as much an art as a science.\nIt depends some firm rules, but it also depends on our preferences.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/optimalchunkselection#the-many-considerations-for-optimal-chunk-shape-and-size-selection","position":3},{"hierarchy":{"lvl1":"Choosing an Optimal Chunk Size and Shape","lvl2":"Optimal Chunk Shape Algorithm"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/201/optimalchunkselection#optimal-chunk-shape-algorithm","position":4},{"hierarchy":{"lvl1":"Choosing an Optimal Chunk Size and Shape","lvl2":"Optimal Chunk Shape Algorithm"},"content":"As chunk shape and size selection is relatively subjective, let’s create an algorithm using some of our preferences and restrictions to automate the chunk selection process.\nTo take into account the first two considerations (size and partial chunks), this algorithm will focus on determining a chunk shape from the dataset shape given a maximum (uncompressed) chunk size and minimum partial chunk fraction.\nIt will return a chunk shape that has a size as close the maximum size as possible, while ensuring any final chunks contain partial chunk fractions above the specified minimum.\nFor reads across dimensions, we will limit our algorithm to 3D data only and try and balance 1D-to-2D read times.\nFor example, if we have a 3D spatiotemporal dataset, it would propose an “optimal” shape that allows for almost equal read times for time-series and spatial reads.\nFinally, we will assume the algorithm will only be applied to static datasets and that we do not need to worry about any future data additions.\n\nSo, let’s take a look at the algorithm, which we will write as a function.\n\ndef chunk_shape_3D(var_shape, chunk_size=4096, var_value_size=4, partial_chunk_frac=0.5, verbose=False):\n    \"\"\"\n    Return a \"optimal\" shape for a 3D variable, assuming balanced 1D-to-2D access.\n\n    Parameters\n    ----------\n    var_shape : tuple[int]\n        Length 3 list giving the variable shape in terms of (T, X, Y).\n    chunk_size : int\n        Maximum chunk size desired, in bytes (default = 4096).\n    var_value_size : int\n        Size of each variable data value, in bytes (default = 4).\n    partial_chunk_frac : float\n        The minimum fraction of data that the partial final chunk must\n        contain using the returned chunk shape (default = 0.5).\n    verbose : bool\n        If True, info on other candidate chunk shapes will be printed\n        (default = False).\n\n    Returns\n    -------\n    chunk_shape : tuple[int]\n        The optimal chunk shape that provides balanced access of 1D subsets\n        and 2D subsets of a variable with shape (T, X, Y), T is the typically\n        the time dimension and X and Y are the spatial dimensions. An \"optimal\"\n        shape for chunks means that the number of chunks accessed to read\n        either a full time series (1D) or full spatial map (2D) is\n        approximately equal, and the size of each chunk (uncompressed) is no\n        more than `chunk_size`, which is often a disk block size.\n    \"\"\"\n    rank = len(var_shape)\n    # Get ideal chunk info using only chunk size and balanced read\n    ideal_num_vals_per_chunk = chunk_size / float(var_value_size)\n    ideal_num_chunks = np.prod(var_shape) / ideal_num_vals_per_chunk\n    if ideal_num_chunks < 1:\n        ideal_num_chunks = 1\n    ideal_1d_num_chunks = np.sqrt(ideal_num_chunks)\n    ideal_2d_num_chunks = np.sqrt(ideal_1d_num_chunks)\n\n    if verbose:\n        print(f'Ideal number of values in a chunk = {ideal_num_vals_per_chunk:0.1f}')\n        print(f'Ideal number of chunks = {ideal_num_chunks:0.1f}')\n        print(f'Ideal number of chunks along the 1D axis = {ideal_1d_num_chunks:0.1f}')\n        print(f'Ideal number of chunks along each 2D axis = {ideal_2d_num_chunks:0.1f}')\n\n    # Get 1D optimal chunk shape along dimension\n    # Check if the first dimension has smaller shape then number of chunks\n    # If so, set to chunk shape to 1 and adjust 2D ideal chunks\n    if var_shape[0] < ideal_1d_num_chunks:\n        chunk_dim = 1.0\n        ideal_2d_num_chunks = (ideal_2d_num_chunks\n                               / np.sqrt(var_shape[0] / ideal_1d_num_chunks))\n    else:\n        chunk_dim = var_shape[0] // ideal_1d_num_chunks\n    # Add chunk dim to optimal chunk shape list\n    optimal_chunk_shape = [chunk_dim]\n\n    # Get 2D optimal chunk shape along each dimension\n    prod = 1.0  # factor to increase other dims if some must be increased to 1.0\n    for i in range(1, rank):\n        if var_shape[i] < ideal_2d_num_chunks:\n            prod *= ideal_2d_num_chunks / var_shape[i]\n            \n    for i in range(1, rank):\n        if var_shape[i] < ideal_2d_num_chunks:\n            chunk_dim = 1.0\n        else:\n            chunk_dim = (prod * var_shape[i]) // ideal_2d_num_chunks\n        optimal_chunk_shape.append(chunk_dim)\n\n    # Calculate the partial chunk fraction from the remainder\n    remainder_frac_per_dim = np.remainder(var_shape, optimal_chunk_shape) / optimal_chunk_shape\n    # If the remainder fraction is 0, swap with 1 for multiplication of parial chunk fraction\n    optimal_chunk_frac = np.where(remainder_frac_per_dim == 0, 1, remainder_frac_per_dim).prod()\n\n    if verbose:\n        print(f'Ideal chunk shape = {tuple(map(int, optimal_chunk_shape))}')\n        print(f'Ideal chunk, partial chunk fraction = {optimal_chunk_frac}')\n\n    \"\"\"\n    Optimal_chunk_shape is typically too small, size(optimal_chunk_shape) < chunk_size\n    and may have small partial chunks. So, we adjust by adding 1 to some chunk shape\n    dimensions to get as close as possible to chunk_size without exceeding it and then\n    check if the shape is over the minimum partial chunk fraction. If it is acceptable,\n    that is our chunk shape. Otherwise, we continually subtract one from the 1D dimension\n    to get to the partial fraction minimum, while adding one to the 2D dimensions to\n    maintain the size request. We then reverse this and subtract from the 2D demensions\n    and add to the 1D demensions. The optimal chunk is then the one that is the most\n    balanced of these two increment and decrement methods.\n    \"\"\"\n    # Increment the optimal chunk shape by 1\n    best_chunk_size = 0\n    best_chunk_shape = []\n    if verbose:\n        print('\\n--- Candidates ---')\n    for dim_increment in itertools.product([0, 1], repeat=3):\n        candidate_chunk_shape = np.add(optimal_chunk_shape, dim_increment)\n        \n        this_chunk_size = int(var_value_size * np.prod(candidate_chunk_shape))\n        remainder = np.remainder(var_shape, candidate_chunk_shape) / candidate_chunk_shape\n        this_chunk_frac = np.where(remainder == 0, 1, remainder).prod()\n        \n        if verbose:\n            if (this_chunk_size <= chunk_size) and (this_chunk_frac >= partial_chunk_frac):\n                print(f'{tuple(map(int, candidate_chunk_shape))}; '\n                      f'Total size per chunk (MB): {this_chunk_size/2**20:0.3f} '\n                      f'(ratio: {np.prod(candidate_chunk_shape) / ideal_num_vals_per_chunk:0.3f}); '\n                      f'Partial chunk fraction: {this_chunk_frac}')\n\n        # Only keep if closest to chunk size limit and above partial fraction limit\n        if (best_chunk_size < this_chunk_size <= chunk_size) and (this_chunk_frac >= partial_chunk_frac):\n            best_chunk_size = this_chunk_size\n            best_chunk_shape = list(candidate_chunk_shape) # make a copy of best candidate so far\n\n    # Return if a shape was found\n    if best_chunk_shape:\n        return list(map(int, best_chunk_shape))\n\n    # Increment and decrement 1D and 2D from optimal chunk shape to get a best shape    \n    increments_decrements = [[[-1, 0, 0], [0, 1, 0], [0, 0, 1]],\n                             [[1, 0, 0], [0, -1, 0], [0, 0, -1]]]\n    # Use Euclidean distance to estimate balanced shape\n    best_shape_balance = np.linalg.norm(np.array(optimal_chunk_shape) - np.array([0, 0, 0]))\n    for increment_decrement in increments_decrements:\n        best_chunk_frac = optimal_chunk_frac\n        candidate_chunk_shape = list(optimal_chunk_shape)\n        while best_chunk_frac < partial_chunk_frac:\n            # Quit if any candidate is too big or too small in a dimension\n            if ((np.array(candidate_chunk_shape) < 1).any()\n                or (candidate_chunk_shape > np.array(var_shape)).any()):\n                break\n\n            for dim_increment in increment_decrement:\n                candidate_chunk_shape = np.add(candidate_chunk_shape, dim_increment)\n                \n                this_chunk_size = int(var_value_size * np.prod(candidate_chunk_shape))\n                remainder = np.remainder(var_shape, candidate_chunk_shape) / candidate_chunk_shape\n                this_chunk_frac = np.where(remainder == 0, 1, remainder).prod()\n                \n                if (this_chunk_size <= chunk_size) and (this_chunk_frac >= partial_chunk_frac):\n                    if verbose:\n                        print(f'{tuple(map(int, candidate_chunk_shape))}; '\n                              f'Total size per chunk (MB): {this_chunk_size/2**20:0.3f} '\n                              f'(ratio: {np.prod(candidate_chunk_shape) / ideal_num_vals_per_chunk:0.3f}); '\n                              f'Partial chunk fraction: {this_chunk_frac}')\n                \n                    best_chunk_frac = this_chunk_frac\n                    shape_balance = np.linalg.norm(np.array(optimal_chunk_shape) - candidate_chunk_shape)\n                    # Only save candidate if it is more balanced than previous one\n                    if shape_balance < best_shape_balance:\n                        best_shape_balance = shape_balance\n                        best_chunk_shape = list(candidate_chunk_shape)\n\n\n    return tuple(map(int, best_chunk_shape))\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/optimalchunkselection#optimal-chunk-shape-algorithm","position":5},{"hierarchy":{"lvl1":"Choosing an Optimal Chunk Size and Shape","lvl2":"Examples"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/201/optimalchunkselection#examples","position":6},{"hierarchy":{"lvl1":"Choosing an Optimal Chunk Size and Shape","lvl2":"Examples"},"content":"","type":"content","url":"/dataset-processing/tutorials/chunking/201/optimalchunkselection#examples","position":7},{"hierarchy":{"lvl1":"Choosing an Optimal Chunk Size and Shape","lvl3":"Toy Example","lvl2":"Examples"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/201/optimalchunkselection#toy-example","position":8},{"hierarchy":{"lvl1":"Choosing an Optimal Chunk Size and Shape","lvl3":"Toy Example","lvl2":"Examples"},"content":"Okay, now that we have our algorithm, let’s give it a test using some made up data.\nWe will use a variable of shape (365, 240, 150), aim for a chunk of 1 MiB (2**20 bytes) in size, and a partial chunk fraction of 0.5 (0.8 in each dimensions; 0.8^3 \\approx 0.5).\n\nvar_shape = (365, 240, 150)\nchunk_shape = chunk_shape_3D(\n    var_shape, chunk_size=2**20, partial_chunk_frac=0.5, verbose=True\n)\nprint(f'\\n\"Optimal\" chunk shape: {chunk_shape}')\n\n\n\nNice!\nFrom these results, we can see that the “ideal” chunk shape given only our chunk size would have been (51, 90, 56).\nHowever, this had some partial chunks that would have been very small.\nSo, its choice would not have been good given this additional constraint.\nTherefore, it adjusted the chunk shape to (53, 88, 54) to meet the partial chunk constraint and provide us with a reasonably close alternative in terms of both dimension balance and chunk size.\nThis is great, exactly what we wanted!\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/optimalchunkselection#toy-example","position":9},{"hierarchy":{"lvl1":"Choosing an Optimal Chunk Size and Shape","lvl3":"PRISM Example","lvl2":"Examples"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/201/optimalchunkselection#prism-example","position":10},{"hierarchy":{"lvl1":"Choosing an Optimal Chunk Size and Shape","lvl3":"PRISM Example","lvl2":"Examples"},"content":"As a real world example, let’s apply the algorithm to the PRISM data from the \n\nBasics of Chunk Shape and Size notebook.\nFirst, we need to read in the data.\n\nfs = fsspec.filesystem(\n    's3',\n    anon=True,   # anonymous = does not require credentials\n    client_kwargs={'endpoint_url': 'https://usgs.osn.mghpcc.org/'}\n)\nds = xr.open_dataset(\n    fs.get_mapper('s3://mdmf/gdp/PRISM_v2.zarr/'),\n    engine='zarr',\n    chunks={}\n)\nds\n\n\n\nNow, we can estimate the “optimal” chunk shape using our algorithm.\nWe will only do this for the precipitaiton data variable as the others all have the same shape and data type. If you had additional variables with different data types, you would want to run this algorithm on a variable of each type to find its optimal size.\nAlso, we will use a maximum chunk size that matches the same chunk size of the dataset currently to see how our algorithm compares to the chosen chunking.\n\ncurrent_chunk_size = np.prod([chunksize[0] for chunksize in ds['ppt'].chunksizes.values()])\ncurrent_chunk_size *= ds['ppt'].dtype.itemsize\n\nchunk_shape = chunk_shape_3D(\n    ds['ppt'].shape,\n    chunk_size=current_chunk_size,\n    var_value_size=ds['ppt'].dtype.itemsize,\n    verbose=True\n)\nprint(f'\\n\"Optimal\" chunk shape: {chunk_shape}')\n\n\n\nAs we can see, our algorithm struggles with this dataset to come up with a balanced chunk shape that meets our size and partial fraction restrictions.\nRather than having the “ideal” chunk shape of (125, 178, 404), we got a chunk shape of (168, 135, 362), which is quite different than the current chunk shape of (72, 354, 354).\nThe primary driver of this discrepancy is our restriction on balancing the dimensions, as the current chunk shape has a parital fraction of 0.73.\nTherefore, enforcing the balance can be very restrictive.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/optimalchunkselection#prism-example","position":11},{"hierarchy":{"lvl1":"Choosing an Optimal Chunk Size and Shape","lvl2":"Further Considerations"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/201/optimalchunkselection#further-considerations","position":12},{"hierarchy":{"lvl1":"Choosing an Optimal Chunk Size and Shape","lvl2":"Further Considerations"},"content":"As noted in the PRISM example, this algorithm is a very niche case for chunk shape selection.\nIt assumes you want even read times for the temporal and spatial dimensions, and it only allows for 3D data.\nWhat if we had more dimensions or fewer?\nWhat if we wanted a different read pattern that was unbalanced?\nWhat if we wanted to not have any partial chunks and only use a chunk shape that is a divisor of the variable shape?\nTherefore, this algorithm is not general by any means, but does give us an idea on how to formulate more general algorithms in the future.\nFor example, one of these more general algorithms can be found in \n\nthis repo on dynamic rechunking, which has some algorithms that allow for the user to specify the balancing of the dimensions and a chunk size limit.\nAs algorithms like these become more developed, selecting an optimal chunk size should become easier.\nHowever, the subjective component will never go away and the need for someone to make a decision on what critera that influences chunk shape and size is more important will always be required.","type":"content","url":"/dataset-processing/tutorials/chunking/201/optimalchunkselection#further-considerations","position":13},{"hierarchy":{"lvl1":"Rechunking Larger Datasets with Dask"},"type":"lvl1","url":"/dataset-processing/tutorials/chunking/201/rechunkingwithdask","position":0},{"hierarchy":{"lvl1":"Rechunking Larger Datasets with Dask"},"content":"The goal of this notebook is to expand on the rechunking performed in the \n\nIntroductory Rechunking tutorial.\nThis notebook will perfrom the same operations, but will work on the much larger dataset and involve some parallelization using \n\ndask.\n\nWarning\n\nYou should only run workflows like this tutorial on a cloud or HPC compute node.\nIn application, this will require reading and writing enormous amounts of data.\nUsing a typical network connection and simple compute environment, you would saturate your bandwidth and max out your processor, thereby taking days for the rechunking to complete.\n\nimport os\n# Needed when boto3 >= 1.36.0 or the rechunking process will fail\n# This needs to be set before the boto3 library gets loaded\n# See: https://github.com/aws/aws-cli/issues/9214#issuecomment-2606619168\nos.environ['AWS_REQUEST_CHECKSUM_CALCULATION'] = 'when_required'\nos.environ['AWS_RESPONSE_CHECKSUM_VALIDATION'] = 'when_required'\nimport xarray as xr\nimport fsspec\nfrom rechunker import rechunk\nimport zarr\nimport dask.diagnostics\nimport logging\nimport dask\nimport configparser\n\ndask.config.set({'temporary_directory':'../'})\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/rechunkingwithdask","position":1},{"hierarchy":{"lvl1":"Rechunking Larger Datasets with Dask","lvl2":"Read in a Zarr Store"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/201/rechunkingwithdask#read-in-a-zarr-store","position":2},{"hierarchy":{"lvl1":"Rechunking Larger Datasets with Dask","lvl2":"Read in a Zarr Store"},"content":"Like the \n\nIntroductory Rechunking tutorial, we will use the data from the National Water Model Retrospective Version 2.1.\nThe full dataset is part of the \n\nAWS Open Data Program, available via the S3 bucket at: s3://noaa-nwm-retro-v2-zarr-pds/.\n\nAs this is a Zarr store, let’s read it in with \n\nxarray.open_dataset() and engine='zarr'.\n\nfile = fsspec.get_mapper('s3://noaa-nwm-retro-v2-zarr-pds', anon=True)\nds = xr.open_dataset(file, chunks={}, engine='zarr')\nds\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/rechunkingwithdask#read-in-a-zarr-store","position":3},{"hierarchy":{"lvl1":"Rechunking Larger Datasets with Dask","lvl2":"Restrict for Tutorial"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/201/rechunkingwithdask#restrict-for-tutorial","position":4},{"hierarchy":{"lvl1":"Rechunking Larger Datasets with Dask","lvl2":"Restrict for Tutorial"},"content":"As we saw in the \n\nIntroductory Rechunking tutorial, this dataset is massive, taking up almost 32 TiB uncompressed.\nAs this is a tutorial, we will still restrict the data to a subset - we don’t really need to work on the entire dataset to demonstrate how to use dask to scale up.\nFollowing the \n\nIntroductory Rechunking tutorial let’s only look at streamflow and velocity for the first 15,000 feature_ids, |but we will look at the entire 2000s decade of water years (October 1999 through September 2009) instead of a single water year this time.\nThis will make our dataset larger-than-memory, but it should still run in a reasonable amount of time.\n\nFor processing the full-sized dataset, you’d just skip this step where we slice off a representative example of the data.\nExpect run time to increase in proportion to the size of the data being processed.\n\nds = ds[['streamflow', 'velocity']]\nds = ds.isel(feature_id=slice(0, 15000))\nds = ds.sel(time=slice('1999-10-01', '2009-09-30'))\nds\n\n\n\nNow, our subset of data is only about 10 GiB per data variable and has a chunk shape of {'time': 672, 'feature_id': 15000} with size of 76.9 MiB.\nHowever, the chunk shape is not an optimal choice for our analysis as it is chunked completely by feature_id (i.e., all feature IDs for a given time can be read in a single chunk).\nFollowing the \n\nIntroductory Rechunking tutorial, let’s get chunk shapes that are time-series wise chunking (i.e., all time for a given feature_id in one chunk) for streamflow and balanced for velocity.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/rechunkingwithdask#restrict-for-tutorial","position":5},{"hierarchy":{"lvl1":"Rechunking Larger Datasets with Dask","lvl2":"Rechunk Plan"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/201/rechunkingwithdask#rechunk-plan","position":6},{"hierarchy":{"lvl1":"Rechunking Larger Datasets with Dask","lvl2":"Rechunk Plan"},"content":"Using our general strategy of time-series wise chunking for streamflow and balanced for velocity,\nlet’s compute how large the chunk sizes will be if we have chunk shapes of {'time': 87672, 'feature_id': 1} for streamflow and 3 chunks per dimension for velocity.\n\nnfeature = len(ds.feature_id)\nntime = len(ds.time)\n\nstreamflow_chunk_plan = {'time': ntime, 'feature_id': 1}\nbytes_per_value = ds.streamflow.dtype.itemsize\ntotal_bytes = streamflow_chunk_plan['time'] * streamflow_chunk_plan['feature_id'] * bytes_per_value\nstreamflow_MiB = total_bytes / (2 ** 10) ** 2\npartial_chunks = {'time': ntime -  streamflow_chunk_plan['time'] * (ntime / streamflow_chunk_plan['time']),\n                  'feature_id': nfeature -  streamflow_chunk_plan['feature_id'] * (nfeature / streamflow_chunk_plan['feature_id']),}\nprint(\"STREAMFLOW \\n\"\n      f\"Chunk of shape {streamflow_chunk_plan} \\n\"\n      f\"Partial 'time' chunk remainder: {partial_chunks['time']} ({partial_chunks['time']/streamflow_chunk_plan['time']:.3f}% of a chunk)\\n\"\n      f\"Partial 'feature_id' chunk remainder: {partial_chunks['feature_id']} ({partial_chunks['feature_id']/streamflow_chunk_plan['feature_id']:.3f}% of a chunk)\\n\"\n      f\"Chunk size: {streamflow_MiB:.2f} [MiB] \\n\")\n\nchunks_per_dim = 3\nvelocity_chunk_plan = {'time': ntime // chunks_per_dim, 'feature_id': nfeature // chunks_per_dim}\nbytes_per_value = ds.velocity.dtype.itemsize\ntotal_bytes = velocity_chunk_plan['time'] * velocity_chunk_plan['feature_id'] * bytes_per_value\nvelocity_MiB = total_bytes / (2 ** 10) ** 2\npartial_chunks = {'time': ntime -  velocity_chunk_plan['time'] * chunks_per_dim,\n                  'feature_id': nfeature -  velocity_chunk_plan['feature_id'] * chunks_per_dim,}\nprint(\"VELOCITY \\n\"\n      f\"Chunk of shape {velocity_chunk_plan} \\n\"\n      f\"Partial 'time' chunk remainder: {partial_chunks['time']} ({partial_chunks['time']/velocity_chunk_plan['time']:.3f}% of a chunk)\\n\"\n      f\"Partial 'feature_id' chunk remainder: {partial_chunks['feature_id']} ({partial_chunks['feature_id']/velocity_chunk_plan['feature_id']:.3f}% of a chunk)\\n\"\n      f\"Chunk size: {velocity_MiB:.2f} [MiB]\")\n\n\n\nOkay, we can see that the streamflow chunk size is way to small by a factor of ~100.\nSo, let’s include 100 feature IDs per chunk.\nAs for velocity, it is ~10x too big.\nAs it is an even chunk split, that means we need to increase the number of chunks per dimension by ~\\sqrt{10} \\approx 3.\nHowever knowing that the time dimension is hourly, we can get no partial chunks if our chunk per dimension is a divisor of 24.\nLuckily, this also applies to the feature ID dimension as 15000 is a multiple of 24.\nSo, rather than increasing our chunks per dimension by a factor of 3 to 9, let’s increase them to 12 as this will give no partial chunks.\n\nnfeature = len(ds.feature_id)\nntime = len(ds.time)\n\nstreamflow_chunk_plan = {'time': ntime, 'feature_id': 100}\nbytes_per_value = ds.streamflow.dtype.itemsize\ntotal_bytes = streamflow_chunk_plan['time'] * streamflow_chunk_plan['feature_id'] * bytes_per_value\nstreamflow_MiB = total_bytes / (2 ** 10) ** 2\npartial_chunks = {'time': ntime -  streamflow_chunk_plan['time'] * (ntime / streamflow_chunk_plan['time']),\n                  'feature_id': nfeature -  streamflow_chunk_plan['feature_id'] * (nfeature / streamflow_chunk_plan['feature_id']),}\nprint(\"STREAMFLOW \\n\"\n      f\"Chunk of shape {streamflow_chunk_plan} \\n\"\n      f\"Partial 'time' chunk remainder: {partial_chunks['time']} ({partial_chunks['time']/streamflow_chunk_plan['time']:.3f}% of a chunk)\\n\"\n      f\"Partial 'feature_id' chunk remainder: {partial_chunks['feature_id']} ({partial_chunks['feature_id']/streamflow_chunk_plan['feature_id']:.3f}% of a chunk)\\n\"\n      f\"Chunk size: {streamflow_MiB:.2f} [MiB] \\n\")\n\nchunks_per_dim = 12\nvelocity_chunk_plan = {'time': ntime // chunks_per_dim, 'feature_id': nfeature // chunks_per_dim}\nbytes_per_value = ds.velocity.dtype.itemsize\ntotal_bytes = velocity_chunk_plan['time'] * velocity_chunk_plan['feature_id'] * bytes_per_value\nvelocity_MiB = total_bytes / (2 ** 10) ** 2\npartial_chunks = {'time': ntime -  velocity_chunk_plan['time'] * chunks_per_dim,\n                  'feature_id': nfeature -  velocity_chunk_plan['feature_id'] * chunks_per_dim,}\nprint(\"VELOCITY \\n\"\n      f\"Chunk of shape {velocity_chunk_plan} \\n\"\n      f\"Partial 'time' chunk remainder: {partial_chunks['time']} ({partial_chunks['time']/velocity_chunk_plan['time']:.3f}% of a chunk)\\n\"\n      f\"Partial 'feature_id' chunk remainder: {partial_chunks['feature_id']} ({partial_chunks['feature_id']/velocity_chunk_plan['feature_id']:.3f}% of a chunk)\\n\"\n      f\"Chunk size: {velocity_MiB:.2f} [MiB]\")\n\n\n\nNice!\nNow, our chunks are a reasonable size and have no remainders.\nSo, let’s use these chunk plans for our rechunking.\n\nchunk_plan = {\n    'streamflow': streamflow_chunk_plan,\n    'velocity': velocity_chunk_plan,\n     # We don't want any of the coordinates chunked\n    'latitude': (nfeature,),\n    'longitude': (nfeature,),    \n    'time': (ntime,),\n    'feature_id': (nfeature,)\n}\nchunk_plan\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/rechunkingwithdask#rechunk-plan","position":7},{"hierarchy":{"lvl1":"Rechunking Larger Datasets with Dask","lvl2":"Rechunk with Rechunker"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/201/rechunkingwithdask#rechunk-with-rechunker","position":8},{"hierarchy":{"lvl1":"Rechunking Larger Datasets with Dask","lvl2":"Rechunk with Rechunker"},"content":"With this plan, we can now ask Rechunker to re-write the data using the prescribed chunking pattern.","type":"content","url":"/dataset-processing/tutorials/chunking/201/rechunkingwithdask#rechunk-with-rechunker","position":9},{"hierarchy":{"lvl1":"Rechunking Larger Datasets with Dask","lvl3":"Set up output location","lvl2":"Rechunk with Rechunker"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/201/rechunkingwithdask#set-up-output-location","position":10},{"hierarchy":{"lvl1":"Rechunking Larger Datasets with Dask","lvl3":"Set up output location","lvl2":"Rechunk with Rechunker"},"content":"Unlike with the smaller dataset in our previous rechunking tutorial, we will write this larger dataset to an object store (similar to an S3 bucket) on the USGS OSN Pod.\nSo, we need to set that up so that Rechunker will have a suitable place to write data.\n\nFirst, we need to set up our credentials that allow us to write to the OSN Pod and direct it to the endpoint where the OSN Pod is located (while it is available via the S3 API, it is not in AWS, so we have to tell it where to find the pod!). If you do not already have access to the credentials to write to the OSN Pod and you are a USGS staff member or collaborator, please reach out to \n\nasnyder@usgs.gov to request them. If you are not a USGS staff or collaborator, you will need to set up a different location that to you have write permissions to. This could be an AWS S3 bucket that you have access too or you could even set up an fsspec LocalFileSystem.\n\nawsconfig = configparser.ConfigParser()\nawsconfig.read(\n    os.path.expanduser('~/.aws/credentials') \n    # default location... if yours is elsewhere, change this.\n)\n_profile_nm  = 'osn-hytest-scratch'\n_endpoint = 'https://usgs.osn.mghpcc.org/'\n\n# Set environment vars based on parsed awsconfig\ntry:\n    os.environ['AWS_ACCESS_KEY_ID']     = awsconfig[_profile_nm]['aws_access_key_id']\n    os.environ['AWS_SECRET_ACCESS_KEY'] = awsconfig[_profile_nm]['aws_secret_access_key']\n    os.environ['AWS_S3_ENDPOINT']       = _endpoint\n    os.environ['AWS_PROFILE'] = _profile_nm\n    os.environ['AWS_DEFAULT_PROFILE'] = _profile_nm\nexcept KeyError as e:\n    logging.error(\"Problem parsing the AWS credentials file. \")\n\n\n\nNext, we make our S3 fsspec.filesystem with the required user info and get the mapper to this file to pass to Rechunker.\n\nfrom getpass import getuser\nuname=getuser()\n\nfs = fsspec.filesystem(\n    's3', \n    anon=False, \n    default_fill_cache=False, \n    skip_instance_cache=True, \n    client_kwargs={'endpoint_url': os.environ['AWS_S3_ENDPOINT'], }\n)\n\noutput_dir = f's3://hytest-scratch/rechunking_tutorial/{uname}/'\n\ntemp_store = fs.get_mapper(output_dir + 'temp_store.zarr')\ntarget_store = fs.get_mapper(output_dir + 'tutorial_rechunked.zarr')\n# Check if the objects exist and remove if they do\nfor filename in [temp_store, target_store]:\n    try:\n        fs.rm(filename.root, recursive=True)\n    except:\n        FileNotFoundError\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/rechunkingwithdask#set-up-output-location","position":11},{"hierarchy":{"lvl1":"Rechunking Larger Datasets with Dask","lvl3":"Spin up Dask Cluster","lvl2":"Rechunk with Rechunker"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/201/rechunkingwithdask#spin-up-dask-cluster","position":12},{"hierarchy":{"lvl1":"Rechunking Larger Datasets with Dask","lvl3":"Spin up Dask Cluster","lvl2":"Rechunk with Rechunker"},"content":"Our rechunking operation will be able to work in parallel.\nTo do that, we will spin up a dask cluster to schedule the various workers.\n\nThis cluster will be configured differently depending on where you compute is performed. We have set up this notebook to run on \n\nNebari. If you are working on USGS systems, you can likely use one of our \n\nexample codes to spin up a dask cluster. Otherwise, please refer to the \n\ndask deployment docs for details.\n\n%run ../../../../environment_set_up/Start_Dask_Cluster_Nebari.ipynb\n## If this notebook is not being run on Nebari, replace the above \n## path name with a helper appropriate to your compute environment.  Examples:\n# %run ../../../../environment_set_up/Start_Dask_Cluster_Denali.ipynb\n# %run ../../../../environment_set_up/Start_Dask_Cluster_Tallgrass.ipynb\n# %run ../../../../environment_set_up/Start_Dask_Cluster_Desktop.ipynb\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/rechunkingwithdask#spin-up-dask-cluster","position":13},{"hierarchy":{"lvl1":"Rechunking Larger Datasets with Dask","lvl3":"Rechunk","lvl2":"Rechunk with Rechunker"},"type":"lvl3","url":"/dataset-processing/tutorials/chunking/201/rechunkingwithdask#rechunk","position":14},{"hierarchy":{"lvl1":"Rechunking Larger Datasets with Dask","lvl3":"Rechunk","lvl2":"Rechunk with Rechunker"},"content":"Now, we are ready to rechunk!\n\nresult = rechunk(\n    # Make sure the base chunks are correct\n    ds.chunk({'time': 672, 'feature_id': 15000}),\n    target_chunks=chunk_plan,\n    max_mem=\"2GB\",\n    target_store=target_store,\n    temp_store=temp_store\n)\nresult\n\n\n\nRemember that merely invoking Rechunker does not do any work.\nIt just sorts out the rechunking plan and writes metadata.\nWe need to call .execute on the result object to actually run the rechunking.\n\nwith dask.diagnostics.ProgressBar():\n    r = result.execute(retries=10)  \n\n# Also consolidate the metadata for fast reading into xarray\n_ = zarr.consolidate_metadata(target_store)\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/rechunkingwithdask#rechunk","position":15},{"hierarchy":{"lvl1":"Rechunking Larger Datasets with Dask","lvl2":"Confirm the Creation of the Zarr Store by Rechunker"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/201/rechunkingwithdask#confirm-the-creation-of-the-zarr-store-by-rechunker","position":16},{"hierarchy":{"lvl1":"Rechunking Larger Datasets with Dask","lvl2":"Confirm the Creation of the Zarr Store by Rechunker"},"content":"Let’s read in the resulting re-chunked dataset to confirm it turned out how we intended.\n\nds_rechunked = xr.open_zarr(target_store)\nds_rechunked\n\n\n\nNice, looks good!\nYou may have noticed that the only difference between the \n\nintroductory tutorial on rechunking and this is the inclusion of creating the dask cluster and where we saved the files.\nPicking your compute environment and output location will typically be the only things that vary in other workflows requiring rechunking.\nTherefore, if you understand this rechunking process you should be able to apply it to your own data efficiently.\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/rechunkingwithdask#confirm-the-creation-of-the-zarr-store-by-rechunker","position":17},{"hierarchy":{"lvl1":"Rechunking Larger Datasets with Dask","lvl2":"Clean Up"},"type":"lvl2","url":"/dataset-processing/tutorials/chunking/201/rechunkingwithdask#clean-up","position":18},{"hierarchy":{"lvl1":"Rechunking Larger Datasets with Dask","lvl2":"Clean Up"},"content":"As we don’t want to keep this rechunked Zarr, let’s go ahead and delete it.\nWe will also conform with best practices and close our Dask client and cluster.\n\nfs.rm(temp_store.root, recursive=True)\nfs.rm(target_store.root, recursive=True)\n        \nclient.close()\ncluster.close()\n\n","type":"content","url":"/dataset-processing/tutorials/chunking/201/rechunkingwithdask#clean-up","position":19},{"hierarchy":{"lvl1":"Advanced Topics in Chunking"},"type":"lvl1","url":"/dataset-processing/tutorials/chunking/201","position":0},{"hierarchy":{"lvl1":"Advanced Topics in Chunking"},"content":"In this second series of notebooks, we will go over more advanced topics associated with chunking.\nThe key concepts you should understand have after this series include:\n\nHyTEST Project\n\nEnvironment Set-Up\n\nCompute Environments\n\nGeneral QuickStart\n\nCloud: Nebari Quick-Start\n\nHPC Server: Open OnDemand Quick-Start\n\nHPC Server: Jupyter Forward Quick-Start\n\nHPC Server: Start Script Quick-Start\n\nAWS Credentials\n\nStarting a Dask Cluster\n\nNebari Dask Cluster\n\nDenali HPC Dask Cluster\n\nHovenweep/Tallgrass HPC Dask Cluster\n\nLocal Desktop Dask Cluster\n\nGetting Started\n\nEssential Reading\n\nFoundations\n\nData Sources\n\nData/APIs\n\nData/Cloud Storage\n\nParallelism with Dask\n\nDataset Access\n\nData Catalogs\n\nData Storage Locations\n\nWater Mission Area STAC Catalog\n\nHyTEST Data Catalog (Intake)\n\nHyTEST Intake Sub-Catalogs\n\nCONUS404 Products Data Access\n\nCONUS404 Zarr Changelog\n\nExplore CONUS404 Dataset\n\nZarr Creation\n\nData Chunking Tutorial Overview\n\nIntroduction to Chunking\n\nWhy (re)Chunk Data?\n\nHow to Examine a Stored Dataset’s Chunk Shape\n\nBasics of Chunk Shape and Size\n\nWriting Chunked Files\n\nRechunking\n\nAdvanced Topics in Chunking\n\nRechunking Larger Datasets with Dask\n\nGenerating a Virtual Zarr Store\n\nAdding Coordinate Reference Systems (CRS) to Zarr Datasets\n\nChoosing an Optimal Chunk Size and Shape\n\nChunking References\n\nGlossary\n\nAdditional Resources\n\nDataset Processing\n\nCONUS404 Temporal Aggregation\n\nSpatial Aggregation\n\ngdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s\n\ngdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1\n\ngdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s\n\nPangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s\n\ngdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation\n\nCONUS404 Site Data Selection\n\nCONUS404 Regridding (Curvilinear => Rectilinear)\n\nModel Evaluation\n\nEvaluation Metrics\n\nD-Score Suite (v1) Benchmark\n\nD-Score Suite (v1) Benchmark -- Usage Examples\n\nStandard Suite (v1) Metrics\n\nStandard Suite (v1) Metrics -- Usage Examples\n\nStreamflow Model Evaluation\n\nStreamflow Eval :: Data Preparation\n\nStreamflow Eval :: StdSuite Analysis\n\nSteamflow Eval :: DScore Analysis\n\nStreamflow Eval :: Visualization\n\nBack Matter\n\nContributing\n\nLicense","type":"content","url":"/dataset-processing/tutorials/chunking/201","position":1},{"hierarchy":{"lvl1":"Data Chunking Tutorial Overview"},"type":"lvl1","url":"/dataset-processing/tutorials/chunking/readme","position":0},{"hierarchy":{"lvl1":"Data Chunking Tutorial Overview"},"content":"Chunking data is an important foundational step in preparing data for HyTEST workflows. It helps massive datasets be easily subsetted (without having to download the whole dataset), and it supports efficient parallelized workflows. Organizing and chunking the data appropriately will dramatically affect the performance of analyses.\n\nIn this tutorial, we will go over all levels of information on data chunking,\nfrom the basic introductions on the topic to complex methods of selecting optimal chunk sizes and rechunking on the cloud.\nMuch of what is covered in this tutorial replicates concepts covered in a variety of materials that we cite as we go.\nHowever, that material has been adapted to use data that looks like data you might encounter in a HyTEST workflow.\n\nThe content is split into two primary sections:\n\nIntroduction to Chunking\n\nAdvanced Topics in Chunking\n\nIn \n\nIntroduction to Chunking, we discuss all of the basic introductory topics associated with chunking. As for \n\nAdvanced Topics in Chunking, we dive into some more advanced topics related to chunking, which require a firm understanding of introductory topics. Feel free to read this tutorial in order (which has been set up for those new to chunking) or jump directly to the topic that interests you.\n\nIf you would like to run this tutorial on your own, you can find the jupyter notebooks that are used in this tutorial in our \n\nGithub repository. We also include a python environment file \n\nhere. The “101: Introduction to Chunking” topics can be run on your local computer, but some of the “201: Advanced Topics in Chunking” notebooks will likely require an HPC or cloud computing resources to complete. If you are a USGS staff member, you can work in HyTEST’s preconfigured cloud environment which will provide both the python environment and access to the needed cloud computing resources. Please reach out to \n\nasnyder@usgs.gov if you would like access to this space.\n\nIf you find any issues or errors in this tutorial, please open an \n\nissue in our Github repository. If you have questions or want to talk more about data chunking, please feel free to ask on our \n\nData Processing Discussion Board.","type":"content","url":"/dataset-processing/tutorials/chunking/readme","position":1},{"hierarchy":{"lvl1":"Additional Resources"},"type":"lvl1","url":"/dataset-processing/tutorials/chunking/back/additionalresources","position":0},{"hierarchy":{"lvl1":"Additional Resources"},"content":"As we can’t cover all possible material on chunking in one series of notebooks,\nlisted below are some links to additional resources on chunking that can be referred to for additional information.\n\nZarr docs on chunk optimization\n\nDask docs on chunk selection\n\nRechunker docs\n\nKerchunk docs\n\nVirtualiZarr docs\n\nTutorial notebook on adding CRS to Zarr\n\nUCAR Blog on Difference between NetCDF and Zarr\n\nUCAR Blog on Why Chunk Data\n\nUCAR Blog on Choosing Chunk Shapes\n\nProject Pythia Cookbook on Kerchunk with Dask","type":"content","url":"/dataset-processing/tutorials/chunking/back/additionalresources","position":1},{"hierarchy":{"lvl1":"Glossary"},"type":"lvl1","url":"/dataset-processing/tutorials/chunking/back/glossary","position":0},{"hierarchy":{"lvl1":"Glossary"},"content":"A glossary of common terms used throughout Jupyter Book.\n\nChunk\n\nSmaller, more manageable pieces of a larger dataset.\n\nChunking\n\nThe process of breaking down large amounts of data into smaller, more manageable pieces.\n\nChunk shape\n\nThe actual shape of a chunk, specifying the number of elements in each dimension.\n\nChunk size\n\nThe size of the chunk in terms of memory, which depends on the chunk shape.\n\nCoordinate Reference System\n\nA framework used to precisely measure locations on the surface of Earth as coordinates.\n\nLarger-than-memory\n\nA dataset whose memory footprint is too large to fit into memory all at once.\n\nPartial Chunk\n\nThe final chunk along a dimensions of a dataset that is not completely full of data due to the chosen chunk shape not being an integer divisor of the dataset’s dimensions.\n\nRechunking\n\nThe process of changing the current chunk shape of a dataset to another chunk shape.\n\nStored chunks\n\nThe chunks that are physically stored on disk.\n\nVirtual Zarr Store\n\nA virtual representation of a Zarr store generated by mapping any number of real datasets in individual files (e.g., NetCDF/HDF5, GRIB2, TIFF) together into a single, sliceable dataset via an interface layer, which contains information about the original files (e.g., chunking, compression, etc.).","type":"content","url":"/dataset-processing/tutorials/chunking/back/glossary","position":1},{"hierarchy":{"lvl1":"Chunking References"},"type":"lvl1","url":"/dataset-processing/tutorials/chunking/back","position":0},{"hierarchy":{"lvl1":"Chunking References"},"content":"HyTEST Project\n\nEnvironment Set-Up\n\nCompute Environments\n\nGeneral QuickStart\n\nCloud: Nebari Quick-Start\n\nHPC Server: Open OnDemand Quick-Start\n\nHPC Server: Jupyter Forward Quick-Start\n\nHPC Server: Start Script Quick-Start\n\nAWS Credentials\n\nStarting a Dask Cluster\n\nNebari Dask Cluster\n\nDenali HPC Dask Cluster\n\nHovenweep/Tallgrass HPC Dask Cluster\n\nLocal Desktop Dask Cluster\n\nGetting Started\n\nEssential Reading\n\nFoundations\n\nData Sources\n\nData/APIs\n\nData/Cloud Storage\n\nParallelism with Dask\n\nDataset Access\n\nData Catalogs\n\nData Storage Locations\n\nWater Mission Area STAC Catalog\n\nHyTEST Data Catalog (Intake)\n\nHyTEST Intake Sub-Catalogs\n\nCONUS404 Products Data Access\n\nCONUS404 Zarr Changelog\n\nExplore CONUS404 Dataset\n\nZarr Creation\n\nData Chunking Tutorial Overview\n\nIntroduction to Chunking\n\nWhy (re)Chunk Data?\n\nHow to Examine a Stored Dataset’s Chunk Shape\n\nBasics of Chunk Shape and Size\n\nWriting Chunked Files\n\nRechunking\n\nAdvanced Topics in Chunking\n\nRechunking Larger Datasets with Dask\n\nGenerating a Virtual Zarr Store\n\nAdding Coordinate Reference Systems (CRS) to Zarr Datasets\n\nChoosing an Optimal Chunk Size and Shape\n\nChunking References\n\nGlossary\n\nAdditional Resources\n\nDataset Processing\n\nCONUS404 Temporal Aggregation\n\nSpatial Aggregation\n\ngdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s\n\ngdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1\n\ngdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s\n\nPangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s\n\ngdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation\n\nCONUS404 Site Data Selection\n\nCONUS404 Regridding (Curvilinear => Rectilinear)\n\nModel Evaluation\n\nEvaluation Metrics\n\nD-Score Suite (v1) Benchmark\n\nD-Score Suite (v1) Benchmark -- Usage Examples\n\nStandard Suite (v1) Metrics\n\nStandard Suite (v1) Metrics -- Usage Examples\n\nStreamflow Model Evaluation\n\nStreamflow Eval :: Data Preparation\n\nStreamflow Eval :: StdSuite Analysis\n\nSteamflow Eval :: DScore Analysis\n\nStreamflow Eval :: Visualization\n\nBack Matter\n\nContributing\n\nLicense","type":"content","url":"/dataset-processing/tutorials/chunking/back","position":1},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection"},"type":"lvl1","url":"/dataset-processing/tutorials/conus404-point-selection","position":0},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection"},"content":"Pull CONUS404 data at a set of lat/lon point locations.\n\nimport fsspec\nimport xarray as xr\nimport hvplot.xarray\nimport intake\nimport pystac\nfrom packaging.version import Version\nimport zarr\nimport os\nimport cartopy.crs as ccrs\nimport metpy\nimport warnings\nimport pandas as pd\nimport s3fs\nimport xoak\nimport dask\nimport hvplot.pandas # to add .hvplot to DataFrames\nfrom dask.distributed import LocalCluster, Client\nwarnings.filterwarnings('ignore')\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-point-selection","position":1},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl2":"Open dataset from WMA STAC Catalog"},"type":"lvl2","url":"/dataset-processing/tutorials/conus404-point-selection#open-dataset-from-wma-stac-catalog","position":2},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl2":"Open dataset from WMA STAC Catalog"},"content":"\n\ndef get_children(catalog, collection_id=None):\n    \"\"\"\n    This function retrieves a specified collection from a STAC catalog/collection and prints key metadata \n    for exploring/accessing the datasets contained within it.\n    If there is no collection ID provided, the collections in the top level of the catalog will be printed.\n    If a collection ID is provided, it will retrieve the collection with that ID from the input catalog/collection.\n    If the collection ID points to a dataset, it will print the assets available for the dataset.\n    If the collection ID points to another collection, it will list the child collections in the IDed collection.\n\n    Args:\n        catalog (pystac.Catalog | pystac.Collection): The STAC catalog/collection object.\n        collection_id (str): The ID of the collection or dataset to retrieve from catalog.\n    \n    Returns:\n        collection (pystac.Catalog | pystac.Collection): The collection object corresponding to the provided ID\n                                                         or the top-level catalog if no ID is provided.\n    \"\"\"\n    dataset = False\n    if collection_id:\n        collection = catalog.get_child(collection_id)\n        if collection.assets:\n            dataset = True\n            print(f\"{collection_id} is a dataset. Please review the assets below and select one to open.\")\n\n        else:\n            print(f\"{collection_id} is a collection. Please review the child items and select one to open in the next cell.\")\n    else:\n        collection = catalog\n    if dataset==True:\n        # List the assets\n        for asset in collection.assets:\n            print(f\"Asset ID: {asset}\")\n            print(f\"    Title: {collection.assets[asset].title}\")\n            print(f\"    Description: {collection.assets[asset].description}\")\n    else:\n        collections = list(collection.get_collections())\n        print(f\"Number of collections: {len(collections)}\")\n        print(\"Collections IDs:\")\n        for child_collection in collections:\n            id = child_collection.id\n            cite_as = \"Not available\"\n            for link in child_collection.links:\n                if link.rel == \"cite-as\":\n                    cite_as = link.target\n            print(f\"- {id}, Source: {cite_as}\")\n    return collection\n\n\n\n# url for the WMA STAC Catalog\ncatalog_url = \"https://api.water.usgs.gov/gdp/pygeoapi/stac/stac-collection/\"\n\n# use pystac to read the catalog\ncatalog = pystac.Catalog.from_file(catalog_url)\n\n# list the collections in the catalog\ncatalog = get_children(catalog)\n\n\n\n# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(catalog, collection_id=\"conus404\")\n\n\n\n# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(collection, collection_id=\"conus404_xtrm_daily\")\n\n\n\n# replace with the asset ID you want to use:\nselected_asset_id = \"zarr-s3-osn\"\n\n# read the asset metadata\nasset = collection.assets[selected_asset_id]\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-point-selection#open-dataset-from-wma-stac-catalog","position":3},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl2":"Parallelize with Dask"},"type":"lvl2","url":"/dataset-processing/tutorials/conus404-point-selection#parallelize-with-dask","position":4},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl2":"Parallelize with Dask"},"content":"Some of the steps we will take are aware of parallel clustered compute environments\nusing dask. We’re going to start a cluster now so that future steps can take advantage\nof this ability.\n\nThis is an optional step, but speed ups data loading significantly, especially\nwhen accessing data from the cloud.\n\nWe have documentation on how to start a Dask Cluster in different computing environments \n\nhere.\n\n#%run ../../environment_set_up/Start_Dask_Cluster_Nebari.ipynb\n## If this notebook is not being run on Nebari, replace the above \n## path name with a helper appropriate to your compute environment.  Examples:\n# %run ../../environment_set_up/Start_Dask_Cluster_Denali.ipynb\n# %run ../../environment_set_up/Start_Dask_Cluster_Tallgrass.ipynb\n# %run ../../environment_set_up/Start_Dask_Cluster_Desktop.ipynb\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-point-selection#parallelize-with-dask","position":5},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl2":"Explore the dataset"},"type":"lvl2","url":"/dataset-processing/tutorials/conus404-point-selection#explore-the-dataset","position":6},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl2":"Explore the dataset"},"content":"\n\nif Version(zarr.__version__) < Version(\"3.0.0\"):\n    ds = xr.open_dataset(\n        asset.href,\n        storage_options=asset.extra_fields['xarray:storage_options'],\n        **asset.extra_fields['xarray:open_kwargs']\n    )\nelse:\n    ds = xr.open_dataset(\n    asset.href,\n    storage_options=asset.extra_fields['xarray:storage_options'],\n    **asset.extra_fields['xarray:open_kwargs'],\n    zarr_format=2\n    )\n\n\n\nds\n\n\n\n# variables of interest\nvar = ['SKINTEMPMEAN', 'SKINTEMPMAX', 'SKINTEMPMIN', 'SKINTEMPSTD', 'TSKINTEMPMAX', 'TSKINTEMPMIN']\n\n\n\nds_var = ds[var]\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-point-selection#explore-the-dataset","position":7},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl2":"Read in point data and clean"},"type":"lvl2","url":"/dataset-processing/tutorials/conus404-point-selection#read-in-point-data-and-clean","position":8},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl2":"Read in point data and clean"},"content":"\n\nhytest_cat = intake.open_catalog(\"https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml\")\npoints_df = hytest_cat['pointsample-tutorial-sites-osn'].read()\nprint(len(points_df))\npoints_df.head()\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-point-selection#read-in-point-data-and-clean","position":9},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl3":"Drop rows will null lat, lon","lvl2":"Read in point data and clean"},"type":"lvl3","url":"/dataset-processing/tutorials/conus404-point-selection#drop-rows-will-null-lat-lon","position":10},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl3":"Drop rows will null lat, lon","lvl2":"Read in point data and clean"},"content":"\n\npoints_df[points_df.isnull().any(axis=1)]\n\n\n\n# drop rows will null lat, lon\npoints_df.dropna(subset = ['longitude', 'latitude'], inplace=True)\nprint(len(points_df))\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-point-selection#drop-rows-will-null-lat-lon","position":11},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl3":"Set site_id as index","lvl2":"Read in point data and clean"},"type":"lvl3","url":"/dataset-processing/tutorials/conus404-point-selection#set-site-id-as-index","position":12},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl3":"Set site_id as index","lvl2":"Read in point data and clean"},"content":"\n\n#points_df = points_df.set_index(['site_id', 'longitude', 'latitude'])\npoints_df = points_df.set_index(['site_id'])\nprint(len(points_df))\n\n\n\npoints_df.head()\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-point-selection#set-site-id-as-index","position":13},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl3":"Make sure no site ids are duplicated","lvl2":"Read in point data and clean"},"type":"lvl3","url":"/dataset-processing/tutorials/conus404-point-selection#make-sure-no-site-ids-are-duplicated","position":14},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl3":"Make sure no site ids are duplicated","lvl2":"Read in point data and clean"},"content":"\n\npoints_df[points_df.index.duplicated()]\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-point-selection#make-sure-no-site-ids-are-duplicated","position":15},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl3":"Transform into xarray dataset","lvl2":"Read in point data and clean"},"type":"lvl3","url":"/dataset-processing/tutorials/conus404-point-selection#transform-into-xarray-dataset","position":16},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl3":"Transform into xarray dataset","lvl2":"Read in point data and clean"},"content":"\n\npoints_ds = xr.Dataset.from_dataframe(points_df)\n\n\n\npoints_ds\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-point-selection#transform-into-xarray-dataset","position":17},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl2":"Find data values at point locations"},"type":"lvl2","url":"/dataset-processing/tutorials/conus404-point-selection#find-data-values-at-point-locations","position":18},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl2":"Find data values at point locations"},"content":"First we will use xoak.set_index (\n\ndocs to set up an index tree that will enable efficient indexing for the lat and lon coordinates in the CONUS404 data subset. We will choose the sklearn_geo_balltree method for indexing, which uses the Haversine distance metric and is a good choice for indexing latitude / longitude points.\n\nds_var.xoak.set_index(['lat', 'lon'], 'sklearn_geo_balltree')\n\n\n\nds_var.xoak.index\n\n\n\n#from dask.diagnostics import ProgressBar\n#with ProgressBar(), dask.config.set(scheduler='processes'):\nds_selection = ds_var.xoak.sel(lat=points_ds.latitude, lon=points_ds.longitude)\n\n\n\nds_selection\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-point-selection#find-data-values-at-point-locations","position":19},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl2":"Join selected data back to gage data with site ID, source, lat, lon"},"type":"lvl2","url":"/dataset-processing/tutorials/conus404-point-selection#join-selected-data-back-to-gage-data-with-site-id-source-lat-lon","position":20},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl2":"Join selected data back to gage data with site ID, source, lat, lon"},"content":"\n\nds_selection = xr.merge([points_ds, ds_selection])\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-point-selection#join-selected-data-back-to-gage-data-with-site-id-source-lat-lon","position":21},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl2":"Visualize data to verify results"},"type":"lvl2","url":"/dataset-processing/tutorials/conus404-point-selection#visualize-data-to-verify-results","position":22},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl2":"Visualize data to verify results"},"content":"\n\nidx = 300\n\n\n\nda = ds_selection.isel(time=idx).load()\ndf = da.to_dataframe()\n\n\n\ndf.hvplot.scatter(x='lon', y='lat', c=var[0], colormap='viridis').opts(clim=(260, 300))\n\n\n\nda_grid = ds_var[var[0]].isel(time=idx).load()\nda_grid.hvplot.quadmesh(x='lon', y='lat', rasterize=True, geo=True, tiles='OSM', cmap='viridis').opts('Image', clim=(260, 300))\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-point-selection#visualize-data-to-verify-results","position":23},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl2":"Clean up data for saving"},"type":"lvl2","url":"/dataset-processing/tutorials/conus404-point-selection#clean-up-data-for-saving","position":24},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl2":"Clean up data for saving"},"content":"\n\n# drop CONUS404 grid cell lat/lon, x/y values that data were pulled from, keeping only site's lat/lon to reduce confusion\nds_save = ds_selection.drop_vars([\"lat\", \"lon\", \"x\", \"y\"])\n\n\n\nds_save\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-point-selection#clean-up-data-for-saving","position":25},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl2":"Save netcdf to OSN pod"},"type":"lvl2","url":"/dataset-processing/tutorials/conus404-point-selection#save-netcdf-to-osn-pod","position":26},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl2":"Save netcdf to OSN pod"},"content":"\n\nfs_write = fsspec.filesystem(\n    's3',\n    profile='osn-hytest',  ## This is the name of the AWS profile with credentials to write to the output bucket\n    client_kwargs={'endpoint_url': 'https://usgs.osn.mghpcc.org/'}\n)\n\n\n\nfs_write.ls('hytest')\n# get intake catalog entry url (intake catalog entry has already been created) to use to write the file\noutfile = hytest_cat['pointsample-tutorial-output-osn']._entry._yaml()['sources']['pointsample-tutorial-output-osn']['args']['urlpath']\nlocal_file = outfile.split('/')[-1]\n\n\n\nUncomment next two cells to save\n\n# %%time\n# ds_save.to_netcdf(local_file)\n\n\n\n# %%time\n# fs_write.upload(local_file, outfile)\n\n\n\n# check that file has been written\nfs_write.ls(outfile.split(local_file)[0])\n\n\n\nprint(f'netcdf file size is {fs_write.size(outfile) / 1048576} MB')\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-point-selection#save-netcdf-to-osn-pod","position":27},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl3":"Read back in the nc data to verify it saved correctly","lvl2":"Save netcdf to OSN pod"},"type":"lvl3","url":"/dataset-processing/tutorials/conus404-point-selection#read-back-in-the-nc-data-to-verify-it-saved-correctly","position":28},{"hierarchy":{"lvl1":"CONUS404 Site Data Selection","lvl3":"Read back in the nc data to verify it saved correctly","lvl2":"Save netcdf to OSN pod"},"content":"\n\nwith fs_write.open(outfile) as f:\n    ds_final = xr.open_dataset(f)\n\n\n\nds_final\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-point-selection#read-back-in-the-nc-data-to-verify-it-saved-correctly","position":29},{"hierarchy":{"lvl1":"CONUS404 Regridding (Curvilinear => Rectilinear)"},"type":"lvl1","url":"/dataset-processing/tutorials/conus404-regrid","position":0},{"hierarchy":{"lvl1":"CONUS404 Regridding (Curvilinear => Rectilinear)"},"content":"Create a rectilinear grid (1D lon/lat coordinates) for a specific region. Extract spatial and temporal subset of regridded data to a netcdf file. (Extraction to netcdf may also be done for curvilinear grid.)\n\n%%time\nimport os \n# Needed when boto3 >= 1.36.0 or the rechunking process will fail\n# This needs to be set before the boto3 library gets loaded\n# See: https://github.com/aws/aws-cli/issues/9214#issuecomment-2606619168\nos.environ['AWS_REQUEST_CHECKSUM_CALCULATION'] = 'when_required'\nos.environ['AWS_RESPONSE_CHECKSUM_VALIDATION'] = 'when_required'\nimport xarray as xr\nimport xesmf as xe\nimport numpy as np\nimport fsspec\nimport hvplot.xarray\nimport geoviews as gv\nfrom matplotlib import path \nimport pystac\nfrom packaging.version import Version\nimport zarr\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-regrid","position":1},{"hierarchy":{"lvl1":"CONUS404 Regridding (Curvilinear => Rectilinear)","lvl2":"Open dataset from WMA STAC Catalog"},"type":"lvl2","url":"/dataset-processing/tutorials/conus404-regrid#open-dataset-from-wma-stac-catalog","position":2},{"hierarchy":{"lvl1":"CONUS404 Regridding (Curvilinear => Rectilinear)","lvl2":"Open dataset from WMA STAC Catalog"},"content":"\n\ndef get_children(catalog, collection_id=None):\n    \"\"\"\n    This function retrieves a specified collection from a STAC catalog/collection and prints key metadata \n    for exploring/accessing the datasets contained within it.\n    If there is no collection ID provided, the collections in the top level of the catalog will be printed.\n    If a collection ID is provided, it will retrieve the collection with that ID from the input catalog/collection.\n    If the collection ID points to a dataset, it will print the assets available for the dataset.\n    If the collection ID points to another collection, it will list the child collections in the IDed collection.\n\n    Args:\n        catalog (pystac.Catalog | pystac.Collection): The STAC catalog/collection object.\n        collection_id (str): The ID of the collection or dataset to retrieve from catalog.\n    \n    Returns:\n        collection (pystac.Catalog | pystac.Collection): The collection object corresponding to the provided ID\n                                                         or the top-level catalog if no ID is provided.\n    \"\"\"\n    dataset = False\n    if collection_id:\n        collection = catalog.get_child(collection_id)\n        if collection.assets:\n            dataset = True\n            print(f\"{collection_id} is a dataset. Please review the assets below and select one to open.\")\n\n        else:\n            print(f\"{collection_id} is a collection. Please review the child items and select one to open in the next cell.\")\n    else:\n        collection = catalog\n    if dataset==True:\n        # List the assets\n        for asset in collection.assets:\n            print(f\"Asset ID: {asset}\")\n            print(f\"    Title: {collection.assets[asset].title}\")\n            print(f\"    Description: {collection.assets[asset].description}\")\n    else:\n        collections = list(collection.get_collections())\n        print(f\"Number of collections: {len(collections)}\")\n        print(\"Collections IDs:\")\n        for child_collection in collections:\n            id = child_collection.id\n            cite_as = \"Not available\"\n            for link in child_collection.links:\n                if link.rel == \"cite-as\":\n                    cite_as = link.target\n            print(f\"- {id}, Source: {cite_as}\")\n    return collection\n\n\n\n# url for the WMA STAC Catalog\ncatalog_url = \"https://api.water.usgs.gov/gdp/pygeoapi/stac/stac-collection/\"\n\n# use pystac to read the catalog\ncatalog = pystac.Catalog.from_file(catalog_url)\n\n# list the collections in the catalog\ncatalog = get_children(catalog)\n\n\n\n# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(catalog, collection_id=\"conus404\")\n\n\n\n# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(collection, collection_id=\"conus404_hourly\")\n\n\n\n# replace with the asset ID you want to use:\nselected_asset_id = \"zarr-s3-osn\"\n\n# read the asset metadata\nasset = collection.assets[selected_asset_id]\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-regrid#open-dataset-from-wma-stac-catalog","position":3},{"hierarchy":{"lvl1":"CONUS404 Regridding (Curvilinear => Rectilinear)","lvl2":"2) Set Up AWS Credentials (Optional)"},"type":"lvl2","url":"/dataset-processing/tutorials/conus404-regrid#id-2-set-up-aws-credentials-optional","position":4},{"hierarchy":{"lvl1":"CONUS404 Regridding (Curvilinear => Rectilinear)","lvl2":"2) Set Up AWS Credentials (Optional)"},"content":"This notebook reads data from the OSN pod by default, which is object store data on a high speed internet connection that is free to access from any environment. If you change this notebook to use one of the CONUS404 datasets stored on S3 (options ending in -cloud), you will be pulling data from a requester-pays S3 bucket. This means you have to set up your AWS credentials, else we won’t be able to load the data. Please note that reading the -cloud data from S3 may incur charges if you are reading data outside of the us-west-2 region or running the notebook outside of the cloud altogether. If you would like to access one of the -cloud options, uncomment and run the following code snippet to set up your AWS credentials. You can find more info about this AWS helper function \n\nhere.\n\n# uncomment the lines below to read in your AWS credentials if you want to access data from a requester-pays bucket (-cloud)\n# os.environ['AWS_PROFILE'] = 'default'\n# %run ../../environment_set_up/Help_AWS_Credentials.ipynb\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-regrid#id-2-set-up-aws-credentials-optional","position":5},{"hierarchy":{"lvl1":"CONUS404 Regridding (Curvilinear => Rectilinear)","lvl2":"Parallelize with Dask"},"type":"lvl2","url":"/dataset-processing/tutorials/conus404-regrid#parallelize-with-dask","position":6},{"hierarchy":{"lvl1":"CONUS404 Regridding (Curvilinear => Rectilinear)","lvl2":"Parallelize with Dask"},"content":"Some of the steps we will take are aware of parallel clustered compute environments\nusing dask. We’re going to start a cluster now so that future steps can take advantage\nof this ability.\n\nThis is an optional step, but speed ups data loading significantly, especially\nwhen accessing data from the cloud.\n\nWe have documentation on how to start a Dask Cluster in different computing environments \n\nhere.\n\n%run ../../environment_set_up/Start_Dask_Cluster_Nebari.ipynb\n## If this notebook is not being run on Nebari/, replace the above \n## path name with a helper appropriate to your compute environment.  Examples:\n# %run ../../environment_set_up/Start_Dask_Cluster_Denali.ipynb\n# %run ../../environment_set_up/Start_Dask_Cluster_Tallgrass.ipynb\n# %run ../../environment_set_up/Start_Dask_Cluster_Desktop.ipynb\n\n\n\nif Version(zarr.__version__) < Version(\"3.0.0\"):\n    ds = xr.open_dataset(\n        asset.href,\n        storage_options=asset.extra_fields['xarray:storage_options'],\n        **asset.extra_fields['xarray:open_kwargs']\n    )\nelse:\n    ds = xr.open_dataset(\n    asset.href,\n    storage_options=asset.extra_fields['xarray:storage_options'],\n    **asset.extra_fields['xarray:open_kwargs'],\n    zarr_format=2\n    )\n\n\n\nds\n\n\n\nnc_outfile = 'CONUS404_DRB_rectilinear.nc'\nbbox = [-75.9, -74.45, 38.7, 42.55]\ndx = dy = 3./111.    # 3km grid\nvars_out = ['T2', 'SNOW']\nstart = '2017-04-01 00:00'\nstop  = '2017-05-01 00:00'\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-regrid#parallelize-with-dask","position":7},{"hierarchy":{"lvl1":"CONUS404 Regridding (Curvilinear => Rectilinear)","lvl3":"Use xESMF to regrid","lvl2":"Parallelize with Dask"},"type":"lvl3","url":"/dataset-processing/tutorials/conus404-regrid#use-xesmf-to-regrid","position":8},{"hierarchy":{"lvl1":"CONUS404 Regridding (Curvilinear => Rectilinear)","lvl3":"Use xESMF to regrid","lvl2":"Parallelize with Dask"},"content":"xESMF is a xarray-enabled interface to the ESMF regridder from NCAR.\nESMF has options for regridding between curvilinear, rectilinear, and unstructured grids, with conservative regridding options, and much more\n\ndef bbox2ij(lon,lat,bbox=[-160., -155., 18., 23.]):\n    \"\"\"Return indices for i,j that will completely cover the specified bounding box.     \n    i0,i1,j0,j1 = bbox2ij(lon,lat,bbox)\n    lon,lat = 2D arrays that are the target of the subset\n    bbox = list containing the bounding box: [lon_min, lon_max, lat_min, lat_max]\n\n    Example\n    -------  \n    >>> i0,i1,j0,j1 = bbox2ij(lon_rho,[-71, -63., 39., 46])\n    >>> h_subset = nc.variables['h'][j0:j1,i0:i1]       \n    \"\"\"\n    bbox=np.array(bbox)\n    mypath=np.array([bbox[[0,1,1,0]],bbox[[2,2,3,3]]]).T\n    p = path.Path(mypath)\n    points = np.vstack((lon.ravel(),lat.ravel())).T   \n    n,m = np.shape(lon)\n    inside = p.contains_points(points).reshape((n,m))\n    ii,jj = np.meshgrid(range(m),range(n))\n    return min(ii[inside]),max(ii[inside]),min(jj[inside]),max(jj[inside])\n\n\n\nBefore we regrid to rectilinear, let’s subset a region that covers our area of interest.  Becuase lon,lat are 2D arrays, we can’t just use xarray to slice these coordinate variables.  So we have a routine that finds the i,j locations of a specified bounding box, and then slice on those.\n\ni0,i1,j0,j1 = bbox2ij(ds['lon'].values, ds['lat'].values, bbox=bbox)\nprint(i0,i1,j0,j1)\n\n\n\nds_subset = ds.isel(x=slice(i0-1,i1+1), y=slice(j0-1,j1+1))\n\n\n\nds_subset = ds_subset.sel(time=slice(start,stop))\n\n\n\nds_subset\n\n\n\nds_subset.nbytes/1e9\n\n\n\nda = ds_subset.T2.sel(time='2017-04-25 00:00', method='nearest')\nviz = da.hvplot.quadmesh(x='lon', y='lat', geo=True, rasterize=True, cmap='turbo')\nbase = gv.tile_sources.OSM\nbase * viz.opts(alpha=0.5)\n\n\n\nds_subset.nbytes/1e9\n\n\n\n%%time\nds_subset = ds_subset.chunk({'x':-1, 'y':-1, 'time':24})\n\n\n\n%%time\nds_out = xr.Dataset({'lon': (['lon'], np.arange(bbox[0], bbox[1], dx)),\n                     'lat': (['lat'], np.arange(bbox[2], bbox[3], dy))})\n\nregridder = xe.Regridder(ds_subset, ds_out, 'bilinear')\nregridder\n\n\n\n%%time\nds_out = regridder(ds_subset[vars_out])\nprint(ds_out)\n\n\n\nds_out['SNOW']\n\n\n\nlist(ds_out.variables)\n\n\n\nlist(ds_out.data_vars)\n\n\n\nds_out['T2'].encoding\n\n\n\nds_out.time\n\n\n\nencoding={}\nfor var in ds_out.variables:\n    encoding[var] = dict(zlib=True, complevel=2, \n                         fletcher32=False, shuffle=True,\n                         _FillValue=None\n                        )\n\n\n\n# you will need to update the filepaths and uncomment the following line to save out your data.\nds_out.load().to_netcdf(nc_outfile, encoding=encoding, mode='w')\n\n\n\nds_nc = xr.open_dataset(nc_outfile)\n\n\n\nds_nc\n\n\n\n(ds_nc['T2']-273.15).hvplot(x='lon',y='lat', geo=True,\n                rasterize=True, cmap='turbo', \n                tiles='OSM', clim=(2,15))\n\n\n\nds_outcl = ds_subset[vars_out]\n\n\n\nlist(ds_outcl.data_vars)\n\n\n\nencoding={}\nfor var in ds_outcl.variables:\n    encoding[var] = dict(zlib=True, complevel=2, \n                         fletcher32=False, shuffle=True,\n                         _FillValue=None\n                        )\n\n\n\n# you will need to update the filepaths and uncomment the following line to save out your data.\n# ds_outcl.load().to_netcdf('CONUS404_DRB_curvilinear.nc', encoding=encoding, mode='w')\n\n\n\nclient.close(); cluster.shutdown()\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-regrid#use-xesmf-to-regrid","position":9},{"hierarchy":{"lvl1":"CONUS404 Temporal Aggregation"},"type":"lvl1","url":"/dataset-processing/tutorials/conus404-temporal-aggregation","position":0},{"hierarchy":{"lvl1":"CONUS404 Temporal Aggregation"},"content":"Create daily averages from hourly data, write to a zarr dataset\n\nimport os\n# Needed when boto3 >= 1.36.0 or the rechunking process will fail\n# This needs to be set before the boto3 library gets loaded\n# See: https://github.com/aws/aws-cli/issues/9214#issuecomment-2606619168\nos.environ['AWS_REQUEST_CHECKSUM_CALCULATION'] = 'when_required'\nos.environ['AWS_RESPONSE_CHECKSUM_VALIDATION'] = 'when_required'\nimport fsspec\nimport xarray as xr\nimport hvplot.xarray\nimport pystac\nfrom packaging.version import Version\nimport zarr\nimport warnings\nfrom dask.distributed import LocalCluster, Client\nwarnings.filterwarnings('ignore')\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-temporal-aggregation","position":1},{"hierarchy":{"lvl1":"CONUS404 Temporal Aggregation","lvl2":"Open dataset from WMA STAC Catalog"},"type":"lvl2","url":"/dataset-processing/tutorials/conus404-temporal-aggregation#open-dataset-from-wma-stac-catalog","position":2},{"hierarchy":{"lvl1":"CONUS404 Temporal Aggregation","lvl2":"Open dataset from WMA STAC Catalog"},"content":"\n\ndef get_children(catalog, collection_id=None):\n    \"\"\"\n    This function retrieves a specified collection from a STAC catalog/collection and prints key metadata \n    for exploring/accessing the datasets contained within it.\n    If there is no collection ID provided, the collections in the top level of the catalog will be printed.\n    If a collection ID is provided, it will retrieve the collection with that ID from the input catalog/collection.\n    If the collection ID points to a dataset, it will print the assets available for the dataset.\n    If the collection ID points to another collection, it will list the child collections in the IDed collection.\n\n    Args:\n        catalog (pystac.Catalog | pystac.Collection): The STAC catalog/collection object.\n        collection_id (str): The ID of the collection or dataset to retrieve from catalog.\n    \n    Returns:\n        collection (pystac.Catalog | pystac.Collection): The collection object corresponding to the provided ID\n                                                         or the top-level catalog if no ID is provided.\n    \"\"\"\n    dataset = False\n    if collection_id:\n        collection = catalog.get_child(collection_id)\n        if collection.assets:\n            dataset = True\n            print(f\"{collection_id} is a dataset. Please review the assets below and select one to open.\")\n\n        else:\n            print(f\"{collection_id} is a collection. Please review the child items and select one to open in the next cell.\")\n    else:\n        collection = catalog\n    if dataset==True:\n        # List the assets\n        for asset in collection.assets:\n            print(f\"Asset ID: {asset}\")\n            print(f\"    Title: {collection.assets[asset].title}\")\n            print(f\"    Description: {collection.assets[asset].description}\")\n    else:\n        collections = list(collection.get_collections())\n        print(f\"Number of collections: {len(collections)}\")\n        print(\"Collections IDs:\")\n        for child_collection in collections:\n            id = child_collection.id\n            cite_as = \"Not available\"\n            for link in child_collection.links:\n                if link.rel == \"cite-as\":\n                    cite_as = link.target\n            print(f\"- {id}, Source: {cite_as}\")\n    return collection\n\n\n\n# url for the WMA STAC Catalog\ncatalog_url = \"https://api.water.usgs.gov/gdp/pygeoapi/stac/stac-collection/\"\n\n# use pystac to read the catalog\ncatalog = pystac.Catalog.from_file(catalog_url)\n\n# list the collections in the catalog\ncatalog = get_children(catalog)\n\n\n\n# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(catalog, collection_id=\"conus404\")\n\n\n\n# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(collection, collection_id=\"conus404_hourly\")\n\n\n\n# replace with the asset ID you want to use:\nselected_asset_id = \"zarr-s3-osn\"\n\n# read the asset metadata\nasset = collection.assets[selected_asset_id]\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-temporal-aggregation#open-dataset-from-wma-stac-catalog","position":3},{"hierarchy":{"lvl1":"CONUS404 Temporal Aggregation","lvl2":"2) Set Up AWS Credentials (Optional)"},"type":"lvl2","url":"/dataset-processing/tutorials/conus404-temporal-aggregation#id-2-set-up-aws-credentials-optional","position":4},{"hierarchy":{"lvl1":"CONUS404 Temporal Aggregation","lvl2":"2) Set Up AWS Credentials (Optional)"},"content":"This notebook reads data from the OSN pod by default, which is object store data on a high speed internet connection that is free to access from any environment. If you change this notebook to use one of the CONUS404 datasets stored on S3 (options ending in -cloud), you will be pulling data from a requester-pays S3 bucket. This means you have to set up your AWS credentials, else we won’t be able to load the data. Please note that reading the -cloud data from S3 may incur charges if you are reading data outside of the us-west-2 region or running the notebook outside of the cloud altogether. If you would like to access one of the -cloud options, uncomment and run the following code snippet to set up your AWS credentials. You can find more info about this AWS helper function \n\nhere.\n\n# uncomment the lines below to read in your AWS credentials if you want to access data from a requester-pays bucket (-cloud)\n# os.environ['AWS_PROFILE'] = 'default'\n# %run ../../environment_set_up/Help_AWS_Credentials.ipynb\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-temporal-aggregation#id-2-set-up-aws-credentials-optional","position":5},{"hierarchy":{"lvl1":"CONUS404 Temporal Aggregation","lvl2":"Parallelize with Dask"},"type":"lvl2","url":"/dataset-processing/tutorials/conus404-temporal-aggregation#parallelize-with-dask","position":6},{"hierarchy":{"lvl1":"CONUS404 Temporal Aggregation","lvl2":"Parallelize with Dask"},"content":"Some of the steps we will take are aware of parallel clustered compute environments\nusing dask. We’re going to start a cluster now so that future steps can take advantage\nof this ability.\n\nThis is an optional step, but speed ups data loading significantly, especially\nwhen accessing data from the cloud.\n\nWe have documentation on how to start a Dask Cluster in different computing environments \n\nhere.\n\n%run ../../environment_set_up/Start_Dask_Cluster_Nebari.ipynb\n## If this notebook is not being run on Nebari, replace the above \n## path name with a helper appropriate to your compute environment.  Examples:\n# %run ../../environment_set_up/Start_Dask_Cluster_Denali.ipynb\n# %run ../../environment_set_up/Start_Dask_Cluster_Tallgrass.ipynb\n# %run ../../environment_set_up/Start_Dask_Cluster_Desktop.ipynb\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-temporal-aggregation#parallelize-with-dask","position":7},{"hierarchy":{"lvl1":"CONUS404 Temporal Aggregation","lvl2":"Explore the dataset"},"type":"lvl2","url":"/dataset-processing/tutorials/conus404-temporal-aggregation#explore-the-dataset","position":8},{"hierarchy":{"lvl1":"CONUS404 Temporal Aggregation","lvl2":"Explore the dataset"},"content":"\n\nif Version(zarr.__version__) < Version(\"3.0.0\"):\n    ds = xr.open_dataset(\n        asset.href,\n        storage_options=asset.extra_fields['xarray:storage_options'],\n        **asset.extra_fields['xarray:open_kwargs']\n    )\nelse:\n    ds = xr.open_dataset(\n    asset.href,\n    storage_options=asset.extra_fields['xarray:storage_options'],\n    **asset.extra_fields['xarray:open_kwargs'],\n    zarr_format=2\n    )\n\n\n\nds\n\n\n\nds.T2\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-temporal-aggregation#explore-the-dataset","position":9},{"hierarchy":{"lvl1":"CONUS404 Temporal Aggregation","lvl2":"Daily averages"},"type":"lvl2","url":"/dataset-processing/tutorials/conus404-temporal-aggregation#daily-averages","position":10},{"hierarchy":{"lvl1":"CONUS404 Temporal Aggregation","lvl2":"Daily averages"},"content":"Time averages of any type are easy to do with xarray.   Here we do 24 hour averages, and set the time offset to 12 hours, so that the time values are in the middle of the averaging period.\n\nDigital Earth Africa has a great \n\nWorking with Time in Xarray tutorial.\n\nIn the example below we just do a few days with a few variables as a quick demo.\n\n%%time\nds_subset = ds[['T2','U10']].sel(time=slice('2017-01-02','2017-01-13'))\n\n\n\nds_subset_daily = ds_subset.resample(time=\"24H\", offset=\"12h\", label='right').mean()\n\n\n\nds_subset_daily\n\n\n\nds_subset_daily.hvplot.quadmesh(x='lon', y='lat', rasterize=True, \n                             geo=True, tiles='OSM', alpha=0.7, cmap='turbo')\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-temporal-aggregation#daily-averages","position":11},{"hierarchy":{"lvl1":"CONUS404 Temporal Aggregation","lvl3":"Write daily values as a Zarr dataset (to onprem or cloud)","lvl2":"Daily averages"},"type":"lvl3","url":"/dataset-processing/tutorials/conus404-temporal-aggregation#write-daily-values-as-a-zarr-dataset-to-onprem-or-cloud","position":12},{"hierarchy":{"lvl1":"CONUS404 Temporal Aggregation","lvl3":"Write daily values as a Zarr dataset (to onprem or cloud)","lvl2":"Daily averages"},"content":"You will need to to turn the following cell from raw to code and update the filepaths in order to save out your data.\n\n%%time\nif 'SLURM_CLUSTER_NAME' in os.environ:     # on prem (Caldera filesystem)\n    ds_subset_daily.to_zarr('/caldera/usgs/change-me/conus_subset_daily.zarr', mode='w', consolidated=True)\nelse:                                      # cloud (AWS S3 nhgf-development bucket)\n    fs_s3 = fsspec.filesystem('s3', anon=False)\n    ds_subset_daily.to_zarr(fs_s3.get_mapper('s3://esip-qhub/testing/conus_subset_daily.zarr'), mode='w', consolidated=True)\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-temporal-aggregation#write-daily-values-as-a-zarr-dataset-to-onprem-or-cloud","position":13},{"hierarchy":{"lvl1":"CONUS404 Temporal Aggregation","lvl2":"Shutdown cluster"},"type":"lvl2","url":"/dataset-processing/tutorials/conus404-temporal-aggregation#shutdown-cluster","position":14},{"hierarchy":{"lvl1":"CONUS404 Temporal Aggregation","lvl2":"Shutdown cluster"},"content":"\n\nclient.close(); cluster.shutdown()\n\n","type":"content","url":"/dataset-processing/tutorials/conus404-temporal-aggregation#shutdown-cluster","position":15},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s"},"type":"lvl1","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation","position":0},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s"},"content":"In this notebook, we will be showing how to aggregate gridded data to polygons. The method aggregates gridded data conservatively, i.e. by exactly partitioning each grid cell into the precise region boundaries. The method makes use of two key packages \n\nxarray and \n\ngeopandas. Our implementation is based off of this \n\nPangeo Discourse, which we have updated using more streamlined methods.\n\nThe overall approach consists of:\n\nRepresent both the original gridded data and target polygons as \n\ngeopandas.GeoSeries objects (with vector geometries).\n\nCompute their area overlay and turn it into a sparse matrix of cell weights.\n\nPerform weighted aggregation using \n\nxarray.Dataset.weighted along the spatial dimensions.\n\nIt is quite fast and transparent.\n\nThe spatial polygons used in this notebook come from the \n\nNHDPlusV2 snapshot of the Watershed Boundary Dataset HUC12 boundaries provided through the \n\nPyGeoHydro python package.\n\nWe use the HyTest intake catalog to access CONUS404 from the OSN pod. This notebook provides a relatively simple and efficient workflow that can be easily run on a local computer.\n\n%xmode minimal\nimport os\n# Needed when boto3 >= 1.36.0 or the rechunking process will fail\n# This needs to be set before the boto3 library gets loaded\n# See: https://github.com/aws/aws-cli/issues/9214#issuecomment-2606619168\nos.environ['AWS_REQUEST_CHECKSUM_CALCULATION'] = 'when_required'\nos.environ['AWS_RESPONSE_CHECKSUM_VALIDATION'] = 'when_required'\nimport xarray as xr\nimport geopandas as gp\nimport pandas as pd\nimport numpy as np\nimport sparse\n\nimport hvplot.pandas\nimport hvplot.xarray\nimport dask\nimport cf_xarray\n\nfrom pynhd import NLDI, WaterData\nfrom pygeohydro import watershed\nimport pystac\nfrom packaging.version import Version\nimport zarr\nimport cartopy.crs as ccrs\nfrom shapely.geometry import Polygon\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation","position":1},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Open Dataset from Intake Catalog"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#open-dataset-from-intake-catalog","position":2},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Open Dataset from Intake Catalog"},"content":"First, let’s begin by loading the CONUS404 daily data.\n\ndef get_children(catalog, collection_id=None):\n    \"\"\"\n    This function retrieves a specified collection from a STAC catalog/collection and prints key metadata \n    for exploring/accessing the datasets contained within it.\n    If there is no collection ID provided, the collections in the top level of the catalog will be printed.\n    If a collection ID is provided, it will retrieve the collection with that ID from the input catalog/collection.\n    If the collection ID points to a dataset, it will print the assets available for the dataset.\n    If the collection ID points to another collection, it will list the child collections in the IDed collection.\n\n    Args:\n        catalog (pystac.Catalog | pystac.Collection): The STAC catalog/collection object.\n        collection_id (str): The ID of the collection or dataset to retrieve from catalog.\n    \n    Returns:\n        collection (pystac.Catalog | pystac.Collection): The collection object corresponding to the provided ID\n                                                         or the top-level catalog if no ID is provided.\n    \"\"\"\n    dataset = False\n    if collection_id:\n        collection = catalog.get_child(collection_id)\n        if collection.assets:\n            dataset = True\n            print(f\"{collection_id} is a dataset. Please review the assets below and select one to open.\")\n\n        else:\n            print(f\"{collection_id} is a collection. Please review the child items and select one to open in the next cell.\")\n    else:\n        collection = catalog\n    if dataset==True:\n        # List the assets\n        for asset in collection.assets:\n            print(f\"Asset ID: {asset}\")\n            print(f\"    Title: {collection.assets[asset].title}\")\n            print(f\"    Description: {collection.assets[asset].description}\")\n    else:\n        collections = list(collection.get_collections())\n        print(f\"Number of collections: {len(collections)}\")\n        print(\"Collections IDs:\")\n        for child_collection in collections:\n            id = child_collection.id\n            cite_as = \"Not available\"\n            for link in child_collection.links:\n                if link.rel == \"cite-as\":\n                    cite_as = link.target\n            print(f\"- {id}, Source: {cite_as}\")\n    return collection\n\n\n\n# url for the WMA STAC Catalog\ncatalog_url = \"https://api.water.usgs.gov/gdp/pygeoapi/stac/stac-collection/\"\n\n# use pystac to read the catalog\ncatalog = pystac.Catalog.from_file(catalog_url)\n\n# list the collections in the catalog\ncatalog = get_children(catalog)\n\n\n\n# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(catalog, collection_id=\"conus404\")\n\n\n\n# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(collection, collection_id=\"conus404_daily\")\n\n\n\n# replace with the asset ID you want to use:\nselected_asset_id = \"zarr-s3-osn\"\n\n# read the asset metadata\nasset = collection.assets[selected_asset_id]\n\n\n\nAs we can see there are two different locations for the conus404_daily data set. The locations are (1) -hovenweep meaning it is stored on the USGS Hovenweep HPC and (2) -osn meaning the data is on the USGS open storage network (OSN). As the OSN is free to access from any environment, we will use that for this example, but the location can easily be changed depending on your needs.\n\n# uncomment the lines below to read in your AWS credentials if you want to access data from a requester-pays bucket (-cloud)\n# os.environ['AWS_PROFILE'] = 'default'\n# %run ../../../environment_set_up/Help_AWS_Credentials.ipynb\n\n\n\nFinally, read in the daily CONUS404 data set.\n\nif Version(zarr.__version__) < Version(\"3.0.0\"):\n    conus404 = xr.open_dataset(\n        asset.href,\n        storage_options=asset.extra_fields['xarray:storage_options'],\n        **asset.extra_fields['xarray:open_kwargs']\n    )\nelse:\n    conus404 = xr.open_dataset(\n    asset.href,\n    storage_options=asset.extra_fields['xarray:storage_options'],\n    **asset.extra_fields['xarray:open_kwargs'],\n    zarr_format=2\n    )\n\nconus404\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#open-dataset-from-intake-catalog","position":3},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Parallelize with Dask (optional)"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#parallelize-with-dask-optional","position":4},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Parallelize with Dask (optional)"},"content":"Some of the steps we will take are aware of parallel clustered compute environments using \n\ndask. We can start a cluster now so that future steps take advantage\nof this ability. This is an optional step, but speed ups data loading significantly, especially when accessing data from the cloud.\n\nWe have documentation on how to start a Dask Cluster in different computing environments \n\nhere. Uncomment the cluster start up that works for your compute environment.\n\n%run ../../../environment_set_up/Start_Dask_Cluster_Nebari.ipynb\n## If this notebook is not being run on Nebari, replace the above \n## path name with a helper appropriate to your compute environment.  Examples:\n# %run ../../../environment_set_up/Start_Dask_Cluster_Denali.ipynb\n# %run ../../../environment_set_up/Start_Dask_Cluster_Tallgrass.ipynb\n# %run ../../../environment_set_up/Start_Dask_Cluster_Desktop.ipynb\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#parallelize-with-dask-optional","position":5},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Load the Feature Polygons"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#load-the-feature-polygons","position":6},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Load the Feature Polygons"},"content":"Now that we have read in the CONUS404 data, we need to read in some polygons to aggregate the data. For this example, we will use the HUC12 basins within the Delaware River Basin. To get these HUC12 polygons, we can use \n\npygeohydro.watershed to query the Hydro Network Linked Data Index (NLDI). All we need to get the basins is the general IDs of the HUC12 basins. For the Delaware Basin those are ones that start with 020401 or 020402.\n\n%%time\nwbd = watershed.WBD(\"huc4\")\ndelaware_basin = wbd.byids(field=\"huc4\", fids=\"0204\")\nhuc12_basins = WaterData('wbd12').bygeom(delaware_basin.iloc[0].geometry)\nhuc12_basins = huc12_basins[huc12_basins['huc12'].str.startswith(('020401', '020402'))]\nhuc12_basins\n\n\n\nLet’s plot the HUC12 basins to see how they look.\n\nhuc12_basins.hvplot(\n    c='huc12', title=\"Delaware River HUC12 basins\",\n    coastline='50m', geo=True,\n    aspect='equal', legend=False, frame_width=300\n)\n\n\n\nAn important thing to note is that all geodataframes should have a coordinate reference system (CRS). Let’s check to make sure our geodataframe has a CRS.\n\nhuc12_basins.crs\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#load-the-feature-polygons","position":7},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Limit CONUS404 Spatial Range"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#limit-conus404-spatial-range","position":8},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Limit CONUS404 Spatial Range"},"content":"With the HUC12 basins read in, we only need the CONUS404 data that spans these polygons as they are the regions we will be aggregating. So, let’s limit the CONUS404 spatial range to that of the basins. This will save on memory and computation. Note doing this is mainly useful when the region’s footprint is much smaller than the footprint of the gridded model.\n\nTo limit the spatial range, we first need to convert the CRS of the basins to that of CONUS404. Then extract the bounding box of the basins.\n\nhuc12_basins_conus404_crs = huc12_basins.to_crs(conus404.crs.crs_wkt)\nbbox = huc12_basins_conus404_crs.total_bounds\nbbox\n\n\n\nThen select the CONUS404 data within the bounding box. However, when we do this, we will extend the bounds out by 5% of their range to ensure all of our basins are within the spatially limited data. We do this as the reprojections of the CRS can cause slight distortions that make polygons on the bounds not fall fully within the data.\n\nbbox_x_range = bbox[2] - bbox[0]\nbbox_y_range = bbox[3] - bbox[1]\nx_range = slice(bbox[0] - bbox_x_range * 0.05,\n                bbox[2] + bbox_x_range * 0.05)\ny_range = slice(bbox[1] - bbox_y_range * 0.05,\n                bbox[3] + bbox_y_range * 0.05)\n\nconus404 = conus404.sel(x=x_range, y=y_range)\nconus404\n\n\n\nTo make sure this worked as intended, let’s plot the full basin over the extracted CONUS404 data footprint.\n\n# Get the footprint of the grid\ncutout = xr.ones_like(conus404.isel(time=0).drop_vars(['lat', 'lon'])['ACDEWC'])\n# We need to write the CRS to the CONUS404 dataset and\n# reproject to the crs of the HUC12 basins dataframe for clean plotting with hvplot\ncutout = cutout.rio.write_crs(conus404.crs.crs_wkt).rio.reproject('EPSG:4326')\n\ncutout_plt = cutout.hvplot(\n    coastline='50m', geo=True,\n    aspect='equal', frame_width=300, colorbar=False\n)\nhuc12_plt = huc12_basins.hvplot(\n    geo=True, c='r'\n)\ncutout_plt * huc12_plt\n\n\n\nLooks good!\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#limit-conus404-spatial-range","position":9},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Aggregate CONUS404 to HUC12 Polygons"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#aggregate-conus404-to-huc12-polygons","position":10},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Aggregate CONUS404 to HUC12 Polygons"},"content":"\n\nNow that we have our dataset and basin polygons prepared, we are ready to aggregate.","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#aggregate-conus404-to-huc12-polygons","position":11},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl3":"Create Grid Polygons","lvl2":"Aggregate CONUS404 to HUC12 Polygons"},"type":"lvl3","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#create-grid-polygons","position":12},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl3":"Create Grid Polygons","lvl2":"Aggregate CONUS404 to HUC12 Polygons"},"content":"The first step here is to extract the CONUS404 grid information, which consists of getting the grid center points and the grid bounds.\n\ngrid = conus404[['x', 'y']].drop_vars(['lat', 'lon']).reset_coords()\ngrid = grid.cf.add_bounds(['x', 'y'])\ngrid\n\n\n\nThen, we “stack” the data into a single 1D array. This creates an index of (x, y) pairs of the center points that links to the bounds. This will make generating polygons of the grid cells from the bounds much simpler than any manual looping.\n\npoints = grid.stack(point=('y', 'x'))\npoints\n\n\n\nNext, we can use the point pairs we just created to make polygons from the bounds. To do this, we we will make a simple function that takes the x and y bound to generate a polygon for the given grid cell. We can then apply it in parallel using \n\nxarray.apply_ufunc. Note that this step will get slower as we increase the grid size from our limited range. Perhaps could be vectorized using \n\npygeos...\n\n%%time\ndef bounds_to_poly(x_bounds, y_bounds):\n    return Polygon([\n        (x_bounds[0], y_bounds[0]),\n        (x_bounds[0], y_bounds[1]),\n        (x_bounds[1], y_bounds[1]),\n        (x_bounds[1], y_bounds[0])\n    ])\n    \nboxes = xr.apply_ufunc(\n    bounds_to_poly,\n    points.x_bounds,\n    points.y_bounds,\n    input_core_dims=[(\"bounds\",),  (\"bounds\",)],\n    output_dtypes=[np.dtype('O')],\n    vectorize=True\n)\nboxes\n\n\n\nFinally, we convert this xarray.DataArray to a geopandas.GeoDataframe, specifying the projected CRS to be the same as the CONUS404 dataset.\n\ngrid_polygons= gp.GeoDataFrame(\n    data={\"geometry\": boxes.values, \"y\": boxes['y'], \"x\": boxes['x']},\n    index=boxes.indexes[\"point\"],\n    crs=conus404.crs.crs_wkt\n)\ngrid_polygons\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#create-grid-polygons","position":13},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl3":"Key Step: Overlay the Two Geometries","lvl2":"Aggregate CONUS404 to HUC12 Polygons"},"type":"lvl3","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#key-step-overlay-the-two-geometries","position":14},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl3":"Key Step: Overlay the Two Geometries","lvl2":"Aggregate CONUS404 to HUC12 Polygons"},"content":"We are finally ready for the magic of this method, the weight generation using geopandas. To calculate the weights, we will use the \n\noverlay method in geopandas. It calculates the area overlap between polygons in two different GeoDataFrames, i.e. the original grid polygons and the HUC12 polygons. An important thing to note is that when generating the weights we need to use an equal area projection (i.e., equal area CRS). So, before we overlay, we will need to convert the CRS to an area preserving projection. Here we use the \n\nNSIDC EASE-Grid 2.0 grid for the Northern Hemisphere.\n\nAs long as the feature polygons only cover a few 100s of grid polygons, this is an extremely fast operation. However, if 1000s of grid cells are covered this can be a wasteful calculation, as we really only need it for the grid polygons that are partially covered. Otherwise, we could use a faster computational method for fully covered cells, but we will leave that complex topic for another notebook.\n\n%%time\ncrs_area = \"EPSG:6931\" # Good for northern hemisphere\n# crs_area = \"EPSG:5070\" # Good for CONUS\n\nhuc12_basins_area = huc12_basins.to_crs(crs_area)\ngrid_polygons = grid_polygons.to_crs(crs_area)\n\n# overlay the polygons\noverlay = grid_polygons.overlay(huc12_basins_area, keep_geom_type=True)\n\n\n\nThis is essentially already a sparse matrix, mapping one grid space to the other. How sparse? Let’s check.\n\nsparsity = len(overlay) / (len(grid_polygons) * len(huc12_basins_area))\nsparsity\n\n\n\nLet’s explore these overlays a little bit more. Mainly, we can verify that each basin’s area is preserved in the overlay operation.\n\n# calculate areas of HUC12s from overlay and original polygons\noverlay_area = overlay.geometry.area.groupby(overlay['huc12']).sum()\nhuc12_area = huc12_basins_area.geometry.area.groupby(huc12_basins_area['huc12']).sum()\n# find the max fractional difference\n(np.abs(overlay_area - huc12_area) / huc12_area).max()\n\n\n\nNice! So, it worked and only have differences within machine precision.\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#key-step-overlay-the-two-geometries","position":15},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl3":"Calculate the Weights (i.e., Area Fraction for each Region)","lvl2":"Aggregate CONUS404 to HUC12 Polygons"},"type":"lvl3","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#calculate-the-weights-i-e-area-fraction-for-each-region","position":16},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl3":"Calculate the Weights (i.e., Area Fraction for each Region)","lvl2":"Aggregate CONUS404 to HUC12 Polygons"},"content":"Now that we have the overlay of the grid polygons with the HUC12 polygons, we only need to transform this to weights for each grid cell to aggregate. This transform tells us how much of a HUC12 polygon’s total area is within each of the grid cells. This is accurate because we used an area-preserving CRS. Calculating this fractional area is again simple with geopandas.\n\ngrid_cell_fraction = overlay.geometry.area.groupby(overlay['huc12']).transform(lambda x: x / x.sum())\ngrid_cell_fraction\n\n\n\nWe can verify that these all sum up to one.\n\ngrid_cell_fraction.groupby(overlay['huc12']).sum().unique()\n\n\n\nHowever, in their current Series form, the weights aren’t very useful. What we need is to convert them to a sparse array within xarray. Thankfully, xarray can easily do this if we add a MultiIndex for the grid cells’ center points and HUC12 IDs to the cell fraction DataFrame.\n\nmulti_index = overlay.set_index(['y', 'x', 'huc12']).index\ndf_weights = pd.DataFrame({\"weights\": grid_cell_fraction.values}, index=multi_index)\ndf_weights\n\n\n\nWe can bring this directly into xarray as a 1D Dataset and then unstack it into a sparse array.\n\nds_weights = xr.Dataset(df_weights)\nweights_sparse = ds_weights.unstack(sparse=True, fill_value=0.).weights\nweights_sparse\n\n\n\nAgain, we can clearly see that this is a sparse matrix from the density. This is now all we need to do our aggregation.\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#calculate-the-weights-i-e-area-fraction-for-each-region","position":17},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl4":"Perform the Aggregation","lvl3":"Calculate the Weights (i.e., Area Fraction for each Region)","lvl2":"Aggregate CONUS404 to HUC12 Polygons"},"type":"lvl4","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#perform-the-aggregation","position":18},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl4":"Perform the Aggregation","lvl3":"Calculate the Weights (i.e., Area Fraction for each Region)","lvl2":"Aggregate CONUS404 to HUC12 Polygons"},"content":"Unlike deriving the weights, actually performing the aggregation is a simple one line of code. This is because we utilize \n\nxarray.Dataset.weighted to do our weighted calculations. It will happily take a sparse array as weights and compute the aggregation.\n\nHowever, rather than aggregating all variables, let’s only aggregate two in order to reduce the computational time.\n\n%%time\n# Note the .compute() at the end to actually do the computation here vs lazy computing\nhuc12_aggregation = conus404[['T2', 'PREC_ACC_NC']].weighted(weights_sparse).sum(dim=['x', 'y']).compute()\nhuc12_aggregation\n\n\n\nNote that our aggregations are still sparse arrays, which is the cost of using a sparse array as our weights. However, the density of these sparse arrays is large, meaning we want to convert them out of sparse arrays and back to dense arrays. To do this, we can use apply_ufunc with a lambda function to convert.\n\nhuc12_aggregation = xr.apply_ufunc(lambda data: data.todense(), huc12_aggregation)\n\n\n\nIt important to note that we used a sum aggregation above rather than a mean. In theory, the two methods should be the same as a weighted sum is a weighted mean if the sum of weights is one (which they are in our case):\\textrm{weighted mean:\\ }\\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}\\textrm{weighted sum:\\ } \\sum_{i=1}^n w_i x_i\\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i} = \\sum_{i=1}^n w_i x_i \\ \\textrm{if} \\sum_{i=1}^n w_i = 1\n\nHowever, in practice, this will matter if your data contains NaN values. When using a sum with NaNs, the NaNs will effectively be treated as zeros, meaning any all NaN aggregation will result in a 0 value. For the mean, NaNs are ignored in the calculation, but all NaN aggregations will result in a NaN value being returned. Therefore, the two methods are the same when you have data mixed with NaNs, but if you want all NaN aggregations to return NaN use a mean. Otherwise, if you want it to return 0, use a sum.\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#perform-the-aggregation","position":19},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Explore the Aggregation"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#explore-the-aggregation","position":20},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Explore the Aggregation"},"content":"Now that we have the aggregated data and converted it to dense form, let’s make some plots!\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#explore-the-aggregation","position":21},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl3":"Mean of Variable by HUC12","lvl2":"Explore the Aggregation"},"type":"lvl3","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#mean-of-variable-by-huc12","position":22},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl3":"Mean of Variable by HUC12","lvl2":"Explore the Aggregation"},"content":"First, we will calculate and plot mean value over all time steps for every HU12.\n\ndf_mean = huc12_aggregation.mean(dim='time').to_dataframe()\ndf_mean\n\n\n\nWe need to merge this with the HUC12 basin GeoDataFrame to get the geometry info.\n\ndf_mean = pd.merge(huc12_basins, df_mean, left_on='huc12', right_on='huc12')\ndf_mean.head(3)\n\n\n\nTime to plot our two example variables!\n\ntemp_plt = df_mean.hvplot(\n    c='T2', geo=True, coastline='50m', cmap='viridis',\n    title='Mean Temperature at 2m [K]', frame_width=300, aspect='equal'\n)\nprecip_plt = df_mean.hvplot(\n    c='PREC_ACC_NC', geo=True, coastline='50m', cmap='viridis',\n    title='Mean 24hr Accumulated Precipitation [mm]', frame_width=300,\n    aspect='equal'\n)\n\ntemp_plt + precip_plt\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#mean-of-variable-by-huc12","position":23},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl3":"Mean Monthly Time Series","lvl2":"Explore the Aggregation"},"type":"lvl3","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#mean-monthly-time-series","position":24},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl3":"Mean Monthly Time Series","lvl2":"Explore the Aggregation"},"content":"Finally, let’s plot the mean monthly time series for each HUC12.\n\nmonthly_timeseries = huc12_aggregation.resample(time=\"MS\").mean()\nmonthly_timeseries\n\n\n\nmonthly_timeseries.hvplot(x='time', grid=True)\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#mean-monthly-time-series","position":25},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Shut Down the Dask Client"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#shut-down-the-dask-client","position":26},{"hierarchy":{"lvl1":"Pangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Shut Down the Dask Client"},"content":"If utilized, we should shut down the dask client.\n\nclient.close()\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation#shut-down-the-dask-client","position":27},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s"},"type":"lvl1","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-drb","position":0},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s"},"content":"This tutorial demonstrates the use of gdptools, a python package for area-weighted interpolation of source gridded datasets, such as conus404, to target polygonal geospatial fabrics.  Source datasets can be any gridded dataset that can be opened in XArray.  However it’s important to note that gdptools, operations on XArray Datasets or DataArrays with dimensions of (Y,X,Time) generally.  As such climate datasets that have ensemble dimensions will require subsetting by ensemble to obtain the a dataset with the proper dimensions.  The target dataset can be any polygonal dataset that can be read by GeoPandas.  GDPtools also has capabilities of interpolating gridded data to lines as well, but our focus here is interpolating to polygons.\n\nIn this workflow, CONUS404 is aggregated to Deleware River Basin (DRB) HUC12s. The spatial polygons used in this notebook come from the \n\nNHDPlusV2 snapshot of the Watershed Boundary Dataset HUC12 boundaries provided through the \n\nPyGeoHydro python package.\n\nWe use the HyTest intake catalog to access CONUS404 from the OSN pod. This notebook provides a relatively simple and efficient workflow that can be easily run on a local computer.\n\n# Common python packages\nimport xarray as xr\nimport hvplot.xarray\nimport hvplot.pandas\nimport hvplot.dask\nimport intake\nimport pystac\nfrom packaging.version import Version\nimport zarr\nimport warnings\nimport datetime\nimport holoviews as hv\nimport geoviews as gv\nfrom holoviews import opts\nimport cartopy.crs as ccrs\nimport panel as pn\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\n\n# HyRiver packages\nfrom pynhd import NLDI, WaterData\nimport pygeohydro as gh\n# GDPTools packages\nfrom gdptools import AggGen, UserCatData, WeightGen\nimport os\n# Until gdptools updates it's numpy dependency to v2, the environment statement below is required\nos.environ[\"HYRIVER_CACHE_DISABLE\"] = \"true\"\n\nhv.extension(\"bokeh\")\npn.extension()\n\nwarnings.filterwarnings('ignore')\n\n\n\nHere we setup a variable the sets our local context, working on the HPC or working locally on your Desktop.  This just modifies the access point of the conus404 data, using the Hovenweep access for HPC and the OSN pod access for the Desktop.\n\nt_sys = \"Desktop\"  # \"HPC\"  # or \"Desktop\"\n\n\n\n\nWe can use a subset of the \n\nHyRiver Python packages to access HUC12 geometries representing the Delaware River Basin. The process involves several steps:\n\nSelect the HUC4 Mid-Atlantic Region:\n\nThis region encompasses the Delaware River Basin (HUC4 code: 0204).\n\nRetrieve HUC12 Basins within the Selected HUC4:\n\nObtain all HUC12 basins that fall within the HUC4 Mid-Atlantic region.\n\nFilter HUC12 Basins:\n\nFocus on the HUC12 basins within the two HUC6 regions whose drainages terminate in the Delaware River Basin (DRB).\n\nExclude basins with drainages that terminate at the coast.\n\nWe used \n\nScience in Your Watershed to help identify the HUC6 regions that drain directly into the DRB.\n\n\nwbd = gh.WBD(\"huc4\")\ndel_huc4 = wbd.byids(field=\"huc4\", fids=\"0204\")\nhuc12_basins = WaterData('wbd12').bygeom(del_huc4.geometry[0])\nfiltered_gdf = huc12_basins[huc12_basins['huc12'].str.startswith(('020401', '020402'))]\nfiltered_gdf\n\n\n\nfrom holoviews.element.tiles import OSM\ndrb = filtered_gdf.hvplot(\n    geo=True, coastline='50m', alpha=0.2,  frame_width=300,\n    xlabel=\"longitude\", ylabel=\"latitude\",\n    title=\"Delaware River HUC12 basins\", aspect='equal'\n)\nOSM() * drb\n\n\n\nAccess conus404 via the WMA STAC catalog.\n\ndef get_children(catalog, collection_id=None):\n    \"\"\"\n    This function retrieves a specified collection from a STAC catalog/collection and prints key metadata \n    for exploring/accessing the datasets contained within it.\n    If there is no collection ID provided, the collections in the top level of the catalog will be printed.\n    If a collection ID is provided, it will retrieve the collection with that ID from the input catalog/collection.\n    If the collection ID points to a dataset, it will print the assets available for the dataset.\n    If the collection ID points to another collection, it will list the child collections in the IDed collection.\n\n    Args:\n        catalog (pystac.Catalog | pystac.Collection): The STAC catalog/collection object.\n        collection_id (str): The ID of the collection or dataset to retrieve from catalog.\n    \n    Returns:\n        collection (pystac.Catalog | pystac.Collection): The collection object corresponding to the provided ID\n                                                         or the top-level catalog if no ID is provided.\n    \"\"\"\n    dataset = False\n    if collection_id:\n        collection = catalog.get_child(collection_id)\n        if collection.assets:\n            dataset = True\n            print(f\"{collection_id} is a dataset. Please review the assets below and select one to open.\")\n\n        else:\n            print(f\"{collection_id} is a collection. Please review the child items and select one to open in the next cell.\")\n    else:\n        collection = catalog\n    if dataset==True:\n        # List the assets\n        for asset in collection.assets:\n            print(f\"Asset ID: {asset}\")\n            print(f\"    Title: {collection.assets[asset].title}\")\n            print(f\"    Description: {collection.assets[asset].description}\")\n    else:\n        collections = list(collection.get_collections())\n        print(f\"Number of collections: {len(collections)}\")\n        print(\"Collections IDs:\")\n        for child_collection in collections:\n            id = child_collection.id\n            cite_as = \"Not available\"\n            for link in child_collection.links:\n                if link.rel == \"cite-as\":\n                    cite_as = link.target\n            print(f\"- {id}, Source: {cite_as}\")\n    return collection\n\n\n\n# url for the WMA STAC Catalog\ncatalog_url = \"https://api.water.usgs.gov/gdp/pygeoapi/stac/stac-collection/\"\n\n# use pystac to read the catalog\ncatalog = pystac.Catalog.from_file(catalog_url)\n\n# list the collections in the catalog\ncatalog = get_children(catalog)\n\n\n\n# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(catalog, collection_id=\"conus404\")\n\n\n\n# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(collection, collection_id=\"conus404_xtrm_daily\")\n\n\n\nThere are a couple of options for accessing conus404:\n\nHPC Setting (t_sys = HPC):\n\nAssumption: The notebook is run on the USGS HPC Hovenweep.\n\nAccess Method: Utilizes the on-premises version of the data.\n\nBenefits:\n\nWorkflow Association: The workflow is directly linked to the data.\n\nSpeed: Eliminates the need to download data, significantly reducing access and processing time.\n\nDesktop Setting (t_sys = Desktop):\n\nUse Case: Suitable for workflows that do not require HPC resources or for developing workflows locally before deploying them to the HPC.\n\nAccess Method: Connects to the conus404 data via the OSN pod.\n\nBenefits:\n\nFlexibility: Allows for local development and testing.\n\nPerformance: Provides a fast connection to the data.\n\n## Select the dataset you want to read into your notebook and preview its metadata\nif t_sys == \"HPC\":\n    selected_asset_id = 'zarr-disk-hovenweep'\nelif t_sys == \"Desktop\":\n    selected_asset_id = 'zarr-s3-osn' \nelse:\n    print(\"Please set the variable t_sys above to one of 'HPC' or 'Desktop'\")        \nasset = collection.assets[selected_asset_id]\nasset\n\n\n\n# read in the dataset and use metpy to parse the crs information on the dataset\nprint(f\"Reading {selected_asset_id} metadata...\", end='')\nif Version(zarr.__version__) < Version(\"3.0.0\"):\n    ds = xr.open_dataset(\n        asset.href,\n        storage_options=asset.extra_fields['xarray:storage_options'],\n        **asset.extra_fields['xarray:open_kwargs']\n    )\nelse:\n    ds = xr.open_dataset(\n    asset.href,\n    storage_options=asset.extra_fields['xarray:storage_options'],\n    **asset.extra_fields['xarray:open_kwargs'],\n    zarr_format=2\n    )\n\n\n\nds = ds.metpy.parse_cf()\nds\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-drb","position":1},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"GDPTools Background"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-drb#gdptools-background","position":2},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"GDPTools Background"},"content":"In this section, we utilize three data classes from the gdptools package: UserCatData, WeightGen, and AggGen.\n\nUserCatData:Serves as a data container for both the source and target datasets, along with their associated metadata. The instantiated object user_data is employed by both the WeightGen and AggGen classes.\n\nWeightGen:Responsible for calculating the intersected areas between the source and target datasets. It generates normalized area-weights, which are subsequently used by the AggGen class to compute interpolated values between the datasets.\n\nAggGen:Facilitates the interpolation of target data to match the source data using the areal weights calculated by WeightGen. This process is conducted over the time period specified in the UserCatData object.","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-drb#gdptools-background","position":3},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl3":"Instantiation of the UserCatData class.","lvl2":"GDPTools Background"},"type":"lvl3","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-drb#instantiation-of-the-usercatdata-class","position":4},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl3":"Instantiation of the UserCatData class.","lvl2":"GDPTools Background"},"content":"\n\n# Coordinate Reference System (CRS) of the conus404 dataset\nsource_crs = ds.crs.crs_wkt\n\n# Coordinate names of the conus404 dataset\nx_coord = \"x\"\ny_coord = \"y\"\nt_coord = \"time\"\n\n# Time period of interest for areal interpolation of conus404 to DRB HUC12s\n# using the AggGen class below. Note: The dates follow the same format as the\n# time values in the conus404 dataset.\nsdate = \"1979-10-01T00:00:00.000000000\"\nedate = \"2022-10-01T00:00:00.000000000\"\n\n# Variables from the conus404 dataset used for areal interpolation\nvariables = [\"T2MIN\", \"T2MAX\", \"RAINNCVMEAN\"]\n\n# CRS of the DRB HUC12 polygons\ntarget_crs = 5070\n\n# Column name for the unique identifier associated with target polygons.\n# This ID is used in both the generated weights file and the areal interpolated output.\ntarget_poly_idx = \"huc12\"\n\n# Common equal-area CRS for reprojecting both source and target data.\n# This CRS is used for calculating areal weights in the WeightGen class.\nweight_gen_crs = 5070\n\n# Instantiate the UserCatData class, which serves as a container for both\n# source and target datasets, along with associated metadata. The UserCatData\n# object provides methods used by the WeightGen and AggGen classes to subset\n# and reproject the data.\nuser_data = UserCatData(\n    ds=ds,\n    proj_ds=source_crs,\n    x_coord=x_coord,\n    y_coord=y_coord,\n    t_coord=t_coord,\n    var=variables,\n    f_feature=filtered_gdf,\n    proj_feature=target_crs,\n    id_feature=target_poly_idx,\n    period=[sdate, edate],\n)\n\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-drb#instantiation-of-the-usercatdata-class","position":5},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Weight Generation with WeightGen"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-drb#weight-generation-with-weightgen","position":6},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Weight Generation with WeightGen"},"content":"In this section, we utilize the WeightGen class from the gdptools package to calculate the normalized areal weights necessary for interpolating the source gridded data (conus404) to the target polygonal boundaries (DRB HUC12s). The areal weights represent the proportion of each grid cell that overlaps with each polygon, facilitating accurate areal interpolation of the data. These weights are calculated using the calculate_weights() method.\n\nWeight Calculation Process:\n\nSubset Source Data: The source data is subset based on the bounds of the target data, with an additional small buffer to ensure coverage. The buffer size is determined based on the cell size of the source data. The buffer is 2 times the max(dx, dy). In other words, the buffer is essentially 2 source grid cells in the x and y dimensions.\n\nCreate cell boundary GeoDataFrame: A GeoDataFrame of the cell boundaries is created for each node in the subsetted source data, enabling spatial operations.\n\nValidate Geometries: The target file is checked for invalid geometries, which can occur due to various reasons such as topology errors. Invalid geometries are fixed using Shapely’s make_valid() method to prevent failures during intersection calculations.\n\nCalculate and Normalize Areas: For each polygon, gdptools calculates the area of each intersecting grid cell and normalizes it by the total area of the target polygon. This ensures that the weights for each polygon sum to 1, provided the polygon is entirely covered by the source data.\n\nValidation: A quick check on the weights can be performed by grouping the resulting weights by the target_poly_idx and calculating the sum. For all polygons completely covered by the source data, the weights will sum to 1.\n\nNote: The method parameter can be set to one of \"serial\", \"parallel\", or \"dask\". Given the scale of the gridded conus404 data (4 km × 4 km) and the number and spatial footprint of the DRB HUC12s, using \"serial\" in this case is the most efficient method. In subsequent sections, we will explore how the \"parallel\" and \"dask\" methods can provide speed-ups in the areal interpolation process, as well as in the computation of weights for broader CONUS-wide targets.\n\n%%time\nwght_gen = WeightGen(\n    user_data=user_data,\n    method=\"serial\",\n    output_file=\"wghts_drb_ser_c404daily.csv\",\n    weight_gen_crs=6931\n)\n\nwdf = wght_gen.calculate_weights()\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-drb#weight-generation-with-weightgen","position":7},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Areal Interpolation with the AggGen Class"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-drb#areal-interpolation-with-the-agggen-class","position":8},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Areal Interpolation with the AggGen Class"},"content":"In this section, we demonstrate the use of the AggGen class and its calculate_agg() method from the gdptools package to perform areal interpolation. We will explore all three agg_engine options: \"serial\", \"parallel\", and \"dask\". The following links provide detailed documentation on the available parameter options:\n\nagg_engines\n\nagg_writers\n\nstat_methods\n\nWhen using AggGen and the calculate_agg() method, it is important to consider the overlap between the source and target data when selecting the stat_method parameter value. All statistical methods have a masked variant in addition to the standard method; for example, \"mean\" and \"masked_mean\". In cases where the source data has partial overlap with a target polygon, the \"mean\" method will return a missing value for the polygon, whereas the \"masked_mean\" method will calculate the statistic based on the available overlapping source cells. These considerations help users determine whether using a masked statistic is desirable or if a missing value would be preferred, allowing for post-processing of missing values (e.g., using nearest-neighbor or other approaches to handle the lack of overlap). In the case here conus404 completely covers the footprint of the DRB HUC12s, as such the \"mean\" method would be sufficient.\n\n%%time\nagg_gen = AggGen(\n    user_data=user_data,\n    stat_method=\"mean\",\n    agg_engine=\"parallel\",\n    jobs=4,\n    agg_writer=\"netcdf\",\n    weights='wghts_drb_ser_c404daily.csv',\n    out_path='.',\n    file_prefix=\"serial_weights\",\n    precision=8\n)\nngdf, ds_out = agg_gen.calculate_agg()\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-drb#areal-interpolation-with-the-agggen-class","position":9},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Output"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-drb#output","position":10},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Output"},"content":"The \n\ncalculate_agg() method returns two objects: ngdf and ds_out.\n\nngdf: A GeoDataFrame derived from the target GeoDataFrame (filtered_gdf) specified in the UserCatData container. This GeoDataFrame has been both sorted and dissolved based on the identifiers in the \"huc12\" column, as defined by the target_poly_idx parameter.\n\nds_out: The areally weighted interpolation output as an XArray Dataset. The Dataset consists of dimensions time and huc12, and the data variables are T2MIN, T2MAX, and RAINNCVMEAN.\n\nA preview of the ngdf GeoDataFrame below shows that it is sorted by \"huc12\". In this case, there are no duplicate \"huc12\" values, resulting in the original and output GeoDataFrames having the same number of rows.  Some target datasets such as the GFv1.1, will result in many dissolved geometries.\n\nngdf.head()\n\n\n\nds_out\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-drb#output","position":11},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Plot the results as a quick sanity check"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-drb#plot-the-results-as-a-quick-sanity-check","position":12},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Plot the results as a quick sanity check"},"content":"Here we plot the results along with the corresponding conus404 values.  To make the plot a little more interesting we choose the time step with the most precipitation.  This provides a quick qualitative sanity check.  If one is intersted in looking in more detail in a graphic presentation of a target polygon, overlayed on the intersecting grid cells, with the grid-cell values and weights shown for each intersection, please look at the tail end of notebook \n\nClimateR-Catalog - Terraclime data for some example code to generate a plot which give a more quantitative expression of the result.\n\nFormat the interpolated results for plotting\n\n# Convert processed xarray Dataset to DataFrame\ndf = ds_out.to_dataframe().reset_index()\n# Pivot the DataFrame to have variables as separate columns\ndf_pivot = df.pivot_table(index=['time', 'huc12'], values=['T2MAX', 'T2MIN', 'RAINNCVMEAN']).reset_index()\n\n# Merge GeoDataFrame with DataFrame\nmerged_gdf = ngdf.to_crs(5070).merge(df_pivot, on='huc12')\n\n# Convert RAINNCVMEAN from kg/m²/s to mm/day\nmerged_gdf['RAINNCVMEAN_mm_day'] = merged_gdf['RAINNCVMEAN'] * 86400\n# Convert T2MAX and T2MIN from Kelvin to Celsius\nmerged_gdf['T2MAX_C'] = merged_gdf['T2MAX'] - 273.15\nmerged_gdf['T2MIN_C'] = merged_gdf['T2MIN'] - 273.15\n\n# Calculate total precipitation for each time step\nrain_sum = merged_gdf.groupby('time')['RAINNCVMEAN_mm_day'].sum()\n\n# Identify the time step with the maximum total precipitation\nmax_rain_time = rain_sum.idxmax()\nprint(f\"Time step with maximum total precipitation: {max_rain_time}\")\n\n# Subset the GeoDataFrame for the selected time step\nsubset = merged_gdf[merged_gdf['time'] == max_rain_time]\n\n\n\n\nProcess the sub-setted conus404 data for plotting\n\n\n# We can use our agg_gen object to retrieve the subsetted conus404 data\nda_t2max = agg_gen.agg_data.get(\"T2MAX\").da\nda_t2min = agg_gen.agg_data.get(\"T2MIN\").da\nda_rain = agg_gen.agg_data.get(\"RAINNCVMEAN\").da\n\n# Get the subsetted raw conus404 cubes used in the areal interpolation\nda_t2max = agg_gen.agg_data.get(\"T2MAX\").da.sel(time=max_rain_time) - 273.15\nda_t2min = agg_gen.agg_data.get(\"T2MIN\").da.sel(time=max_rain_time) - 273.15\nda_rain = agg_gen.agg_data.get(\"RAINNCVMEAN\").da.sel(time=max_rain_time) * 86400\n\n\n\nGenerate the plot\n\nHere we use \n\nGeoViews and \n\nHoloViews\n\n# Define Cartopy CRS using EPSG code 5070 (NAD83 / Conus Albers)\ncrs_cartopy = ccrs.epsg(5070)\n\n# Define color maps for each variable\ncolor_maps = {\n    'T2MAX_C': 'Reds',\n    'T2MIN_C': 'Blues',\n    'RAINNCVMEAN_mm_day': 'Greens'\n}\n# Define color maps for raw data\ncolor_maps_raw = {\n    'T2MAX': 'Reds',\n    'T2MIN': 'Blues',\n    'RAINNCVMEAN': 'Greens'\n}\n# Create Polygons for T2MAX_C\nmap_T2MAX = gv.Polygons(\n    subset.to_crs(5070), \n    vdims=['T2MAX_C'],\n    crs=crs_cartopy  # Use Cartopy CRS here\n).opts(\n    cmap=color_maps['T2MAX_C'],\n    colorbar=True,\n    tools=['hover'],\n    title='T2MAX (°C)',  # Included units\n    alpha=0.7,\n    frame_width=200,\n    aspect='equal',\n    padding=0,\n)\n\n# Create Polygons for T2MIN_C\nmap_T2MIN = gv.Polygons(\n    subset.to_crs(5070), \n    vdims=['T2MIN_C'],\n    crs=crs_cartopy\n).opts(\n    cmap=color_maps['T2MIN_C'],\n    colorbar=True,\n    tools=['hover'],\n    title='T2MIN (°C)',  # Included units\n    alpha=0.7,\n    frame_width=200,\n    aspect='equal',\n    padding=0,\n)\n\n# Create Polygons for RAINNCVMEAN_mm_day\nmap_RAINNCVMEAN = gv.Polygons(\n    subset.to_crs(5070), \n    vdims=['RAINNCVMEAN_mm_day'],\n    crs=crs_cartopy\n).opts(\n    cmap=color_maps['RAINNCVMEAN_mm_day'],\n    colorbar=True,\n    tools=['hover'],\n    title='RAINNCVMEAN (mm/day)',  # Included units\n    alpha=0.7,\n    frame_width=200,\n    aspect='equal',\n    padding=0,\n)\n\n# Create raw sub-setted data frames\n# Assuming 'lon' and 'lat' are the coordinate names; adjust if necessary\ncoord_x = 'x'  # Replace with actual x-coordinate name\ncoord_y = 'y'  # Replace with actual y-coordinate name\n\n# Create HoloViews Image for T2MAX (Raw)\nimage_T2MAX_raw = hv.Image(\n    da_t2max, \n    kdims=[coord_x, coord_y],\n    vdims=['T2MAX']\n).opts(\n    cmap=color_maps_raw['T2MAX'],\n    colorbar=True,\n    tools=['hover'],\n    title='T2MAX Raw (°C)',\n    alpha=0.7,\n    frame_width=200,\n    aspect='equal'\n)\n\n# Create HoloViews Image for T2MIN (Raw)\nimage_T2MIN_raw = hv.Image(\n    da_t2min, \n    kdims=[coord_x, coord_y],\n    vdims=['T2MIN']\n).opts(\n    cmap=color_maps_raw['T2MIN'],\n    colorbar=True,\n    tools=['hover'],\n    title='T2MIN Raw (°C)',\n    alpha=0.7,\n    frame_width=200,\n    aspect='equal'\n)\n\n# Create HoloViews Image for RAINNCVMEAN (Raw)\nimage_RAIN_raw = hv.Image(\n    da_rain, \n    kdims=[coord_x, coord_y],\n    vdims=['RAINNCVMEAN']\n).opts(\n    cmap=color_maps_raw['RAINNCVMEAN'],\n    colorbar=True,\n    tools=['hover'],\n    title='RAINNCVMEAN Raw (mm/day)',\n    alpha=0.7,\n    frame_width=200,\n    aspect='equal'\n)\n\n# Create a GridSpec with 3 rows and 3 columns\ngrid = pn.GridSpec(ncols=3, nrows=2, sizing_mode='fixed', width=900, height=1200)\n\n# Add title spanning all columns in the first row\ntitle = pn.pane.Markdown(\n    f\"<h2 style='text-align: center;'>Areal Interpolation for {max_rain_time.strftime('%Y-%m-%d')}</h2>\",\n    width=900,\n    height=20\n)\n\n# Add raw data plots in the first row\ngrid[0, 0] = image_T2MAX_raw\ngrid[0, 1] = image_T2MIN_raw\ngrid[0, 2] = image_RAIN_raw\n\n# Add processed data plots in the second row\ngrid[1, 0] = map_T2MAX\ngrid[1, 1] = map_T2MIN\ngrid[1, 2] = map_RAINNCVMEAN\n\n\n# Display the grid\n# final_layout\ngrid\n\n\n# Combine the three maps into a single layout arranged horizontally\nmaped_layout = pn.Row(\n    map_T2MAX,\n    pn.Spacer(sizing_mode=\"stretch_width\"),\n    map_T2MIN,\n    pn.Spacer(sizing_mode=\"stretch_width\"),\n    map_RAINNCVMEAN,\n    width=800\n)\n\n# Combine raw data plots horizontally\nraw_layout = pn.Row(\n    image_T2MAX_raw,\n    pn.Spacer(sizing_mode=\"stretch_width\"),\n    image_T2MIN_raw,\n    pn.Spacer(sizing_mode=\"stretch_width\"),\n    image_RAIN_raw,\n    width=800\n)\n\n# Create a main title using HTML for center alignment within Markdown\ntitle = pn.pane.Markdown(\n    f\"<h3 style='text-align: center;'>Areal Interpolation for {max_rain_time.strftime('%Y-%m-%d')}</h3>\",\n    # width=1800  # Adjust width as needed\n)\n\n# Combine both rows vertically\ncombined_layout = pn.Column(\n    raw_layout,\n    maped_layout\n)\n# Combine the title and the layout vertically\nfinal_layout = pn.Column(\n    title,\n    combined_layout,\n    width=850\n)\n\nfinal_layout\n\n\n\nThere is a clear increasing gradient in temperature from north to south that is visible in the interpolated results.\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-drb#plot-the-results-as-a-quick-sanity-check","position":13},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Parallel and Dask Methods"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-drb#parallel-and-dask-methods","position":14},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s","lvl2":"Parallel and Dask Methods"},"content":"The domain of this workflow is small enough that using either the parallel or dask methods are not necessary.  However there is a speedup that we illustrate.  The parallel and dask engines used in the AggGen object operate in a similar manner using multiprocessing and dask bag respectivly. Using the jobs parameter the user can specify the number of processes to run.  The target data is chunked by the number of processes and each processor recieves a chunked GeoDataFrame along with a copy of the sub-setted source data.  This creates an overhead that can determine how effiently the parallel processing runs.\n\nThe tradeoff in using parallel processing lies in the balance between the number of processors and the overhead of copying data. While increasing the number of processors can significantly reduce computation time by dividing the workload, it also increases the amount of memory used for duplicate datasets and the coordination time between processes. There is a ‘sweet spot’ where the number of processors maximizes performance but beyond this point, additional processors may slow down the operation due to the overhead of managing more processes. The optimal number of processors depends on the size of the data, available memory, and system architecture, and can typically be found through experimentation.\n\nImportantly, most of the time in processing here is dominated by downloading the data, so the speedup is relatively small.  For larger domains the processing will be a larger percentage of the total time and the speedup should be more pronounced.  Well explore that in the CONUS scale processing of conus404 on Hovenweep.\n\n%%time\ngg_gen = AggGen(\n    user_data=user_data,\n    stat_method=\"masked_mean\",\n    agg_engine=\"parallel\",\n    agg_writer=\"netcdf\",\n    weights='wghts_drb_ser_c404daily.csv',\n    out_path='.',\n    file_prefix=\"testing_p2\",\n    precision=8,\n    jobs=4\n)\nngdf, ds_out = agg_gen.calculate_agg()\n\n\n\n%%time\nagg_gen = AggGen(\n    user_data=user_data,\n    stat_method=\"masked_mean\",\n    agg_engine=\"dask\",\n    agg_writer=\"netcdf\",\n    weights='wghts_drb_ser_c404daily.csv',\n    out_path='.',\n    file_prefix=\"testing_p3\",\n    precision=8,\n    jobs=4\n)\nngdf, ds_out = agg_gen.calculate_agg()\n\n\n\nds_agg = xr.open_dataset(\"testing_p3.nc\")\nds_agg\n\n\n\nds_agg.T2MAX.values\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-drb#parallel-and-dask-methods","position":15},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1"},"type":"lvl1","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-gfv1-1","position":0},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1"},"content":"This tutorial demonstrates the use of gdptools, a python package for area-weighted interpolation of source gridded datasets, such as conus404, to target polygonal geospatial fabrics.  Source datasets can be any gridded dataset that can be opened in XArray.  However it’s important to note that gdptools, operations on XArray Datasets or DataArrays with dimensions of (Y,X,Time) generally.  As such climate datasets that have ensemble dimensions will require subsetting by ensemble to obtain the a dataset with the proper dimensions.  The target dataset can be any polygonal dataset that can be read by GeoPandas.  GDPtools also has capabilities of interpolating gridded data to lines as well, but our focus here is interpolating to polygons.\n\nIn this workflow, CONUS404 is aggregated to \n\nGeoSpatialFabric v1.1 (GFv1.1).  This is a CONUS scale spatial fabric with ~115,000 polygons. Access to this dataset is provided through a copy of the data release stored on the OSN pod (geofabric_v1_1_nhru_v1_1_simp-osn).\n\nWe use the HyTest intake catalog to access the conus404-daily-diagnostic-onprem-hw version of CONUS404 on Hovenweep so that we could also run the workflow there to be co-located with the data. However, a user could adapt this workflow to run in other computing environments if they use the version of CONUS404 on the OSN pod instead.\n\nCompared to the gdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s tutorial, the main difference is that to manage file size and memory overhead we process CONUS404 by year, generating 43 annual netcdf files of the interpolated data.\n\n# Common python packages\nimport xarray as xr\nimport hvplot.xarray\nimport hvplot.pandas\nimport hvplot.dask\nimport pystac\nfrom packaging.version import Version\nimport zarr\nimport warnings\nimport intake\nimport intake_parquet\nimport intake_geopandas\nimport datetime\nimport holoviews as hv\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\n\n# HyRiver packages\nfrom pynhd import NLDI, WaterData\nimport pygeohydro as gh\n# GDPTools packages\nfrom gdptools import AggGen, UserCatData, WeightGen\nimport os\nos.environ[\"HYRIVER_CACHE_DISABLE\"] = \"true\"\n\nhv.extension(\"bokeh\")\nwarnings.filterwarnings('ignore')\n\n\n\nHere we setup a variable the sets our local context, working on the HPC or working locally on your Desktop.  This just modifies the access point of the conus404 data, using the Hovenweep access for HPC and the OSN pod access for the Desktop.\n\nt_sys = \"HPC\"  # \"HPC\" or \"Desktop\"\n\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-gfv1-1","position":1},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1","lvl2":"Access data with HyTest intake catalog and the WMA STAC Catalog."},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-gfv1-1#access-data-with-hytest-intake-catalog-and-the-wma-stac-catalog","position":2},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1","lvl2":"Access data with HyTest intake catalog and the WMA STAC Catalog."},"content":"Use the geofabric_v1_1-zip-osn intake entry to read the Geospatial Fabric v1.1\n\nUse the conus404_xtrm_daily STAC collection to read conus404\n\n# open the hytest data intake catalog\n# hytest_cat = intake.open_catalog(\"../dataset_catalog/hytest_intake_catalog.yml\")\nhytest_cat = intake.open_catalog(\"https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml\")\nlist(hytest_cat)\n\n\n\n# open the gfv1.1_simp file\ngfv11_file = hytest_cat['geofabric_v1_1_nhru_v1_1_simp-osn']\ngfv11 = gfv11_file.read()\ngfv11\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-gfv1-1#access-data-with-hytest-intake-catalog-and-the-wma-stac-catalog","position":3},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1","lvl2":"Generate a quick plot of GFv11"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-gfv1-1#generate-a-quick-plot-of-gfv11","position":4},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1","lvl2":"Generate a quick plot of GFv11"},"content":"\n\n# Plot using hvplot with datashading\nplot = gfv11.hvplot(\n    datashade=True,  # Enable datashading for large datasets\n    aspect='equal'\n)\n# Display the plot\nplot\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-gfv1-1#generate-a-quick-plot-of-gfv11","position":5},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1","lvl2":"Load the conus404 dataset using the HyTest catalog"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-gfv1-1#load-the-conus404-dataset-using-the-hytest-catalog","position":6},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1","lvl2":"Load the conus404 dataset using the HyTest catalog"},"content":"In this case we are running this notebook on Hovenweep.\n\ndef get_children(catalog, collection_id=None):\n    \"\"\"\n    This function retrieves a specified collection from a STAC catalog/collection and prints key metadata \n    for exploring/accessing the datasets contained within it.\n    If there is no collection ID provided, the collections in the top level of the catalog will be printed.\n    If a collection ID is provided, it will retrieve the collection with that ID from the input catalog/collection.\n    If the collection ID points to a dataset, it will print the assets available for the dataset.\n    If the collection ID points to another collection, it will list the child collections in the IDed collection.\n\n    Args:\n        catalog (pystac.Catalog | pystac.Collection): The STAC catalog/collection object.\n        collection_id (str): The ID of the collection or dataset to retrieve from catalog.\n    \n    Returns:\n        collection (pystac.Catalog | pystac.Collection): The collection object corresponding to the provided ID\n                                                         or the top-level catalog if no ID is provided.\n    \"\"\"\n    dataset = False\n    if collection_id:\n        collection = catalog.get_child(collection_id)\n        if collection.assets:\n            dataset = True\n            print(f\"{collection_id} is a dataset. Please review the assets below and select one to open.\")\n\n        else:\n            print(f\"{collection_id} is a collection. Please review the child items and select one to open in the next cell.\")\n    else:\n        collection = catalog\n    if dataset==True:\n        # List the assets\n        for asset in collection.assets:\n            print(f\"Asset ID: {asset}\")\n            print(f\"    Title: {collection.assets[asset].title}\")\n            print(f\"    Description: {collection.assets[asset].description}\")\n    else:\n        collections = list(collection.get_collections())\n        print(f\"Number of collections: {len(collections)}\")\n        print(\"Collections IDs:\")\n        for child_collection in collections:\n            id = child_collection.id\n            cite_as = \"Not available\"\n            for link in child_collection.links:\n                if link.rel == \"cite-as\":\n                    cite_as = link.target\n            print(f\"- {id}, Source: {cite_as}\")\n    return collection\n\n\n\n# url for the WMA STAC Catalog\ncatalog_url = \"https://api.water.usgs.gov/gdp/pygeoapi/stac/stac-collection/\"\n\n# use pystac to read the catalog\ncatalog = pystac.Catalog.from_file(catalog_url)\n\n# list the collections in the catalog\ncatalog = get_children(catalog)\n\n\n\n# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(catalog, collection_id=\"conus404\")\n\n\n\n# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(collection, collection_id=\"conus404_xtrm_daily\")\n\n\n\nThere are a couple of options for accessing conus404:\n\nHPC Setting (t_sys = HPC):\n\nAssumption: The notebook is run on the USGS HPC Hovenweep.\n\nAccess Method: Utilizes the on-premises version of the data.\n\nBenefits:\n\nWorkflow Association: The workflow is directly linked to the data.\n\nSpeed: Eliminates the need to download data, significantly reducing access and processing time.\n\nDesktop Setting (t_sys = Desktop):\n\nUse Case: Suitable for workflows that do not require HPC resources or for developing workflows locally before deploying them to the HPC.\n\nAccess Method: Connects to the conus404 data via the OSN pod.\n\nBenefits:\n\nFlexibility: Allows for local development and testing.\n\nPerformance: Provides a fast connection to the data.\n\n## Select the dataset you want to read into your notebook and preview its metadata\nif t_sys == \"HPC\":\n    selected_asset_id = 'zarr-disk-hovenweep'\nelif t_sys == \"Desktop\":\n    selected_asset_id = 'zarr-s3-osn' \nelse:\n    print(\"Please set the variable t_sys above to one of 'HPC' or 'Desktop'\")        \nasset = collection.assets[selected_asset_id]\nasset\n\n\n\n# read in the dataset and use metpy to parse the crs information on the dataset\nprint(f\"Reading {selected_asset_id} metadata...\", end='')\nif Version(zarr.__version__) < Version(\"3.0.0\"):\n    ds = xr.open_dataset(\n        asset.href,\n        storage_options=asset.extra_fields['xarray:storage_options'],\n        **asset.extra_fields['xarray:open_kwargs']\n    )\nelse:\n    ds = xr.open_dataset(\n    asset.href,\n    storage_options=asset.extra_fields['xarray:storage_options'],\n    **asset.extra_fields['xarray:open_kwargs'],\n    zarr_format=2\n    )\n\n\n\nds = ds.metpy.parse_cf()\nds\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-gfv1-1#load-the-conus404-dataset-using-the-hytest-catalog","position":7},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1","lvl2":"GDPTools Background"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-gfv1-1#gdptools-background","position":8},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1","lvl2":"GDPTools Background"},"content":"In this section, we utilize three data classes from the gdptools package: UserCatData, WeightGen, and AggGen.\n\nUserCatData:Serves as a data container for both the source and target datasets, along with their associated metadata. The instantiated object user_data is employed by both the WeightGen and AggGen classes.\n\nWeightGen:Responsible for calculating the intersected areas between the source and target datasets. It generates normalized area-weights, which are subsequently used by the AggGen class to compute interpolated values between the datasets.\n\nAggGen:Facilitates the interpolation of target data to match the source data using the areal weights calculated by WeightGen. This process is conducted over the time period specified in the UserCatData object.","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-gfv1-1#gdptools-background","position":9},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1","lvl2":"Instantiation of the UserCatData class."},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-gfv1-1#instantiation-of-the-usercatdata-class","position":10},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1","lvl2":"Instantiation of the UserCatData class."},"content":"\n\n# Coordinate Reference System (CRS) of the conus404 dataset\nsource_crs = ds.crs.crs_wkt\n\n# Coordinate names of the conus404 dataset\nx_coord = \"x\"\ny_coord = \"y\"\nt_coord = \"time\"\n\n# Time period of interest for areal interpolation of conus404 to DRB HUC12s\n# using the AggGen class below. Note: The dates follow the same format as the\n# time values in the conus404 dataset.\nsdate = \"1979-10-01T00:00:00.000000000\"\nedate = \"2022-10-01T00:00:00.000000000\"\n\n# Variables from the conus404 dataset used for areal interpolation\nvariables = [\"T2MIN\", \"T2MAX\", \"RAINNCVMEAN\"]\n\n# CRS of the DRB HUC12 polygons\ntarget_crs = 5070\n\n# Column name for the unique identifier associated with target polygons.\n# This ID is used in both the generated weights file and the areal interpolated output.\ntarget_poly_idx = \"nhru_v1_1\"\n\n# Common equal-area CRS for reprojecting both source and target data.\n# This CRS is used for calculating areal weights in the WeightGen class.\nweight_gen_crs = 5070\n\n# Instantiate the UserCatData class, which serves as a container for both\n# source and target datasets, along with associated metadata. The UserCatData\n# object provides methods used by the WeightGen and AggGen classes to subset\n# and reproject the data.\nuser_data = UserCatData(\n    ds=ds,  # conus404 read from the intake catalog\n    proj_ds=source_crs,\n    x_coord=x_coord,\n    y_coord=y_coord,\n    t_coord=t_coord,\n    var=variables,\n    f_feature=gfv11,  # GFv1.1 read above from the intake catalog\n    proj_feature=target_crs,\n    id_feature=target_poly_idx,\n    period=[sdate, edate],\n)\n\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-gfv1-1#instantiation-of-the-usercatdata-class","position":11},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1","lvl2":"Weight Generation with WeightGen"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-gfv1-1#weight-generation-with-weightgen","position":12},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1","lvl2":"Weight Generation with WeightGen"},"content":"In this section, we utilize the WeightGen class from the gdptools package to calculate the normalized areal weights necessary for interpolating the source gridded data (conus404) to the target polygonal boundaries (DRB HUC12s). The areal weights represent the proportion of each grid cell that overlaps with each polygon, facilitating accurate areal interpolation of the data. These weights are calculated using the calculate_weights() method.\n\nWeight Calculation Process:\n\nSubset Source Data: The source data is subset based on the bounds of the target data, with an additional small buffer to ensure coverage. The buffer size is determined based on the cell size of the source data. The buffer is 2 times the max(dx, dy). In other words, the buffer is essentially 2 source grid cells in the x and y dimensions.\n\nCreate cell boundary GeoDataFrame: A GeoDataFrame of the cell boundaries is created for each node in the subsetted source data, enabling spatial operations.\n\nValidate Geometries: The target file is checked for invalid geometries, which can occur due to various reasons such as topology errors. Invalid geometries are fixed using Shapely’s make_valid() method to prevent failures during intersection calculations.\n\nCalculate and Normalize Areas: For each polygon, gdptools calculates the area of each intersecting grid cell and normalizes it by the total area of the target polygon. This ensures that the weights for each polygon sum to 1, provided the polygon is entirely covered by the source data.\n\nValidation: A quick check on the weights can be performed by grouping the resulting weights by the target_poly_idx and calculating the sum. For all polygons completely covered by the source data, the weights will sum to 1.\n\nNote: The method parameter in calculate_weights() can be set to one of \"serial\", \"parallel\", or \"dask\". Given the scale of the gridded conus404 data (4 km × 4 km) and the spatial footprint of the  GFv1.1 HRUs, using \"parallel\"or \"dask\" in this case is the most efficient method.","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-gfv1-1#weight-generation-with-weightgen","position":13},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1","lvl2":"Parallel and Dask Methods"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-gfv1-1#parallel-and-dask-methods","position":14},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1","lvl2":"Parallel and Dask Methods"},"content":"The domain in this workflow is large as defined by the number of polygons, the polygon complexity, and the relatively small scale of the conus404 cell geometries.  We can take advantage of the parallel methods to improve performance in both the weight calculation and the interpolation.  The parallel and dask engines used in the WeightGen class operate in a similar manner, utilizing Python’s multiprocessing module and dask.bag, respectively.\n\nUsing the jobs parameter, users can specify the number of processes to run. The target data is divided into chunks based on the number of processes, and each processor receives a chunked GeoDataFrame along with a copy of the subsetted source data. This setup introduces overhead that can affect how efficiently the parallel processing runs.\n\nTrade-offs in Parallel Processing:\n\nThe use of parallel processing involves balancing the number of processors with the overhead of copying data:\n\nBenefits: Increasing the number of processors can reduce computation time by dividing the workload.\n\nCosts: More processors increase memory usage due to duplicate datasets and add coordination overhead between processes.\n\nOptimal Performance: There is a ‘sweet spot’ where the number of processors maximizes performance. Beyond this point, additional processors may slow down the operation due to overhead.\n\nThe optimal number of processors depends on factors such as data size, available memory, and system architecture. It often requires experimentation to determine the most efficient configuration.\n\n%%time\nwght_gen = WeightGen(\n    user_data=user_data,\n    method=\"serial\",\n    output_file=\"wghts_gfv11_c404daily.csv\",\n    weight_gen_crs=weight_gen_crs\n)\n\nwdf = wght_gen.calculate_weights()\n\n\n\n%%time\nwght_gen = WeightGen(\n    user_data=user_data,\n    method=\"parallel\",\n    output_file=\"wghts_gfv11_c404daily_p.csv\",\n    weight_gen_crs=weight_gen_crs,\n    jobs=4\n)\n\nwdf = wght_gen.calculate_weights()\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-gfv1-1#parallel-and-dask-methods","position":15},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1","lvl2":"Compute the areal weighted spatial interpolation"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-gfv1-1#compute-the-areal-weighted-spatial-interpolation","position":16},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1","lvl2":"Compute the areal weighted spatial interpolation"},"content":"Because the result will be rather large.  To manage the file size and memory requirements for processing we process by year.  Additionaly, The conus404 data starts and ends on the water year dates, so we chose to process by water year in this case.  The code below generates a list of start_dates, end_dates, and years that we iterate over to process the data by year.\n\nt_start_series = pd.date_range(pd.to_datetime(\"1979-10-01\"), periods=43, freq=\"YS-OCT\")\nt_end_series = pd.date_range(pd.to_datetime(\"1980-09-30\"), periods=43, freq=\"Y-SEP \")\nf_time_series = pd.date_range(pd.to_datetime(\"1980\"), periods=43, freq=\"Y\")\n\ntime_start = [t.strftime(\"%Y-%m-%dT%H:%M:%S.%f\") for t in t_start_series]\ntime_end = [t.strftime(\"%Y-%m-%dT%H:%M:%S.%f\") for t in t_end_series]\nfile_time = [t.strftime(\"%Y\") for t in f_time_series]\ntime_start[:4], time_end[:4]\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-gfv1-1#compute-the-areal-weighted-spatial-interpolation","position":17},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1","lvl2":"Areal Interpolation with the AggGen Class"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-gfv1-1#areal-interpolation-with-the-agggen-class","position":18},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1","lvl2":"Areal Interpolation with the AggGen Class"},"content":"In this section, we demonstrate the use of the AggGen class and its calculate_agg() method from the gdptools package to perform areal interpolation. We will explore all three agg_engine options: \"serial\", \"parallel\", and \"dask\". The following links provide detailed documentation on the available parameter options:\n\nagg_engines\n\nagg_writers\n\nstat_methods\n\nWhen using AggGen and the calculate_agg() method, it is important to consider the overlap between the source and target data when selecting the stat_method parameter value. All statistical methods have a masked variant in addition to the standard method; for example, \"mean\" and \"masked_mean\". In cases where the source data has partial overlap with a target polygon, the \"mean\" method will return a missing value for the polygon, whereas the \"masked_mean\" method will calculate the statistic based on the available overlapping source cells. These considerations help users determine whether using a masked statistic is desirable or if a missing value would be preferred, allowing for post-processing of missing values (e.g., using nearest-neighbor or other approaches to handle the lack of overlap). In the case here conus404 completely covers the footprint of the DRB HUC12s, as such the \"mean\" method would be sufficient.\n\nBecause we are processing by year, we have to create a new UserCatData object for each year processed.\n\n%%time\nfor index, _ts in enumerate(time_start):\n    sdate = time_start[index]\n    edate = time_end[index]\n    print(sdate, edate)\n    user_data = UserCatData(\n        ds=ds,  # conus404 read from the intake catalog\n        proj_ds=source_crs,\n        x_coord=x_coord,\n        y_coord=y_coord,\n        t_coord=t_coord,\n        var=variables,\n        f_feature=gfv11,  # GFv1.1 read above from the intake catalog\n        proj_feature=target_crs,\n        id_feature=target_poly_idx,\n        period=[sdate, edate],\n    )\n    \n    agg_gen = AggGen(\n        user_data=user_data,\n        stat_method=\"mean\",\n        agg_engine=\"parallel\",\n        agg_writer=\"netcdf\",\n        weights='wghts_gfv11_c404daily_p.csv',\n        out_path='.',\n        file_prefix=f\"{file_time[index]}_gfv11_c404_daily_diagnostic\",\n        jobs=4\n    )\n    ngdf, ds_out = agg_gen.calculate_agg()\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-gfv1-1#areal-interpolation-with-the-agggen-class","position":19},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s"},"type":"lvl1","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-wbd12","position":0},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s"},"content":"This tutorial demonstrates the use of gdptools, a python package for area-weighted interpolation of source gridded datasets, such as conus404, to target polygonal geospatial fabrics.  Source datasets can be any gridded dataset that can be opened in XArray.  However it’s important to note that gdptools, operations on XArray Datasets or DataArrays with dimensions of (Y,X,Time) generally.  As such climate datasets that have ensemble dimensions will require subsetting by ensemble to obtain the a dataset with the proper dimensions.  The target dataset can be any polygonal dataset that can be read by GeoPandas.  GDPtools also has capabilities of interpolating gridded data to lines as well, but our focus here is interpolating to polygons.\n\nIn this workflow, CONUS404 is aggregated to \n\nNHDPlusV2 snapshot of the Watershed Boundary Dataset HUC12 boundaries. This is a CONUS scale spatial fabric with ~102,000 polygons. Access to this dataset is provided through a copy of the data release stored on the OSN pod ('huc12-geoparquet-osn').\n\nWe use the HyTest intake catalog to access the conus404-daily-diagnostic-onprem-hw version of CONUS404 on Hovenweep so that we could also run the workflow there to be co-located with the data. However, a user could adapt this workflow to run in other computing environments if they use the version of CONUS404 on the OSN pod instead.\n\nCompared to the gdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s tutorial, the main difference is that to manage file size and memory overhead we process CONUS404 by year, generating 43 annual netcdf files of the interpolated data.\n\n# Common python packages\nimport xarray as xr\nimport hvplot.xarray\nimport hvplot.pandas\nimport hvplot.dask\nimport warnings\nimport pystac\nfrom packaging.version import Version\nimport zarr\nimport intake\nimport intake_parquet\nimport intake_geopandas\nimport datetime\nimport holoviews as hv\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\n\n# HyRiver packages\nfrom pynhd import NLDI, WaterData\nimport pygeohydro as gh\n# GDPTools packages\nfrom gdptools import AggGen, UserCatData, WeightGen\nimport os\nos.environ[\"HYRIVER_CACHE_DISABLE\"] = \"true\"\n\nhv.extension(\"bokeh\")\nwarnings.filterwarnings('ignore')\n\n\n\nHere we setup a variable the sets our local context, working on the HPC or working locally on your Desktop.  This just modifies the access point of the conus404 data, using the Hovenweep access for HPC and the OSN pod access for the Desktop.\n\nt_sys = \"HPC\"  # \"HPC\" or \"Desktop\"\n\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-wbd12","position":1},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s","lvl2":"Access data with HyTest intake catalog and the WMA STAC Catalog."},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-wbd12#access-data-with-hytest-intake-catalog-and-the-wma-stac-catalog","position":2},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s","lvl2":"Access data with HyTest intake catalog and the WMA STAC Catalog."},"content":"Use the huc12-geoparquet-osn intake entry to read the NHDPlusV2 snapshot of the Watershed Boundary Dataset HUC12 boundaries\n\nUse the conus404_xtrm_daily STAC collection to read conus404\n\n# open the hytest data intake catalog\n# hytest_cat = intake.open_catalog(\"../dataset_catalog/hytest_intake_catalog.yml\")\nhytest_cat = intake.open_catalog(\"https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml\")\nlist(hytest_cat)\n\n\n\nWe need a column to use as our identifyer.  Printing huc12.columns below to view all the possible columns, we choose the HUC12 column as our identifyer.\n\n# open the huc12-geoparquet-osn\nhuc12_access = hytest_cat['huc12-geoparquet-osn']\nhuc12 = huc12_access.read()\nprint(huc12.columns)\nhuc12\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-wbd12#access-data-with-hytest-intake-catalog-and-the-wma-stac-catalog","position":3},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s","lvl2":"Load the conus404 dataset using the HyTest catalog"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-wbd12#load-the-conus404-dataset-using-the-hytest-catalog","position":4},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s","lvl2":"Load the conus404 dataset using the HyTest catalog"},"content":"In this case we are running this notebook on Hovenweep.\n\ndef get_children(catalog, collection_id=None):\n    \"\"\"\n    This function retrieves a specified collection from a STAC catalog/collection and prints key metadata \n    for exploring/accessing the datasets contained within it.\n    If there is no collection ID provided, the collections in the top level of the catalog will be printed.\n    If a collection ID is provided, it will retrieve the collection with that ID from the input catalog/collection.\n    If the collection ID points to a dataset, it will print the assets available for the dataset.\n    If the collection ID points to another collection, it will list the child collections in the IDed collection.\n\n    Args:\n        catalog (pystac.Catalog | pystac.Collection): The STAC catalog/collection object.\n        collection_id (str): The ID of the collection or dataset to retrieve from catalog.\n    \n    Returns:\n        collection (pystac.Catalog | pystac.Collection): The collection object corresponding to the provided ID\n                                                         or the top-level catalog if no ID is provided.\n    \"\"\"\n    dataset = False\n    if collection_id:\n        collection = catalog.get_child(collection_id)\n        if collection.assets:\n            dataset = True\n            print(f\"{collection_id} is a dataset. Please review the assets below and select one to open.\")\n\n        else:\n            print(f\"{collection_id} is a collection. Please review the child items and select one to open in the next cell.\")\n    else:\n        collection = catalog\n    if dataset==True:\n        # List the assets\n        for asset in collection.assets:\n            print(f\"Asset ID: {asset}\")\n            print(f\"    Title: {collection.assets[asset].title}\")\n            print(f\"    Description: {collection.assets[asset].description}\")\n    else:\n        collections = list(collection.get_collections())\n        print(f\"Number of collections: {len(collections)}\")\n        print(\"Collections IDs:\")\n        for child_collection in collections:\n            id = child_collection.id\n            cite_as = \"Not available\"\n            for link in child_collection.links:\n                if link.rel == \"cite-as\":\n                    cite_as = link.target\n            print(f\"- {id}, Source: {cite_as}\")\n    return collection\n\n\n\n# url for the WMA STAC Catalog\ncatalog_url = \"https://api.water.usgs.gov/gdp/pygeoapi/stac/stac-collection/\"\n\n# use pystac to read the catalog\ncatalog = pystac.Catalog.from_file(catalog_url)\n\n# list the collections in the catalog\ncatalog = get_children(catalog)\n\n\n\n# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(catalog, collection_id=\"conus404\")\n\n\n\n# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(collection, collection_id=\"conus404_xtrm_daily\")\n\n\n\nThere are a couple of options for accessing conus404:\n\nHPC Setting (t_sys = HPC):\n\nAssumption: The notebook is run on the USGS HPC Hovenweep.\n\nAccess Method: Utilizes the on-premises version of the data.\n\nBenefits:\n\nWorkflow Association: The workflow is directly linked to the data.\n\nSpeed: Eliminates the need to download data, significantly reducing access and processing time.\n\nDesktop Setting (t_sys = Desktop):\n\nUse Case: Suitable for workflows that do not require HPC resources or for developing workflows locally before deploying them to the HPC.\n\nAccess Method: Connects to the conus404 data via the OSN pod.\n\nBenefits:\n\nFlexibility: Allows for local development and testing.\n\nPerformance: Provides a fast connection to the data.\n\n## Select the dataset you want to read into your notebook and preview its metadata\nif t_sys == \"HPC\":\n    selected_asset_id = 'zarr-disk-hovenweep'\nelif t_sys == \"Desktop\":\n    selected_asset_id = 'zarr-s3-osn' \nelse:\n    print(\"Please set the variable t_sys above to one of 'HPC' or 'Desktop'\")        \nasset = collection.assets[selected_asset_id]\nasset\n\n\n\n# read in the dataset and use metpy to parse the crs information on the dataset\nprint(f\"Reading {selected_asset_id} metadata...\", end='')\nif Version(zarr.__version__) < Version(\"3.0.0\"):\n    ds = xr.open_dataset(\n        asset.href,\n        storage_options=asset.extra_fields['xarray:storage_options'],\n        **asset.extra_fields['xarray:open_kwargs']\n    )\nelse:\n    ds = xr.open_dataset(\n    asset.href,\n    storage_options=asset.extra_fields['xarray:storage_options'],\n    **asset.extra_fields['xarray:open_kwargs'],\n    zarr_format=2\n    )\n\n\n\nds = ds.metpy.parse_cf()\nds\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-wbd12#load-the-conus404-dataset-using-the-hytest-catalog","position":5},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s","lvl2":"GDPTools Background"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-wbd12#gdptools-background","position":6},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s","lvl2":"GDPTools Background"},"content":"In this section, we utilize three data classes from the gdptools package: UserCatData, WeightGen, and AggGen.\n\nUserCatData:Serves as a data container for both the source and target datasets, along with their associated metadata. The instantiated object user_data is employed by both the WeightGen and AggGen classes.\n\nWeightGen:Responsible for calculating the intersected areas between the source and target datasets. It generates normalized area-weights, which are subsequently used by the AggGen class to compute interpolated values between the datasets.\n\nAggGen:Facilitates the interpolation of target data to match the source data using the areal weights calculated by WeightGen. This process is conducted over the time period specified in the UserCatData object.","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-wbd12#gdptools-background","position":7},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s","lvl3":"Instantiation of the UserCatData class.","lvl2":"GDPTools Background"},"type":"lvl3","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-wbd12#instantiation-of-the-usercatdata-class","position":8},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s","lvl3":"Instantiation of the UserCatData class.","lvl2":"GDPTools Background"},"content":"\n\n# Coordinate Reference System (CRS) of the conus404 dataset\nsource_crs = ds.crs.crs_wkt\n\n# Coordinate names of the conus404 dataset\nx_coord = \"x\"\ny_coord = \"y\"\nt_coord = \"time\"\n\n# Time period of interest for areal interpolation of conus404 to DRB HUC12s\n# using the AggGen class below. Note: The dates follow the same format as the\n# time values in the conus404 dataset.\nsdate = \"1979-10-01T00:00:00.000000000\"\nedate = \"2022-10-01T00:00:00.000000000\"\n\n# Variables from the conus404 dataset used for areal interpolation\nvariables = [\"T2MIN\", \"T2MAX\", \"RAINNCVMEAN\"]\n\n# CRS of the DRB HUC12 polygons\ntarget_crs = 5070\n\n# Column name for the unique identifier associated with target polygons.\n# This ID is used in both the generated weights file and the areal interpolated output.\ntarget_poly_idx = \"HUC12\"\n\n# Common equal-area CRS for reprojecting both source and target data.\n# This CRS is used for calculating areal weights in the WeightGen class.\nweight_gen_crs = 5070\n\n# Instantiate the UserCatData class, which serves as a container for both\n# source and target datasets, along with associated metadata. The UserCatData\n# object provides methods used by the WeightGen and AggGen classes to subset\n# and reproject the data.\nuser_data = UserCatData(\n    ds=ds,  # conus404 read from the intake catalog\n    proj_ds=source_crs,\n    x_coord=x_coord,\n    y_coord=y_coord,\n    t_coord=t_coord,\n    var=variables,\n    f_feature=huc12,  # huc121 read above from the intake catalog\n    proj_feature=target_crs,\n    id_feature=target_poly_idx,\n    period=[sdate, edate],\n)\n\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-wbd12#instantiation-of-the-usercatdata-class","position":9},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s","lvl2":"Weight Generation with WeightGen"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-wbd12#weight-generation-with-weightgen","position":10},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s","lvl2":"Weight Generation with WeightGen"},"content":"In this section, we utilize the WeightGen class from the gdptools package to calculate the normalized areal weights necessary for interpolating gridded data (conus404) to polygonal boundaries (CONUS HUC12s). The areal weights represent the proportion of each grid cell that overlaps with each polygon, facilitating accurate areal interpolation of the data. These weights are calculated using the calculate_weights() method.\n\nWeight Calculation Process:\n\nSubset Source Data: The source data is subset based on the bounds of the target data, with an additional small buffer to ensure coverage. The buffer size is determined based on the cell size of the source data. The buffer is 2 times the max(dx, dy). In other words, the buffer is essentially 2 source grid cells in the x and y dimensions.\n\nCreate cell boundary GeoDataFrame: A GeoDataFrame of the cell boundaries is created for each node in the subsetted source data, enabling spatial operations.\n\nValidate Geometries: The target file is checked for invalid geometries, which can occur due to various reasons such as topology errors. Invalid geometries are fixed using Shapely’s make_valid() method to prevent failures during intersection calculations.\n\nCalculate and Normalize Areas: For each polygon, gdptools calculates the area of each intersecting grid cell and normalizes it by the total area of the target polygon. This ensures that the weights for each polygon sum to 1, provided the polygon is entirely covered by the source data.\n\nValidation: A quick check on the weights can be performed by grouping the resulting weights by the target_poly_idx and calculating the sum. For all polygons completely covered by the source data, the weights will sum to 1.\n\nNote: The method parameter in calculate_weights() can be set to one of \"serial\", \"parallel\", or \"dask\". Given the scale of the gridded conus404 data (4 km × 4 km) and the spatial footprint of the CONUS HUC12s, using \"parallel\"or \"dask\" in this case is the most efficient method.","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-wbd12#weight-generation-with-weightgen","position":11},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s","lvl2":"Parallel and Dask Methods"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-wbd12#parallel-and-dask-methods","position":12},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s","lvl2":"Parallel and Dask Methods"},"content":"The domain in this workflow is large as defined by the number of polygons, the polygon complexity, and the relatively small scale of the conus404 cell geometries.  We can take advantage of the parallel methods to improve performance in both the weight calculation and the interpolation.  The parallel and dask engines used in the WeightGen class operate in a similar manner, utilizing Python’s multiprocessing module and dask.bag, respectively.\n\nUsing the jobs parameter, users can specify the number of processes to run. The target data is divided into chunks based on the number of processes, and each processor receives a chunked GeoDataFrame along with a copy of the subsetted source data. This setup introduces overhead that can affect how efficiently the parallel processing runs.\n\nTrade-offs in Parallel Processing:\n\nThe use of parallel processing involves balancing the number of processors with the overhead of copying data:\n\nBenefits: Increasing the number of processors can reduce computation time by dividing the workload.\n\nCosts: More processors increase memory usage due to duplicate datasets and add coordination overhead between processes.\n\nOptimal Performance: There is a ‘sweet spot’ where the number of processors maximizes performance. Beyond this point, additional processors may slow down the operation due to overhead.\n\nThe optimal number of processors depends on factors such as data size, available memory, and system architecture. It often requires experimentation to determine the most efficient configuration.\n\n%%time\nwght_gen = WeightGen(\n    user_data=user_data,\n    method=\"parallel\",\n    output_file=\"wghts_huc12_c404daily_p.csv\",\n    weight_gen_crs=weight_gen_crs,\n    jobs=4\n)\n\nwdf = wght_gen.calculate_weights()\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-wbd12#parallel-and-dask-methods","position":13},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s","lvl2":"Compute the areal weighted spatial interpolation"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-wbd12#compute-the-areal-weighted-spatial-interpolation","position":14},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s","lvl2":"Compute the areal weighted spatial interpolation"},"content":"Because the result will be rather large.  To manage the file size and memory requirements for processing we process by year.  Additionaly, The conus404 data starts and ends on the water year dates, so we chose to process by water year in this case.  The code below generates a list of start_dates, end_dates, and years that we iterate over to process the data by year.\n\nt_start_series = pd.date_range(pd.to_datetime(\"1979-10-01\"), periods=43, freq=\"YS-OCT\")\nt_end_series = pd.date_range(pd.to_datetime(\"1980-09-30\"), periods=43, freq=\"Y-SEP \")\nf_time_series = pd.date_range(pd.to_datetime(\"1980\"), periods=43, freq=\"Y\")\n\ntime_start = [t.strftime(\"%Y-%m-%dT%H:%M:%S.%f\") for t in t_start_series]\ntime_end = [t.strftime(\"%Y-%m-%dT%H:%M:%S.%f\") for t in t_end_series]\nfile_time = [t.strftime(\"%Y\") for t in f_time_series]\ntime_start[:4], time_end[:4]\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-wbd12#compute-the-areal-weighted-spatial-interpolation","position":15},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s","lvl2":"Areal Interpolation with the AggGen Class"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-wbd12#areal-interpolation-with-the-agggen-class","position":16},{"hierarchy":{"lvl1":"gdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s","lvl2":"Areal Interpolation with the AggGen Class"},"content":"In this section, we demonstrate the use of the AggGen class and its calculate_agg() method from the gdptools package to perform areal interpolation. We will explore all three agg_engine options: \"serial\", \"parallel\", and \"dask\". The following links provide detailed documentation on the available parameter options:\n\nagg_engines\n\nagg_writers\n\nstat_methods\n\nWhen using AggGen and the calculate_agg() method, it is important to consider the overlap between the source and target data when selecting the stat_method parameter value. All statistical methods have a masked variant in addition to the standard method; for example, \"mean\" and \"masked_mean\". In cases where the source data has partial overlap with a target polygon, the \"mean\" method will return a missing value for the polygon, whereas the \"masked_mean\" method will calculate the statistic based on the available overlapping source cells. These considerations help users determine whether using a masked statistic is desirable or if a missing value would be preferred, allowing for post-processing of missing values (e.g., using nearest-neighbor or other approaches to handle the lack of overlap). In the case here conus404 completely covers the footprint of the DRB HUC12s, as such the \"mean\" method would be sufficient.\n\nBecause we are processing by year, we have to create a new UserCatData object for each year processed.\n\n%%time\nfor index, _ts in enumerate(time_start):\n    sdate = time_start[index]\n    edate = time_end[index]\n    print(sdate, edate)\n    user_data = UserCatData(\n        ds=ds,  # conus404 read from the intake catalog\n        proj_ds=source_crs,\n        x_coord=x_coord,\n        y_coord=y_coord,\n        t_coord=t_coord,\n        var=variables,\n        f_feature=huc12,  # GFv1.1 read above from the intake catalog\n        proj_feature=target_crs,\n        id_feature=target_poly_idx,\n        period=[sdate, edate],\n    )\n    \n    agg_gen = AggGen(\n        user_data=user_data,\n        stat_method=\"mean\",\n        agg_engine=\"parallel\",\n        agg_writer=\"netcdf\",\n        weights='wghts_huc12_c404daily_p.csv',\n        out_path='.',\n        file_prefix=f\"{file_time[index]}_huc12_c404_daily_diagnostic\",\n        jobs=4\n    )\n    ngdf, ds_out = agg_gen.calculate_agg()\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-wbd12#areal-interpolation-with-the-agggen-class","position":17},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation"},"type":"lvl1","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison","position":0},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation"},"content":"In this notebook, we will be comparing two spatial aggregation methods to aggregate from grids to polygons. One uses \n\ngdptools. The other uses conservative regional methods with \n\nxarray and \n\ngeopandas natively (see this \n\nPangeo Discourse for details).\n\nThe goal of this comparision is to see how the results of the two methods compare to help judge the efficacy of one versus the other.\n\n%xmode minimal\nimport os\n# Needed when boto3 >= 1.36.0 or the rechunking process will fail\n# This needs to be set before the boto3 library gets loaded\n# See: https://github.com/aws/aws-cli/issues/9214#issuecomment-2606619168\nos.environ['AWS_REQUEST_CHECKSUM_CALCULATION'] = 'when_required'\nos.environ['AWS_RESPONSE_CHECKSUM_VALIDATION'] = 'when_required'\nimport time\nimport xarray as xr\nimport geopandas as gp\nimport pandas as pd\nimport numpy as np\nimport sparse\n\nimport hvplot.pandas\nimport hvplot.xarray\nimport dask\nimport cf_xarray\n\nfrom pynhd import NLDI, WaterData\nfrom pygeohydro import watershed\nimport cartopy.crs as ccrs\nfrom shapely.geometry import Polygon\n\nimport pyproj\nfrom gdptools import WeightGen, AggGen, UserCatData\nimport pystac\nfrom packaging.version import Version\nimport zarr\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison","position":1},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl2":"Open dataset from Intake Catalog"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#open-dataset-from-intake-catalog","position":2},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl2":"Open dataset from Intake Catalog"},"content":"First, let’s begin by loading the CONUS404 daily data.\n\ndef get_children(catalog, collection_id=None):\n    \"\"\"\n    This function retrieves a specified collection from a STAC catalog/collection and prints key metadata \n    for exploring/accessing the datasets contained within it.\n    If there is no collection ID provided, the collections in the top level of the catalog will be printed.\n    If a collection ID is provided, it will retrieve the collection with that ID from the input catalog/collection.\n    If the collection ID points to a dataset, it will print the assets available for the dataset.\n    If the collection ID points to another collection, it will list the child collections in the IDed collection.\n\n    Args:\n        catalog (pystac.Catalog | pystac.Collection): The STAC catalog/collection object.\n        collection_id (str): The ID of the collection or dataset to retrieve from catalog.\n    \n    Returns:\n        collection (pystac.Catalog | pystac.Collection): The collection object corresponding to the provided ID\n                                                         or the top-level catalog if no ID is provided.\n    \"\"\"\n    dataset = False\n    if collection_id:\n        collection = catalog.get_child(collection_id)\n        if collection.assets:\n            dataset = True\n            print(f\"{collection_id} is a dataset. Please review the assets below and select one to open.\")\n\n        else:\n            print(f\"{collection_id} is a collection. Please review the child items and select one to open in the next cell.\")\n    else:\n        collection = catalog\n    if dataset==True:\n        # List the assets\n        for asset in collection.assets:\n            print(f\"Asset ID: {asset}\")\n            print(f\"    Title: {collection.assets[asset].title}\")\n            print(f\"    Description: {collection.assets[asset].description}\")\n    else:\n        collections = list(collection.get_collections())\n        print(f\"Number of collections: {len(collections)}\")\n        print(\"Collections IDs:\")\n        for child_collection in collections:\n            id = child_collection.id\n            cite_as = \"Not available\"\n            for link in child_collection.links:\n                if link.rel == \"cite-as\":\n                    cite_as = link.target\n            print(f\"- {id}, Source: {cite_as}\")\n    return collection\n\n\n\n# url for the WMA STAC Catalog\ncatalog_url = \"https://api.water.usgs.gov/gdp/pygeoapi/stac/stac-collection/\"\n\n# use pystac to read the catalog\ncatalog = pystac.Catalog.from_file(catalog_url)\n\n# list the collections in the catalog\ncatalog = get_children(catalog)\n\n\n\n# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(catalog, collection_id=\"conus404\")\n\n\n\n# select a collection from the catalog, replace the collection ID with the one you want to use:\ncollection = get_children(collection, collection_id=\"conus404_daily\")\n\n\n\nAs we can see there are two different locations for the conus404_daily data set. The locations are (1) -hovenweep meaning it is stored on the USGS Hovenweep HPC and (2) -osn meaning the data is on the USGS open storage network (OSN). As the OSN is free to access from any environment, we will use that for this example, but the location can easily be changed depending on your needs.\n\n# replace with the asset ID you want to use:\nselected_asset_id = \"zarr-s3-osn\"\n\n# read the asset metadata\nasset = collection.assets[selected_asset_id]\n\n\n\n# uncomment the lines below to read in your AWS credentials if you want to access data from a requester-pays bucket (-cloud)\n# os.environ['AWS_PROFILE'] = 'default'\n# %run ../../../environment_set_up/Help_AWS_Credentials.ipynb\n\n\n\nFinally, read in the daily CONUS404 data set and select the accumulated grid scale precipitation. We select the precipitation rather then all variables to keep things simple for this example, but aggregation of other variables would follow the same methodology.\n\nif Version(zarr.__version__) < Version(\"3.0.0\"):\n    conus404 = xr.open_dataset(\n        asset.href,\n        storage_options=asset.extra_fields['xarray:storage_options'],\n        **asset.extra_fields['xarray:open_kwargs']\n    )\nelse:\n    conus404 = xr.open_dataset(\n    asset.href,\n    storage_options=asset.extra_fields['xarray:storage_options'],\n    **asset.extra_fields['xarray:open_kwargs'],\n    zarr_format=2\n    )\n\n# Include the crs as we will need it later\nconus404 = conus404[['PREC_ACC_NC', 'crs']]\nconus404\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#open-dataset-from-intake-catalog","position":3},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl2":"Parallelize with Dask (optional)"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#parallelize-with-dask-optional","position":4},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl2":"Parallelize with Dask (optional)"},"content":"Some of the steps we will take are aware of parallel clustered compute environments using \n\ndask. We can start a cluster now so that future steps take advantage\nof this ability. This is an optional step, but speed ups data loading significantly, especially when accessing data from the cloud.\n\nWe have documentation on how to start a Dask Cluster in different computing environments \n\nhere. Uncomment the cluster start up that works for your compute environment.\n\n%run ../../../environment_set_up/Start_Dask_Cluster_Nebari.ipynb\n## If this notebook is not being run on Nebari, replace the above \n## path name with a helper appropriate to your compute environment.  Examples:\n# %run ../../../environment_set_up/Start_Dask_Cluster_Denali.ipynb\n# %run ../../../environment_set_up/Start_Dask_Cluster_Tallgrass.ipynb\n# %run ../../../environment_set_up/Start_Dask_Cluster_Desktop.ipynb\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#parallelize-with-dask-optional","position":5},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl2":"Load the Feature Polygons"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#load-the-feature-polygons","position":6},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl2":"Load the Feature Polygons"},"content":"Now that we have read in the CONUS404 data, we need to read in some polygons to aggregate the data. For this example, we will use the HUC12 basins within the Delaware River Basin. To get these HUC12 polygons, we can use \n\npygeohydro.watershed to query the Hydro Network Linked Data Index (NLDI). All we need to get the basins is the general IDs of the HUC12 basins. For the Delaware Basin those are ones that start with 020401 or 020402.\n\n%%time\nwbd = watershed.WBD(\"huc4\")\ndelaware_basin = wbd.byids(field=\"huc4\", fids=\"0204\")\nhuc12_basins = WaterData('wbd12').bygeom(delaware_basin.iloc[0].geometry)\nhuc12_basins = huc12_basins[huc12_basins['huc12'].str.startswith(('020401', '020402'))]\n\n\n\nLet’s plot the HUC12 basins to see how they look.\n\nhuc12_basins.hvplot(\n    c='huc12', title=\"Delaware River HUC12 basins\",\n    coastline='50m', geo=True,\n    aspect='equal', legend=False, frame_width=300\n)\n\n\n\nAn important thing to note is that all geodataframes should have a coordinate reference system (CRS). Let’s check to make sure our geodataframe has a CRS.\n\nhuc12_basins.crs\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#load-the-feature-polygons","position":7},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl2":"Limit CONUS404 Spatial Range"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#limit-conus404-spatial-range","position":8},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl2":"Limit CONUS404 Spatial Range"},"content":"With the HUC12 basins read in, we only need the CONUS404 data that spans these polygons as they are the regions we will be aggregating. So, let’s limit the CONUS404 spatial range to that of the basins. This will save on memory and computation. Note doing this is mainly useful when the regions footprint is much smaller than the footprint of the gridded model.\n\nTo limit the spatial range, we first need to convert the CRS of the basins to that of CONUS404. Then extract the bounding box of the basins.\n\nhuc12_basins_conus404_crs = huc12_basins.to_crs(conus404.crs.crs_wkt)\nbbox = huc12_basins_conus404_crs.total_bounds\nbbox\n\n\n\nThen select the CONUS404 data within the bounding box. However, when we do this, we will extend the bounds out by 5% of their range to ensure all of our basins are within the spatially limited data. We do this as the reprojections of the CRS can cause slight distortions that make polygons on the bounds not fall fully within the data.\n\nbbox_x_range = bbox[2] - bbox[0]\nbbox_y_range = bbox[3] - bbox[1]\nx_range = slice(bbox[0] - bbox_x_range * 0.05,\n                bbox[2] + bbox_x_range * 0.05)\ny_range = slice(bbox[1] - bbox_y_range * 0.05,\n                bbox[3] + bbox_y_range * 0.05)\n\nconus404 = conus404.sel(x=x_range, y=y_range)\nconus404\n\n\n\nTo make sure this worked as intended, let’s plot the full basin over the extracted CONUS404 data.\n\n# Select a single timestamp for simple plotting\ntimestamp = '2000-5-02'\ncutout = conus404.sel(time=timestamp).drop_vars(['lat', 'lon'])\n# We need to write the CRS to the CONUS404 dataset and\n# reproject for clean plotting with hvplot\ncutout = cutout.rio.write_crs(conus404.crs.crs_wkt).rio.reproject('EPSG:4326')\n\ncutout_plt = cutout.hvplot(\n    coastline='50m', geo=True,\n    aspect='equal', cmap='viridis', frame_width=300\n)\nhuc12_plt = huc12_basins.hvplot(\n    geo=True, alpha=0.3, c='r'\n)\ncutout_plt * huc12_plt\n\n\n\nLooks good!\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#limit-conus404-spatial-range","position":9},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl2":"Aggregate CONUS404 to Feature Polygons"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#aggregate-conus404-to-feature-polygons","position":10},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl2":"Aggregate CONUS404 to Feature Polygons"},"content":"Now that we have our gridded data and polygons, it is time to aggregate them using gdptools and what we consider the native method that uses geopandas and xarray.\n\nNOTE: gdptools handles a number of pre-processing steps for the user:\n\nSubsets the gridded data to a buffered bounding box of the targets polygons.\n\nChecks latitude bounds and if it’s in the interval 0-360, it’s rotated into -180 - 180.\n\nChecks the order of the longitude bounds, i.e. top-to-bottom or bottom-to-top, and autmatically acconts for this is the sub-setting operation above.\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#aggregate-conus404-to-feature-polygons","position":11},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl3":"gdptools Aggregation","lvl2":"Aggregate CONUS404 to Feature Polygons"},"type":"lvl3","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#gdptools-aggregation","position":12},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl3":"gdptools Aggregation","lvl2":"Aggregate CONUS404 to Feature Polygons"},"content":"Let’s start by using the gdptools aggregation method, where we use three data classes provided by gdptools, in the order discussed below.\n\nUserCatData stores the data required to perfom the aggregation.\n\nWeightGen is a class used to generate the areal-weights used to calculate the areal-weighted interpolation. The weights generated between a source and target dataset can be reused as long as the source and target are consistent. For example, If a new time-period became available, or a different set of variables is needed, the same weights can be used.\n\nAggGen is a class that is used to calculate the aggregation.\n\nThe first step to aggregating with gdptools is to convert the input data to a UserCatData class. Note additionally that the var parameter could be a list of variables, such that when the user_data object is used in AggGen, the calculuate_agg() method will perform the aggregation over all the list of variables.\n\nuser_data = UserCatData(\n    ds=conus404,\n    proj_ds=conus404.crs.crs_wkt,\n    x_coord='x',\n    y_coord='y',\n    t_coord='time',\n    var='PREC_ACC_NC',\n    f_feature=huc12_basins,\n    proj_feature=huc12_basins.crs,\n    id_feature='huc12',\n    period=[pd.Timestamp(conus404.time.values.min()),\n            pd.Timestamp(conus404.time.values.max())],\n)\n\n\n\nThe UserCatData can then be used to generate weights for each polygon. An important thing to note is that when generating the weights we need to use an equal area projection (i.e., equal area CRS).\n\ncrs_area = \"EPSG:6931\" # Good for northern hemisphere\n# crs_area = \"EPSG:5070\" # Good for CONUS\n\n# time the weight generation for later comparison\nt0 = time.time()\n\nweight_gen = WeightGen(\n    user_data=user_data,\n    # use serial here vs dask as the dask overhead would cause\n    # a slow down since our example is relatively small scale\n    method=\"serial\",\n    weight_gen_crs=crs_area,\n)\n\ndf_gdptools_weights = weight_gen.calculate_weights()\n\ngdptools_weights_time = time.time() - t0\n\ndf_gdptools_weights\n\n\n\nWith the weights, we can now perform the aggregation.\n\nNote that the return values of calculate_agg() are:\n\nngdf the target GeoDataFrame, sorted by id_feature and filtered to only those ids that have weights.\nIn other words if, there was not complete overlay of the source to target datasets, some target ids will not have values.\nIf the user wishes to plot the resulting interpolated data, the returned GeoDataFrame’s id order is the same as the gdptools_aggregation.\n\ngdptools_aggregation, which is an xarray.Dataset containing the interpolated output with dimensions of time and id_feature.\nIn the case below, the agg_writer parameter is set to 'none', it can be set to 'netcdf', 'csv', or 'parquet' for archiving the results to a file.\n\nt0 = time.time()\n\nagg_gen = AggGen(\n    user_data=user_data,\n    # Use masked to ignore NaNs\n    # Note that a we use mean vs sum as sum seems to ignore\n    # weights even though they should be equivalent methods\n    # (i.e., weighted sum = weighted mean)\n    stat_method=\"masked_mean\",\n    agg_engine=\"dask\",\n    weights=df_gdptools_weights,\n    agg_writer='none',\n)\n_, gdptools_aggregation = agg_gen.calculate_agg()\n\ngdptools_agg_time = time.time() - t0\n\ngdptools_aggregation\n\n\n\nLet’s make a nice plot of the aggregated HUC12 basins to make sure the aggregation worked as expected.\n\n# xarray holds the huc12s in sorted order\ngdptools_huc12_basins = huc12_basins.copy().sort_values('huc12')\ngdptools_huc12_basins['aggregation'] = gdptools_aggregation.sel(time=timestamp)['PREC_ACC_NC']\n\ngdptools_plt = gdptools_huc12_basins.hvplot(\n    c='aggregation', title=\"Accumulated Precipitation over HUC12 basins\",\n    coastline='50m', geo=True, cmap='viridis',\n    aspect='equal', legend=False, frame_width=300\n)\n\ncutout_plt * gdptools_plt + cutout_plt\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#gdptools-aggregation","position":13},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl3":"Native Method","lvl2":"Aggregate CONUS404 to Feature Polygons"},"type":"lvl3","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#native-method","position":14},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl3":"Native Method","lvl2":"Aggregate CONUS404 to Feature Polygons"},"content":"For the native method, we first need to extract the grid information from our CONUS404 data set. We then use it to create polygon boxes that we overlay with the basin polygons to generate weights. Finally like gdptools, we use the weights to aggregate via a weighted sum.\n\nTo give a fair computational time comparison with gdptools, we will group all steps to generate the weights into one timed cell.\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#native-method","position":15},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl4":"Create Weights","lvl3":"Native Method","lvl2":"Aggregate CONUS404 to Feature Polygons"},"type":"lvl4","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#create-weights","position":16},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl4":"Create Weights","lvl3":"Native Method","lvl2":"Aggregate CONUS404 to Feature Polygons"},"content":"To generate the weights, we (1) extract grid information (includes extracting the x and y grid and getting their bounds), (2) use these bounds to create polygons of the grid, (3) assign the polygons to a GeoDataFrame with the CONUS404 dataset’s CRS, (4) overlay the grid polygons and basin polygons, (5) use the overlay to get fractional area weights.\n\n%%time\nt0 = time.time()\n# (1) extract grid info\ngrid = conus404[['x', 'y']].drop_vars(['lat', 'lon']).reset_coords()\ngrid = grid.cf.add_bounds(['x', 'y'])\n\n\n# (2) create polygons of the grid\n# use a simple helper function. This way we can use xarray to parallelize.\ndef bounds_to_poly(x_bounds, y_bounds):\n    return Polygon([\n        (x_bounds[0], y_bounds[0]),\n        (x_bounds[0], y_bounds[1]),\n        (x_bounds[1], y_bounds[1]),\n        (x_bounds[1], y_bounds[0])\n    ])\n\n# Stack the grid cells into a single stack (i.e., x-y pairs)\npoints = grid.stack(point=('y', 'x'))\n\n# Apply the function to create polygons from bounds\nboxes = xr.apply_ufunc(\n    bounds_to_poly,\n    points.x_bounds,\n    points.y_bounds,\n    input_core_dims=[(\"bounds\",),  (\"bounds\",)],\n    output_dtypes=[np.dtype('O')],\n    vectorize=True\n)\n\n\n# (3) assign polygons to geodataframe with CRS\ngrid_polygons = gp.GeoDataFrame(\n    data={\"geometry\": boxes.values, \"y\": boxes['y'], \"x\": boxes['x']},\n    index=boxes.indexes[\"point\"],\n    crs=conus404.crs.crs_wkt\n)\n\n\n# (4) overlay the grid polygons with basin polygons\n# transform both to an area preserving projection\nhuc12_basins_area = huc12_basins.to_crs(crs_area)\ngrid_polygons = grid_polygons.to_crs(crs_area)\n\n# overlay the polygons.\noverlay = grid_polygons.overlay(huc12_basins_area, keep_geom_type=True)\n\n\n# (5)calculate the area fraction for each region\ngrid_cell_fraction = overlay.geometry.area.groupby(overlay['huc12']).transform(lambda x: x / x.sum())\n\n# turn this into a series\nmulti_index = overlay.set_index(['y', 'x', 'huc12']).index\ndf_native_weights = pd.Series(grid_cell_fraction.values, index=multi_index)\n\nda_native_weights_stacked = xr.DataArray(df_native_weights)\n\n# unstack to a sparse array.\nnative_weights = da_native_weights_stacked.unstack(sparse=True, fill_value=0.)\n\nnative_weights_time = time.time() - t0\n\nnative_weights\n\n\n\nNow that we have our weights, we can clearly see that this is a sparse matrix, with a density of ~0.0025 (i.e., only 0.25% of values are non-zero). So, maintaining it as a sparse martix is the right move for memory conservation, especially as this process scales up.\n\nAlso, this process is area conserving. We can verify this for each basin’s area with a simple area calculation.\n\n# calculate areas of HUC12s from overlay and original polygons\noverlay_area = overlay.geometry.area.groupby(overlay['huc12']).sum()\nhuc12_area = huc12_basins_area.geometry.area.groupby(huc12_basins_area['huc12']).sum()\n# find the max fractional difference\n(np.abs(overlay_area - huc12_area) / huc12_area).max()\n\n\n\nNice! This means the differences can be attributed to machine precision.\n\nWe can also verify that the cell fractions all sum up to one.\n\ngrid_cell_fraction.groupby(overlay['huc12']).sum().unique()\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#create-weights","position":17},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl4":"Perform Aggregation","lvl3":"Native Method","lvl2":"Aggregate CONUS404 to Feature Polygons"},"type":"lvl4","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#perform-aggregation","position":18},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl4":"Perform Aggregation","lvl3":"Native Method","lvl2":"Aggregate CONUS404 to Feature Polygons"},"content":"To aggregate the data, we can use \n\nxarray.Dataset.weighted to do our weighted calculations. This is simple as it will take a sparse array as weights and compute the aggregation.\n\n%%time\nt0 = time.time()\n\nnative_aggregation = conus404.drop_vars('crs').weighted(native_weights).sum(dim=['x', 'y']).compute()\n\nnative_agg_time = time.time() - t0\n\nnative_aggregation\n\n\n\nLike the gdptools aggregation results, let’s make some plots to make sure this worked as expected.\n\n# xarray holds the huc12s in sorted order\nnative_huc12_basins = huc12_basins.copy().sort_values('huc12')\nnative_huc12_basins['aggregation'] = native_aggregation.sel(time=timestamp)['PREC_ACC_NC'].data.todense()\nnative_plt = native_huc12_basins.hvplot(\n    c='aggregation', title=\"Accumulated Precipitation over HUC12 basins\",\n    coastline='50m', geo=True, cmap='viridis',\n    aspect='equal', legend=False, frame_width=300\n)\n\ncutout_plt * native_plt + cutout_plt\n\n\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#perform-aggregation","position":19},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl2":"Compare the Results"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#compare-the-results","position":20},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl2":"Compare the Results"},"content":"With both aggregation methods complete, we are now ready to compare the results. We can do this both for the final output and the intermediate weights.\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#compare-the-results","position":21},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl3":"Weight Comparison","lvl2":"Compare the Results"},"type":"lvl3","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#weight-comparison","position":22},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl3":"Weight Comparison","lvl2":"Compare the Results"},"content":"To do the weight comparison, we first need to standardize the weight outputs. This is relatively simple as we just need to convert the gdptools DataFrame weights into an xarray.DataArray. We can do this just like we did for the conservative method, but assigning the x and y values to the gdptools data frame using the given indices.\n\n# Due to the buffer region, gdptools weights i index values\n# are off by 3 and j are off by 1. This was found from a manual inspection\ndf_gdptools_weights['y'] = conus404['y'].isel(y=df_gdptools_weights['i']+3).data\ndf_gdptools_weights['x'] = conus404['x'].isel(x=df_gdptools_weights['j']+1).data\ngdptool_weights = xr.DataArray(\n    df_gdptools_weights.set_index(['y', 'x', 'huc12'])['wght']\n).unstack(sparse=True, fill_value=0)\n\n\n\nNow, a simple max fractional difference is the simple check for how they compare.\n\n(np.abs(gdptool_weights - native_weights) / native_weights).max()\n\n\n\nLook at that. They are identical (up to machine precision). So, the only other thing to compare would be the time required for the computation.\n\nprint(f'gdptools weights computation time: {gdptools_weights_time:0.3f} seconds')\nprint(f'native weights computation time: {native_weights_time:0.3f} seconds')\nprint(f'computation time difference: {(gdptools_weights_time - native_weights_time):0.3f} seconds')\nprint(f'computation time ratio: {(gdptools_weights_time / native_weights_time):0.3f}')\n\n\n\nSo, from this comparison, we can see that both methods give the same weights, but the method using xarray and geopandas slightly faster (and likely not significantly). However, this does not test how well either of the two methods scale.\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#weight-comparison","position":23},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl3":"Aggregation Comparison","lvl2":"Compare the Results"},"type":"lvl3","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#aggregation-comparison","position":24},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl3":"Aggregation Comparison","lvl2":"Compare the Results"},"content":"To do the aggregated data comparison, there is no need for any data formatting, as both gdptools and the native method have matching xarray.Dataset formats. So, let’s start with the simple max fractional difference to compare.\n\n(np.abs(gdptools_aggregation - native_aggregation) / native_aggregation).max()\n\n\n\nWell, as expected, they are nearly identical, since they had nearly identical weights.\n\nLet’s plot the fractional difference for a timestamp just to see how they compare.\n\n# xarray holds the huc12s in sorted order\ndiff_huc12_basins = huc12_basins.copy().sort_values('huc12')\ndiff_huc12_basins['aggregation'] = (np.abs(gdptools_aggregation - native_aggregation) / native_aggregation).sel(time=timestamp)['PREC_ACC_NC']\n\ndiff_huc12_basins.hvplot(\n    c='aggregation', title=\"Difference in Precipitation over HUC12 basins\",\n    coastline='50m', geo=True, cmap='viridis',\n    aspect='equal', legend=False, frame_width=300\n)\n\n\n\nFinally, let’s compare the computational times.\n\nprint(f'gdptools aggregation computation time: {gdptools_agg_time:0.3f} seconds')\nprint(f'native aggregation computation time: {native_agg_time:0.3f} seconds')\nprint(f'computation time difference: {(gdptools_agg_time - native_agg_time):0.3f} seconds')\nprint(f'computation time ratio: {(gdptools_agg_time / native_agg_time):0.3f}')\n\n\n\nIt looks like this step takes about the same time for both. So, let’s compare the total computation time (weights and aggregation).\n\ngdptools_total_time = gdptools_weights_time+gdptools_agg_time\nnative_total_time = native_weights_time + native_agg_time\nprint(f'gdptools total computation time: {gdptools_total_time:0.3f} seconds')\nprint(f'native total computation time: {native_total_time:0.3f} seconds')\nprint(f'total computation time difference: {(gdptools_total_time - native_total_time):0.3f} seconds')\nprint(f'total computation time ratio: {(gdptools_total_time / native_total_time):0.3f}')\n\n\n\nAlright, since both the aggregation and weights times are about equal, the overall performance of both is equal as well. Therefore, it appears that either method is a solid choice. The only other thing to test would be how well each method scales to larger feature polygons and larger grids. However, we will leave that comparison for another notebook.\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#aggregation-comparison","position":25},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl2":"Shut down the Dask Client"},"type":"lvl2","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#shut-down-the-dask-client","position":26},{"hierarchy":{"lvl1":"gdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation","lvl2":"Shut down the Dask Client"},"content":"If utilized, we should shut down the dask client.\n\nclient.close()\n\n","type":"content","url":"/dataset-processing/tutorials/spatial-aggregation/conus404-spatial-aggregation-comparison#shut-down-the-dask-client","position":27},{"hierarchy":{"lvl1":"What is HyTEST?"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"What is HyTEST?"},"content":"The Hydro-Terrestrial Earth System Testbed (HyTEST) Organization is a community promoting open, reproducible, and scalable tools and workflows for hydrological modeling on advanced computing platforms. Initiated as a collaboration between the U.S. Geological Survey (USGS) and the National Center for Atmospheric Research (NCAR), HyTEST seeks to build a diverse community of researchers and open-source developers focused on improving the usability of hydro-terrestrial data and models, creating an interdisciplinary team that cuts across academia, government, and industry.\n\nFollowing the successful approach of the \n\nPangeo Community, instead of building monolithic software environments, we seek out tools and approaches widely used in the open-source development community. We add value by testing them on our domain-specific workflows and by contributing enhancements, building modeling testbeds that support open and reproducible science. This approach benefits end-users by enabling them to more easily leverage contributions by others, while improving the flexibility and future-proofing of how they use these. With a diverse community of developers, users, and stakeholders participating in the HyTEST Organization, there will be a pathway for sharing the best modeling tools and workflows into the future.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"What is HyTEST?","lvl2":"About the HyTEST Repository"},"type":"lvl2","url":"/#about-the-hytest-repository","position":2},{"hierarchy":{"lvl1":"What is HyTEST?","lvl2":"About the HyTEST Repository"},"content":"The HyTEST organization encompasses a multitude of repositories, however the \n\nhytest repo is where users can find demonstrations, tutorials, and explanatory Jupyter Notebooks for common workflows encountered in the hydrologic and atmospheric modeling fields. These workflows cover development, evaluation, and visualization of small to large scale modeling applications on computational environments such as the cloud, high performance computers, or local environments.","type":"content","url":"/#about-the-hytest-repository","position":3},{"hierarchy":{"lvl1":"What is HyTEST?","lvl2":"How to contribute?"},"type":"lvl2","url":"/#how-to-contribute","position":4},{"hierarchy":{"lvl1":"What is HyTEST?","lvl2":"How to contribute?"},"content":"The HyTEST Organization is excited to foster collaboration amongst members and newcomers. We hope to hear more about real-world applications of HyTEST workflows, workflow improvements and suggestions, bug fixes, demo and tutorial additions. We are happy to grow membership in HyTEST and connect with developers and researchers that aim for similar principles of open-source, reproducible science.\n\nIf you have bug fixes, suggestions, ideas for helpful tutorials or demos, course material, or want to share new developments in the hydrologic and atmospheric software community, please see further \n\ndetails on contributing. Please don’t hesitate to reach out to HyTEST members with any questions or problems with submitting an issue to the repo.","type":"content","url":"/#how-to-contribute","position":5},{"hierarchy":{"lvl1":"AWS Credentials"},"type":"lvl1","url":"/environment-set-up/help-aws-credentials","position":0},{"hierarchy":{"lvl1":"AWS Credentials"},"content":"If a workflow needs to access S3, it will be important to establish AWS credentials before first access to the object storage. If you are working in a workflow using a dask cluster, you will need to establish AWS credentials before the cluster starts up.\n\nThe following code block will handle*general configuration for establishing AWS credentials for the notebook and any spawned cluster workers.\n\nYou will need to make sure your AWS_PROFILE or AWS_S3_ENDPOINT environmental variables are established before this is run. If they are not, the defaults will be set to the access the OSN storage pod (with a profile named osn-hytest-scratch and an endpoint of https://usgs.osn.mghpcc.org), which will only work if you have this AWS profile configured. Please reach out to \n\nasnyder@usgs.gov if you are a USGS staff member and you would like access to the credentials for this scratch space to test out our tutorials.\n\nimport os\nimport logging\nimport configparser\n\nawsconfig = configparser.ConfigParser()\nawsconfig.read(\n    os.path.expanduser('~/.aws/credentials') # default location... if yours is elsewhere, change this.\n)\n## NOTE:  The default will be for the OSN / RENCI profile and endpoint. Override this\n## by setting environment variables before executing this cell/notebook.\n_profile_nm  = os.environ.get('AWS_PROFILE', 'osn-hytest-scratch')\n_endpoint = os.environ.get('AWS_S3_ENDPOINT', 'https://usgs.osn.mghpcc.org')\n# Set environment vars based on parsed awsconfig\ntry:\n    os.environ['AWS_ACCESS_KEY_ID']     = awsconfig[_profile_nm]['aws_access_key_id']\n    os.environ['AWS_SECRET_ACCESS_KEY'] = awsconfig[_profile_nm]['aws_secret_access_key']\n    os.environ['AWS_S3_ENDPOINT']       = _endpoint\n    os.environ['AWS_PROFILE'] = _profile_nm\n    os.environ['AWS_DEFAULT_PROFILE'] = _profile_nm\n    os.environ['AWS_S3_REGION'] = _profile_nm\nexcept KeyError:\n    logging.error(\"Problem parsing the AWS credentials file. \")\n\n\n","type":"content","url":"/environment-set-up/help-aws-credentials","position":1},{"hierarchy":{"lvl1":"HPC Server: Jupyter Forward Quick-Start"},"type":"lvl1","url":"/environment-set-up/jupyterforward","position":0},{"hierarchy":{"lvl1":"HPC Server: Jupyter Forward Quick-Start"},"content":"This option will configure your HPC account to access the correct Jupyter server software.  That server\nwill be invoked by a command installed on your PC.  This option minimizes the configuration changes\nneeded on the HPC host, but still permits relatively easy access to that hardware via a personal Jupyter\nServer.","type":"content","url":"/environment-set-up/jupyterforward","position":1},{"hierarchy":{"lvl1":"HPC Server: Jupyter Forward Quick-Start","lvl2":"1) Configure your HPC account to access the HyTEST conda environment :"},"type":"lvl2","url":"/environment-set-up/jupyterforward#id-1-configure-your-hpc-account-to-access-the-hytest-conda-environment","position":2},{"hierarchy":{"lvl1":"HPC Server: Jupyter Forward Quick-Start","lvl2":"1) Configure your HPC account to access the HyTEST conda environment :"},"content":"* Open a terminal window and log in to your HPC host. If you do not know how to do this, you can consult the [HPC User Docs](https://hpcportal.cr.usgs.gov/hpc-user-docs/guides/connecting/ssh.html).\n* Edit your `.bashrc` file to include these two lines at the bottom:\n  ```bash\n  module use --append /caldera/projects/usgs/water/impd/hytest/modules\n  module load hytest\n  ```\n* NOTE: Edit your `.bashrc` with care. It controls what happens when you log in.  If it gets mangled,\n  your account may not work correctly .","type":"content","url":"/environment-set-up/jupyterforward#id-1-configure-your-hpc-account-to-access-the-hytest-conda-environment","position":3},{"hierarchy":{"lvl1":"HPC Server: Jupyter Forward Quick-Start","lvl2":"2) Install jupyter-forward on your PC"},"type":"lvl2","url":"/environment-set-up/jupyterforward#id-2-install-jupyter-forward-on-your-pc","position":4},{"hierarchy":{"lvl1":"HPC Server: Jupyter Forward Quick-Start","lvl2":"2) Install jupyter-forward on your PC"},"content":"This is a one-time operation to get the right software on your desktop.\nThe \n\njupyter-forward software will\nwrap up a series of commands necessary to to execute a jupyter server on the\nHPC host we just configured. It is a convenience package intended to make\nHPC-based jupyter servers easy.* Install Python on your PC. <br>\n  You will need to have python installed on your PC, along with either `pip` or\n  `conda` to help manage the python environments. We recommend anaconda.\n  You can request anaconda from IT, or you can download the installer from [anaconda.com](https://www.anaconda.com/)\n  to install it in user space (i.e. admin is not required).\n* Add `jupyter-forward` to your PC:<br>\n  Launch an `Anaconda Prompt` from your Start menu, then execute:\n  ```text\n  > conda install -c conda-forge jupyter-forward\n  ```\n\nAnaconda provides the option for multiple virtual environments and configurations.  It does not\nmatter which one is active when you install jupyter-forward ( the “base” or some other).  You\njust need to have that enviroment active when you attempt to launch the command later.","type":"content","url":"/environment-set-up/jupyterforward#id-2-install-jupyter-forward-on-your-pc","position":5},{"hierarchy":{"lvl1":"HPC Server: Jupyter Forward Quick-Start","lvl2":"3) Launch Server"},"type":"lvl2","url":"/environment-set-up/jupyterforward#id-3-launch-server","position":6},{"hierarchy":{"lvl1":"HPC Server: Jupyter Forward Quick-Start","lvl2":"3) Launch Server"},"content":"With all of that set up, you are now ready to launch a session on the HPC using\njupyter-forward on your PC. Do this every time you would like to run notebooks\nhoused on the HPC host.* Launch an `Anaconda Prompt` from your start menu on your local computer\n* Run the following command to connect compute node. If you are not in the `impd` group, you will need to replace `impd` with a group you are a part of. You may also need to update the number of hours you want to have access to the compute node (in this example, we are requesting 1 hour).:  jupyter-forward --launch-command \"srun -A impd -N 1 -t 01:00:00\"  denali* If prompted, log in with your AD username and password.\n* The output of the script will also provide the URL where your PC's browser will find the jupter server. You can open one of these urls in your browser.","type":"content","url":"/environment-set-up/jupyterforward#id-3-launch-server","position":7},{"hierarchy":{"lvl1":"HPC Server: Jupyter Forward Quick-Start","lvl2":"4) Shut Down Server"},"type":"lvl2","url":"/environment-set-up/jupyterforward#id-4-shut-down-server","position":8},{"hierarchy":{"lvl1":"HPC Server: Jupyter Forward Quick-Start","lvl2":"4) Shut Down Server"},"content":"After a daily session, you will need to shut down the jupyter server.\nIn the terminal session where you started jupyter-forward, merely press Ctl-C\nto signal the server to shut down.","type":"content","url":"/environment-set-up/jupyterforward#id-4-shut-down-server","position":9},{"hierarchy":{"lvl1":"HPC Server: Open OnDemand Quick-Start"},"type":"lvl1","url":"/environment-set-up/openondemand","position":0},{"hierarchy":{"lvl1":"HPC Server: Open OnDemand Quick-Start"},"content":"This is a custom service provided by the USGS ARC team that is available to anyone who has an account on the USGS high performance computers (HPCs). If you are a USGS staff member who does not yet have an account on the HPCs, you can request one \n\nhere. Once your USGS HPC account has been created, you will be able to follow the instructions below to use this service.\n\nThis is the easiest option for running Jupyter notebooks on the HPCs, as there is no configuration needed on your part. This option provides reasonable compute resources via the tallgrass and hovenweep HPC hosts:\n\nTo log in to OnDemand, select the appropriate login link from the OnDemand section of the \n\nHPC User Docs. Note that you must be on the USGS VPN to access this host. Denali/Tallgrass share one disk for data storage and Hovenweep has a different disk. If you have data stored on the HPCs, you will want to choose whichever resource is attached to where your data is stored. If you are accessing data from a different, publicly accessible storage location, you can choose either option.\n\nFrom the OnDemand landing page, choose Interactive Apps. If you are using Hovenweep, select the Jupyter option from this dropdown. If you are using Tallgrass, you can either select Jupyter or you can launch the HyTEST Jupyter server app, which will include a conda environment pre-configured with the packages you need to run the workflows in this JupyterBook. If you do not use our pre-configured environment (if you selected Jupyter), you will need to build your own once your connect to the HPC. This process is described below in [Conda Environment Set Up](#Conda Environment Set Up).\n\nFill in the form to customize the allocation in which the Jupyter Server will execute.\n\nFor light duty work (i.e. tutorials), a Viz node is likely adequate in your allocation request.  If you\nwill be doing heavier processing, you may want to request a compute node.  None of the HyTEST tutorials\nutilize GPU code; a GPU-enabled node is not necessary.\n\nYou may want to consider adding the git and/or aws modules if you plan to use them during your session. You will just need to type module load git and/or module load aws in the Module loads section.\n\nIf you expect to run code in parallel on multiple compute nodes, you have two options. (1) You can use the form to request the number of cores you need and then run a \n\nDask Local Cluster on those cores, or (2) you can request the standard 2 cores, and then use a \n\nDask SLURMCluster to submit new jobs to the SLURM scheduler, giving you access to additional compute nodes. Please see the HPC docs to learn more about the available compute and memory resources for \n\nTallgrass and \n\nHovenweep.\n\nClick Submit\n\nOnce your server is ready, a Connect to Jupyter button will appear that you can click to start your session.\n\nThe Jupyter Server will run in an allocation on tallgrass or hovenweep. This server will have access to your home\ndirectory/folder on that host, which is where your notebooks will reside.","type":"content","url":"/environment-set-up/openondemand","position":1},{"hierarchy":{"lvl1":"HPC Server: Open OnDemand Quick-Start","lvl2":"Conda Environment Set Up"},"type":"lvl2","url":"/environment-set-up/openondemand#conda-environment-set-up","position":2},{"hierarchy":{"lvl1":"HPC Server: Open OnDemand Quick-Start","lvl2":"Conda Environment Set Up"},"content":"If you need to set up your own conda environment to run the notebooks, proceed with the following steps:\n\nmake sure you have the \n\nHyTEST environment file uploaded to the HPCs. You can manually upload just this file, or you can clone the \n\nHyTEST repository, which contains this file.\n\nload the conda module with module load conda\n\nnavigate in to the directory containing the environment yaml file from step 1, and create the hytest environment with conda env create -f hytest.yml\n\nactivate your environment with conda activate hytest\n\ncreate kernel of hytest environment by running python -m ipykernel install --user --name hytest --display-name \"hytest\"\n\nNow, you will be able to see the environment you just built as an available kernel from your Jupyter notebook.\n\nFor additional help setting up and using conda environments on the HPCs, please see the \n\nHPC User Docs.\n\nNote: The code to build the HyTEST Jupyter app is in \n\nthis repository; however, this repo is only visible on the internal USGS network.","type":"content","url":"/environment-set-up/openondemand#conda-environment-set-up","position":3},{"hierarchy":{"lvl1":"HPC Server: Open OnDemand Quick-Start","lvl2":"Dask Dashboard"},"type":"lvl2","url":"/environment-set-up/openondemand#dask-dashboard","position":4},{"hierarchy":{"lvl1":"HPC Server: Open OnDemand Quick-Start","lvl2":"Dask Dashboard"},"content":"If you plan to use the dask dashboard with OnDemand, you will need to launch your interactive session in a conda environment that includes the dask_labextension package. If you are using the HyTEST environment provided in the \n\nhytest.yml file in our repository, we have included this package. You will just need to choose the hytest environment when you are launching your interactive session (after you have built it) using the I have a custom conda env I'd like to use option during launch.","type":"content","url":"/environment-set-up/openondemand#dask-dashboard","position":5},{"hierarchy":{"lvl1":"Cloud: Nebari Quick-Start"},"type":"lvl1","url":"/environment-set-up/quickstart-cloud-nebari","position":0},{"hierarchy":{"lvl1":"Cloud: Nebari Quick-Start"},"content":"If you are a USGS employee, and you would like to run HyTEST workflows in a cloud environment, we recommend that you work with the CHS’s deployment of \n\nNebari, which can be accessed \n\nhere. This JupyterHub instance is deployed on USGS’s AWS account, and any USGS employee can request access to this space.","type":"content","url":"/environment-set-up/quickstart-cloud-nebari","position":1},{"hierarchy":{"lvl1":"Cloud: Nebari Quick-Start","lvl2":"File Space"},"type":"lvl2","url":"/environment-set-up/quickstart-cloud-nebari#file-space","position":2},{"hierarchy":{"lvl1":"Cloud: Nebari Quick-Start","lvl2":"File Space"},"content":"This resource offers both a private (per user) and a shared file space in the JupyterHub environment. The private space is only visible to the user who created it, while the shared space allows for file sharing and collaboration among team members.","type":"content","url":"/environment-set-up/quickstart-cloud-nebari#file-space","position":3},{"hierarchy":{"lvl1":"Cloud: Nebari Quick-Start","lvl2":"Kernels"},"type":"lvl2","url":"/environment-set-up/quickstart-cloud-nebari#kernels","position":4},{"hierarchy":{"lvl1":"Cloud: Nebari Quick-Start","lvl2":"Kernels"},"content":"USGS’s Cloud Hosting Solutions (CHS) team provides a global “pangeo” environment that is available to all users in the Nebari space by default. This environment includes many of the packages needed to run pangeo-style workflows.\n\nHowever, most users will need to add custom packages for their workflows at some point. Thankfully, Nebari also allows any user to set up and modify a set of python kernels in their personal namespace.\n\nWe also have a HyTEST team set up in this space, where we manage a set of environments needed to run our HyTEST workflows. Please reach out to \n\nasnyder@usgs.gov if you have your Nebari account set up and would like to be added to the HyTEST team.","type":"content","url":"/environment-set-up/quickstart-cloud-nebari#kernels","position":5},{"hierarchy":{"lvl1":"Cloud: Nebari Quick-Start","lvl2":"Accessing the Nebari JupyterHub Instance"},"type":"lvl2","url":"/environment-set-up/quickstart-cloud-nebari#accessing-the-nebari-jupyterhub-instance","position":6},{"hierarchy":{"lvl1":"Cloud: Nebari Quick-Start","lvl2":"Accessing the Nebari JupyterHub Instance"},"content":"To use this JupyterHub instance:\n\nYou will need to request an account before you can log in. If you previously used the pangeo.chs.usgs.gov endpoint, you will have an account and you can proceed to log in with the USGS Login button. If you do not have an account yet, you can submit a request for one \n\nhere.\n\nOpen \n\nhttps://​nebari​.chs​.usgs​.gov/ in your browser (you will need to be on the USGS network or VPN to access this resource)\n\nLogin using Keycloak  and the USGS Login button\n\nLaunch JupyterLab from the landing page, and spawn a server with the appropriate specifications from the menu of options that appears (the smallest one is probably fine if you are just testing things out)\n\nLaunch a terminal, configure your Github credentials, and clone this repository (or another that you are working on) into your working space\n\nUse the navigation panel on the left to open a notebook, and select the hytest-pangeo conda env kernel in the dropdown menu in the upper right\n\nNow you are ready to run/develop your notebooks.","type":"content","url":"/environment-set-up/quickstart-cloud-nebari#accessing-the-nebari-jupyterhub-instance","position":7},{"hierarchy":{"lvl1":"General QuickStart"},"type":"lvl1","url":"/environment-set-up/quickstart-general","position":0},{"hierarchy":{"lvl1":"General QuickStart"},"content":"","type":"content","url":"/environment-set-up/quickstart-general","position":1},{"hierarchy":{"lvl1":"General QuickStart","lvl2":"Jupyter Server"},"type":"lvl2","url":"/environment-set-up/quickstart-general#jupyter-server","position":2},{"hierarchy":{"lvl1":"General QuickStart","lvl2":"Jupyter Server"},"content":"If you are working in an environment that already has a pathway to run Jupyter notebooks, you will not need to worry about this section. You will simply launch the Jupyter notebook in whatever way you know how.\n\nIf you do need to set up your own Jupyter server, this is a harder problem to solve -- primarily because\nthere are so many variables and options to consider when setting it up. The \n\nJupyter\ndocs give some help for setting up; however, these deployments, generally, are meant for a single user to run notebooks on a semi-private host.\n\nIf multiple users will be using the same server, consider deploying a \n\nJupyter Hub on a centralized computing host. There are many resources online for deploying a JupyterHub, including:\n\nBuilding multi-tenant JupyterHub Platforms on Amazon EKS\n\nMicrosoft Planetrary Computer’s Guide: Deploy your own JupyterHub","type":"content","url":"/environment-set-up/quickstart-general#jupyter-server","position":3},{"hierarchy":{"lvl1":"General QuickStart","lvl2":"Virtual Environment"},"type":"lvl2","url":"/environment-set-up/quickstart-general#virtual-environment","position":4},{"hierarchy":{"lvl1":"General QuickStart","lvl2":"Virtual Environment"},"content":"We recommend conda (or its functional equivalent, mamba) as the environment configuration tool. If you are a USGS Staff Member, you will want to install \n\nminiforge, which includes conda and mamba.\n\nThe HyTEST workflows are built largely on the \n\nPangeo software stack. The software stack we use is defined in \n\nthis environment file.\n\nUse conda to create an environment called hytest with this command:conda env create -f ./hytest.yml\n\nYou will then need to select the hytest environment as your kernel when you try to run the notebook.\n\nWhile is is possible to run most of these workflows on a desktop system, they have been developed to run on a Linux variant.  This will affect which versions and which architectures of the various software packages are installed.","type":"content","url":"/environment-set-up/quickstart-general#virtual-environment","position":5},{"hierarchy":{"lvl1":"Compute Environments"},"type":"lvl1","url":"/environment-set-up/readme","position":0},{"hierarchy":{"lvl1":"Compute Environments"},"content":"This section contains instructional materials and helper scripts to set up your computational environment for running HyTEST workflows and scripts in this repository.\n\nThe HyTEST workflows are built largely on the \n\nPangeo software stack, and are designed to be portable between the cloud, HPC clusters, and your local computer (though some workflows may take an unreasonable amount of time to complete if you are working only on a local computer). Most of our workflows are contained in Jupyter notebooks, and we recommend running them in \n\nJupyterLab or another development environment of your choice that supports Jupyter notebooks. If you are working on a team, you may want to consider using a shared \n\nJupyterHub instance, which allows multiple users to work in the same environment and share files easily. Below, we describe a few options for setting up or connecting to JupyterHub resources.","type":"content","url":"/environment-set-up/readme","position":1},{"hierarchy":{"lvl1":"Compute Environments","lvl2":"Setting Up Your Own Computing Environment"},"type":"lvl2","url":"/environment-set-up/readme#setting-up-your-own-computing-environment","position":2},{"hierarchy":{"lvl1":"Compute Environments","lvl2":"Setting Up Your Own Computing Environment"},"content":"If you do not have access to a JupyterLab/JupyterHub environment that has already been deployed, we recommend you follow \n\nthese instructions to set up a computational environment that can run our workflows.","type":"content","url":"/environment-set-up/readme#setting-up-your-own-computing-environment","position":3},{"hierarchy":{"lvl1":"Compute Environments","lvl2":"USGS Cloud Environment (only available to USGS staff)"},"type":"lvl2","url":"/environment-set-up/readme#usgs-cloud-environment-only-available-to-usgs-staff","position":4},{"hierarchy":{"lvl1":"Compute Environments","lvl2":"USGS Cloud Environment (only available to USGS staff)"},"content":"If you are a USGS employee, and you would like to run HyTEST workflows in a cloud environment, we recommend that you work with the CHS’s deployment of \n\nNebari, which can be accessed \n\nhere. This JupyterHub instance is deployed on USGS’s AWS account, backed by a Kubernetes cluster for scaled compute, and any USGS employee can request access to this space. This resource offers both a private (per user) and a shared file space in the JupyterHub environment. The private space is only visible to the user who created it, while the shared space allows for file sharing and collaboration among team members. Learn how to use the Nebari JupyterHub instance in the \n\nQuick-Start Guide.","type":"content","url":"/environment-set-up/readme#usgs-cloud-environment-only-available-to-usgs-staff","position":5},{"hierarchy":{"lvl1":"Compute Environments","lvl2":"USGS HPC Environment (only available to USGS staff)"},"type":"lvl2","url":"/environment-set-up/readme#usgs-hpc-environment-only-available-to-usgs-staff","position":6},{"hierarchy":{"lvl1":"Compute Environments","lvl2":"USGS HPC Environment (only available to USGS staff)"},"content":"While we prioritize the development of workflows to run on “cloud” infrastructure, we also provide some guidance for running them on USGS’s supercomputers. First, make sure you have an account on Denali, Tallgrass, and/or Hovenweep. We recommend requesting accounts on all of these systems, as at times compute nodes may be unavailable on one system or the other. To get an account, fill out the \n\nrequest form and wait for approval (this may take a day or so).\n\nNote that to access any of the HPCs, you must be on the DOI Network. If you are not in the office, please make sure you access these resources on the VPN.\n\nYou will need to access an enviroment which emulates the Jupyter server -- where the notebooks will reside and execute -- using the HPC hardware. There are many ways to do this. Here are three options, in increasing order of complexity and flexibility:\n\nOpen OnDemand\nThis option provides the most effortless access to HPC hardware using a web interface. However, this only runs on the \n\nTallgrass and \n\nHovenweep supercomputers.\n\nJupyter Forward\nThis option gives you more control over how to launch the server, and on which host (can be run on \n\nDenali, \n\nTallgrass, \n\nHovenweep, or other hosts to which you have a login). This setup requires that you install the jupyter-forward software on your PC.\n\nCustom Server Script\nThis option lets you completely customize your HPC compute environment and invoke the Jupyter server from a command shell on the HPC. This requires familiarity with the HPC command line, file editing, etc. This (can be run on \n\nDenali, \n\nTallgrass, \n\nHovenweep, or other hosts to which you have a login).","type":"content","url":"/environment-set-up/readme#usgs-hpc-environment-only-available-to-usgs-staff","position":7},{"hierarchy":{"lvl1":"HPC Server: Start Script Quick-Start"},"type":"lvl1","url":"/environment-set-up/startscript","position":0},{"hierarchy":{"lvl1":"HPC Server: Start Script Quick-Start"},"content":"Use this option if you would like more control over the allocation request and the environment in\nwhich the jupyter server runs. This option requires that you log into the HPC host to run a custom\nscript.  Copy \n\nthis example and edit to your liking.\n\nLaunching in this way will start your custom jupyter server and plumb the port forwarding necessary\nto make it available to your desktop web browser.\n\nOpen a terminal window and log in to your HPC host. If you do not know how to do this, you can consult the \n\nHPC User Docs.\n\nClone the \n\nhytest repository into the directory you want to work in.\n\nRequest a compute node with a command like salloc -A impd -t 1:00:00 -N 1 srun --pty bash. If you are not in the impd group, you will need to replace impd with a group you are a part of. You may also need to update the number of hours you want to have access to the compute node (in this example, we are requesting 1 hour).\n\nNavigate into the environment_set_up directory (cd environment_set_up/) and execute the \n\njupyter-start.sh script (bash jupyter-start.sh).\n\nThe jupyter-start.sh script will direct you on how to start port forwarding (that’s the ssh command that you are directed to run from a new terminal on your desktop). You will want to open a new terminal window on your local computer an run this command. If prompted, log in with your AD username and password. The cursor will hang after entering your password. This is expected - you can minimize this window and move on to the next step.\n\nThe output of the script will also provide the URL where your PC’s browser will find the jupter server. You can open one of these urls in your browser.\n\nNow you are set up with a Jupyter server connected to the HPC you logged in to.\n\nThis option can be useful if you want to build your own conda environment, rather than using the provided\nHyTEST environment. The central HyTEST environment is built using \n\nthis environment definition. We recommend that you\nuse it as the baseline if you will be building your own environment.\n\nIf you choose to do that, you will want to edit the jupyter-start.sh script to reflect\nthe particulars of your custom conda environment.","type":"content","url":"/environment-set-up/startscript","position":1},{"hierarchy":{"lvl1":"Denali HPC Dask Cluster"},"type":"lvl1","url":"/environment-set-up/start-dask-cluster-denali","position":0},{"hierarchy":{"lvl1":"Denali HPC Dask Cluster"},"content":"import os\nimport logging\n\nfrom dask.distributed import LocalCluster, Client\n\n## Denali is treated like a very, very, very big PC.  \ncluster = LocalCluster(threads_per_worker=1)\nclient = Client(cluster)\n    \nprint(\"The 'cluster' object can be used to adjust cluster behavior.  i.e. 'cluster.adapt(minimum=10)'\")\nprint(\"The 'client' object can be used to directly interact with the cluster.  i.e. 'client.submit(func)' \")\nprint(f\"The link to view the client dashboard is:\\n>  {client.dashboard_link}\")\n\n","type":"content","url":"/environment-set-up/start-dask-cluster-denali","position":1},{"hierarchy":{"lvl1":"Local Desktop Dask Cluster"},"type":"lvl1","url":"/environment-set-up/start-dask-cluster-desktop","position":0},{"hierarchy":{"lvl1":"Local Desktop Dask Cluster"},"content":"import os\nfrom dask.distributed import Client, LocalCluster\n\ncluster = LocalCluster(threads_per_worker=os.cpu_count())\nclient = Client(cluster)\n\nprint(f'dask dashboard available at: {client.dashboard_link}')\n\n","type":"content","url":"/environment-set-up/start-dask-cluster-desktop","position":1},{"hierarchy":{"lvl1":"Nebari Dask Cluster"},"type":"lvl1","url":"/environment-set-up/start-dask-cluster-nebari","position":0},{"hierarchy":{"lvl1":"Nebari Dask Cluster"},"content":"import os\nimport logging\n\n# Needed when boto3 >= 1.36.0 or the rechunking process will fail\n# This needs to be set before the boto3 library gets loaded\n# See: https://github.com/aws/aws-cli/issues/9214#issuecomment-2606619168\nos.environ['AWS_REQUEST_CHECKSUM_CALCULATION'] = 'when_required'\nos.environ['AWS_RESPONSE_CHECKSUM_VALIDATION'] = 'when_required'\n\ntry:\n    from dask_gateway import Gateway\nexcept ImportError:\n    logging.error(\"Unable to import Dask Gateway.  Are you running in a cloud compute environment?\\n\")\n    raise\nos.environ['DASK_DISTRIBUTED__SCHEDULER__WORKER_SATURATION'] = \"1.0\"\n\ngateway = Gateway()\n_options = gateway.cluster_options()\n_options.conda_environment='hytest/hytest-pangeo'  ##<< this is the conda environment we use on nebari.\n_options.profile = 'Medium Worker'\n_env_to_add={}\naws_env_vars=['AWS_ACCESS_KEY_ID',\n              'AWS_SECRET_ACCESS_KEY',\n              'AWS_SESSION_TOKEN',\n              'AWS_DEFAULT_REGION',\n              'AWS_S3_ENDPOINT',\n              'AWS_REQUEST_CHECKSUM_CALCULATION',\n              'AWS_RESPONSE_CHECKSUM_VALIDATION']\nfor _e in aws_env_vars:\n    if _e in os.environ:\n        _env_to_add[_e] = os.environ[_e]\n_options.environment_vars = _env_to_add    \ncluster = gateway.new_cluster(_options)          ##<< create cluster via the dask gateway\ncluster.scale(30)             ##<< Sets scaling parameters. \n\nclient = cluster.get_client()\n\nprint(\"The 'cluster' object can be used to adjust cluster behavior.  i.e. 'cluster.adapt(minimum=10)'\")\nprint(\"The 'client' object can be used to directly interact with the cluster.  i.e. 'client.submit(func)' \")\nprint(f\"The link to view the client dashboard is:\\n>  {client.dashboard_link}\")\n\n","type":"content","url":"/environment-set-up/start-dask-cluster-nebari","position":1},{"hierarchy":{"lvl1":"Hovenweep/Tallgrass HPC Dask Cluster"},"type":"lvl1","url":"/environment-set-up/start-dask-cluster-tallgrass","position":0},{"hierarchy":{"lvl1":"Hovenweep/Tallgrass HPC Dask Cluster"},"content":"import os\nimport logging\n\nfrom dask.distributed import Client\nfrom dask_jobqueue import SLURMCluster   \n\ntry:\n    project = os.environ['SLURM_JOB_ACCOUNT']\nexcept KeyError:\n    logging.error(\"SLURM_JOB_ACCOUNT is not set in the active environment. Are you on the login node? You should not be running this there.\")\n    raise\n\ncluster = SLURMCluster(\n    account='hytest', \n    processes=1, \n    cores=1, \n    memory='8GB', \n    interface='ib0',\n    walltime='01:00:00',      \n    shared_temp_directory='/home/asnyder/tmp'\n)\ncluster.adapt(minimum=2, maximum=30)\n\nclient = Client(cluster)\n\nif os.environ['SLURM_CLUSTER_NAME']=='hovenweep':\n    ood_pre = 'hw'\nelif os.environ['SLURM_CLUSTER_NAME']=='tallgrass':\n    ood_pre = 'tg'\n\nood_dashboard_link = f\"https://{ood_pre}-ood.cr.usgs.gov/node/{os.environ['JUPYTER_SERVER_NODE']}/{os.environ['JUPYTER_SERVER_PORT']}/proxy/{client.dashboard_link.split(':')[2]}\"\nprint(f'Dask Dashboard for OnDemand is available at: {ood_dashboard_link}')\n\n","type":"content","url":"/environment-set-up/start-dask-cluster-tallgrass","position":1},{"hierarchy":{"lvl1":"Starting a Dask Cluster"},"type":"lvl1","url":"/environment-set-up/clusters","position":0},{"hierarchy":{"lvl1":"Starting a Dask Cluster"},"content":"Many of the workflows in this repository are designed to take advantage of parallelism via dask clusters. The details of spinning up a cluster will differ, depending on the \n\ncompute platform. We have boilerplate code to help in starting a suitable cluster for many of the computing environments where HyTEST workflows are likely to run.\n\nNebari\n\nDenali HPC\n\nHovenweep/Tallgrass HPC\n\nLocal Desktop","type":"content","url":"/environment-set-up/clusters","position":1},{"hierarchy":{"lvl1":"Data/APIs"},"type":"lvl1","url":"/essential-reading/datasources/data-apis","position":0},{"hierarchy":{"lvl1":"Data/APIs"},"content":"Application Programming Interfaces (API) are a common way of accessing and processing\ndata over the internet between a distant application and the application (or script)\nyou are working on. Another method is File Transfer Protocol calls (FTP), which are\nused to transfer large amounts of data over the internet. As FTPs are not communications\nbetween applications but just simple data tranfers with no processing capabilities, they\nare being used less and less over time, though some data is still only available over\nthe internet in this fashion.","type":"content","url":"/essential-reading/datasources/data-apis","position":1},{"hierarchy":{"lvl1":"Data/APIs","lvl2":"API"},"type":"lvl2","url":"/essential-reading/datasources/data-apis#api","position":2},{"hierarchy":{"lvl1":"Data/APIs","lvl2":"API"},"content":"In Python, there are different ways of getting data with APIs and we will examine two of the more common: using a library such as requests to query the API or use a library a developer has created to make interacting with an API easier, such as pygeohydro, through different helper functions or improved documentation.","type":"content","url":"/essential-reading/datasources/data-apis#api","position":3},{"hierarchy":{"lvl1":"Data/APIs","lvl3":"requests","lvl2":"API"},"type":"lvl3","url":"/essential-reading/datasources/data-apis#requests","position":4},{"hierarchy":{"lvl1":"Data/APIs","lvl3":"requests","lvl2":"API"},"content":"The requests library is a \n\nwell documented library for issuing get requests.\n\nA common example in geospatial work is getting data from ArcGIS Feature Service layers and\ncreating a geopandas GeoDataFrame from it. The typical process for accomplishing this is\nusing a combination of the REST API page, the REST API query page to help build a query\n(if available), and an ArcGIS Online view of the data for help understanding the contents\nof fields. An example is the\n\n\nHUC10 Watershed\nby Columbia County Planning. The dataset can be explored using the included map and an comes\nwith an \n\nAPI Explorer to help build the API.\n\nPutting this all together would look like this:import requests\nimport geopandas as gpd\n\n# create base url and paylaod\nbase_url = \"https://services3.arcgis.com/F5762GA30C3SyZFy/arcgis/rest/services/Watershed_Boundary_Dataset/FeatureServer/4/query?\"\npayload = {\"where\":\"1=1\", #all records that match\n    \"returnGeometry\":\"true\",\n    \"outFields\": \"*\",\n    \"outSR\":\"4326\", #WGS84\n    \"f\": \"pjson\" #f=format\n}\n\n# send request, which will create URL\nr = requests.get(base_url, params=payload)\n\n# check that get was successful before creating GeoDataFrame\nif r.status_code == 200: #200 == success\n    gdf = gpd.read_file(r.text)\nelse:\n    print(f\"Request failed. Status code = {r.status_code}\")\n\n# plot to see the geometries\ngdf.plot()\n\nSome FeatureServers have preformatted data calls that return the data as a GeoJSON without\nhaving to construct a payload as above.  The request URL has the query parameters built in.\nThis can be read directly into a GeoDataFrame.\n\nA request similar to the above would look like this with a GeoJSON API request using such a service:import geopandas as gpd\n\nurl = \"https://services3.arcgis.com/F5762GA30C3SyZFy/arcgis/rest/services/Watershed_Boundary_Dataset/FeatureServer/4/query?outFields=*&where=1%3D1&f=geojson\"\n\n# create GeoDataFrame\ngdf = gpd.read_file(url)\n\n# plot to see the geometries\ngdf.plot()","type":"content","url":"/essential-reading/datasources/data-apis#requests","position":5},{"hierarchy":{"lvl1":"Data/APIs","lvl3":"pygeohydro","lvl2":"API"},"type":"lvl3","url":"/essential-reading/datasources/data-apis#pygeohydro","position":6},{"hierarchy":{"lvl1":"Data/APIs","lvl3":"pygeohydro","lvl2":"API"},"content":"Libraries like \n\npygeohydro provide\neasy ways to format data calls to data services via a customize API.  Libraries such as this\nformat the URL behind the scenes.\nThe example below is being made to the\n\n\nWBD HUC6 Feature Layer\nand is being done using the already known HUC6 ids 020401 and 020402.import pygeohydro\n\n# bring in HUC6 boundaries based on known IDs\ngdf = pygeohydro.WBD(\"huc6\").byids(\"huc6\", [\"020401\", \"020402\"])\n\ngdf.plot()\n\nYou can also use the bysql method to send a condition to the where field in the\n\n\nquery. If we\nwanted the HUC6 boundaries found only in the state of Michigan (MI), we would create a\nSQL call with column='value' to query the database. (e.g. states='MI'). Note the\nsingle quotes, which SQL uses in this instance.  It will not work with double quotes (\" \").import pygeohydro\n\n# bring in HUC boundaries found only in Michigan\ngdf = pygeohydro.WBD(\"huc6\").bysql(\"states='MI'\")\n\ngdf.plot()","type":"content","url":"/essential-reading/datasources/data-apis#pygeohydro","position":7},{"hierarchy":{"lvl1":"Data/APIs","lvl3":"NWIS","lvl2":"API"},"type":"lvl3","url":"/essential-reading/datasources/data-apis#nwis","position":8},{"hierarchy":{"lvl1":"Data/APIs","lvl3":"NWIS","lvl2":"API"},"content":"This API is a member of the hyriver suite (as is pygeohydro, above).  It facilitates\nqueries to the NWIS service to obtain historical readings for stream gages.from pygeohydro import NWIS\nnwis = NWIS()\nDATE_RANGE= (\"2000-01-01\", \"2010-12-31\")\n\n# returns data as pandas DataFrame\nas_dastaframe = nwis.get_streamflow('USGS-13317000', DATE_RANGE)\n\n# returns data as xarray DataSet\nas_xarray = nwis.get_streamflow('USGS-13317000', DATE_RANGE, to_xarray=True)","type":"content","url":"/essential-reading/datasources/data-apis#nwis","position":9},{"hierarchy":{"lvl1":"Data/APIs","lvl2":"FTP"},"type":"lvl2","url":"/essential-reading/datasources/data-apis#ftp","position":10},{"hierarchy":{"lvl1":"Data/APIs","lvl2":"FTP"},"content":"FTP is often the access method for many legacy datasets. An example is bringing in\ndata from NOAA’s\n\n\nGlobal Climate Reference Network FTP Server.\nHere, the fsspec library is used to create an FTPFileSystem instance locally that speaks to\nthe FTP server, read in a text file that contains station data using pandas, and the data is\nused to create a GeoDataFrame.import geopandas as gpd\nfrom fsspec.implementations.ftp import FTPFileSystem\nimport pandas as pd\n\nfs = FTPFileSystem(\"ftp.ncei.noaa.gov\")\ndata = pd.read_table(fs.open(\"/pub/data/uscrn/products/stations.tsv\"))\n\ngdf = gpd.GeoDataFrame(data, geometry=gpd.points_from_xy(data[\"LONGITUDE\"], data[\"LATITUDE\"]), crs=\"EPSG:4326\")\n\ngdf.plot()\n\nSay you just wanted the daily tabular data from a single station, you could use a combination\nof fsspec functions, including glob to search for files.from fsspec.implementations.ftp import FTPFileSystem\nimport pandas as pd\n\nstat_name = \"Avondale\"\n\n# instantiate FTP system\nfs = FTPFileSystem(\"ftp.ncei.noaa.gov\")\n\n# get list of all files with Avondale in name\nfile_list_glob = fs.glob(f\"/pub/data/uscrn/products/daily01/**/*{stat_name}*\")\n\n# create dataframe of data\ndf = pd.DataFrame()\n\nfor file in file_list_glob:\n    stat_data = pd.read_csv(fs.open(file), header=None, sep=\"\\t\")\n    df = pd.concat([df, stat_data])\n\n# the data is all crammed into a single column so spread it out\ndf = df[0].str.split(\" +\",expand = True)\n\ndf.head()","type":"content","url":"/essential-reading/datasources/data-apis#ftp","position":11},{"hierarchy":{"lvl1":"Data/Cloud Storage"},"type":"lvl1","url":"/essential-reading/datasources/data-s3","position":0},{"hierarchy":{"lvl1":"Data/Cloud Storage"},"content":"One of the main storage locations for HyTest data is in ‘The Cloud’. This is sometimes referred to as ‘Object Storage’.\nThe data is kept in data centers operated by Amazon, Microsoft, or similar, which makes it easily available to network-connected devices.\nThe main advantage of doing this is that if your compute engine is also in that same data center (or nearby, as is the case with many JupyterHub services), the data doesn’t have to go very far to get to the compute power. This brings the computation to the data, rather than shipping large datasets across the internet to get to the compute engine.\n\nThe HyTEST project is collaborating with the \n\nWoods Hole Oceanographic Institution to demonstrate the utility of using an \n\nOpen Storage Network (OSN) pod for providing access to data within scientific workflows. This OSN pod will provide 1 PB of usable \n\nCeph Object Storage and will be housed at the Massachusetts Green High Performance Computing Center on a high-speed (100+ GbE) network. This piece of hardware provides the opportunity to host data without the storage or egress fees that come along with other forms of cloud object storage, such as \n\nAmazon S3 object storage. Ceph object storage supports an \n\nAPI that is compatible with the basic data access model of the Amazon S3 API.\n\nWhat follows is a brief demo of how data can be read from or written to object storage through the S3 API and some pitfalls to watch out for. These methods should be generally applicable to both S3 object storage and the OSN pod object storage.\n\nThe permissions scheme for S3 allows for anonymous/global read access, as well as secured access via specific credentials.\nWe’ll look at generic workflows using an anonymous-access store, then finish off with some private/credentialed operations.\n\nThe easiest way to access data with the S3 API through a Python program is via the\n\n\nfsspec module.\nThis is a layer of abstraction that lets us interact with arbitrary storage mechanisms as if\nthey are conventional file systems. It makes this object storage ‘look’ like a conventional file system.","type":"content","url":"/essential-reading/datasources/data-s3","position":1},{"hierarchy":{"lvl1":"Data/Cloud Storage","lvl2":"Anonymous Reads from S3"},"type":"lvl2","url":"/essential-reading/datasources/data-s3#anonymous-reads-from-s3","position":2},{"hierarchy":{"lvl1":"Data/Cloud Storage","lvl2":"Anonymous Reads from S3"},"content":"A lot of data is available for global read, which does not require credentials or a profile.\nIn this case, we set anon=True when plumbing the fsspec object. Here we demonstrate how to list the contents of an AWS S3 bucket with global read access:fs = fsspec.filesystem(\n    's3',\n    anon=True   # Does not require credentials\n    )\n# 'fs' is an object which provides methods access to the virtual filesystem\n# 'ls' method == list ; list files in a virtual folder to test if we really have access\nfs.ls('s3://noaa-nwm-retrospective-2-1-zarr-pds/')\n\nOutput:['noaa-nwm-retrospective-2-1-zarr-pds/chrtout.zarr',\n 'noaa-nwm-retrospective-2-1-zarr-pds/gwout.zarr',\n 'noaa-nwm-retrospective-2-1-zarr-pds/lakeout.zarr',\n 'noaa-nwm-retrospective-2-1-zarr-pds/ldasout.zarr',\n 'noaa-nwm-retrospective-2-1-zarr-pds/precip.zarr',\n 'noaa-nwm-retrospective-2-1-zarr-pds/rtout.zarr']\n\nWe can call other methods on the fsspec object “fs” to interact with filesystem objects.fs.info('s3://noaa-nwm-retrospective-2-1-zarr-pds/index.html')\n\nOutput:{'Key': 'noaa-nwm-retrospective-2-1-zarr-pds/index.html',\n'LastModified': datetime.datetime(2021, 10, 1, 20, 48, 48, tzinfo=tzutc()),\n'ETag': '\"3b4b4277037c1127ed4ba68e26461b2e\"',\n'Size': 32357,\n'StorageClass': 'STANDARD',\n'type': 'file',\n'size': 32357,\n'name': 'noaa-nwm-retrospective-2-1-zarr-pds/index.html'}\n\nLow-level operations with fsspec are designed to behave similarly to the ‘native’ file operations in Python.# Use open() to get something that behaves like a file handle for\n# low-level Python read/write operations:\nwith fs.open('s3://noaa-nwm-retrospective-2-1-zarr-pds/index.html') as f:\n    # print first 5 lines...\n    for i in range(0, 5):\n        line = f.readline()\n        print(line)\n\nOutput:b'<!DOCTYPE html>\\r\\n'\nb'\\r\\n'\nb'<!--\\r\\n'\nb'Copyright 2014-2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\\r\\n'\nb'\\r\\n'\n\nHowever -- most of the time, we don’t need such low level access.\nA more general, and useful, mechanism is to ‘map’ an S3 object to a virtual file using get_mapper().\nThat map object can then be treated as if it is a local file -- the fsspec mapping mechanism handles\nall of the translation to interact with S3.m = fs.get_mapper('s3://noaa-nwm-retrospective-2-1-zarr-pds/chrtout.zarr')\n# 'm' looks like a file as far as Python is concerned.\n# It is now a stand-in for those functions that require a file as argument.\ng = zarr.convenience.open_consolidated(m) # read zarr metadata for named file.\nprint(g.tree())\n\nOutput:/\n ├── crs () |S1\n ├── elevation (2776738,) float32\n ├── feature_id (2776738,) int32\n ├── gage_id (2776738,) |S15\n ├── latitude (2776738,) float32\n ├── longitude (2776738,) float32\n ├── order (2776738,) int32\n ├── streamflow (367439, 2776738) int32\n ├── time (367439,) int64\n └── velocity (367439, 2776738) int32\n\nWarning\n\nWe are deliberately demonstrating using commands that read only the zarr metadata from the\nobject storage -- not the full dataset.\n\nThe above example is a very, very, very large dataset, which you don’t want to load over the network to your desktop.\nExecute full data read operations only if this notebook is being hosted and run out of the same AWS center where the data lives.\n\nThe good news about some of the larger science-oriented libraries (xarray, dask, pandas, zarr, etc), is that\nthey can automatically handle the fsspec operations for you IF YOUR ACCESS IS ANONYMOUS.\nThis is a convenience, but is a special case for read-only data where anon=True and requester_pays=False.\n\nNote that this is a feature of\n\n\nspecific libraries,\nand doesn’t work everywhere.\n\nBecause it isn’t universally available, and only applies to anonymous reads, examples in HyTEST will\nalways explicitly plumb fsspec ‘longhand’ using get_mapper(). You may see example code elsewhere\nthat takes the shortcut if it is available.","type":"content","url":"/essential-reading/datasources/data-s3#anonymous-reads-from-s3","position":3},{"hierarchy":{"lvl1":"Data/Cloud Storage","lvl2":"Anonymous Reads from Endpoints Outside of S3 (that use the S3 API)"},"type":"lvl2","url":"/essential-reading/datasources/data-s3#anonymous-reads-from-endpoints-outside-of-s3-that-use-the-s3-api","position":4},{"hierarchy":{"lvl1":"Data/Cloud Storage","lvl2":"Anonymous Reads from Endpoints Outside of S3 (that use the S3 API)"},"content":"For storage operations, the S3 API needs the web address of the access point, or endpoint where it should address filesystem operations. If your storage is completely within the Amazon ecosystem, you will likely not need to specify an endpoint. However, for 3rd-party storage (such as the OSN pod), you will need to explicitly declare the endpoint when the filesystem is first referenced using fsspec. We can list the files stored in the usgs-scratch bucket of the OSN pod with the following:fs_osn = fsspec.filesystem(\n    's3',\n    anon=True,   # Does not require credentials\n    client_kwargs={'endpoint_url': 'https://usgs.osn.mghpcc.org'}\n)\nfs_osn.ls('s3://hytest')","type":"content","url":"/essential-reading/datasources/data-s3#anonymous-reads-from-endpoints-outside-of-s3-that-use-the-s3-api","position":5},{"hierarchy":{"lvl1":"Data/Cloud Storage","lvl2":"Credentialed Access"},"type":"lvl2","url":"/essential-reading/datasources/data-s3#credentialed-access","position":6},{"hierarchy":{"lvl1":"Data/Cloud Storage","lvl2":"Credentialed Access"},"content":"For some data storage within the HyTEST workflows, access will not be anonymous.\nPermissions are set by the owners of that data, and the rules governing your ability to read from or write to certain locations may be defined with a set of credentials assigned to an AWS ‘profile’.\n\nProfile credentials are usually stored outside of the Python program, typically in a file in your HOME folder on the compute/jupyter server. You need to have this credential file set up before you can work with data in buckets requiring credentialed access. This section will demonstrate how to configure your OSN pod credentials in the same way that we would configure an AWS account profile - with the aws \n\ncommand line interface.\n\nTo create a new AWS profile, which we will name osn-hytest:> aws configure --profile osn-hytest\nAWS Access Key ID :\nAWS Secret Access Key :\nDefault region name : us-east-1\nDefault output format: json\n\nThe first two prompts will ask for key/security information you were assigned for access to object storage. Credentials for the OSN pod should have been provided by the HyTEST team if you need credentialed access to the OSN pod.\nThe default region for the OSN pod should be “us-east-1” because this is the region in which the pod is physically located, and default output format should be “json”.\n\nNote that this configuration is specific to the OSN ‘pod’ storage.\nYour profile name and region may be different if you are setting up your credentials for an AWS S3 object storage bucket.\n\nWe can now set up a virtual filesystem to access the OSN pod with the credentials stored in the osn-hytest profile you just created. This credentialed access will grant you additional permissions that you did not have with the anonymous access we used above.fs_osn = fsspec.filesystem(\n    's3',\n    profile='osn-hytest',  ## This is the profile name you configured above.\n    client_kwargs={'endpoint_url': 'https://usgs.osn.mghpcc.org'}\n)","type":"content","url":"/essential-reading/datasources/data-s3#credentialed-access","position":7},{"hierarchy":{"lvl1":"Data/Cloud Storage","lvl2":"Read-only access with “requester pays”"},"type":"lvl2","url":"/essential-reading/datasources/data-s3#read-only-access-with-requester-pays","position":8},{"hierarchy":{"lvl1":"Data/Cloud Storage","lvl2":"Read-only access with “requester pays”"},"content":"Some datasets, such as the first example above (anonymous reading), are offered with ‘egress fees’ paid by the data owner.\nThis means that all access is free to anybody.\nThe cost of the network bandwidth used to serve the data is covered by the data’s host.\nNot all “public” data is offered this way: it is still available to anybody who wants to read it, but\nthe access fees must be paid by the reader (i.e. the ‘requester’).\n\nWhen you access a requester-pays dataset, your profile identifies the account which will be\nbilled for access.  Open such datasets with an extra option to fsspec:fs_requesterpays = fsspec.filesystem(\n    's3',\n    profile='profile_name',\n    anon=False,\n    requester_pays=True\n)\n\nThe fsspec call identifies how you will be interacting with object storage (your identity and what\nyou are willing to pay for).  File-system operations using that fs handle will be made using that\nconfiguration.","type":"content","url":"/essential-reading/datasources/data-s3#read-only-access-with-requester-pays","position":9},{"hierarchy":{"lvl1":"Data/Cloud Storage","lvl2":"Writing Data to S3"},"type":"lvl2","url":"/essential-reading/datasources/data-s3#writing-data-to-s3","position":10},{"hierarchy":{"lvl1":"Data/Cloud Storage","lvl2":"Writing Data to S3"},"content":"From within your Python program, writes to object storage can be achieved a few different ways, which we will demonstrate below. Writing data will typically require credentials. If this is the case, you should use an fsspec filesystem with a profile defined, similar to the fs_osn object that we created in the Credentialed Access section above.","type":"content","url":"/essential-reading/datasources/data-s3#writing-data-to-s3","position":11},{"hierarchy":{"lvl1":"Data/Cloud Storage","lvl3":"Uploading a Local File to S3","lvl2":"Writing Data to S3"},"type":"lvl3","url":"/essential-reading/datasources/data-s3#uploading-a-local-file-to-s3","position":12},{"hierarchy":{"lvl1":"Data/Cloud Storage","lvl3":"Uploading a Local File to S3","lvl2":"Writing Data to S3"},"content":"If you have a file saved on your local file system, and you would like to upload it directly to object storage, you can use the upload method.fs_osn.upload('outfile.nc', 'hytest-scratch/testing/outfile.nc')","type":"content","url":"/essential-reading/datasources/data-s3#uploading-a-local-file-to-s3","position":13},{"hierarchy":{"lvl1":"Data/Cloud Storage","lvl3":"Writing a Pandas Dataframe to a CSV File on S3","lvl2":"Writing Data to S3"},"type":"lvl3","url":"/essential-reading/datasources/data-s3#writing-a-pandas-dataframe-to-a-csv-file-on-s3","position":14},{"hierarchy":{"lvl1":"Data/Cloud Storage","lvl3":"Writing a Pandas Dataframe to a CSV File on S3","lvl2":"Writing Data to S3"},"content":"If you would like to write a pandas dataframe object directly to a csv file on object storage, you can do something like:with fs_osn.open(\"hytest-scratch/testing/outfile.csv\", mode='wt') as f:\n    pandas_df.to_csv(f)","type":"content","url":"/essential-reading/datasources/data-s3#writing-a-pandas-dataframe-to-a-csv-file-on-s3","position":15},{"hierarchy":{"lvl1":"Data/Cloud Storage","lvl3":"Writing Zarr Data to S3","lvl2":"Writing Data to S3"},"type":"lvl3","url":"/essential-reading/datasources/data-s3#writing-zarr-data-to-s3","position":16},{"hierarchy":{"lvl1":"Data/Cloud Storage","lvl3":"Writing Zarr Data to S3","lvl2":"Writing Data to S3"},"content":"For zarr data, the most convenient way to write data to object storage is to use a mapper to connect a file-like python object to\nthe object storage location:fname='s3://hytest-scratch/testing/outfile.zarr'\noutfile=fs_osn.get_mapper(fname)\nxarray_dataset.to_zarr(outfile, mode='w', consolidated=True)\n\nThe outfile variable can be used most anywhere that a file-like object is needed for\nwriting.","type":"content","url":"/essential-reading/datasources/data-s3#writing-zarr-data-to-s3","position":17},{"hierarchy":{"lvl1":"Data/Cloud Storage","lvl3":"Additional Commands","lvl2":"Writing Data to S3"},"type":"lvl3","url":"/essential-reading/datasources/data-s3#additional-commands","position":18},{"hierarchy":{"lvl1":"Data/Cloud Storage","lvl3":"Additional Commands","lvl2":"Writing Data to S3"},"content":"With adequate permissions, you may be able to do more destructive activities to objects in a bucket (overwriting, removing, etc).\nExamples:\n\nmkdir -- makes a new directory / folder\n\nmv -- moves/renames a file or folder\n\nrm -- removes a file or folder\n\nSee the \n\nAPI documentation for the full details of available operations.","type":"content","url":"/essential-reading/datasources/data-s3#additional-commands","position":19},{"hierarchy":{"lvl1":"Data Sources"},"type":"lvl1","url":"/essential-reading/datasources/readme","position":0},{"hierarchy":{"lvl1":"Data Sources"},"content":"This section describes foundational methods for the few ways that data is accessed\nin HyTEST workflows:\n\nObject Storage (S3)\n\nAPI","type":"content","url":"/essential-reading/datasources/readme","position":1},{"hierarchy":{"lvl1":"Parallelism with Dask"},"type":"lvl1","url":"/essential-reading/parallel-dask","position":0},{"hierarchy":{"lvl1":"Parallelism with Dask"},"content":"The dask folks have a very good introduction to dask data structures and parallelism in their\n\n\ndask tutorial.  In that tutorial, you’ll get exposure to\nthe general dask architecture, as well as specific hands-on examples with two of the three\nkey dask data structures:  dask arrays, and dask dataframes.\n\nThe above tutorial does not cover dask bag in any detail -- this is a key data structure\nthat we use to distribute parallel tasks in clustered environments.  Here’s a quick demo of\na dask bag and how we use it. A bag is analagous to a standard Python list... let’s start there\nwith some nomenclature:\n\n","type":"content","url":"/essential-reading/parallel-dask","position":1},{"hierarchy":{"lvl1":"Parallelism with Dask","lvl2":"Lists and Maps"},"type":"lvl2","url":"/essential-reading/parallel-dask#lists-and-maps","position":2},{"hierarchy":{"lvl1":"Parallelism with Dask","lvl2":"Lists and Maps"},"content":"A common pattern in Python is a ‘list comprehension’ -- a way of transforming a list of values into a new list of values using a transformation pattern.\n\nmyList = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\nnewList = [x**2 for x in myList]\n\nnewList\n\n\n\nThis feature of python maps an action onto each element in a list. The result is a new list\nholding the result of each action -- one action per element in the list.\n\nThe syntax of the above list comprehension is purely for us\nhumans. The python implementation behind the scenes uses a special built-in python function\ncalled map:\n\ndef myFunc(i):\n    return i**2\n\nlist(map(myFunc, myList))\n# maps each element of myList to a separate invokation of myFunc. \n\n\n\nThe map() call handles the work of calling myFunc() once each for the elements of myList.You can see that doing it this way involves a lot of extra hoops and parenthesis to jump through.\nWhich is why it is almost never written that way.\n\nThe list element is given to the function as a positional argument.  In essence, the above map is the same as:\n\nresult=[]\nfor x in myList:\n    result.append(myFunc(x))\nresult\n\n\n\n","type":"content","url":"/essential-reading/parallel-dask#lists-and-maps","position":3},{"hierarchy":{"lvl1":"Parallelism with Dask","lvl2":"Dask Bag"},"type":"lvl2","url":"/essential-reading/parallel-dask#dask-bag","position":4},{"hierarchy":{"lvl1":"Parallelism with Dask","lvl2":"Dask Bag"},"content":"A dask bag datastructure is much like a list, but it has the map function built into it as\nan object method. The invocation is slightly different, but the concept is the same: it\npairs up elements of a list with a function call to execute against those elements.  A dask\nbag has the ability to spawn the myFunc() calls in parallel, distributing those calls to\nworkers around the dask cluster.\n\nLet’s look at an example... first, let’s make myFunc() simulate time-consuming work by having\nit pause for a while.\n\nfrom time import sleep\ndef myFunc(i):\n    sleep(1) ## Simulates dense computation taking one second\n    return i**2\n\n\n\nThis function is one that we want to distribute across a dask compute cluster.\n\nFor this small demonstration, we’ll build a cluster on the local host; no need to\nmake a distributed cluster.\n\nimport os\nimport dask.bag as db\n\nfrom dask.distributed import Client, LocalCluster\ncluster = LocalCluster(threads_per_worker=os.cpu_count())\nclient = Client(cluster)\n\n\n\n# fill the bag with our list of items\nmyBag = db.from_sequence(myList)\n# then 'map' the elements to the worker function we want to execute on each element in the bag\nresult = myBag.map(myFunc)\n\n\n\nNote that this returned immediately (i.e very small execution time). That’s because the\ncomputation has not been done yet. Dask is a ‘lazy’ system in which computations are\ndelayed as long as possible.\n\nAs with other dask operations, result at this point just contains the ‘task graph’ -- what\ndask is going to do, and in what order.  We can see that task graph with the visualize\nmethod:\n\nresult.visualize()\n\n\n\nNote that each of those elements is independent from the others, so they can in theory execute in parallel.\n‘Thread’ zero will call myFunc with some arguments to produce result zero.  Same with threads 1 throuth 9.\nBecause these threads do not depend on one another, they can operate in parallel, on separate workers.\n\nIn the original, pure-python operation, these calls to myFunc would be serialized: one at a time.\n\nLet’s compare run times:\n\n%%time  \n# should take about 10 seconds: one second per call to myFunc\n## Pure python mapping:\nlist( map( myFunc, myList))\n\n\n\n%%time\n## using the dask bag task-graph\nresult.compute()\n\n\n\nNote that the pure python execution took right around 10 seconds.  Each call to myFunc, remember, is artificially set to take one second each.  Calling them serially should take 10 seconds (plus whatever internal overhead python needs).\n\nThe dask bag approach took substantially less time. If it were perfectly parallel, the result would have been computed in one second (ten simultaneous executions of myFunc, each taking one second).  But that parallelism depends on how many cpus/cores you have.  If you only have one core, then parallelism isn’t going to help  you -- the dispatched workers still need to take turns on the CPU.\n\nIf you had 10 cores (the number of elements in the bag), then you might get close to the perfect parallelism.  Dask does involve more overhead than pure python in order to achieve its parallelism; the administrative overhead for operating the cluster scheduler will prevent this from being perfectly parallel.\n\nThe above example uses a local cluster, meaning that the work is scheduled among the CPUs on the local hardware.  Dask can also utilize distributed clusters, meaning that workers can be organized among several computers connected via network. This allows for many more CPUs to attach to a problem, but the overhead of network communication will impact the administrative costs of coordinating the workers.\n\n## shut down cluster scheduler\nclient.close(); del client\ncluster.close(); del cluster\n\n\n\n","type":"content","url":"/essential-reading/parallel-dask#dask-bag","position":5},{"hierarchy":{"lvl1":"Parallelism with Dask","lvl2":"Dask Bags in HyTEST Workflows"},"type":"lvl2","url":"/essential-reading/parallel-dask#dask-bags-in-hytest-workflows","position":6},{"hierarchy":{"lvl1":"Parallelism with Dask","lvl2":"Dask Bags in HyTEST Workflows"},"content":"This pattern is used extensively in the benchmarking workflows.  A series of statistics are calculated for each streamgage in a list of 5000+ gages. With a dask bag (the contents of which is the list of gages), the stats package can be dispatched to workers operating independently and in parallel.","type":"content","url":"/essential-reading/parallel-dask#dask-bags-in-hytest-workflows","position":7},{"hierarchy":{"lvl1":"Essential Reading"},"type":"lvl1","url":"/essential-reading/readme","position":0},{"hierarchy":{"lvl1":"Essential Reading"},"content":"This section holds reference documentation for foundational technologies used in HyTEST workflows.\n\nMany tutorials and readings are available on the internet, some of which are collected \n\nhere.\n\nWe have a particular reliance on Dask for parallel processing workflows. The common ways we use Dask parallelism are described \n\nhere.\n\nWe describe some foundational methods for some of the ways that data is accessed in HyTEST workflows, such as through APIs, object storage (S3), and STAC catalogs in \n\nData Sources.","type":"content","url":"/essential-reading/readme","position":1},{"hierarchy":{"lvl1":"Foundations"},"type":"lvl1","url":"/essential-reading/readingstutorials","position":0},{"hierarchy":{"lvl1":"Foundations"},"content":"References to books, websites, youtube vids, podcasts, etc...\n\nIf you like a particular tutorial that you want to see included\nhere, create an issue to submit the recommendation.\n\n♜ == internal resource","type":"content","url":"/essential-reading/readingstutorials","position":1},{"hierarchy":{"lvl1":"Foundations","lvl2":"HPC & Linux"},"type":"lvl2","url":"/essential-reading/readingstutorials#hpc-linux","position":2},{"hierarchy":{"lvl1":"Foundations","lvl2":"HPC & Linux"},"content":"https://​hpcportal​.cr​.usgs​.gov​/training​/courses​/Intro​_to​_HPC​/04​_basics​.html ♜\n\nhttps://​hpcportal​.cr​.usgs​.gov​/training​/courses​/Intro​_to​_HPC​/index​.html ♜\n\nIntroduction to HPC ♜\n\nTraining GitLab Landing Page ♜\n\nIntroduction to Linux ♜","type":"content","url":"/essential-reading/readingstutorials#hpc-linux","position":3},{"hierarchy":{"lvl1":"Foundations","lvl2":"Python"},"type":"lvl2","url":"/essential-reading/readingstutorials#python","position":4},{"hierarchy":{"lvl1":"Foundations","lvl2":"Python"},"content":"https://​docs​.python​-guide​.org/\n\nhttps://​github​.com​/jiffyclub​/scipy​-2021​-intro​-to​-python\n\nhttps://​hpcportal​.cr​.usgs​.gov​/training​/courses​/Parallel​_Python​/index​.html ♜\n\nHitchhiker’s Guide to Python\n\nParallel Python ♜","type":"content","url":"/essential-reading/readingstutorials#python","position":5},{"hierarchy":{"lvl1":"Foundations","lvl2":"Numerical/Scientific Computing"},"type":"lvl2","url":"/essential-reading/readingstutorials#numerical-scientific-computing","position":6},{"hierarchy":{"lvl1":"Foundations","lvl2":"Numerical/Scientific Computing"},"content":"General\n\nScientific Python ♜\n\nhttps://​hpcportal​.cr​.usgs​.gov​/training​/courses​/Scientific​_Python​/index​.html ♜\n\nhttps://​jakevdp​.github​.io​/PythonDataScienceHandbook/\n\nThink Stats — Essential Statistics & Data Analysis in Python\n\nNumPy\n\nhttps://​github​.com​/enthought​/Numpy​-Tutorial​-SciPyConf​-2022\n\nhttps://​github​.com​/jpivarski​-talks​/2022​-07​-11​-scipy​-loopy​-tutorial\n\nxarray\n\nhttps://​tutorial​.xarray​.dev​/overview​/get​-started​.html\n\ndask\n\nhttps://​tutorial​.dask​.org/\n\nhttps://​github​.com​/ramonpzg​/scipyus21​_dask​_analytics\n\nscipy\n\nhttps://​docs​.scipy​.org​/doc​/scipy​/tutorial​/index​.html\n\nhttps://​faculty​.washington​.edu​/otoomet​/machinelearning​-py​/numpy​-and​-pandas​.html\n\nVisualizations:\n\nhttps://​matplotlib​.org​/stable​/tutorials​/index​.html\n\nhttps://​holoviz​.org​/tutorial\n\nhttps://​github​.com​/symmy596​/PythonMaps​-Scipy​-2022","type":"content","url":"/essential-reading/readingstutorials#numerical-scientific-computing","position":7},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark"},"type":"lvl1","url":"/evaluation/metrics-dscore-suite-v1","position":0},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark"},"content":"This notebook implements the D-Score metrics from \n\nHodson et al., 2021, which decompose mean squared error into orthogonal metrics like bias and variance.  This notebook briefly describes each decomposition along with providing Python implementations.  This notebook can be sourced into analysis notebooks to retrieve access to these functions. See \n\nD-Score Usage for an example of using these functions with a demonstration dataset.\n\nDecomposition Name\n\nMetric\n\nDescription\n\nTop-level Error Metric\n\nmse\n\nMean Squared Error; error metric that is decomposed into sets of decompositions below.\n\nBias-Variance\n\ne_bias (Bias)\n\nBias corresponds to error in the expected magnitude\n\nBias-Variance\n\ne_variance (Variance)\n\nVariance corresponds to errors in distribution and timing\n\nBias, Distribution, Sequence\n\ne_bias (Bias)\n\nBias corresponds to error in magnitudes\n\nBias, Distribution, Sequence\n\ne_dist (Distribution)\n\nDistribution corresponds to distributional error independent of timing\n\nBias, Distribution, Sequence\n\ne_seq (Sequence)\n\nSequence related to error in ordering of values, timing\n\nTrend, Seasonality, Residual Variability\n\ntrend\n\nError related to overall trend\n\nTrend, Seasonality, Residual Variability\n\nseasonality\n\nError related to shifts in seasonality such as phase and amplitude variation\n\nTrend, Seasonality, Residual Variability\n\nresidual\n\nRemainder error, not attributed to anything in particular but still exists\n\nSeasonal\n\nwinter\n\nNorthern Hemisphere Seasons: Dec, Jan, Feb\n\nSeasonal\n\nspring\n\nNorthern Hemisphere Seasons: Mar, Apr, May\n\nSeasonal\n\nsummer\n\nNorthern Hemisphere Seasons: Jun, Jul, Aug\n\nSeasonal\n\nfall\n\nNorthern Hemisphere Seasons: Sep, Oct, Nov\n\nPercentile\n\nlow\n\n5th percentile and lower\n\nPercentile\n\nbelow_avg\n\n>25th percentile and <= 50th percentile\n\nPercentile\n\nabove_avg\n\n>50th percentile and <= 75th percentile\n\nPercentile\n\nhigh\n\n>75th percentile\n\nSource: These statistics are adapted from the original functions found in \n\nhttps://​github​.com​/thodson​-usgs​/dscore\n\nHodson et al., 2021, Mean Squared Error, Deconstructed. Journal of Advances in Modeling Earth Systems, 13(12), \n\nHodson et al. (2021).\n\n","type":"content","url":"/evaluation/metrics-dscore-suite-v1","position":1},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark","lvl2":"Import libraries"},"type":"lvl2","url":"/evaluation/metrics-dscore-suite-v1#import-libraries","position":2},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark","lvl2":"Import libraries"},"content":"\n\nimport logging\nimport numpy as np\nimport pandas as pd\n\n\n\n","type":"content","url":"/evaluation/metrics-dscore-suite-v1#import-libraries","position":3},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark","lvl2":"Mean Squared Error"},"type":"lvl2","url":"/evaluation/metrics-dscore-suite-v1#mean-squared-error","position":4},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark","lvl2":"Mean Squared Error"},"content":"\n\ndef mse(obs, sim) -> float:\n    \"\"\"\n     Mean Square Error --   Compute MSE over all paired values observed (x) and simulated/modeled (x_hat)\n        .. math::\n            \\\\sum_{i=1}^{n}(x_i - \\\\hat{x}_i)^2\n\n    :return: mean square error\n    :rtype: float\n\n    NOTE: this and all functions below rely upon the obs and sim datatypes implementing \n          certain math methods on themselves.  That is, obs.sum() must be defined by \n          typeof(obs). Pandas Series and DataFrames do this, but other array_like\n          may not.\n    \"\"\"\n    e = obs - sim\n    \n    return (e**2).mean()\n\n\n\n","type":"content","url":"/evaluation/metrics-dscore-suite-v1#mean-squared-error","position":5},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark","lvl2":"Percent Bias"},"type":"lvl2","url":"/evaluation/metrics-dscore-suite-v1#percent-bias","position":6},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark","lvl2":"Percent Bias"},"content":"\n\ndef pbias(obs, sim) -> float:\n    \"\"\"\n    percent bias -- a measure of the mean tendency of simulated values to be\n    greater than or less than associated observed values.\n\n    Returns:\n        float: calculated percent bias / units = percent (i.e. 90 rather than 0.90)\n    \"\"\"\n    return 100 * (sim - obs).sum() / obs.sum()    \n\n\n\n","type":"content","url":"/evaluation/metrics-dscore-suite-v1#percent-bias","position":7},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark","lvl2":"Bias-Variance Decomposition"},"type":"lvl2","url":"/evaluation/metrics-dscore-suite-v1#bias-variance-decomposition","position":8},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark","lvl2":"Bias-Variance Decomposition"},"content":"From ‘Bias-Variance Trade-off’. See \n\nGeman et al., 1992 for more details.\n\nGeman et al., 1992, Neural networks and the bias/variance dilemma. Neural Computation, 4(1), 158. \n\nGeman et al. (1992)\n\ndef e_bias(obs, sim) -> float:\n    \"\"\"\n    Bias = square of mean error\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    e = sim - obs\n    return e.mean()**2\n\n\ndef e_variance(obs, sim) -> float:\n    \"\"\"\n    Variance of error\n    \n    Args:\n        obs (pd.Series - like): data representing observed values\n        sim (pd.Series - like): data representing simulated/modeled values\n\n    Returns:\n        float: variance of the error\n    \"\"\"\n    e = sim - obs\n    return e.var(ddof=0) # use the maximum likelihood estimator for the population variance; 1/n * RSS\n\n\n\n","type":"content","url":"/evaluation/metrics-dscore-suite-v1#bias-variance-decomposition","position":9},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark","lvl2":"Bias-Distribution-Sequence Decomposition"},"type":"lvl2","url":"/evaluation/metrics-dscore-suite-v1#bias-distribution-sequence-decomposition","position":10},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark","lvl2":"Bias-Distribution-Sequence Decomposition"},"content":"Hodson et al., 2021, Mean Squared Error, Deconstructed. Journal of Advances in Modeling Earth Systems, 13(12), \n\nHodson et al. (2021).\n\ndef bias_distribution_sequence(obs, sim) -> float:\n    \"\"\"\n    Decomposition into bias, distribution, sequence\n\n    Args:\n        obs (pd.Series - like): data representing observed values\n        sim (pd.Series - like): data representing simulated/modeled values\n\n    Returns:\n        pd.Series: three metrics, indexed by name\n    \"\"\"\n    \n    e = sim - obs\n    s = np.sort(sim) - np.sort(obs)\n    var_s = s.var(ddof=0) # use the maximum likelihood estimator for the population variance; 1/n * RSS\n    var_e = e.var(ddof=0) # use the maximum likelihood estimator for the population variance; 1/n * RSS\n    e_seq = var_e - var_s\n    e_dist = var_s\n    e_bias = e.mean()**2\n    return pd.Series(\n        [e_bias, e_dist, e_seq],\n        index=[\"e_bias\", \"e_dist\", \"e_seq\"])\n\n\n\n","type":"content","url":"/evaluation/metrics-dscore-suite-v1#bias-distribution-sequence-decomposition","position":11},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark","lvl2":"Trend, Seasonality, Residual Variability Decomposition"},"type":"lvl2","url":"/evaluation/metrics-dscore-suite-v1#trend-seasonality-residual-variability-decomposition","position":12},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark","lvl2":"Trend, Seasonality, Residual Variability Decomposition"},"content":"The trend, seasonality, and residual variability components are not exactly orthogonal, thus the total percent error for this decomposition will sum to less than 100%. See \n\nCleveland et al., 1990 and \n\nHodson et al., 2021 for more information.\n\nCleveland et al., 1990, STL: A seasonal-trend decomposition procedure based on loess. Journal of Official Statistics, 6(1), 3-73.\n\nHodson et al., 2021, Mean Squared Error, Deconstructed. Journal of Advances in Modeling Earth Systems, 13(12), \n\nHodson et al. (2021).\n\ntry:\n    from statsmodels.tsa.seasonal import STL\n    _SEASONAL = True\nexcept ImportError:\n    logging.debug(\"STL library not available.\")\n    _SEASONAL = False\n\ndef stl(obs, sim):\n    \"\"\"\n    Decompose error using STL.\n\n    Seasonal and trend decomposition using Loess (STL).\n    Note that STL is not perfectly orthogonal.\n\n    References\n    ----------\n    .. [1] Cleveland et al., 1990, STL: A seasonal-trend decomposition\n    procedure based on loess. Journal of Official Statistics, 6(1), 3-73.\n    \"\"\"\n    if not _SEASONAL:\n        logging.warning(\"STL statistics not available.\")\n        return None\n    e = sim - obs\n    res = STL(e, period=365, seasonal=9).fit()\n    E = pd.DataFrame(\n        {\n            'trend': res.trend,\n            'seasonality': res.seasonal,\n            'residual': res.resid\n        }\n    )\n    return (E**2).mean()\n\n\n\n","type":"content","url":"/evaluation/metrics-dscore-suite-v1#trend-seasonality-residual-variability-decomposition","position":13},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark","lvl2":"Seasonal Decomposition"},"type":"lvl2","url":"/evaluation/metrics-dscore-suite-v1#seasonal-decomposition","position":14},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark","lvl2":"Seasonal Decomposition"},"content":"Seasonal decomposition uses Northern Hemisphere seasons as a basis, this can be customized below in the function for different seasonal divisions.\n\ndef seasonal_mse(obs, sim):\n    \"\"\"\n    Decompose error by season.\n\n    Args:\n        obs (pd.Series - like): data representing observed values\n        sim (pd.Series - like): data representing simulated/modeled values\n\n    Both obs and sim should be time-indexed, such that we can pick out months\n    from the time value.\n\n    Returns:\n        pd.Series : mse for 4 major seasons\n    \n    NOTE: 'season' is viewed from a northern-hemisphere perspective\n    \"\"\"\n    e = sim - obs\n    idx = (e.index.month == 12) | (e.index.month <= 2) #Dec, Jan, Feb\n    winter = ((e*idx)**2).mean()\n\n    idx = (e.index.month > 2) & (e.index.month <= 5) #Mar, Apr, May\n    spring = ((e*idx)**2).mean()\n\n    idx = (e.index.month > 5) & (e.index.month <= 8) #Jun, Jul, Aug\n    summer = ((e*idx)**2).mean()\n    \n    idx = (e.index.month > 8) & (e.index.month <= 11) #Sep, Oct, Nov\n    fall = ((e*idx)**2).mean()\n\n    return pd.Series([winter, spring, summer, fall], index=['winter', 'spring', 'summer', 'fall'])\n\n\n\n","type":"content","url":"/evaluation/metrics-dscore-suite-v1#seasonal-decomposition","position":15},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark","lvl2":"Quantile Decomposition"},"type":"lvl2","url":"/evaluation/metrics-dscore-suite-v1#quantile-decomposition","position":16},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark","lvl2":"Quantile Decomposition"},"content":"MSE is decomposed by quantile range:\n\nQ from\n\nQ to\n\nLabel\n\n0.00\n\n0.25\n\nLow\n\n0.25\n\n0.50\n\nBelow-Average\n\n0.50\n\n0.75\n\nAbove-Average\n\n0.75\n\n1.00\n\nHigh\n\ndef quantile_mse(obs, sim):\n    \"\"\"\n    Decomposes MSE by quantile rangess 0.00-0.25; 0.25-0.5; 0.5-0.75; 0.75-1.00\n\n    Args:\n        obs (pd.Series - like  ): series of observed values\n        sim (_type_): series of simulated/modeled values\n    Both share a common index\n\n    Returns:\n        pd.Series : decomposed MSE, one value per quantile range\n    \"\"\"\n    breaks=[0, 0.25, 0.5, 0.75, 1]\n    labels=['low', 'below_avg', 'above_avg', 'high']\n    e = sim - obs\n    scores = []\n    ranks = obs.rank(method='first')\n    quants = pd.qcut(ranks, q=breaks)\n    for i in range(len(breaks) - 1):\n        quant = e * (quants == quants.cat.categories[i])  # select quantile\n        mse_q = ((quant)**2).mean()\n        scores.append(mse_q)\n    return pd.Series(scores, index=labels)\n\n\n\n","type":"content","url":"/evaluation/metrics-dscore-suite-v1#quantile-decomposition","position":17},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark","lvl2":"Scoring Function - ILAMB Scoring"},"type":"lvl2","url":"/evaluation/metrics-dscore-suite-v1#scoring-function-ilamb-scoring","position":18},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark","lvl2":"Scoring Function - ILAMB Scoring"},"content":"Collier et al., 2018, The International Land Model Benchmarking (ILAMB) system: Design, theory, and implementation. Journal of Advances in Modeling Earth Systems, 10(11), \n\nCollier et al. (2018)\n\ndef score(e, a=1.0):\n    \"\"\"\n    Scores an error\n\n    Exponential scoring function that maps MSE to the unit interval.\n\n    Parameters\n    ----------\n    a : float\n        Positive tuning parameter.\n\n    References\n    ----------\n    .. [1] Collier et al., 2018, The International Land Model Benchmarking\n    (ILAMB) system: Design, theory, and implementation. Journal of Advances\n    in Modeling Earth Systems, 10(11), http://dx.doi.org/10.1029/2018ms001354\n    \"\"\"\n    if a <= 0.0:\n        raise ValueError(\"Tuning parameter must be a positive float\")\n    return np.exp(-1 * a * e)\n\n\n\n","type":"content","url":"/evaluation/metrics-dscore-suite-v1#scoring-function-ilamb-scoring","position":19},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark","lvl2":"ScoreCard Visualizations"},"type":"lvl2","url":"/evaluation/metrics-dscore-suite-v1#scorecard-visualizations","position":20},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark","lvl2":"ScoreCard Visualizations"},"content":"These routines help with formatting scorecards.\n\nimport matplotlib.pyplot as plt\n\ndef ilamb_card_II(df, ax=None, clim=(0,100), cmap='RdYlBu'):\n    axs = ax or plt.gca()\n    try:\n        axs.set_xlabel(df.name)  # note that name is not a standard dataframe attribute.\n    except NameError:\n        pass # Don't set xaxis label name\n        \n    axs.set_ylabel(\"Component\")\n    axs.set_yticks(np.arange(len(df.index)), labels=df.index, fontsize=6)\n    axs.yaxis.set_tick_params(length=0, which='minor')\n\n    axs.set_xticks(np.arange(len(df.columns)), labels=df.columns, fontsize=6, rotation=90, ha='center', va='bottom')\n    axs.xaxis.tick_top()\n    axs.xaxis.set_tick_params(length=0)\n    axs.xaxis.set_label_position('top') \n\n    # Thick black lines between decompositions\n    axs.axhline(y=0.5, color='k') #betwen cells 0 and 1\n    axs.axhline(y=3.5, color='k') # between 3 and 4 \n    axs.axhline(y=6.5, color='k') # between 6 and 7 \n    axs.axhline(y=10.5, color='k') # between 9 and 10\n\n    im = axs.imshow(df, cmap=cmap, vmin=clim[0], vmax=clim[1], alpha=0.80)\n    cbar = axs.figure.colorbar(im, ax=axs, location='bottom', ticks=[0, 50, 100], ticklocation='bottom', pad=0.05, fraction=0.15, shrink=0.5)\n    cbar.ax.tick_params(labelsize=4, width=0.2, length=2, pad=1, labelbottom=True)\n    cbar.outline.set_linewidth(0.2)\n\n    ## Annotates each cell...\n    txtattrs = dict(ha=\"center\", va=\"center\", fontsize=5)\n    i=0\n    for col in df.columns:\n        j=0\n        for row in df.index:\n            text = im.axes.text(i, j, df[col][row], **txtattrs)\n            j+=1\n        i+=1\n    \n    ## Offset minor gridlines.... paint them white. \n    axs.set_yticks(np.arange(len(df.index)+1)-.5, minor=True)\n    axs.set_xticks(np.arange(len(df.columns)+1)-.5, minor=True)\n    axs.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=0.75)\n    \n    return axs\n\n","type":"content","url":"/evaluation/metrics-dscore-suite-v1#scorecard-visualizations","position":21},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics"},"type":"lvl1","url":"/evaluation/metrics-stdsuite-v1","position":0},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics"},"content":"These are custom-defined Python functions to calculate metrics using timeseries data. These are mainly intended to be used with streamflow timeseries data, though may be applicable elsewhere.\n\nSee \n\nTowler et al 2023 for a published application.\n\nTowler, E., Foks, S.S., Dugger, A.L., Dickinson, J.E., Essaid, H.I., Gochis, D., Viger, R.J., and Zhang, Y. (2023). Benchmarking High-Resolution, Hydrologic Performance of Long-Term Retrospectives in the United States, Hydrology and Earth System Sciences.\n\nThese functions have been adapted from the originals in \n\ngallery​/streamflow​/02​_nwm​_benchmark​_analysis​.ipynb\n\n","type":"content","url":"/evaluation/metrics-stdsuite-v1","position":1},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl2":"The Metrics:"},"type":"lvl2","url":"/evaluation/metrics-stdsuite-v1#the-metrics","position":2},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl2":"The Metrics:"},"content":"This suite of metrics describes the Standard Suite v1 benchmarking metrics:\n\nMetric\n\nUsed on these variables\n\nReference\n\nNash-Sutcliffe efficiency (NSE)\n\nall\n\nNash, J. E., & Sutcliffe, J. V. (1970). River flow forecasting through conceptual models part I—A discussion of principles. Journal of hydrology, 10(3), 282-290. \n\nhttps://​www​.sciencedirect​.com​/science​/article​/pii​/0022169470902556​?via​%3Dihub\n\nKling-Gupta efficiency (KGE)\n\nall\n\nGupta, H. V., Kling, H., Yilmaz, K. K., & Martinez, G. F. (2009).  Decomposition of the mean squared error and NSE performance criteria: Implications for improving hydrological modelling. Journal of hydrology, 377(1-2), 80-91. \n\nhttps://​www​.sciencedirect​.com​/science​/article​/pii​/S0022169409004843\n\nlogNSE\n\nall\n\nOudin, L., Andréassian, V., Mathevet, T., Perrin, C., & Michel, C. (2006). Dynamic averaging of rainfall‐runoff model simulations from complementary model parameterizations. Water Resources  Research, 42(7).\n\npercent bias\n\nall\n\nA measure of the mean tendency of simulated values to be greater or less than associated observed values, units of percent\n\nratio of standard deviation\n\nall\n\nstandard deviation of simulated values divided by the standard deviation of observed values\n\nPearson Correlation\n\nall\n\nK. Pearson (1896, 1900, 1920)\n\nSpearman Correlation\n\nall\n\nCharles Spearman (1904, 1910)\n\npercent bias in midsegment slope of the flow-duration curve (FDC) between Q20-Q70\n\nstreamflow\n\nYilmaz, K. K., Gupta, H. V., & Wagener, T. (2008). A process‐based diagnostic approach to model evaluation: Application to the NWS distributed hydrologic model. Water Resources Research, 44(9).\n\npercent bias in FDC low-segment volume (Q0-Q30)\n\nstreamflow\n\nYilmaz, K. K., Gupta, H. V., & Wagener, T. (2008). A process‐based diagnostic approach to model evaluation: Application to the NWS distributed hydrologic model. Water Resources Research, 44(9).\n\npercent bias in FDC high-segment volume (Q98-Q100)\n\nstreamflow\n\nYilmaz, K. K., Gupta, H. V., & Wagener, T. (2008). A process‐based diagnostic approach to model evaluation: Application to the NWS distributed hydrologic model. Water Resources Research, 44(9).\n\nThis notebook will briefly describe each of the above metrics, and show some results using sample data.\nThe specific code to implement each metric is included.  This notebook can be sourced into analysis notebooks\nto get access to these functions natively.\n\nimport numpy as np\nimport pandas as pd\nimport logging\n\n\n\n","type":"content","url":"/evaluation/metrics-stdsuite-v1#the-metrics","position":3},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl2":"Metric Definitions"},"type":"lvl2","url":"/evaluation/metrics-stdsuite-v1#metric-definitions","position":4},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl2":"Metric Definitions"},"content":"\n\n","type":"content","url":"/evaluation/metrics-stdsuite-v1#metric-definitions","position":5},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl3":"Mean Square Error (MSE)","lvl2":"Metric Definitions"},"type":"lvl3","url":"/evaluation/metrics-stdsuite-v1#mean-square-error-mse","position":6},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl3":"Mean Square Error (MSE)","lvl2":"Metric Definitions"},"content":"Many subsequent functions rely on MSE.\n\ndef MSE(obs, sim) -> float:\n    \"\"\"\n    Mean Square Error --   Compute MSE over all paired values obs (x) and sim (x_hat)\n        \n    Returns\n    -------\n    float\n        Mean square error\n    \"\"\"\n    err = obs - sim\n    return np.mean(err**2)\n\n\n\n","type":"content","url":"/evaluation/metrics-stdsuite-v1#mean-square-error-mse","position":7},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl3":"Nash-Sutcliffe efficiency (NSE)","lvl2":"Metric Definitions"},"type":"lvl3","url":"/evaluation/metrics-stdsuite-v1#nash-sutcliffe-efficiency-nse","position":8},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl3":"Nash-Sutcliffe efficiency (NSE)","lvl2":"Metric Definitions"},"content":"Nash, J. E., & Sutcliffe, J. V. (1970). River flow forecasting through conceptual models part I—A discussion of principles. Journal of hydrology, 10(3), 282-290.\n\n\nhttps://​www​.sciencedirect​.com​/science​/article​/pii​/0022169470902556​?via​%3Dihub\n\ndef NSE(obs, sim) -> float:\n    \"\"\"\n    Nash-Sutcliffe efficiency (NSE)\n\n    Returns:\n        float: calculated NSE\n    \"\"\"\n    return 1 - (MSE(obs, sim) / np.var(obs, ddof=0))\n    # See NOTE re:  ddof \n\n\n\nSpecial note on NSE & variance — A component within the calculation of NSE is variance computed over the\nobserved values. Different python libraries calculated this in different ways, so some of the details matter\nwhen calculating.  In particular, numpy assumes that ddof (Delta Degrees of Freedom)\nis \n\nzero, while  pandas\nassumes a ddof of\n\n\none (\n\nBessel’s Correction).\n\nWithout explicit instructions, these two common libraries will return different results for the ‘same’ calculation,\nso it is important not to inter-mix the libraries. If you should decide to build your own functions involving\nvariance, it will matter how you calculate that value:df['obs'].var()  # using pandas\n\nwill yield a different result thannp.var(df['obs']) # using numpy\n\nThe key (in either case) is to explicitly define the ddof:df['obs'].var(ddof=0)\n# or\nnp.var(df['obs'], ddof=0)\n\nThe original codespec for this benchmark series used numpy, with its default DDOF of 0 and explicit definition of DDOF to ensure compatibility with similar metrics.\n\n","type":"content","url":"/evaluation/metrics-stdsuite-v1#nash-sutcliffe-efficiency-nse","position":9},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl3":"Ratio of Standard Deviations","lvl2":"Metric Definitions"},"type":"lvl3","url":"/evaluation/metrics-stdsuite-v1#ratio-of-standard-deviations","position":10},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl3":"Ratio of Standard Deviations","lvl2":"Metric Definitions"},"content":"\n\ndef rSD(obs, sim) -> float:\n    \"\"\"\n    ratio of standard deviation  -- standard deviation of simulated/modeled\n    values divided by the standard deviation of observed values\n\n    Returns:\n        float: calculated ratio\n    \"\"\"\n    try:\n        return np.std(sim) / np.std(obs)\n    except ZeroDivisionError:\n        logging.warning(\"std dev of observed is zero; ratio undefined\")\n        return None\n\n\n\n","type":"content","url":"/evaluation/metrics-stdsuite-v1#ratio-of-standard-deviations","position":11},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl3":"Correlation Coefficients: Pearson and Spearman","lvl2":"Metric Definitions"},"type":"lvl3","url":"/evaluation/metrics-stdsuite-v1#correlation-coefficients-pearson-and-spearman","position":12},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl3":"Correlation Coefficients: Pearson and Spearman","lvl2":"Metric Definitions"},"content":"These standard measures are available in reliable and fast libraries from \n\nSciPy.\n\nfrom scipy.stats import pearsonr, spearmanr\n\ndef pearson_r(obs, sim) -> float:\n    \"\"\"\n    Pearson Correlation -- Pearson's R, calculated using the scipi library method\n\n    Returns\n    -------\n    float\n        Pearson's R\n    \"\"\"\n    return pearsonr(obs, sim)[0]\n\ndef spearman_r(obs, sim) -> float:\n    \"\"\"\n    Spearman Correlation == Spearman's R, calcuated using the scipy method\n\n    Returns\n    -------\n    float\n        Calculated R                                 |\n    \"\"\"\n    return spearmanr(obs, sim)[0]\n\n\n\n\n","type":"content","url":"/evaluation/metrics-stdsuite-v1#correlation-coefficients-pearson-and-spearman","position":13},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl3":"Kling-Gupta efficiency (KGE)","lvl2":"Metric Definitions"},"type":"lvl3","url":"/evaluation/metrics-stdsuite-v1#kling-gupta-efficiency-kge","position":14},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl3":"Kling-Gupta efficiency (KGE)","lvl2":"Metric Definitions"},"content":"Gupta, H. V., Kling, H., Yilmaz, K. K., & Martinez, G. F. (2009).  Decomposition of the mean squared error and NSE performance criteria: Implications for improving hydrological modelling. Journal of hydrology, 377(1-2), 80-91. \n\nhttps://​www​.sciencedirect​.com​/science​/article​/pii​/S0022169409004843\n\ndef KGE(obs, sim) -> float:\n    \"\"\"\n    Kling-Gupta efficiency (KGE)\n\n    Returns:\n        float: Calculated KGE\n    \"\"\"\n    r = pearsonr(obs, sim)[0]\n    alpha = rSD(obs, sim)\n    beta = np.sum(sim) / np.sum(obs)\n    return 1 - np.sqrt((r-1)**2 + (alpha-1)**2 + (beta-1)**2)\n\n\n\n","type":"content","url":"/evaluation/metrics-stdsuite-v1#kling-gupta-efficiency-kge","position":15},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl3":"logNSE","lvl2":"Metric Definitions"},"type":"lvl3","url":"/evaluation/metrics-stdsuite-v1#lognse","position":16},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl3":"logNSE","lvl2":"Metric Definitions"},"content":"Oudin, L., Andréassian, V., Mathevet, T., Perrin, C., & Michel, C. (2006). Dynamic averaging of rainfall‐runoff model simulations from complementary model parameterizations. Water Resources  Research, 42(7).\n\nThis is a NSE metric, run against log-transformed data.  Because we can’t have log work on zero values, the data must be sanitized to ensure only positive values are passed to np.log(). Of the various ways to treat zeros, we use a clipping function to ‘promote’ values below a small threshold up to that threshold value.  By default, any value below 0.01 is treated as 0.01 for purposes of the log transform.\n\nThis data sanitization is handled differently within other libraries, notably hydroeval.  That package uses a slightly more complex strategy to ensure that log() gets clean data to work on. The hydroeval developer\nreferences \n\nPushpalatha et al. (2012) regarding their strategy.  The details of that method are beyond scope here -- just know that if you compare results with hydroeval, this metric may yield very slightly different results.\n\ndef logXform(a, **kwargs):\n    ### we are allowing for the possible future addition of other methods to treat zero values. 'clip' is the default. \n    if 'clip' in kwargs:\n        assert kwargs['clip'] > 0\n        A = a.clip(kwargs['clip'])\n    return np.log(A)\n\ndef logNSE(obs, sim) -> float:\n    \"\"\"\n    logNSE - computes NSE using the log of data (rather than data)\n\n        float: Calculated NSE of log(data)\n    \"\"\"\n    return NSE(logXform(obs, clip=0.01), logXform(sim, clip=0.1))\n\n\n\n\n","type":"content","url":"/evaluation/metrics-stdsuite-v1#lognse","position":17},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl3":"Percent Bias","lvl2":"Metric Definitions"},"type":"lvl3","url":"/evaluation/metrics-stdsuite-v1#percent-bias","position":18},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl3":"Percent Bias","lvl2":"Metric Definitions"},"content":"A measure of the mean tendency of simulated values to be greater or less than associated observed values, units of percent\n\ndef pbias(obs, sim) -> float:\n    \"\"\"\n    percent bias -- a measure of the mean tendency of simulated values to be\n    greater than or less than associated observed values.\n\n    Returns:\n        float: calculated percent bias / units = percent (i.e. 90 rather than 0.90)\n    \"\"\"\n    return 100 * np.sum(sim - obs) / np.sum(obs)\n\n\n\nSpecial Note on pbias -- as relates to hydroeval and other libraries.\n\nThe result we compute here mimics the behavior of the hydroGOF R package, and is the result of the code provided in\nthe \n\nmodel notebook\nmentioned above.\n\nThis differs from the hydroeval Python package in an important way.\n\nhydroGOF (and this benchmark) returns:   100 × \\frac{\\displaystyle\\sum_{i=1}^{n}(\\hat{x}_{i} - x_{i})}{\\displaystyle\\sum_{i=1}^{n}x_{i}} where x is ‘observed’ and \\hat{x} is ‘modeled’\n\nhydroeval on the other hand, returns:   100 × \\frac{\\displaystyle\\sum_{i=1}^{n}(x_{i} - \\hat{x}_{i})}{\\displaystyle\\sum_{i=1}^{n}x_{i}}Note\ntht the numerator has switched the ordering of x and \\hat{x}.\n\nThe end result is that these two libraries return values of different sign. hydroGOF returns a positive value if the ‘modeled’ tends to be higher than ‘observed’, while hydroeval will return a negative number in this case. The absolute values of these calulations are the same.\n\nThe developer for hydroeval points to \n\nthis document as the source of the math used in that package.\n\nThis code library uses the same ordering as hydroGOF, which is describe in EQN A1 of Yilmaz et al. (2008)\n\n","type":"content","url":"/evaluation/metrics-stdsuite-v1#percent-bias","position":19},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl3":"FDC - Flow Duration Curves","lvl2":"Metric Definitions"},"type":"lvl3","url":"/evaluation/metrics-stdsuite-v1#fdc-flow-duration-curves","position":20},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl3":"FDC - Flow Duration Curves","lvl2":"Metric Definitions"},"content":"Metric\n\nReference\n\npercent bias in midsegment slope of the flow-duration curve (FDC) between Q20-Q70\n\nYilmaz, K. K., Gupta, H. V., & Wagener, T. (2008). A process‐based diagnostic approach to model evaluation: Application to the NWS distributed hydrologic model. Water Resources Research, 44(9).\n\npercent bias in FDC low-segment volume (Q0-Q30)\n\nYilmaz, K. K., Gupta, H. V., & Wagener, T. (2008). A process‐based diagnostic approach to model evaluation: Application to the NWS distributed hydrologic model. Water Resources Research, 44(9).\n\npercent bias in FDC high-segment volume (Q98-Q100)\n\nYilmaz, K. K., Gupta, H. V., & Wagener, T. (2008). A process‐based diagnostic approach to model evaluation: Application to the NWS distributed hydrologic model. Water Resources Research, 44(9).\n\n","type":"content","url":"/evaluation/metrics-stdsuite-v1#fdc-flow-duration-curves","position":21},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl4":"pBiasFMS","lvl3":"FDC - Flow Duration Curves","lvl2":"Metric Definitions"},"type":"lvl4","url":"/evaluation/metrics-stdsuite-v1#pbiasfms","position":22},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl4":"pBiasFMS","lvl3":"FDC - Flow Duration Curves","lvl2":"Metric Definitions"},"content":"This is the percent bias of the slope of the FDC in the mid-segment part of the curve. See equation A2 of Yilmaz\n\n\\%BiasFMS = 100 × \\cfrac{ [log(QS_{m1}) - log(QS_{m2})] - [log(QO_{m1}) - log(QO_{m2})] }{ [log(QO_{m1}) - log(QO_{m2})] }\n\ndef pBiasFMS(obs, sim) -> float:\n    \"\"\"\n    calculates percent bias of the slope the mid-segment of FDC.\n\n    Returns:\n        float: percent bias for values in exceedence probability range 0.2-0.7\n    \"\"\"\n    # Exceedence = 1 - percentile  ;  percentile = 1 - exceedence\n    # mid-segment slope is defined as those observations with flow exceedence probabilities between 20% and 70%.\n    # This leads to percentiles/quantiles of 30% and 80% to establish the cut-offs\n    QO_m1, QO_m2 = np.quantile(obs, [0.30, 0.80])\n    QS_m1, QS_m2 = np.quantile(sim, [0.30, 0.80])\n    m = np.log(QS_m1) - np.log(QS_m2)\n    o = np.log(QO_m1) - np.log(QO_m2)\n    return 100 * (m - o ) / o\n\n\n\n","type":"content","url":"/evaluation/metrics-stdsuite-v1#pbiasfms","position":23},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl4":"pBiasFLV","lvl3":"FDC - Flow Duration Curves","lvl2":"Metric Definitions"},"type":"lvl4","url":"/evaluation/metrics-stdsuite-v1#pbiasflv","position":24},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl4":"pBiasFLV","lvl3":"FDC - Flow Duration Curves","lvl2":"Metric Definitions"},"content":"Percent bias in low-flow segment volume.  Note that in low flow segment, a log transform is used to\nincrease sensitivity to very low flows. See equation A4 from Yilmaz.\n\n\\%BiasFLV = -100 × \\cfrac{\n    \\displaystyle\\sum_{l=1}^L[log(QS_l) - log(QS_L)] - \n    \\displaystyle\\sum_{l=1}^L[log(QO_l) - log(QO_l)]\n    }{\n        \\displaystyle\\sum_{l=1}^L[log(QO_l) - log(QO_L)]\n    }\n\ndef pBiasFLV(obs, sim) -> float:\n    \"\"\"\n    calculates percent bias over the low-flow segment volume.\n    Note that for low-flow observations a log transform is done before the\n    pbias calculation.\n\n    Returns:\n        float: percent bias for values in exceedence probability range 0.7-1.0\n    \"\"\"\n    # Exceedence = 1 - percentile  ;  percentile = 1 - exceedence\n    # Low-Volume is defined as those observations with flow exceedence probabilities between 70% and 100%.\n    # This leads to percentiles/quantiles of 0% and 30% to establish the cut-offs\n    _, QO_L = np.quantile(obs, [0.0, 0.30])\n    _, QS_L = np.quantile(sim, [0.0, 0.30])\n    idx = (obs <= QO_L) # defines boolean selector index\n    QS_l = sim[idx]\n    QO_l = obs[idx]\n    m = np.sum(np.log(QS_l) - np.log(QS_L))\n    o = np.sum(np.log(QO_l) - np.log(QO_L))\n    return -100 * (( m - o ) / o)\n\n\n\n\n","type":"content","url":"/evaluation/metrics-stdsuite-v1#pbiasflv","position":25},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl4":"pBiasFHV","lvl3":"FDC - Flow Duration Curves","lvl2":"Metric Definitions"},"type":"lvl4","url":"/evaluation/metrics-stdsuite-v1#pbiasfhv","position":26},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl4":"pBiasFHV","lvl3":"FDC - Flow Duration Curves","lvl2":"Metric Definitions"},"content":"Percent bias in high-flow segment volume.  See equation A3 of Yilmaz\n\n\\%BiasFHV = 100 × \\cfrac{\n    \\displaystyle\\sum_{h=1}^H(QS_h - QO_h)\n    }{\n    \\displaystyle\\sum_{h=1}^H QO_h\n    }\n\ndef pBiasFHV(obs, sim) -> float:\n    \"\"\"\n    calculates percent bias over the high-flow segment volume.\n\n    Returns:\n        float:\n    \"\"\"\n    # Exceedence = 1 - percentile  ; percentile = 1 - exceedence\n    # 'High-Volume' is defined as those observations with flow exceedence probabilities between 0 and 2%.\n    # This leads to percentiles/quantiles of 98% and 100% to establish the cut-offs\n    #\n    minval, maxval = np.quantile(obs, [0.98, 1.0])\n    idx = (obs >= minval) & (obs <= maxval)\n    QS_h = sim[idx]\n    QO_h = obs[idx]\n    # standard pbias over these observations\n    return 100 * ( (QS_h - QO_h).sum() / QO_h.sum() )\n\n\n\n","type":"content","url":"/evaluation/metrics-stdsuite-v1#pbiasfhv","position":27},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl3":"Flow Duration Curve Plotting","lvl2":"Metric Definitions"},"type":"lvl3","url":"/evaluation/metrics-stdsuite-v1#flow-duration-curve-plotting","position":28},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics","lvl3":"Flow Duration Curve Plotting","lvl2":"Metric Definitions"},"content":"We also allow for plots of the FDC, highlighting the segment-specific metric being calculated.\nSee the \n\nusage document to see how this might be called.\n\ndef FDCplot(obs, sim, ax, segment='mid', fill=False):\n    \"\"\"\n    Given an axes within a matplotlib.plot figure, populate it with\n    artists to show the FDC curves for this dataset (modeled and observed).\n    Optionally fills the space between curves.\n    Parameters\n    ----------\n    obs : observed values\n    sim : simulated/modeled values\n    ax : Axes\n        The specific axes within a fig to draw upon\n    segment : str, optional\n        Which segment of the FDC do you want to highlight? ['lo', 'mid', 'hi'], by default 'mid'\n    fill : bool, optional\n        : Do you want to 'fill_between' the observed and modeled curves?. by default False\n    Returns\n    -------\n    Axes\n        The same axes we were passed in.  Likely ignored.\n    \"\"\"\n    obs_sorted = np.sort(obs)[::-1]\n    mod_sorted = np.sort(sim)[::-1]\n    exceedence = np.arange(1.,len(obs_sorted)+1) / len(obs_sorted)\n\n    ax.set_xlabel(\"Exceedence [%]\", fontsize=6)\n    ax.set_ylabel(\"Flow Rate\", fontsize=6)\n    # NOTE: no units are specified in this label -- because we don't know enough about\n    # the source data.  If the user wants units in the y axis labels, they can call\n    # ax.set_ylabel(\"Flow Rate $m^3 / s$\") (or similar) afterwards to manipulate it.\n    ax.set_yscale('log')\n\n    if segment not in ['mid', 'lo', 'hi']:\n        logging.debug(\"Invalid segment identifier '%s'. Options are ['mid', 'lo', 'hi']\", segment)\n        segment = 'mid'\n\n    if segment == 'mid':\n        pb=pBiasFMS(obs, sim)\n        ax.axvline(x = 20, color = 'b', lw=0.25)\n        ax.axvline(x = 70, color = 'b', lw=0.25)\n        ax.axvspan(20, 70, facecolor='lightgrey', alpha=0.25)\n        ax.text( 0.45 , 0.05, \"Mid-Segment\",\n            verticalalignment='bottom', horizontalalignment='center',\n            transform=ax.transAxes,\n            color='b', fontsize=4)\n        ax.text( 0.45 , 0.01, f\"pBiasFMS= {pb:.4f}\",\n            verticalalignment='bottom', horizontalalignment='center',\n            transform=ax.transAxes,\n            color='b', fontsize=3)\n\n    if segment == 'lo':\n        pb=pBiasFLV(obs, sim)\n        ax.axvline(x = 70, color = 'b', lw=0.25)\n        ax.axvline(x = 100, color = 'b', lw=0.25)\n        ax.axvspan(70, 100, facecolor='lightgrey', alpha=0.25)\n        ax.text( 0.99 , 0.05, \"Low-Flow\",\n            verticalalignment='bottom', horizontalalignment='right',\n            transform=ax.transAxes,\n            color='b', fontsize=4)\n        ax.text( 0.99 , 0.01, f\"pBiasFLV= {pb:.4f}\",\n            verticalalignment='bottom', horizontalalignment='right',\n            transform=ax.transAxes,\n            color='b', fontsize=3)\n\n    if segment == 'hi':\n        pb=pBiasFHV(obs, sim)\n        ax.axvline(x = 0, color = 'b', lw=0.25)\n        ax.axvline(x = 2, color = 'b', lw=0.25)\n        ax.axvspan(0, 2, facecolor='lightgrey', alpha=0.25)\n        ax.text( 0.02 , 0.05, \"High-Flow\",\n            verticalalignment='bottom', horizontalalignment='left',\n            transform=ax.transAxes,\n            color='b', fontsize=4)\n        ax.text( 0.02 , 0.01, f\"pBiasFHV= {pb:.4f}\",\n            verticalalignment='bottom', horizontalalignment='left',\n            transform=ax.transAxes,\n            color='b', fontsize=3)\n\n    ax.plot(exceedence*100, mod_sorted, color='r', lw=0.33, label='Modeled')\n    ax.plot(exceedence*100, obs_sorted, color='k', lw=0.33, label='Observed')\n    if fill:\n        ax.fill_between(exceedence*100, obs_sorted, mod_sorted, lw=0.33, color='r', hatch='//////', alpha=0.0625)\n    ax.legend(loc='upper right', fontsize=6, handlelength=3)\n    ax.grid(linewidth=0.05, dashes=[8,12], color='k', which=\"both\", axis='y')\n    ax.grid(linewidth=0.1, dashes=[16, 8], which='major', axis='x')\n    ax.set_xlim((0, 100))\n    ax.tick_params(which='both', labelsize=4, width=0.25, length=2, pad=1)\n    _ = [ax.spines[axis].set_linewidth(0.2) for axis in ['top','bottom','left','right']]\n\n    return ax\n\n","type":"content","url":"/evaluation/metrics-stdsuite-v1#flow-duration-curve-plotting","position":29},{"hierarchy":{"lvl1":"Evaluation Metrics"},"type":"lvl1","url":"/evaluation/metrics","position":0},{"hierarchy":{"lvl1":"Evaluation Metrics"},"content":"The section contains tutorials for evaluating model outputs and forcing datasets. Evaluation workflows rely on specific benchmarks/metrics to assess their models. We offer some standardized suites of metrics, along with samples of how\nthose metrics might be applied to the data:\n\nHyTEST Project\n\nEnvironment Set-Up\n\nCompute Environments\n\nGeneral QuickStart\n\nCloud: Nebari Quick-Start\n\nHPC Server: Open OnDemand Quick-Start\n\nHPC Server: Jupyter Forward Quick-Start\n\nHPC Server: Start Script Quick-Start\n\nAWS Credentials\n\nStarting a Dask Cluster\n\nNebari Dask Cluster\n\nDenali HPC Dask Cluster\n\nHovenweep/Tallgrass HPC Dask Cluster\n\nLocal Desktop Dask Cluster\n\nGetting Started\n\nEssential Reading\n\nFoundations\n\nData Sources\n\nData/APIs\n\nData/Cloud Storage\n\nParallelism with Dask\n\nDataset Access\n\nData Catalogs\n\nData Storage Locations\n\nWater Mission Area STAC Catalog\n\nHyTEST Data Catalog (Intake)\n\nHyTEST Intake Sub-Catalogs\n\nCONUS404 Products Data Access\n\nCONUS404 Zarr Changelog\n\nExplore CONUS404 Dataset\n\nZarr Creation\n\nData Chunking Tutorial Overview\n\nIntroduction to Chunking\n\nWhy (re)Chunk Data?\n\nHow to Examine a Stored Dataset’s Chunk Shape\n\nBasics of Chunk Shape and Size\n\nWriting Chunked Files\n\nRechunking\n\nAdvanced Topics in Chunking\n\nRechunking Larger Datasets with Dask\n\nGenerating a Virtual Zarr Store\n\nAdding Coordinate Reference Systems (CRS) to Zarr Datasets\n\nChoosing an Optimal Chunk Size and Shape\n\nChunking References\n\nGlossary\n\nAdditional Resources\n\nDataset Processing\n\nCONUS404 Temporal Aggregation\n\nSpatial Aggregation\n\ngdptools CONUS404 Spatial Aggregation over DRB-extent HUC12s\n\ngdptools CONUS404 Spatial Aggregation over CONUS-extent GFv1.1\n\ngdptools CONUS404 Spatial Aggregation over CONUS-extent HUC12s\n\nPangeo CONUS404 Spatial Aggregation over DRB-extent HUC12s\n\ngdptools-Pangeo Method Comparison for CONUS404 Spatial Aggregation\n\nCONUS404 Site Data Selection\n\nCONUS404 Regridding (Curvilinear => Rectilinear)\n\nModel Evaluation\n\nEvaluation Metrics\n\nD-Score Suite (v1) Benchmark\n\nD-Score Suite (v1) Benchmark -- Usage Examples\n\nStandard Suite (v1) Metrics\n\nStandard Suite (v1) Metrics -- Usage Examples\n\nStreamflow Model Evaluation\n\nStreamflow Eval :: Data Preparation\n\nStreamflow Eval :: StdSuite Analysis\n\nSteamflow Eval :: DScore Analysis\n\nStreamflow Eval :: Visualization\n\nBack Matter\n\nContributing\n\nLicense","type":"content","url":"/evaluation/metrics","position":1},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark -- Usage Examples"},"type":"lvl1","url":"/evaluation/tutorials/stats-demos/usage-dscore-suite-v1","position":0},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark -- Usage Examples"},"content":"This notebook adapted from originals by Timothy Hodson and Rich Signell. See that upstream work at:\n\nhttps://​github​.com​/thodson​-usgs​/dscore\n\nhttps://​github​.com​/USGS​-python​/hytest​-evaluation​-workflows/\n\nThis notebook will demonstrate how to call the specific functions defined in the d-score metrics suite notebook (Metrics_Dscore_Suite_v1.ipynb), using a small demonstration dataset.\n\nimport pandas as pd\nimport numpy as np\n\n\n\n","type":"content","url":"/evaluation/tutorials/stats-demos/usage-dscore-suite-v1","position":1},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark -- Usage Examples","lvl2":"Sample Data"},"type":"lvl2","url":"/evaluation/tutorials/stats-demos/usage-dscore-suite-v1#sample-data","position":2},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark -- Usage Examples","lvl2":"Sample Data"},"content":"\n\nsampleData = pd.read_csv(r\"../streamflow/NWM_Benchmark_SampleData.csv\", index_col='date', parse_dates=True).dropna()\nprint(len(sampleData.index), \" Records\")\n\n\n\nA quick look at the table shows that this data contains time-series streamflow values for\nobserved (‘obs’), the NWM data model (‘nwm’), and the NHM model (‘nhm’).  This demonstration\ndataset limits to a single gage (“site_no = 01104200”)\n\nsampleData.head()\n\n\n\n","type":"content","url":"/evaluation/tutorials/stats-demos/usage-dscore-suite-v1#sample-data","position":3},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark -- Usage Examples","lvl2":"Import Benchmark Functions"},"type":"lvl2","url":"/evaluation/tutorials/stats-demos/usage-dscore-suite-v1#import-benchmark-functions","position":4},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark -- Usage Examples","lvl2":"Import Benchmark Functions"},"content":"The metric functions are defined and described in\n\n\nD-Score Suite (v1) Benchmark.\nThey are imported here by running that notebook from within the following cell:\n\n%run ../../Metrics_DScore_Suite_v1.ipynb\n\n\n\nThe functions are now available here, to run against our sample data:\n\n# Mean Square Error\nmse(sampleData['obs'], sampleData['nwm'])\n\n\n\nseasonal_mse(sampleData['obs'], sampleData['nwm'])\n\n\n\n","type":"content","url":"/evaluation/tutorials/stats-demos/usage-dscore-suite-v1#import-benchmark-functions","position":5},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark -- Usage Examples","lvl2":"Create Composite Benchmark"},"type":"lvl2","url":"/evaluation/tutorials/stats-demos/usage-dscore-suite-v1#create-composite-benchmark","position":6},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark -- Usage Examples","lvl2":"Create Composite Benchmark"},"content":"It is useful to combine several of these metrics into a single benchmark routine, which returns a pandas Series of the assembled metrics.\n\nThis ‘wrapper’ composite benchmark also handles any transforms of the data before calling the metric functions. In this case, we will log transform the data.\n\ndef compute_benchmark(df):\n    \"\"\"\n    Runs several metrics against the data table in 'df'.  \n\n    NOTE: the 'obs' and 'nwm' columns must exist in df, and that nan's have already been removed.  \n    \"\"\"\n    obs = np.log(df['obs'].clip(lower=0.01)) # clip to remove zeros and negative values\n    sim = np.log(df['nwm'].clip(lower=0.01)) \n    \n    mse_ = pd.Series(\n        [ mse(obs, sim) ], \n        index=[\"mse\"], \n        dtype='float32'\n    )\n    return pd.concat([\n            mse_,\n            bias_distribution_sequence(obs, sim), \n            stl(obs, sim),\n            seasonal_mse(obs, sim),\n            quantile_mse(obs, sim)\n            ],\n        )\n\n\n\ncompute_benchmark(sampleData)\n\n\n\n","type":"content","url":"/evaluation/tutorials/stats-demos/usage-dscore-suite-v1#create-composite-benchmark","position":7},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark -- Usage Examples","lvl2":"Score-Cards"},"type":"lvl2","url":"/evaluation/tutorials/stats-demos/usage-dscore-suite-v1#score-cards","position":8},{"hierarchy":{"lvl1":"D-Score Suite (v1) Benchmark -- Usage Examples","lvl2":"Score-Cards"},"content":"The D-score functions include an ILAMB-style scorecard function to produce a graphic scorecard from these metrics.\n\nNote that a scorecard such as this is typically applied to a composite of D-score metrics computed for many gages.\nThis demos the scorecard for a single gage as if it were the mean of all gages in an evaluation analysis.\n\n# Compute benchmark and 'score' each decomp as percent of total MSE\nbm = compute_benchmark(sampleData)\npercentage_card = pd.DataFrame(data={\n    'NWM' : ((bm / bm['mse']) * 100).round().astype(int)\n    })\npercentage_card.name=\"Percent\"  ## NOTE: `name` is a non-standard attribute for a dataframe. We use it to stash\n                                ## metadata for this dataframe which the ilamb_card_II() func will use to label things.\npercentage_card\n\n\n\nn_cards=1\nfig, ax = plt.subplots(1, n_cards, figsize=(0.5+(1.5*n_cards), 3.25), dpi=150)\nax = ilamb_card_II(percentage_card, ax)\nplt.show()\n\n\n\n## if the score card has columns for multilple models.....  \n# fictitious example:\npercentage_card['XYZ'] = pd.Series([100, 20, 30, 20, 10, 50, 60, 70, 20, 10, 40, 65, 15,10,5], index=percentage_card.index)\nfig, ax = plt.subplots(1, n_cards, figsize=(0.5+(1.5*n_cards), 3.25), dpi=150)\nax = ilamb_card_II(percentage_card, ax)\nplt.show()\n\n","type":"content","url":"/evaluation/tutorials/stats-demos/usage-dscore-suite-v1#score-cards","position":9},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics -- Usage Examples"},"type":"lvl1","url":"/evaluation/tutorials/stats-demos/usage-stdsuite-v1","position":0},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics -- Usage Examples"},"content":"This notebook will demonstrate how to call the specific functions defined in the\n\n\nStandard Suite (v1) Metrics\nnotebook, using a small demonstration dataset.\n\nimport pandas as pd\nimport numpy as np\n\n\n\n","type":"content","url":"/evaluation/tutorials/stats-demos/usage-stdsuite-v1","position":1},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics -- Usage Examples","lvl2":"Sample Data"},"type":"lvl2","url":"/evaluation/tutorials/stats-demos/usage-stdsuite-v1#sample-data","position":2},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics -- Usage Examples","lvl2":"Sample Data"},"content":"\n\nsampleData = pd.read_csv(r\"../streamflow/NWM_Benchmark_SampleData.csv\", index_col='date').dropna()\nprint(len(sampleData.index), \" Records\")\nsampleData.head()\n\n\n\n","type":"content","url":"/evaluation/tutorials/stats-demos/usage-stdsuite-v1#sample-data","position":3},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics -- Usage Examples","lvl2":"Import Metric Functions"},"type":"lvl2","url":"/evaluation/tutorials/stats-demos/usage-stdsuite-v1#import-metric-functions","position":4},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics -- Usage Examples","lvl2":"Import Metric Functions"},"content":"The functions are defined in an\n\n\nStandard Suite (v1) Metrics.They are imported for use here by running that notebook from within the following cell:\n\n%run ../../Metrics_StdSuite_v1.ipynb\n# This brings functions defined in external notebook into this notebook's namespace.\n\n\n\nThe functions are now available here, to run against our sample data.  These are called with two\narguments: an array/series of observed values and an array/series of modeled/simulated values.\n\nA couple of examples:\n\n# Mean Square Error\nMSE(obs=sampleData['obs'], sim=sampleData['nwm'])\n\n\n\n# Kling-Gupta efficiency\nKGE(obs=sampleData['obs'], sim=sampleData['nwm'])\n\n\n\n","type":"content","url":"/evaluation/tutorials/stats-demos/usage-stdsuite-v1#import-metric-functions","position":5},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics -- Usage Examples","lvl2":"Create Composite Benchmark"},"type":"lvl2","url":"/evaluation/tutorials/stats-demos/usage-stdsuite-v1#create-composite-benchmark","position":6},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics -- Usage Examples","lvl2":"Create Composite Benchmark"},"content":"It is useful to combine several of these metrics into a single benchmark routine, which returns a\npandas \n\nSeries of\nthe assembled metrics.\n\nThis example computes those metrics which might apply to the streamflow variable.\n\ndef compute_benchmark(df):\n    obs = df['obs']\n    sim = df['nwm']\n    return pd.Series(\n        data={\n            'NSE': NSE(obs, sim),\n            'KGE': KGE(obs, sim),\n            'logNSE': logNSE(obs, sim),\n            'pbias': pbias(obs, sim),\n            'rSD': rSD(obs, sim),\n            'pearson': pearson_r(obs, sim),\n            'spearman': spearman_r(obs, sim), \n            'pBiasFMS': pBiasFMS(obs, sim),\n            'pBiasFLV': pBiasFLV(obs, sim),\n            'pBiasFHV': pBiasFHV(obs, sim)\n        },\n        name=df['site_no'].iloc[0], # special case -- 'site_no' column\n        dtype='float64'\n    )\n\n\n\ncompute_benchmark(sampleData)\n\n\n\n","type":"content","url":"/evaluation/tutorials/stats-demos/usage-stdsuite-v1#create-composite-benchmark","position":7},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics -- Usage Examples","lvl2":"Streamflow and FDC plots"},"type":"lvl2","url":"/evaluation/tutorials/stats-demos/usage-stdsuite-v1#streamflow-and-fdc-plots","position":8},{"hierarchy":{"lvl1":"Standard Suite (v1) Metrics -- Usage Examples","lvl2":"Streamflow and FDC plots"},"content":"In the case of streamflow, the NWM standard suite offers a way to plot the Flow Duration Curve when calculating the pBias metrics per Yilmaz et al. This mechanism uses \n\nmatplotlib to implement the figures.\n\nSome examples:\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(6, 3), dpi=150)\nax = FDCplot(sampleData['obs'], sampleData['nwm'], ax, segment='mid')\nax.set_title(\"Gage 1104200 - Mid\")\nplt.show()\n\n\n\n# Same fig, but with \"segment='lo'\"\nfig, ax = plt.subplots(1, 1, figsize=(6, 3), dpi=150)\nax = FDCplot(sampleData['obs'], sampleData['nwm'], ax, segment='lo')\nax.set_title(\"Gage 1104200 - Low\")\nplt.show()\n\n\n\n# Same fig, but with \"segment='hi'\"\nfig, ax = plt.subplots(1, 1, figsize=(6, 3), dpi=150)\nax = FDCplot(sampleData['obs'], sampleData['nwm'], ax, segment='hi')\nax.set_title(\"Gage 1104200 - High\")\nplt.show()\n\n\n\nThe default behavior is to plot the Y axis log-scale, and to leave units off of the flow rate.\nIf you would like to manipulate these parameters, you may adjust the ax after calling FDCplot()\n(see example, next cell). In general, any of the matplotlib parameters can be adjusted after\nFDCplot() in order to customize the figure.\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 3), dpi=150)\nax = FDCplot(sampleData['obs'], sampleData['nwm'], ax, segment='mid')\nax.set_yscale('linear')\nax.set_ylabel(\"Flow Rate $m^3 / s$\") # << labels can contain LaTex-style math between $ chars\nplt.show()\n\n","type":"content","url":"/evaluation/tutorials/stats-demos/usage-stdsuite-v1#streamflow-and-fdc-plots","position":9},{"hierarchy":{"lvl1":"Streamflow Model Evaluation"},"type":"lvl1","url":"/evaluation/tutorials/streamflow/overview","position":0},{"hierarchy":{"lvl1":"Streamflow Model Evaluation"},"content":"","type":"content","url":"/evaluation/tutorials/streamflow/overview","position":1},{"hierarchy":{"lvl1":"Streamflow Model Evaluation","lvl2":"Overview"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/overview#overview","position":2},{"hierarchy":{"lvl1":"Streamflow Model Evaluation","lvl2":"Overview"},"content":"In broad strokes, the evaluation workflow contains these components:\n\nThis can be broken down per variable, as there are different reference\ndatasets for variables, as well as aggregation methods.  Examples below will\nemphasize streamflow as an example, comparing historical stream gage readings against the\nNWM streamflow predictions for those gages.\n\nEach of the yellow boxes in the block diagram is mapped to a notebook in\nwhich that bit of processing takes place.","type":"content","url":"/evaluation/tutorials/streamflow/overview#overview","position":3},{"hierarchy":{"lvl1":"Streamflow Model Evaluation","lvl2":"Source Data"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/overview#source-data","position":4},{"hierarchy":{"lvl1":"Streamflow Model Evaluation","lvl2":"Source Data"},"content":"Source datasets include modeled data\n( NWM or NHM, for example)\nand a reference dataset representing the ‘observed’ values covering the\nsame variable and temporal range.\nFor streamflow data, we have actual gage readings.  For other variables, we have other standard\ndatasets representing the reference against which the model will be compared.\n\nSource datasets are found in a variety of storage mechanisms.  The main mechanisms that we\nneed to be able to accomodate are:\n\n‘on-prem’ data stored on /caldera, accessible by one of the HPC hosts.\n\nObject storage in the ‘cloud’ -- typically an \n\nS3 bucket in the AWS cloud computing ecosystem.\n\nAPI request -- Data is offered by a provider as a network service.  An Aplication\nProgramming Interface call is made from within a python\nprogram to fetch the data from the network provider. This is typically via\nhttp or ftp transfer protocols.\n\nIntake catalog -- this is a convenience mechanism which can handle\nmuch of the infrastructure needed to access data, regardless of protocol. We use intake catalogs whenever\npossible, since they simplify the access mechanism and hid implementation details.\n\nThe source datasets may be replecated among two or more of these access methods.  Which copy to\nuse may depend on where the processing takes place (i.e. if running a notebook on denali or\ntallgrass, on-prem data is preferred over S3;  if running on a cloud environment (esip/qhub),\nS3 is preferred.)","type":"content","url":"/evaluation/tutorials/streamflow/overview#source-data","position":5},{"hierarchy":{"lvl1":"Streamflow Model Evaluation","lvl2":"Data Preparation Notebook"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/overview#data-preparation-notebook","position":6},{"hierarchy":{"lvl1":"Streamflow Model Evaluation","lvl2":"Data Preparation Notebook"},"content":"This pre-processing step is needed in order to rectify the data and organize it in preparation\nfor analysis.  Rectifying the data includes measures such as:\n\nOrganizing the time-series index such that the time steps for both simulated and observed are congruent;\n\nCoordinate aggregation units between simulated and observed (e.g. indexed on ‘gage_id’ with similar string formats: ‘USGS-01104200’ vs ‘01104200’)\n\nRe-Chunking the data to make time-series analysis more efficient\n\nObtaining data from an API and storing it in more efficient format for reuse\n\nSee \n\nhere for a primer on re-chunking data, and why we choose to do it before analysis.\n\nAt this stage, a given variable should be represented as a pair of 2D array of values (one\nfor simulated, one for observed).\nOne dimension of the array is indexed by some nominal key (‘gage_id’, ‘HUC-12 ID’,\netc), while the other dimension is indexed by time step.","type":"content","url":"/evaluation/tutorials/streamflow/overview#data-preparation-notebook","position":7},{"hierarchy":{"lvl1":"Streamflow Model Evaluation","lvl2":"Analysis Notebook"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/overview#analysis-notebook","position":8},{"hierarchy":{"lvl1":"Streamflow Model Evaluation","lvl2":"Analysis Notebook"},"content":"The above data organization steps will allow us to extract a time series for a given station\nfrom each of the simulated and observed datasets, and run a series of statistical metrics\nagainst these values to evaluate Goodness Of Fit (GOF).  Benchmarking proceeds\naccording to this general recipe:\n\nA set of predictions and matching observations (i.e. the data, established above);\n\nThe domain (e.g. space or time) over which to benchmark\n\nThis will vary by variable and by dataset.  For streamflow, the list\nof ‘cobalt’ gages (\n\nFoks et al., 2022)\nestablishes the spatial domain identifying which gages to consider.\n\nOther variables will have other definitions for domain, which restrict\nanalysis to a specific set of locations or times.\n\nA set of statistical metrics with which to benchmark.\n\nIn this tutorial, we are focusing on streamflow and the metrics relevant to\nthat variable.\n\nA different set of metrics may be used for other variables.\n\nWe will be using the ‘NWM Standard Suite’ and ‘DScore’ metrics to analyize streamflow.\n\nThe end result of this analysis is a 2D table of values.  One dimension of\nthis array/table is the same nominal data field (i.e. ‘gage_id’), the other\ndimension being the metrics comparing observed vs simulated for that gage.\nIt is this table of values we send to the visualization step.","type":"content","url":"/evaluation/tutorials/streamflow/overview#analysis-notebook","position":9},{"hierarchy":{"lvl1":"Streamflow Model Evaluation","lvl2":"Vizualization Notebook"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/overview#vizualization-notebook","position":10},{"hierarchy":{"lvl1":"Streamflow Model Evaluation","lvl2":"Vizualization Notebook"},"content":"Visualization steps offer different views of the metrics, plotted in various\nways to allow for exploration.  In addition to these interactive visualizations,\na score card is offered as a way of summarizing how well the\nmodel compares against the reference dataset.","type":"content","url":"/evaluation/tutorials/streamflow/overview#vizualization-notebook","position":11},{"hierarchy":{"lvl1":"Streamflow Eval :: Data Preparation"},"type":"lvl1","url":"/evaluation/tutorials/streamflow/data-prep","position":0},{"hierarchy":{"lvl1":"Streamflow Eval :: Data Preparation"},"content":"As a part of the generalized evaluation workflow:\n\nThe pre-processing step is needed in order to align the two datasets for analysis.  The specific\nsteps needed to prepare a given dataset may differ, depending on the source and the variable of\ninterest.\n\nSome steps might include:\n\nOrganizing the time-series index such that the time steps for both simulated and observed are congruent\n\nThis may involve interpolation to estimate a more granular time-step than is found in the source data\n\nMore often, an agregating function is used to ‘down-sample’ the dataset to a coarser time step (days vs hours).\n\nCoordinate aggregation units between simulated and observed\n\nGridded data may be sampled per HUC-12, HUC-6, etc. to match modeled data indexed by these units.\n\nIndex formats may be adjusted (e.g. a ‘gage_id’ may be ‘USGS-01104200’ in one data set, vs ‘01104200’ in another)\n\nRe-Chunking the data to make time-series analysis more efficient (see \n\nhere for a primer on re-chunking).","type":"content","url":"/evaluation/tutorials/streamflow/data-prep","position":1},{"hierarchy":{"lvl1":"Streamflow Eval :: Data Preparation","lvl2":"Streamflow Data Prep"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/data-prep#streamflow-data-prep","position":2},{"hierarchy":{"lvl1":"Streamflow Eval :: Data Preparation","lvl2":"Streamflow Data Prep"},"content":"This document shows one approach to preparing the streamflow data for subsequent analysis (That analysis is outlined \n\nhere).\n\nStreamflow analysis will compare time-series of two aligned datasets:\n\n‘observed’ data values obtained from \n\nNWIS\n\n‘modeled’ data extracted from the \n\nNWM\n\nThese data soruces are accessed using different methods.  We will pull data from their respective sources, reshape and optimize the data structures, then write that data to storage to make later analysis easier.\n\nAn overview of the steps we will take in this notebook:\n\nRead Modeled Data\n\nEstablish AWS Credentials\n\nSource NWIS data via API\n\nCreate a plan to Re-Structure that data for storage as ZARR file on S3.\n\nRename variables\n\nEstablish chunking layout\n\nEstablish encoding\n\nCreate a template to formalize this configuration\n\nIn Parallel (one worker per gage_id):\n\nFetch data from NWIS\n\nWrite data to ZARR file\n\nVerify the data is correctly written to ZARR storage.\n\n","type":"content","url":"/evaluation/tutorials/streamflow/data-prep#streamflow-data-prep","position":3},{"hierarchy":{"lvl1":"Streamflow Eval :: Data Preparation","lvl2":"1) Reading the ‘modeled’ Data"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/data-prep#id-1-reading-the-modeled-data","position":4},{"hierarchy":{"lvl1":"Streamflow Eval :: Data Preparation","lvl2":"1) Reading the ‘modeled’ Data"},"content":"Modeled data for this demonstration tutorial will be sourced from the S3 bucket nhgf-development.\n\nimport os\n# Needed when boto3 >= 1.36.0 or the rechunking process will fail\n# This needs to be set before the boto3 library gets loaded\n# See: https://github.com/aws/aws-cli/issues/9214#issuecomment-2606619168\nos.environ['AWS_REQUEST_CHECKSUM_CALCULATION'] = 'when_required'\nos.environ['AWS_RESPONSE_CHECKSUM_VALIDATION'] = 'when_required'\nos.environ['USE_PYGEOS'] = '0'\nimport fsspec\nimport dask\nimport numpy as np\nimport xarray as xr\nimport intake\nfrom getpass import getuser\nusername = getuser()\nfrom dask_gateway import Gateway\n\n\n\nModeled data is within the HyTEST intake catalog, with name “nwm21-streamflow-usgs-gages-osn”:\n\ncat = intake.open_catalog(r'https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml')\n\nmodeled = cat['nwm21-streamflow-usgs-gages-osn'].to_dask()\n\nmodeled\n\n\n\nSource Data as Template\n\nThis source data set will establish the indices and boundaries for the data we will eventually pull from the NWIS stream gage network.\nThe two dimensions of this data are the Gage ID and Time.  We’ll use these dimensions to fetch the ‘observed’ data later. The\nendpoints and range of these dimensions will establish that future query.\n\n## Gage IDs\nimport re\n_gages = [gage_id.lstrip() for gage_id in modeled['gage_id'].values.astype('str')]\n\nGAGES = [g for g in _gages if re.search('USGS-\\d+$', g) ][0:100] \n#                                                        ^^^^^^^ See NOTES:\n\n## >> NOTE 1: The regex search pattern ensures we get legal gage names only\n## >> NOTE 2: We are limiting the GAGES array to the first 100 gages for this \n##            demo/tutorial. To run this workflow for the entire set of GAGES, \n##            remove the slice notation [0:100]\n\n# Time boundaries for future queries:\nstart_time = modeled.time.values.min()\nstop_time = modeled.time.values.max()\nDATE_RANGE=(start_time, stop_time)\n\n\n\nSome important notes for each of those bounds:\n\nGAGES / Gage IDs :: The list of 7994 gage IDs in the model dataset include some values which the NWIS does not\nrecognize and will not accept. We need to remove them.\n\nGage IDs of the form USGS-\\d+ (A string starting ‘USGS-’ and ending in an arbitrary number of digits)\nare processed by NWIS data requests.\n\nThere are roughly 350 gage IDs in the modeled dataset with letters embedded in the string of digits after the ‘USGS-’. These\nwill be rejected by the API when we try to call NWIS data service to obtain streamflow history for that location.\n\nThis is the reason behind the regular expression search (re.search) to select only gage_id of the correct format.\n\nAfter selecting the NWIS-compliant gage IDs, the GAGES list contains 7647 gages. This tutorial will demonstrate\nthe workflow using only the first 100 gages on that list. If you want to process the whole list, remove the slice notation\nas described in the comments above.\n\nDATE_RANGE / Dates :: This defines the temporal range for the historical data will will fetch from NWIS.\n\nThe NWM modeled data includes time values stepped hourly.\n\nThe historical streamflow data is stepped daily.\n\nWe will resample later to make sure the indices match.\n\n","type":"content","url":"/evaluation/tutorials/streamflow/data-prep#id-1-reading-the-modeled-data","position":5},{"hierarchy":{"lvl1":"Streamflow Eval :: Data Preparation","lvl2":"2) Establish AWS Credentials"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/data-prep#id-2-establish-aws-credentials","position":6},{"hierarchy":{"lvl1":"Streamflow Eval :: Data Preparation","lvl2":"2) Establish AWS Credentials"},"content":"Now that we’ve got a handle on the ‘modeled’ data, we can begin to think about the matching ‘observed’\ndata.  But before we do that, let’s establish credentials for working with our compute environment. Doing\nthis now will streamline future I/O and cluster tasks.\n\n# uncomment the lines below to read in your AWS credentials if you want to access data from a requester-pays bucket (-cloud)\nos.environ['AWS_PROFILE'] = 'osn-hytest-scratch'\n%run ../../../environment_set_up/Help_AWS_Credentials.ipynb\n\n\n\n# With credentials established, instantiate a handle for the writable file system\nfs_write = fsspec.filesystem('s3', \n                         anon=False, # force a consult to environment vars set above.\n                         profile='osn-hytest-scratch',\n                         skip_instance_cache=True, \n                         client_kwargs={'endpoint_url': 'https://usgs.osn.mghpcc.org'}\n                        )\n\nfname=f's3://hytest-scratch/testing/{username}/nwis_out_{username}.zarr'  #<<<< THIS will be our output location.\n\n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/data-prep#id-2-establish-aws-credentials","position":7},{"hierarchy":{"lvl1":"Streamflow Eval :: Data Preparation","lvl2":"3) Sourcing the ‘observed’ data"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/data-prep#id-3-sourcing-the-observed-data","position":8},{"hierarchy":{"lvl1":"Streamflow Eval :: Data Preparation","lvl2":"3) Sourcing the ‘observed’ data"},"content":"Now that we have information about the list of gages and the date range covered by the model, we\ncan use that to query NWIS for matching data points for this same range of dates and station IDs.\nBecause NWIS data is structured a little differently than the modeled streamflow, we’ll need to\nre-arrange the data a little after fetching.\n\nIn addition, a call to NWIS for historical data can be time consuming -- and we will do it roughly\n7500 times.  We will eventually set up a mechanism to do these requests in parallel, once we’ve\nestablished how the data restructuring should happen.\n\nThe first step in that process is to make a NWIS request for just a couple of gages to see how\nthe return data is structured.  We’ll use that information to create the plan by which the full\ndataset is to be fetched and reorganized.\n\nfrom pygeohydro import NWIS\nnwis = NWIS()\n## Fetch data for a couple of gages to see how NWIS formats a response\nobserved = nwis.get_streamflow(GAGES[0:2], DATE_RANGE, to_xarray=True) \n    ## get_streamflow() is an API call to a data server via internet.\nobserved\n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/data-prep#id-3-sourcing-the-observed-data","position":9},{"hierarchy":{"lvl1":"Streamflow Eval :: Data Preparation","lvl2":"4) Examine the Response Data -- Make a plan"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/data-prep#id-4-examine-the-response-data-make-a-plan","position":10},{"hierarchy":{"lvl1":"Streamflow Eval :: Data Preparation","lvl2":"4) Examine the Response Data -- Make a plan"},"content":"Our goal is to use the NWIS service to fetch data for a large number of gages, then assemble that\ninto a dataset that is structured similarly to our modeled data.\n\nWe requested two stream gages from NWIS rather than just one, to ensure that the dataset is multi-dimensional\n(as the final dataset will be). Using what we learned from the above NWIS call, we can make a plan\nfor how to match it to the existing modeled data.  A few considerations:\n\nWe’ll need to rename some variables (i.e. ‘discharge’ --> ‘streamflow’, etc).\n\nWe also need\nto make note of which data variables are strings, but perhaps stored with different encodings.\n\nBecause the composite dataset will be quite large, some care should be taken to chunk it such that it performs well\nfor time-series analysis.\n\nLearn more about chunking \n\nhere\n\nLastly, we note the time range returned by NWIS.  It covers the time range we requested in\nDATE_RAGE, but is stepped daily. We will use this information to interpolate future results.\n\n# Step 4a :  rename variables\nobserved = (observed\n            .rename_dims({'station_id':'gage_id'})\n            .rename({'discharge':'streamflow', 'station_id':'gage_id'})\n           )\nobserved\n\n\n\n# Step 4b : define chunking in a 'template' dataset\nsource_dataset = observed\ntemplate = (xr.zeros_like(source_dataset)  # DataSet just like 'observed'\n             .chunk()           \n             .isel(gage_id=0, drop=True)      # temporarily remove gage_id as a dimension and coordinate\n             .expand_dims(gage_id=len(GAGES), axis=-1) # add it back, reserving space for the full size of GAGES\n             .assign_coords({'gage_id': GAGES}) # add coordinate to match dimension\n             .chunk({                         # define chunk sizes\n                 'time': len(observed.time),  # all time vals in one chunk\n                 'gage_id': 1}                # one gage_id per chunk\n             )\n           )\ntemplate\n\n\n\n## Step 4c :  Commit template to permanent storage: \nif fs_write.exists(fname):\n    print(\"Removing old copy of tutorial/demo output...\", end=\"\")\n    fs_write.rm(fname, recursive=True)\n    print(\"Done\")\n\n\n\n# Step 4c (continued): write the template with specific encodings\noutfile = fs_write.get_mapper(fname)\ntemplate.to_zarr(\n    outfile,\n    compute=False,\n    encoding =  {                                  # encodings sets data types for the disk store\n        'station_nm':  dict( _FillValue=None,        dtype='<U64'), \n        'alt_datum_cd':dict( _FillValue=None,        dtype='<U6'),\n        'alt_acy_va':  dict( _FillValue=-2147483647, dtype=np.int32),\n        'alt_va':      dict( _FillValue=9.96921e+36, dtype=np.float32),\n        'dec_lat_va':  dict( _FillValue=None,        dtype=np.float32),\n        'dec_long_va': dict( _FillValue=None,        dtype=np.float32),\n        'streamflow':  dict( _FillValue=9.96921e+36, dtype=np.float32)\n    },\n    consolidated=True,                             # Consolidate metadata\n    mode='w'\n)\n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/data-prep#id-4-examine-the-response-data-make-a-plan","position":11},{"hierarchy":{"lvl1":"Streamflow Eval :: Data Preparation","lvl2":"5) Parallel Processing"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/data-prep#id-5-parallel-processing","position":12},{"hierarchy":{"lvl1":"Streamflow Eval :: Data Preparation","lvl2":"5) Parallel Processing"},"content":"The above steps were necessary to establish a permanent disk storage space for the output\ndataset.  We’ve established its structure (variables, chunking plan, encodings) and also\ngiven a hint as to its size (by asserting the length of the gage_id index to be len(GAGES)\nitems).\n\nWith that all established, we can now execute a job where each gage’s data is fetched from\nNWIS and inserted into the permanent store. This demo is limited (by default) to only 100\ngages, so could in theory be executed serially.  We want to do it in parallel so as to model\nthe process for an arbitrary number of gages.\n\n## Step 5a) : write the 'worker' function -- this will be called once per gage\n# Globals: \nn_timesteps = len(observed.time)\ntime_steps = observed.time.values\n\ndef write_one_gage(n):\n    \"\"\" \n    Writes one gage's data to the existing zarr file.  Uses the NWIS API call to fetch data.\n    \n    Arguments: \n    n   : integer\n       the index into the GAGES array identifying which gage to fetch and write. \n    \"\"\"\n    site_id = GAGES[n]\n    try:\n        _obs = nwis.get_streamflow(site_id, DATE_RANGE, to_xarray=True).interp(time=time_steps)\n        _obs = _obs.rename_dims({'station_id':'gage_id'}).rename({'station_id':'gage_id','discharge':'streamflow'})\n        ## We must force the returned data into the datatype that we stored to disk. \n        _obs['station_nm'] = xr.DataArray(data=_obs['station_nm'].values.astype('<U64'), dims='gage_id')\n        _obs['alt_datum_cd'] = xr.DataArray(data=_obs['alt_datum_cd'].values.astype('<U6'), dims='gage_id')\n \n        _obs.to_zarr(\n            outfile, \n            region={ #<<< Specifying a region lets us 'insert' data to a specific place in the dataset. \n                'time': slice(0, n_timesteps), \n                'gage_id': slice(n,n+1)\n                }\n            )\n        return n # If success, returns the index into GAGES. \n    except Exception as e: \n        pass\n        #return e  # if failure, return the exception thrown.\n        # This is an extremely broad way to catch exceptions... and in general is to be avoided. \n        # We do it this way in this case to protect the parallel run. it allows a single write_one_gage() \n        # to fail silently without affecting the rest of the run.\n\n\n\n\n# Step 5b) Start up a distributed cluster of workers\n# NOTE: This cluster configuration is VERY specific to the JupyterHub cloud environment on ESIP/QHUB\n%run ../../../environment_set_up/Start_Dask_Cluster_Nebari.ipynb\n\n\n\n%%time\n## Run the list of tasks: \nresults = dask.compute(*[dask.delayed(write_one_gage)(i) for i in range(len(GAGES))], retries=10)\n\n\n\n## Consolidate metadata, to make future reads easier/faster\nfrom zarr.convenience import consolidate_metadata\n_ = consolidate_metadata(outfile)\n\n\n\n## Shut down the cluster\nclient.close()\ncluster.close()\n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/data-prep#id-5-parallel-processing","position":13},{"hierarchy":{"lvl1":"Streamflow Eval :: Data Preparation","lvl2":"6) Verify"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/data-prep#id-6-verify","position":14},{"hierarchy":{"lvl1":"Streamflow Eval :: Data Preparation","lvl2":"6) Verify"},"content":"We can now read the dataset that we just wrote to disk.  Does it have the dimensions, chunking, and encoding that we want?\n\n## fname is already set from above...\noutfile=fs_write.get_mapper(fname)\ndst = xr.open_dataset(outfile, engine='zarr', chunks={}, backend_kwargs=dict(consolidated=True))\n## NOTE: xarray will employ a 'lazy' loader; only metadata will be loaded initially.  Will only \n## read real data when it is actually needed for computation. \ndst\n\n\n\nimport hvplot.xarray\n\ndst.sel(gage_id='USGS-07241800').hvplot(x='time',y='streamflow', grid=True)\n## This select operator is a specific call to read data -- so may take a \n## moment to fetch the full time series for the specified gage. \n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/data-prep#id-6-verify","position":15},{"hierarchy":{"lvl1":"Streamflow Eval :: Data Preparation","lvl2":"DONE"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/data-prep#done","position":16},{"hierarchy":{"lvl1":"Streamflow Eval :: Data Preparation","lvl2":"DONE"},"content":"That dataset is now available for future analysis in which we need a consolidated NWIS\ndataset, chunked to optimize time-series analysis.","type":"content","url":"/evaluation/tutorials/streamflow/data-prep#done","position":17},{"hierarchy":{"lvl1":"Steamflow Eval :: DScore Analysis"},"type":"lvl1","url":"/evaluation/tutorials/streamflow/analysis-dscore","position":0},{"hierarchy":{"lvl1":"Steamflow Eval :: DScore Analysis"},"content":"\n\nNote\n\nThis notebook adapted from originals by Timothy Hodson and Rich Signell. See that upstream work at:\n\nhttps://​github​.com​/thodson​-usgs​/dscore\n\nhttps://​github​.com​/USGS​-python​/hytest​-evaluation​-workflows/","type":"content","url":"/evaluation/tutorials/streamflow/analysis-dscore","position":1},{"hierarchy":{"lvl1":"Steamflow Eval :: DScore Analysis","lvl2":"Essential Benchmark Components"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/analysis-dscore#essential-benchmark-components","position":2},{"hierarchy":{"lvl1":"Steamflow Eval :: DScore Analysis","lvl2":"Essential Benchmark Components"},"content":"This benchmark notebook will present a workflow which follows a canonical model for Essential Benchmark Components:\n\nA set of predictions and matching observations;\n\nThe domain (e.g. space or time) over which to benchmark;\n\nA set of statistical metrics with which to benchmark.\n\nLet’s get started.\n\n","type":"content","url":"/evaluation/tutorials/streamflow/analysis-dscore#essential-benchmark-components","position":3},{"hierarchy":{"lvl1":"Steamflow Eval :: DScore Analysis","lvl2":"Step 0: Load libraries and configure Python computing environment"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/analysis-dscore#step-0-load-libraries-and-configure-python-computing-environment","position":4},{"hierarchy":{"lvl1":"Steamflow Eval :: DScore Analysis","lvl2":"Step 0: Load libraries and configure Python computing environment"},"content":"\n\nimport pandas as pd\nimport logging\nimport os\n# Needed when boto3 >= 1.36.0 or the rechunking process will fail\n# This needs to be set before the boto3 library gets loaded\n# See: https://github.com/aws/aws-cli/issues/9214#issuecomment-2606619168\nos.environ['AWS_REQUEST_CHECKSUM_CALCULATION'] = 'when_required'\nos.environ['AWS_RESPONSE_CHECKSUM_VALIDATION'] = 'when_required'\n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/analysis-dscore#step-0-load-libraries-and-configure-python-computing-environment","position":5},{"hierarchy":{"lvl1":"Steamflow Eval :: DScore Analysis","lvl2":"Step 1: Load Data"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/analysis-dscore#step-1-load-data","position":6},{"hierarchy":{"lvl1":"Steamflow Eval :: DScore Analysis","lvl2":"Step 1: Load Data"},"content":"\n\nEssential Benchmark Components:\n\nA set of predictions and matching observations,  \n\n<<--You are here\n\nThe domain over which to benchmark\n\nA set of statistical metrics with which to benchmark.\n\nFinding and loading data is made easier for this particular workflow (the streamflow variable), in that most of it has been\npre-processed and stored in a cloud-friendly format.  That process is described in the \n\ndata preparation\nnotebook. We will proceed here using the already-prepared data for streamflow, which is included in the HyTEST intake catalog.\n\nLearn more about intake \n\nhere\n\nimport intake\ncat = intake.open_catalog(r'https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml')\nprint(\"Available datasets: \\n\", \"\\n\".join(cat.keys()))\n\n\n\nThe above list represents the processed datasets available for benchmarking.  If a dataset\nyou want is not in that list,\nyou can load data directly from S3 or onprem.\nIf you load data from a source other than this list, you can jump to Step 2: Restrict to a Domain\n\nNote that the interesting datasets in the cataloged dataset above are duplicated: Some are -onprem , some are -cloud, and some are some are -osn. Same data, but the storage location and access protocol will be different. You\nwill definitely want to use the correct copy of the data for your computing environment.\n\nonprem : Direct access via the caldera filesystem from denali or tallgrass\n\ncloud : Network access via S3 bucket, suitable for consumption on cloud-hosted jupyter servers. You could also access using any network-attached computer, but the amount of data will likely saturate your connection.  Use in the cloud (e.g. ESIP QHub)\n\nosn : Network access via OSN pod, which uses the S3 API, suitable for consumption on any jupyter server.\n\nSo... are you on-prem?\n\nimport platform\nonprem = (platform.node() in ['denali', 'tallgrass'])\nif onprem:\n    print(\"Yes : -onprem\")\n    obs_data_src='nwis-streamflow-usgs-gages-onprem'\n    mod_data_src='nwm21-streamflow-usgs-gages-onprem'\nelse:\n    print(\"Not onprem; use '-osn' data source\")\n    obs_data_src='nwis-streamflow-usgs-gages-osn'\n    mod_data_src='nwm21-streamflow-usgs-gages-osn'\nprint(\"Observed : \", obs_data_src)\nprint(\"Modeled  : \", mod_data_src)\n\n\n\nvariable_of_interest = 'streamflow'\ntry:\n    obs = cat[obs_data_src].to_dask()\n    mod = cat[mod_data_src].to_dask()\nexcept KeyError:\n    print(\"Something wrong with dataset names.\")\n    raise\n\ntry:\n    obs_data = obs[variable_of_interest]\n    mod_data = mod[variable_of_interest].astype('float32')\nexcept KeyError:\n    print(f\"{variable_of_interest} was not found in these data.\")\n\nobs_data.name = 'observed'\nmod_data.name = 'predicted'    \n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/analysis-dscore#step-1-load-data","position":7},{"hierarchy":{"lvl1":"Steamflow Eval :: DScore Analysis","lvl2":"Step 2: Restrict to a Domain"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/analysis-dscore#step-2-restrict-to-a-domain","position":8},{"hierarchy":{"lvl1":"Steamflow Eval :: DScore Analysis","lvl2":"Step 2: Restrict to a Domain"},"content":"\n\nEssential Benchmark Components:\n\nA set of predictions and matching observations,\n\nThe domain over which to benchmark \n\n<<--You are here\n\nA set of statistical metrics with which to benchmark.\n\nEach benchmark domain is defined over specific bounds (typically space and/or time).\nBenchmark domain definitions can be defined within the notebook, or sourced from\nelsewhere. For this example, we use the Cobalt gages, avaliable for download on ScienceBase\n(\n\nFoks et al., 2022).\n\nThis is essentially a list of\nstream guages in which we are interested, along with some  metadata about that gage (watershed,\nreach code, etc).  We will use this as a spatial selector to restrict the original data to only\nthose gages found within this benchmarking domain.\n\ncobalt = pd.read_csv(\n    'https://www.sciencebase.gov/catalog/file/get/6181ac65d34e9f2789e44897?f=__disk__22%2F1a%2Fd2%2F221ad2fe9d95de17731ad35d0fc6b417a4b53ee1',\n    dtype={'site_no':str, 'huc_cd':str, 'reachcode':str, 'comid':str, 'gagesII_class':str, 'aggecoregion': str}, \n    index_col='site_no'\n    )\n# Re-format the gage_id/site_no string value.  ex:   \"1000000\"  ==> \"USGS-1000000\"\ncobalt.rename(index=lambda x: f'USGS-{x}', inplace=True)\nprint(f\"{len(cobalt.index)} gages in this benchmark\")\ncobalt.head(2)\n\n\n\nSo now we have a domain dataset representing the stream gages (unique site_no values) identifying the locations making up the spatial domain of this benchmark. While we have good meta-data for these gages (lat/lon location, HUC8 code, etc), we really will only use the list of site_no values to select locations out of the raw data.\n\n","type":"content","url":"/evaluation/tutorials/streamflow/analysis-dscore#step-2-restrict-to-a-domain","position":9},{"hierarchy":{"lvl1":"Steamflow Eval :: DScore Analysis","lvl2":"Step 3: Define Metrics"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/analysis-dscore#step-3-define-metrics","position":10},{"hierarchy":{"lvl1":"Steamflow Eval :: DScore Analysis","lvl2":"Step 3: Define Metrics"},"content":"\n\nEssential Benchmark Components:\n\nA set of predictions and matching observations,\n\nThe domain over which to benchmark\n\nA set of statistical metrics with which to benchmark. \n\n<<--You are here\n\nThe code to calculate the various metrics has been standardized \n\nhere.\nYou can use these or write your own.  To import and use these standard definitions, run this cell:\n\n%run ../../Metrics_DScore_Suite_v1.ipynb\n\n\n\nWhether you use these functions or your own, we need to put all metric computation into a special all-encompasing\nbenchmarking function--a single call which can be assigned to each gage in our domain list. This sort of arrangement\nis well-suited to parallelism with dask.\n\nIf this is done well, the process will benefit enormously from task parallelism -- each gage can be given its own\nCPU to run on.  After all are done, the various results will be collected and assembled into a composite dataset.\n\nTo achieve this, we need a single ‘atomic’ function that can execute independently. It will take the gage identifier\nas input and return a list of metrics.\n\n## Wrapper function -- this func will be called once per gage_id, each call on its own dask worker\ndef compute_benchmark(gage_id):\n    try:\n        ## obs_data and mod_data should be globals...\n        obs = obs_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series()\n        mod = mod_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series().resample('1D', offset='5h').mean() \n        \n        # make sure the indices match\n        obs.index = obs.index.to_period('D')\n        mod.index = mod.index.to_period('D')\n\n        # merge obs and predictions; drop NaNs.\n        gage_df = pd.merge(obs, mod, left_index=True, right_index=True).dropna(how='any')\n        \n        obs_log = np.log(gage_df['observed'].clip(lower=0.01)) # clip to remove zeros and negative values\n        sim_log = np.log(gage_df['predicted'].clip(lower=0.01))\n        \n        scores = pd.concat([\n                pd.Series([ mse(obs_log, sim_log) ], index=[\"mse\"], dtype='float64'),\n                bias_distribution_sequence(obs_log, sim_log), \n                seasonal_mse(obs_log, sim_log),\n                quantile_mse(obs_log, sim_log)\n                ]\n            )\n        scores.name=gage_id\n        return scores\n    except Exception as e:#<-- this is an extremely broad way to catch exceptions.  We only do it this way to ensure \n            #    that a failure on one benchmark (for a single stream gage) will not halt the entire run. \n        logging.info(\"Benchmark failed for %s\", gage_id)\n        return None\n\n\n\nLet’s test to be sure this compute_benchmark() function will return data for a single gage\n\ncompute_benchmark('USGS-01030350')\n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/analysis-dscore#step-3-define-metrics","position":11},{"hierarchy":{"lvl1":"Steamflow Eval :: DScore Analysis","lvl2":"Execute the Analysis"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/analysis-dscore#execute-the-analysis","position":12},{"hierarchy":{"lvl1":"Steamflow Eval :: DScore Analysis","lvl2":"Execute the Analysis"},"content":"We will be doing a lot of work in paralallel, using workers within a ‘cluster’.The details of cluster configuration are handled for us by ‘helper’ notebooks, below.\nYou can override their function by doing your own cluster configuration if you like.\n\n# uncomment the lines below to read in your AWS credentials if you want to access data from a requester-pays bucket (-cloud)\n# os.environ['AWS_PROFILE'] = 'default'\n# %run ../../../environment_set_up/Help_AWS_Credentials.ipynb\n\n\n\n%run ../../../environment_set_up/Start_Dask_Cluster_Nebari.ipynb\n### Executes external 'helper to spin up a cluster of workers. \n\n\n\nWe verified above that the compute_benchmark works on the “hosted” server (where this\nnotebook is being executed. As a sanity check before we give the cluster of workers a lot\nto do, let’s verify that we can have a remote worker process a gage by submitting work\nto one in isolation:\n\nclient.submit(compute_benchmark, 'USGS-01030350').result()\n\n\n\nNow that we’ve got a benchmark function, and can prove that it works in remote workers\nwithin the cluster, we can dispatch a fleet of workers to process our data in parallel.\nWe will mak use of dask to do thisusing a dask ‘bag’.\n\nRead more about task parallelism with Dask and how we are using dask bags \n\nhere\n\n# Set up a dask bag with the contents beging a list of the cobalt gages\nimport dask.bag as db\nbag = db.from_sequence( cobalt.index.tolist() ).map(compute_benchmark)\nresults = bag.compute() \n\n\n\nWith that big task done, we don’t need dask parallelism any more. Let’s shut down the cluster:\n\nclient.close(); del client\ncluster.close(); del cluster\n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/analysis-dscore#execute-the-analysis","position":13},{"hierarchy":{"lvl1":"Steamflow Eval :: DScore Analysis","lvl2":"Assemble the results"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/analysis-dscore#assemble-the-results","position":14},{"hierarchy":{"lvl1":"Steamflow Eval :: DScore Analysis","lvl2":"Assemble the results"},"content":"The bag now contains a collection of return values (one per call to compute_benchmark()).  We can massage that into a table/dataframe for easier processing:\n\nresults = [i for i in results if i is not None] # Drop entries where compute_benchmark failed\nresults_df = pd.concat(results, axis=1).T\nresults_df.index.name = 'site_no'\nresults_df\n\n\n\nThis dataframe/table can be saved to disk as a CSV. It will be used for visualizations \n\nhere.\n\nresults_df.to_csv('DScore_streamflow_example.csv') ##<--- change this to a personalized filename\n\n","type":"content","url":"/evaluation/tutorials/streamflow/analysis-dscore#assemble-the-results","position":15},{"hierarchy":{"lvl1":"Streamflow Eval :: StdSuite Analysis"},"type":"lvl1","url":"/evaluation/tutorials/streamflow/analysis-stdsuite","position":0},{"hierarchy":{"lvl1":"Streamflow Eval :: StdSuite Analysis"},"content":"","type":"content","url":"/evaluation/tutorials/streamflow/analysis-stdsuite","position":1},{"hierarchy":{"lvl1":"Streamflow Eval :: StdSuite Analysis","lvl2":"Essential Benchmark Components"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/analysis-stdsuite#essential-benchmark-components","position":2},{"hierarchy":{"lvl1":"Streamflow Eval :: StdSuite Analysis","lvl2":"Essential Benchmark Components"},"content":"This benchmark notebook will present a workflow which follows a canonical model for Essential Benchmark Components:\n\nA set of predictions and matching observation (the data);\n\nThe domain (e.g. space or time) over which to benchmark;\n\nA set of statistical metrics with which to benchmark.\n\nLet’s get started.\n\n","type":"content","url":"/evaluation/tutorials/streamflow/analysis-stdsuite#essential-benchmark-components","position":3},{"hierarchy":{"lvl1":"Streamflow Eval :: StdSuite Analysis","lvl2":"Step 0: Load libraries and configure Python computing environment"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/analysis-stdsuite#step-0-load-libraries-and-configure-python-computing-environment","position":4},{"hierarchy":{"lvl1":"Streamflow Eval :: StdSuite Analysis","lvl2":"Step 0: Load libraries and configure Python computing environment"},"content":"\n\nimport pandas as pd\nimport logging\nimport os\n# Needed when boto3 >= 1.36.0 or the rechunking process will fail\n# This needs to be set before the boto3 library gets loaded\n# See: https://github.com/aws/aws-cli/issues/9214#issuecomment-2606619168\nos.environ['AWS_REQUEST_CHECKSUM_CALCULATION'] = 'when_required'\nos.environ['AWS_RESPONSE_CHECKSUM_VALIDATION'] = 'when_required'\n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/analysis-stdsuite#step-0-load-libraries-and-configure-python-computing-environment","position":5},{"hierarchy":{"lvl1":"Streamflow Eval :: StdSuite Analysis","lvl2":"Step 1: Load Data"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/analysis-stdsuite#step-1-load-data","position":6},{"hierarchy":{"lvl1":"Streamflow Eval :: StdSuite Analysis","lvl2":"Step 1: Load Data"},"content":"\n\nEssential Benchmark Components:\n\nA set of predictions and matching observations\n\nThe domain over which to benchmark\n\nA set of statistical metrics with which to benchmark.\n\nFinding and loading data is made easier for this particular workflow (the streamflow variable), in that most of it has been\npre-processed and stored in a cloud-friendly format.  That process is described in the \n\ndata preparation\nnotebook. We will proceed here using the already-prepared data for streamflow, which is included in the HyTEST intake catalog.\n\nLearn more about intake \n\nhere\n\nimport intake\ncat = intake.open_catalog(r'https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml')\nprint(\"Available datasets: \\n\", \"\\n\".join(cat.keys()))\n\n\n\nThe above list represents the processed datasets available for benchmarking.  If a dataset\nyou want is not in that list,\nyou can load the data manually via direct connection to ‘on-prem’ or S3 object storage.\nIf you load data from a source other than this list, you can jump to Step 2: Restrict to a Domain\n\nNote that the interesting datasets in the cataloged dataset above are duplicated: Some are -onprem\nand some are -cloud. Same data, but the storage location and access protocol will be different. You\nwill definitely want to use the correct copy of the data for your computing environment.\n\nonprem : Direct access via the caldera filesystem from denali or tallgrass\n\ncloud : Network access via S3 bucket, suitable for consumption on cloud-hosted jupyter servers. You could also access using any network-attached computer, but the amount of data will likely saturate your connection.  Use in the cloud (e.g. ESIP QHub)\n\nosn : Network access via OSN pod, which uses the S3 API, suitable for consumption on any jupyter server.\n\nSo... are you on-prem?\n\nimport platform\nonprem = (platform.node() in ['denali', 'tallgrass'])  # NOTE: these hostnames are not quite correct... this will always return not onprem.\nif onprem:\n    print(\"Yes : -onprem\")\n    obs_data_src='nwis-streamflow-usgs-gages-onprem'\n    mod_data_src='nwm21-streamflow-usgs-gages-onprem'\nelse:\n    print(\"Not onprem; use '-osn' data source\")\n    obs_data_src='nwis-streamflow-usgs-gages-osn'\n    mod_data_src='nwm21-streamflow-usgs-gages-osn'\nprint(\"Observed : \", obs_data_src)\nprint(\"Modeled  : \", mod_data_src)\n\n\n\nvariable_of_interest = 'streamflow'\ntry:\n    obs = cat[obs_data_src].to_dask()\n    mod = cat[mod_data_src].to_dask()\nexcept KeyError:\n    print(\"Something wrong with dataset names.\")\n    raise\n\ntry:\n    obs_data = obs[variable_of_interest]\n    mod_data = mod[variable_of_interest].astype('float32')\nexcept KeyError:\n    print(f\"{variable_of_interest} was not found in these data.\")\n\nobs_data.name = 'observed'\nmod_data.name = 'predicted'    \n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/analysis-stdsuite#step-1-load-data","position":7},{"hierarchy":{"lvl1":"Streamflow Eval :: StdSuite Analysis","lvl2":"Step 2: Restrict to a Domain"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/analysis-stdsuite#step-2-restrict-to-a-domain","position":8},{"hierarchy":{"lvl1":"Streamflow Eval :: StdSuite Analysis","lvl2":"Step 2: Restrict to a Domain"},"content":"\n\nEssential Benchmark Components:\n\nA set of predictions and matching observations,\n\nThe domain over which to benchmark\n\nA set of statistical metrics with which to benchmark.\n\nEach benchmark domain is defined over specific bounds (typically space and/or time).\nBenchmark domain definitions can be defined within the notebook, or sourced from\nelsewhere. For this example, we use the Cobalt gages, avaliable for download on ScienceBase\n(\n\nFoks et al., 2022).\n\nThis is essentially a list of\nstream guages in which we are interested, along with some  metadata about that gage (watershed,\nreach code, etc).  We will use this as a spatial selector to restrict the original data to only\nthose gages found within this benchmarking domain.\n\ncobalt = pd.read_csv(\n    'https://www.sciencebase.gov/catalog/file/get/6181ac65d34e9f2789e44897?f=__disk__22%2F1a%2Fd2%2F221ad2fe9d95de17731ad35d0fc6b417a4b53ee1',\n    dtype={'site_no':str, 'huc_cd':str, 'reachcode':str, 'comid':str, 'gagesII_class':str, 'aggecoregion': str}, \n    index_col='site_no'\n    )\n# Re-format the gage_id/site_no string value.  ex:   \"1000000\"  ==> \"USGS-1000000\"\ncobalt.rename(index=lambda x: f'USGS-{x}', inplace=True)\nprint(f\"{len(cobalt.index)} gages in this benchmark\")\ncobalt.head(2)\n\n\n\nSo now we have a domain dataset representing the stream gages (unique site_no values) identifying the locations making up the spatial domain of this benchmark. While we have good meta-data for these gages (lat/lon location, HUC8 code, etc), we really will only use the list of site_no values to select locations out of the raw data.\n\n","type":"content","url":"/evaluation/tutorials/streamflow/analysis-stdsuite#step-2-restrict-to-a-domain","position":9},{"hierarchy":{"lvl1":"Streamflow Eval :: StdSuite Analysis","lvl2":"Step 3: Define Metrics"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/analysis-stdsuite#step-3-define-metrics","position":10},{"hierarchy":{"lvl1":"Streamflow Eval :: StdSuite Analysis","lvl2":"Step 3: Define Metrics"},"content":"\n\nEssential Benchmark Components:\n\nA set of predictions and matching observations,\n\nThe domain over which to benchmark\n\nA set of statistical metrics with which to benchmark.\n\nThe code to calculate the various NWM metrics has been standardized in\n\n\nStandard Suite (v1) Metrics\nwith usage examples in\n\n\nStandard Suite (v1) Metrics -- Usage Examples.\nYou can use these metrics or write your own.  To import and use these standardized definitions, run this cell:\n\n%run ../../Metrics_StdSuite_v1.ipynb\n\n\n\nWhether you use these functions or your own, we need to put all metric computation into a special all-encompasing\nbenchmarking function--a single call which can be assigned to each gage in our domain list. This sort of arrangement\nis well-suited to parallelism with dask.\n\nIf this is done well, the process will benefit enormously from task parallelism -- each gage can be given its own\nCPU to run on.  After all are done, the various results will be collected and assembled into a composite dataset.\n\nTo achieve this, we need a single ‘atomic’ function that can execute independently. It will take the gage identifier\nas input and return a list of metrics.\n\n## Wrapper function -- this func will be called once per gage_id, each call on its own dask worker\ndef compute_benchmark(gage_id):\n    try:\n        ## obs_data and mod_data should be globals...\n        obs = obs_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series()\n        mod = mod_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series().resample('1D', offset='5h').mean() \n        \n        # make sure the indices match\n        obs.index = obs.index.to_period('D')\n        mod.index = mod.index.to_period('D')\n\n        # merge obs and predictions; drop NaNs.\n        gage_df = pd.merge(obs, mod, left_index=True, right_index=True).dropna(how='any')\n        \n        scores = pd.Series(\n            data={\n                'NSE': NSE(gage_df.observed, gage_df.predicted),\n                'KGE': KGE(gage_df.observed, gage_df.predicted),\n                'logNSE': logNSE(gage_df.observed, gage_df.predicted),\n                'pbias': pbias(gage_df.observed, gage_df.predicted),\n                'rSD': rSD(gage_df.observed, gage_df.predicted),\n                'pearson': pearson_r(gage_df.observed, gage_df.predicted),\n                'spearman': spearman_r(gage_df.observed, gage_df.predicted), \n                'pBiasFMS': pBiasFMS(gage_df.observed, gage_df.predicted),\n                'pBiasFLV': pBiasFLV(gage_df.observed, gage_df.predicted),\n                'pBiasFHV': pBiasFHV(gage_df.observed, gage_df.predicted)\n            },\n            name=gage_id,\n            dtype='float64'\n        )\n        return scores\n    except Exception as e:#<-- this is an extremely broad way to catch exceptions.  We only do it this way to ensure \n                          #    that a failure on one benchmark (for a single stream gage) will not halt the entire run. \n        logging.info(\"Benchmark failed for %s\", gage_id)\n        return None\n\n\n\nLet’s test to be sure this compute_benchmark() function will return data for a single gage\n\ncompute_benchmark('USGS-01030350')\n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/analysis-stdsuite#step-3-define-metrics","position":11},{"hierarchy":{"lvl1":"Streamflow Eval :: StdSuite Analysis","lvl2":"Execute the Analysis"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/analysis-stdsuite#execute-the-analysis","position":12},{"hierarchy":{"lvl1":"Streamflow Eval :: StdSuite Analysis","lvl2":"Execute the Analysis"},"content":"We will be doing a lot of work in parallel, using workers within a ‘cluster’.The details of cluster configuration are handled for us by ‘helper’ notebooks, below.\nYou can override their function by doing your own cluster configuration if you like.\n\n# uncomment the lines below to read in your AWS credentials if you want to access data from a requester-pays bucket (-cloud)\n# os.environ['AWS_PROFILE'] = 'default'\n# %run ../../../environment_set_up/Help_AWS_Credentials.ipynb\n\n\n\n%run ../../../environment_set_up/Start_Dask_Cluster_Nebari.ipynb\n### Executes external 'helper to spin up a cluster of workers. \n\n\n\nWe verified above that the compute_benchmark works on the “hosted” server (where this\nnotebook is being executed. As a sanity check before we give the cluster of workers a lot\nto do, let’s verify that we can have a remote worker process a gage by submitting work\nto one in isolation:\n\nclient.submit(compute_benchmark, 'USGS-01030350').result()\n\n\n\nNow that we’ve got a benchmark function, and can prove that it works in remote workers\nwithin the cluster, we can dispatch a fleet of workers to process our data in parallel.\nWe will make use of dask to do this using a dask ‘bag’.\n\nRead more about task parallelism with Dask and how we are using dask bags \n\nhere\n\n# Set up a dask bag with the contents beging a list of the cobalt gages\nimport dask.bag as db\nbag = db.from_sequence( cobalt.index.tolist() ).map(compute_benchmark)\n\nresults = bag.compute() #<< Dispatch the workers\n\n\n\nWith that big task done, we don’t need dask parallelism any more. Let’s shut down the cluster:\n\nclient.close(); del client\ncluster.close(); del cluster\n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/analysis-stdsuite#execute-the-analysis","position":13},{"hierarchy":{"lvl1":"Streamflow Eval :: StdSuite Analysis","lvl2":"Assemble the results"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/analysis-stdsuite#assemble-the-results","position":14},{"hierarchy":{"lvl1":"Streamflow Eval :: StdSuite Analysis","lvl2":"Assemble the results"},"content":"The bag now contains a collection of return values (one per call to compute_benchmark()).  We can massage that into a table/dataframe for easier processing:\n\nr = [i for i in results if i is not None] # Drop entries where compute_benchmark failed\nresults_df = pd.concat(r, axis=1).T\nresults_df.index.name = 'site_no'\nresults_df\n\n\n\nThis dataframe/table can be saved to disk as a CSV. It will be used for visualizations in \n\nother notebooks.\n\nresults_df.to_csv('NWM_v2.1_streamflow_example.csv') ##<--- change this to a personalized filename\n\n","type":"content","url":"/evaluation/tutorials/streamflow/analysis-stdsuite#assemble-the-results","position":15},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization"},"type":"lvl1","url":"/evaluation/tutorials/streamflow/vizualization","position":0},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization"},"content":"This notebook is an example of how a HyTEST user may examine streamflow benchmark results\nfrom a hydrologic model.\n\nHere, we are viewing daily streamflow benchmark results from the \n\nexample analysis\nof \n\nNational Water Model Retrospective version 2.1,\nforced with AORC, at streamflow benchmark locations (“cobalt gages” \n\nFoks et al., 2022).\n\nTwo benchmark results are examined:\n\nthe standard statistical suite results\n(\n\nTowler et al., 2022)\n\ndecomposition statistical suite, d-score (\n\nHodson et al., 2022).\n\n","type":"content","url":"/evaluation/tutorials/streamflow/vizualization","position":1},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization","lvl2":"Step 0: Load Required Python Libraries"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/vizualization#step-0-load-required-python-libraries","position":2},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization","lvl2":"Step 0: Load Required Python Libraries"},"content":"\n\ntry:\n    import pandas as pd\n    import holoviews as hv\n    import hvplot.pandas\n    import panel as pn\n    from geoviews import tile_sources as gvts\nexcept ImportError:\n    print(\"A required library could not be found. \")\n    raise\n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/vizualization#step-0-load-required-python-libraries","position":3},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization","lvl2":"Step 1: Load Domain Data"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/vizualization#step-1-load-domain-data","position":4},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization","lvl2":"Step 1: Load Domain Data"},"content":"Loading the dataset representing the ‘cobalt’ gages. This includes location and other metadata about each gage. We will need this extra metadata to get lat/lon and other characteristics of the gage locations.\n\ncobalt = pd.read_csv(\n    'https://www.sciencebase.gov/catalog/file/get/6181ac65d34e9f2789e44897?f=__disk__22%2F1a%2Fd2%2F221ad2fe9d95de17731ad35d0fc6b417a4b53ee1',\n    dtype={'site_no':str, 'huc_cd':str, 'reachcode':str, 'comid':str, 'gagesII_class':str, 'aggecoregion': str}, \n    index_col='site_no'\n    )\n# Re-format the gage_id/site_no string value.  ex:   \"1000000\"  ==> \"USGS-1000000\"\ncobalt.rename(index=lambda x: f'USGS-{x}', inplace=True)\ncobalt.rename(columns={'dec_lat_va':'Lat', 'dec_long_va':'Lon'} , inplace=True)\nprint(f\"{len(cobalt.index)} gages in this benchmark\")\ncobalt.head(2)\n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/vizualization#step-1-load-domain-data","position":5},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization","lvl2":"Step 2: Load NWM Analysis"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/vizualization#step-2-load-nwm-analysis","position":6},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization","lvl2":"Step 2: Load NWM Analysis"},"content":"This table is the collection of evaluation metrics comparing observed vs. modeled streamflow of the cobalt gages. This data is the result\nof the \n\nNWM Benchmark Analysis Notebook.\n\nNWM = pd.read_csv(r'./NWM_v2.1_streamflow_example.csv', dtype={'site_no':str} ).set_index('site_no', drop=False)\n\n\n\n# Merge benchmarks with cobalt data to form a single table, indexed by site_no\nmetrics = NWM.columns.tolist()[1:] #list of columns, EXCEPT the first column (site_no)\nNWM = NWM.merge(\n    cobalt, # Table to merge with NWM\n    how='left',            # left join preserves only records which have an index in NWM dataframe.\n    left_index=True, \n    right_index=True\n    )\n# The effect of the left join is that if a cobalt gage does not have a computed benchmark in NWM, \n# (e.g. due to error in the analysis process), it is dropped from the visualization set.\n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/vizualization#step-2-load-nwm-analysis","position":7},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization","lvl2":"Step 3: Visualize NWM Benchmark Results"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/vizualization#step-3-visualize-nwm-benchmark-results","position":8},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization","lvl2":"Step 3: Visualize NWM Benchmark Results"},"content":"A quick look at the joined data table shows that we have a collection of metrics, and the associated metadata,\nall indexed by the site_id.  This table is sortable by column -- you can scroll and select to explore the\nraw result.\n\nNWM.hvplot.table(\n    columns=['site_no', 'drain_sqkm','KGE', 'NSE','logNSE','pearson','spearman','rSD','pbias','pBiasFMS','pBiasFHV','pBiasFLV','complete_yrs','n_days'], \n    sortable=True, \n    selectable=True\n)\n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/vizualization#step-3-visualize-nwm-benchmark-results","position":9},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization","lvl3":"Step 3a: Benchmark Results Mapped Over the Spatial Extent of the Conterminous United States","lvl2":"Step 3: Visualize NWM Benchmark Results"},"type":"lvl3","url":"/evaluation/tutorials/streamflow/vizualization#step-3a-benchmark-results-mapped-over-the-spatial-extent-of-the-conterminous-united-states","position":10},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization","lvl3":"Step 3a: Benchmark Results Mapped Over the Spatial Extent of the Conterminous United States","lvl2":"Step 3: Visualize NWM Benchmark Results"},"content":"\n\nvar_select = pn.widgets.Select(name='Metric', options=metrics, value='pearson')\nbase_map_select = pn.widgets.Select(name='Basemap:', \n                                    options=list(gvts.tile_sources.keys()), \n                                    value='OSM')\n\n@pn.depends(var=var_select, base_map=base_map_select)\ndef plot(var, base_map):\n    return NWM.hvplot.points(x='Lon', y='Lat', color=var, cmap='turbo_r', geo=True, tiles=base_map)\n\ncol = pn.Column(var_select, base_map_select, plot)\ncol.servable('Hydro Assessment Tool')\n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/vizualization#step-3a-benchmark-results-mapped-over-the-spatial-extent-of-the-conterminous-united-states","position":11},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization","lvl3":"Step 3b: Boxplots","lvl2":"Step 3: Visualize NWM Benchmark Results"},"type":"lvl3","url":"/evaluation/tutorials/streamflow/vizualization#step-3b-boxplots","position":12},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization","lvl3":"Step 3b: Boxplots","lvl2":"Step 3: Visualize NWM Benchmark Results"},"content":"Grouped by\n\nGages-II classification,\n\nHUC02 group, or\n\nAggregated Ecoregion\n\n# define which columns are metrics in the widget and which ones are groups\nvar_select = pn.widgets.Select(name='Metric', \n        options=metrics, \n        value='pearson')\n\ngroup_select = pn.widgets.Select(name='Group By:', \n        options=['huc02', 'gagesII_class', 'aggecoregion'], \n        value='aggecoregion')\n\n@pn.depends(var_select, group_select)\ndef plot(var, group):\n        # local scope func to calc summary stats using var/group\n        # build tooltip using the result. \n        # pandas will compute with groupby(). \n    return NWM.hvplot.box(y = var, by = group, height=400, width=800, legend=False)\n\ncol = pn.Column(var_select, group_select, plot)\ncol.servable('NWM Benchmark Box Plots')\n\n## TODO: \n## Hover over box to tell user exactly the number of samples in group (count), \n# median, mean, max, min, and IQR.  # hovertool -- tooltip\n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/vizualization#step-3b-boxplots","position":13},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization","lvl3":"Step 3c: Histograms","lvl2":"Step 3: Visualize NWM Benchmark Results"},"type":"lvl3","url":"/evaluation/tutorials/streamflow/vizualization#step-3c-histograms","position":14},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization","lvl3":"Step 3c: Histograms","lvl2":"Step 3: Visualize NWM Benchmark Results"},"content":"Grouped by\n\nGages-II classification,\n\nHUC02 group, or\n\nAggregated Ecoregion\n\nvar_select = pn.widgets.Select(name='Metric', \n        options=metrics, \n        value='pearson'\n    )\n\ngroup_select = pn.widgets.Select(name='Group By:', \n        options=['huc02', 'gagesII_class', 'aggecoregion'], \n        value='aggecoregion'\n    )\n\n@pn.depends(var_select, group_select)\ndef plot(var, group):\n    return NWM.hvplot.hist(var, group, subplots=True, width=400, bins = 500, legend='top')\n\ncol = pn.Column(var_select, group_select, plot)\ncol.servable('NWM Benchmark Histograms')\n# TODO: Constrain to fixed width for rendering.\n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/vizualization#step-3c-histograms","position":15},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization","lvl3":"Step 3d: Metric by Latitude & Longitude","lvl2":"Step 3: Visualize NWM Benchmark Results"},"type":"lvl3","url":"/evaluation/tutorials/streamflow/vizualization#step-3d-metric-by-latitude-longitude","position":16},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization","lvl3":"Step 3d: Metric by Latitude & Longitude","lvl2":"Step 3: Visualize NWM Benchmark Results"},"content":"\n\nvar_select = pn.widgets.Select(name='Metric', options=metrics, \n                               value='pearson')\n\ngroup_select = pn.widgets.Select(name='Group By:', \n                                    options=['Lon', 'Lat'], \n                                    value='Lon')\n\n@pn.depends(var_select, group_select)\ndef plot(var, group):\n    return NWM.hvplot.scatter(x=group, y=var, height=400, width = 500, legend='top', hover_cols=[\"site_no\",\"Lat\",\"Lon\"])\n\ncol = pn.Column(var_select, group_select, plot)\ncol.servable('Lat/Lon Scatter Plot')\n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/vizualization#step-3d-metric-by-latitude-longitude","position":17},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization","lvl3":"Step 3e: Metric v. Metric","lvl2":"Step 3: Visualize NWM Benchmark Results"},"type":"lvl3","url":"/evaluation/tutorials/streamflow/vizualization#step-3e-metric-v-metric","position":18},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization","lvl3":"Step 3e: Metric v. Metric","lvl2":"Step 3: Visualize NWM Benchmark Results"},"content":"\n\nvar_select = pn.widgets.Select(name='Metric', options=metrics, \n                               value='pearson')\n\nvar2_select = pn.widgets.Select(name='Metric:', \n                                    options=metrics, \n                                    value='spearman')\n\n@pn.depends(var_select, var2_select)\n\ndef plot(var, var2):\n    return NWM.hvplot.scatter(x = var, y = var2, height=400, width = 500, legend='top', hover_cols=['site_no','Lat', 'Lon'])\n\ncol = pn.Column(var_select, var2_select, plot)\ncol.servable('Metric v Metric Scatter Plot')\n\n\n\n","type":"content","url":"/evaluation/tutorials/streamflow/vizualization#step-3e-metric-v-metric","position":19},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization","lvl2":"Step 4: Visualize D-Score Analysis Results"},"type":"lvl2","url":"/evaluation/tutorials/streamflow/vizualization#step-4-visualize-d-score-analysis-results","position":20},{"hierarchy":{"lvl1":"Streamflow Eval :: Visualization","lvl2":"Step 4: Visualize D-Score Analysis Results"},"content":"The above steps are repeated for the DScore analysis.\n\n#Data is loaded \nDScore = pd.read_csv(r'./DScore_streamflow_example.csv', dtype={'site_no':str} ).set_index('site_no', drop=False)\n# Merge benchmarks with cobalt data to form a single table, indexed by site_no\nmetrics = DScore.columns.tolist()[1:] \nDScore = DScore.merge(\n    cobalt, \n    how='left',\n    left_index=True, \n    right_index=True\n    )\n\n\n\n# Examine the data table\ncols = ['site_no']\ncols.extend(metrics)\nDScore.hvplot.table(\n    columns=cols, \n    sortable=True, \n    selectable=True\n)\n\n\n\n# Map\nvar_select = pn.widgets.Select(name='Metric', options=metrics, value='mse')\nbase_map_select = pn.widgets.Select(name='Basemap:', \n                                    options=list(gvts.tile_sources.keys()), \n                                    value='OSM')\n\n@pn.depends(var=var_select, base_map=base_map_select)\ndef plot(var, base_map):\n    return DScore.hvplot.points(x='Lon', y='Lat', color=var, cmap='turbo_r', geo=True, tiles=base_map)\n\ncol = pn.Column(var_select, base_map_select, plot)\ncol.servable('Hydro Assessment Tool')\n\n\n\n# Box Plots\nvar_select = pn.widgets.Select(name='Metric', \n        options=metrics, \n        value='mse')\n\ngroup_select = pn.widgets.Select(name='Group By:', \n        options=['huc02', 'gagesII_class', 'aggecoregion'], \n        value='aggecoregion')\n\n@pn.depends(var_select, group_select)\ndef plot(var, group):\n    return DScore.hvplot.box(y = var, by = group, height=400, width=800, legend=False)\n\ncol = pn.Column(var_select, group_select, plot)\ncol.servable('DScore Benchmark Box Plots')\n\n\n\n# Histograms\nvar_select = pn.widgets.Select(name='Metric', \n        options=metrics, \n        value='mse'\n    )\n\ngroup_select = pn.widgets.Select(name='Group By:', \n        options=['huc02', 'gagesII_class', 'aggecoregion'], \n        value='aggecoregion'\n    )\n\n@pn.depends(var_select, group_select)\ndef plot(var, group):\n    return DScore.hvplot.hist(var, group, subplots=True, width=400, bins = 500, legend='top')\n\ncol = pn.Column(var_select, group_select, plot)\ncol.servable('DScore Benchmark Histograms')\n\n\n\n# metric v lat/lon scatter plot\nvar_select = pn.widgets.Select(name='Metric', options=metrics, \n                               value='mse')\n\ngroup_select = pn.widgets.Select(name='Group By:', \n                                    options=['Lon', 'Lat'], \n                                    value='Lon')\n\n@pn.depends(var_select, group_select)\ndef plot(var, group):\n    return DScore.hvplot.scatter(x=group, y=var, height=400, width = 500, legend='top', hover_cols=[\"site_no\",\"Lat\",\"Lon\"])\n\ncol = pn.Column(var_select, group_select, plot)\ncol.servable('Lat/Lon Scatter Plot')\n\n\n\n# metric vs metric\nvar_select = pn.widgets.Select(name='Metric', options=metrics, \n                               value='mse')\n\nvar2_select = pn.widgets.Select(name='Metric:', \n                                    options=metrics, \n                                    value='high')\n\n@pn.depends(var_select, var2_select)\n\ndef plot(var, var2):\n    return DScore.hvplot.scatter(x = var, y = var2, height=400, width = 500, legend='top', hover_cols=['site_no','Lat', 'Lon'])\n\ncol = pn.Column(var_select, var2_select, plot)\ncol.servable('Metric v Metric Scatter Plot')\n\n","type":"content","url":"/evaluation/tutorials/streamflow/vizualization#step-4-visualize-d-score-analysis-results","position":21}]}